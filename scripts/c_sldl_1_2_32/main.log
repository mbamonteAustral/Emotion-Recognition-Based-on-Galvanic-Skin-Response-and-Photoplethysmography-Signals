2024-05-04 09:31:41.958768: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-04 09:31:45.681808: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-04 09:31:55.031887: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  12.0
step (sec):  12.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_32\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

dep - loss: 0.0139 - mean_squared_error: 0.0139
[1m18/72[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 0.0179 - mean_squared_error: 0.0179 
[1m38/72[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0178 - mean_squared_error: 0.0178
[1m55/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0174 - mean_squared_error: 0.0174
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 0.0169 - mean_squared_error: 0.0169 - val_loss: 0.0076 - val_mean_squared_error: 0.0076
(12000, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 12000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3000, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3000, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 750, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 12000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 12000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 750, 6)    â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 750, 6)    â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 750, 12)   â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 750)   â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 9000)      â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 9000)      â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚      9,001 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 9,823 (38.37 KB)
 Trainable params: 9,823 (38.37 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/72[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:50[0m 2s/step - binary_accuracy: 1.0000 - loss: 0.3399
[1m17/72[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 0.4766 - loss: 0.8231 
[1m35/72[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5078 - loss: 0.7982
[1m53/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.5366 - loss: 0.7735
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.5491 - loss: 0.7632
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 6ms/step - binary_accuracy: 0.5498 - loss: 0.7626 - val_binary_accuracy: 0.7778 - val_loss: 0.5996
Epoch 2/10

[1m 1/72[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 40ms/step - binary_accuracy: 0.0000e+00 - loss: 1.2685
[1m16/72[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5865 - loss: 0.7074     
[1m37/72[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6434 - loss: 0.6611
[1m58/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6739 - loss: 0.6422
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6813 - loss: 0.6380 - val_binary_accuracy: 0.7778 - val_loss: 0.5884
Epoch 3/10

[1m 1/72[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.3097
[1m18/72[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7383 - loss: 0.6259 
[1m39/72[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7384 - loss: 0.6199
[1m59/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7230 - loss: 0.6106
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7121 - loss: 0.6081 - val_binary_accuracy: 0.6667 - val_loss: 0.6196
Epoch 4/10

[1m 1/72[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3962
[1m19/72[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6620 - loss: 0.6318 
[1m38/72[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6731 - loss: 0.6001
[1m58/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6761 - loss: 0.5808
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6764 - loss: 0.5748 - val_binary_accuracy: 0.6667 - val_loss: 0.6205
Epoch 5/10

[1m 1/72[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2726
[1m20/72[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7199 - loss: 0.5283 
[1m41/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6854 - loss: 0.5649
[1m63/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6699 - loss: 0.5730
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6674 - loss: 0.5735 - val_binary_accuracy: 0.5556 - val_loss: 0.6300
Epoch 6/10

[1m 1/72[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2699
[1m18/72[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8815 - loss: 0.3504 
[1m38/72[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8447 - loss: 0.3936
[1m58/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8243 - loss: 0.4181
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8082 - loss: 0.4366 - val_binary_accuracy: 0.5556 - val_loss: 0.6860
Epoch 7/10

[1m 1/72[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.6793
[1m22/72[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6207 - loss: 0.7091 
[1m42/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6502 - loss: 0.6608
[1m63/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6770 - loss: 0.6251
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6880 - loss: 0.6119 - val_binary_accuracy: 0.6667 - val_loss: 0.6686
Epoch 8/10

[1m 1/72[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.0719
[1m18/72[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8101 - loss: 0.4154 
[1m38/72[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7800 - loss: 0.4716
[1m58/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7668 - loss: 0.4837
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7566 - loss: 0.4893 - val_binary_accuracy: 0.5556 - val_loss: 0.6764
Epoch 9/10

[1m 1/72[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.6474
[1m15/72[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 1.0000 - loss: 0.3923 
[1m33/72[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.9564 - loss: 0.3816
[1m53/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.9058 - loss: 0.3935
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8684 - loss: 0.4121 - val_binary_accuracy: 0.5556 - val_loss: 0.6320
Epoch 10/10

[1m 1/72[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 30ms/step - binary_accuracy: 0.0000e+00 - loss: 1.3111
[1m19/72[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6432 - loss: 0.5657     
[1m39/72[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7151 - loss: 0.5234
[1m60/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7348 - loss: 0.5090
[1m72/72[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7433 - loss: 0.5003 - val_binary_accuracy: 0.6667 - val_loss: 0.6948

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.04066255 0.8416705  0.29768088 0.7883656  0.83809775 0.34349424
 0.09899872 0.9287473  0.9195649  0.64954245 0.9573548  0.9274013
 0.69219494 0.09218939 0.9550762  0.8873671  0.92535645 0.6071114
 0.05127043 0.94101655 0.81536347]
predicted [0 1 0 1 1 0 0 1 1 1 1 1 1 0 1 1 1 1 0 1 1]
expected [False False  True  True False  True False  True  True  True  True False
  True False  True  True  True  True False False  True]
accuracy: 0.7142857142857143
confusion matrix: 
[[ 4  4]
 [ 2 11]]
              precision    recall  f1-score   support

       False       0.67      0.50      0.57         8
        True       0.73      0.85      0.79        13

    accuracy                           0.71        21
   macro avg       0.70      0.67      0.68        21
weighted avg       0.71      0.71      0.70        21

macro avg f1-score: 0.6785714285714286
macro avg (UAR): 0.6730769230769231
Sensitivity:  0.5
Specificity:  0.8461538461538461
g-mean:  0.6504436355879909
-------- Model Performance ----------: 
accuracy:  [0.71428571 0.57142857 0.71428571 0.76190476 0.57142857 0.71428571
 0.57142857 0.61904762 0.76190476 0.71428571]
gmean:  [0.65044364 0.43852901 0.5        0.67936622 0.43852901 0.65044364
 0.43852901 0.53708616 0.75955453 0.65044364]
f1_score:  [0.67857143 0.49867374 0.60625    0.72148541 0.49867374 0.67857143
 0.49867374 0.57142857 0.75294118 0.67857143]
UAR:  [0.67307692 0.50961538 0.625      0.71153846 0.50961538 0.67307692
 0.50961538 0.57211538 0.75961538 0.67307692]
Cohen Kappa score:  [0.36363636 0.02072539 0.29213483 0.45595855 0.02072539 0.36363636
 0.02072539 0.15151515 0.50704225 0.36363636]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  12.0
step (sec):  12.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  102
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.651  0.829  0.835  0.812  0.819  0.426  0.713  0.734  0.676  0.699]
 [ 0.424  0.705  0.718  0.704  0.694  0.429  0.733  0.753  0.659  0.718]
 [ 0.464  0.822  0.853  0.762  0.829  0.505  0.796  0.804  0.775  0.784]
 [ 0.     0.5    0.627  0.     0.385  0.     0.5    0.627  0.     0.385]
 [ 0.524  0.752  0.788  0.726  0.756  0.576  0.768  0.802  0.741  0.77 ]
 [ 0.256  0.622  0.671  0.574  0.618 -0.054  0.475  0.552  0.27   0.433]
 [ 0.     0.5    0.627  0.     0.385  0.     0.5    0.627  0.     0.385]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 4.44162509e-01  7.20833333e-01  7.24545455e-01  7.07144429e-01
   7.16826784e-01  3.46965654e-01  6.61904762e-01  7.84545455e-01
   5.37149470e-01  6.57000467e-01]
 [ 3.70985120e-01  6.85833333e-01  6.83636364e-01  6.64606746e-01
   6.74347042e-01  4.63923921e-01  7.20238095e-01  7.64545455e-01
   6.76995369e-01  7.20187729e-01]
 [ 5.29845135e-01  7.60833333e-01  7.84545455e-01  6.94472954e-01
   7.47417582e-01  3.73169399e-01  6.86666667e-01  6.86363636e-01
   6.77565783e-01  6.82171717e-01]
 [ 3.17441110e-01  6.58333333e-01  6.64545455e-01  6.45692226e-01
   6.55186758e-01  1.04410396e-01  5.42857143e-01  6.38181818e-01
   4.04770448e-01  5.35332453e-01]
 [ 4.05819512e-01  7.00714286e-01  7.34545455e-01  6.29279189e-01
   6.88281441e-01  3.84942529e-01  6.91666667e-01  6.97272727e-01
   6.66663262e-01  6.83057498e-01]
 [ 3.91127820e-01  6.96666667e-01  6.97272727e-01  6.44968871e-01
   6.72342380e-01  5.52311896e-01  7.80833333e-01  7.74545455e-01
   7.64117899e-01  7.67908480e-01]
 [ 3.46247318e-01  6.67261905e-01  7.07272727e-01  6.25948411e-01
   6.62150627e-01  2.10039570e-01  6.05000000e-01  6.06363636e-01
   5.64953527e-01  5.86534022e-01]
 [ 8.49528684e-01  9.16071429e-01  9.33636364e-01  9.07144409e-01
   9.22251082e-01  6.45458496e-01  8.31071429e-01  8.23636364e-01
   8.24531953e-01  8.19588190e-01]
 [ 8.38408815e-02  5.46428571e-01  6.56363636e-01  3.12312070e-01
   5.12574517e-01  4.12415134e-02  5.26190476e-01  6.59090909e-01
   2.40309412e-01  4.84666020e-01]
 [ 2.15317797e-01  6.09166667e-01  6.06363636e-01  5.86524338e-01
   5.96461316e-01  1.82218258e-01  5.90000000e-01  5.97272727e-01
   5.65195451e-01  5.81752414e-01]
 [ 7.31784943e-02  5.41071429e-01  5.70000000e-01  4.55247966e-01
   5.22527473e-01  3.56941557e-01  6.77500000e-01  6.87272727e-01
   6.12456364e-01  6.57284799e-01]
 [ 4.40155037e-01  7.12500000e-01  7.41818182e-01  6.79350542e-01
   7.11107226e-01  4.55357569e-01  7.19047619e-01  7.53636364e-01
   6.86833122e-01  7.18621526e-01]
 [ 4.73244382e-01  7.36666667e-01  7.44545455e-01  7.21673912e-01
   7.32016317e-01  3.53060626e-02  5.19166667e-01  5.20000000e-01
   4.56122387e-01  4.94087163e-01]
 [ 6.06763809e-02  5.32142857e-01  6.00909091e-01  3.50627785e-01
   5.00293285e-01  2.77444420e-01  6.39166667e-01  6.54545455e-01
   5.78963982e-01  6.24794650e-01]
 [ 5.40842720e-02  5.22619048e-01  6.05454545e-01  3.65841378e-01
   5.01817227e-01  7.86909361e-02  5.34523810e-01  5.74545455e-01
   4.35544806e-01  5.16549986e-01]
 [ 1.57090909e-01  5.76190476e-01  6.15454545e-01  5.14815689e-01
   5.70277639e-01  3.55689175e-01  6.63095238e-01  7.45454545e-01
   5.44889010e-01  6.59353947e-01]
 [ 3.92558254e-01  6.99166667e-01  7.07272727e-01  6.70683733e-01
   6.85575536e-01  3.41781028e-01  6.59166667e-01  7.02727273e-01
   5.79342384e-01  6.51774614e-01]
 [ 6.17173524e-02  5.34027778e-01  7.84545455e-01  1.62360956e-01
   5.07139663e-01  2.93389972e-01  6.49166667e-01  6.47272727e-01
   6.13475684e-01  6.28787324e-01]
 [ 3.45335631e-01  6.70238095e-01  7.36363636e-01  5.93757050e-01
   6.62362637e-01  3.85402299e-01  6.92500000e-01  6.95454545e-01
   6.84164452e-01  6.89271284e-01]
 [ 3.84297443e-01  6.87500000e-01  7.48181818e-01  6.02946389e-01
   6.77657204e-01  3.09152239e-01  6.54166667e-01  6.97272727e-01
   5.70008412e-01  6.35604396e-01]
 [-4.24773756e-02  4.79166667e-01  6.95454545e-01  1.23878810e-01
   4.57739938e-01  3.38445267e-03  5.01666667e-01  5.00000000e-01
   4.78869537e-01  4.90265290e-01]
 [ 3.30734463e-01  6.65000000e-01  6.66363636e-01  6.13039612e-01
   6.49165002e-01  4.08523940e-01  7.07142857e-01  7.25454545e-01
   6.80323422e-01  6.95320513e-01]
 [ 2.71800249e-01  6.37500000e-01  6.56363636e-01  5.82216794e-01
   6.24807415e-01  2.33955505e-01  6.06250000e-01  7.66363636e-01
   4.02324515e-01  6.02767028e-01]
 [ 6.63413564e-04  5.00000000e-01  5.00909091e-01  4.60651297e-01
   4.88131868e-01  3.41634540e-01  6.72500000e-01  6.87272727e-01
   6.47128574e-01  6.64183039e-01]
 [ 3.59954232e-01  6.80000000e-01  6.88181818e-01  6.38459124e-01
   6.71599234e-01  3.96766209e-01  6.98333333e-01  6.97272727e-01
   6.81035266e-01  6.89975857e-01]
 [ 3.11562270e-01  6.52500000e-01  6.68181818e-01  6.09887907e-01
   6.39582640e-01  4.09233428e-01  7.05000000e-01  7.15454545e-01
   6.55513352e-01  6.93989621e-01]
 [ 2.85936157e-01  6.44642857e-01  7.04545455e-01  5.73630226e-01
   6.34110620e-01  5.08216374e-01  7.55833333e-01  7.54545455e-01
   7.34702302e-01  7.44834055e-01]
 [ 1.45087537e-01  5.70833333e-01  6.53636364e-01  4.34417178e-01
   5.49763031e-01  4.51981242e-01  7.19642857e-01  7.58181818e-01
   6.45431566e-01  7.07295482e-01]
 [ 1.05856568e-01  5.55952381e-01  5.90000000e-01  4.64987242e-01
   5.40721778e-01  2.71478194e-01  6.37500000e-01  6.46363636e-01
   5.71602369e-01  6.14458874e-01]
 [ 6.50663509e-01  8.29166667e-01  8.34545455e-01  8.11889519e-01
   8.18578227e-01  4.25569986e-01  7.13095238e-01  7.33636364e-01
   6.76343620e-01  6.99091187e-01]]
KNN mean:
[0.29388117 0.64630093 0.69018182 0.56161523 0.63309378 0.32148603
 0.65869643 0.68981818 0.59524426 0.64655032]
---------------------------
---------------------------
DT performance:
[[ 0.47851925  0.75083333  0.74636364  0.72311832  0.74336219  0.22114415
   0.63541667  0.71909091  0.42479642  0.61870098]
 [ 0.44863388  0.67916667  0.67636364  0.68823203  0.67013792  0.41848313
   0.70238095  0.71545455  0.65986304  0.68765915]
 [ 0.4485639   0.73583333  0.74545455  0.71103299  0.73000666  0.40784946
   0.68333333  0.68272727  0.71777943  0.67815657]
 [ 0.29375124  0.63333333  0.63636364  0.56505437  0.62533189  0.18990356
   0.6047619   0.65818182  0.53434349  0.60015873]
 [ 0.37692834  0.66559524  0.67727273  0.6388805   0.65650794  0.43311649
   0.6975      0.69545455  0.65791681  0.68899711]
 [ 0.4526813   0.715       0.71545455  0.68782333  0.7043118   0.39417815
   0.71583333  0.71545455  0.69225544  0.71045399]
 [ 0.30866557  0.54345238  0.58454545  0.56664696  0.54459513  0.24820573
   0.63333333  0.63545455  0.53605436  0.61471612]
 [ 0.74170478  0.88511905  0.90363636  0.87062378  0.88796537  0.47954168
   0.72940476  0.72636364  0.70662381  0.71821817]
 [ 0.34748073  0.65297619  0.71363636  0.58485121  0.64776835  0.17142964
   0.61964286  0.69636364  0.49679678  0.61441697]
 [ 0.22855607  0.62        0.61818182  0.6044894   0.6095066   0.16102564
   0.56666667  0.56545455  0.58665566  0.55520646]
 [ 0.0129335   0.57797619  0.57636364  0.50894314  0.54699648  0.24908315
   0.5775      0.57727273  0.61519941  0.57115745]
 [ 0.50695346  0.725       0.72181818  0.72710737  0.71361416  0.35087309
   0.71130952  0.72454545  0.6876338   0.70033397]
 [ 0.33546686  0.65333333  0.65727273  0.64213905  0.64853563  0.26019895
   0.645       0.64727273  0.56132674  0.61478022]
 [-0.02794173  0.51488095  0.56909091  0.39612441  0.48797161  0.18824959
   0.61        0.60818182  0.53349454  0.59967172]
 [ 0.11445769  0.62440476  0.65545455  0.46034837  0.6044185  -0.03897469
   0.47142857  0.50272727  0.40881511  0.46226357]
 [ 0.21625213  0.60833333  0.63090909  0.56907578  0.60048535  0.13399424
   0.5577381   0.61090909  0.48298746  0.54421162]
 [ 0.4892467   0.71583333  0.71636364  0.67674497  0.70509768  0.5292556
   0.7425      0.76454545  0.72379536  0.74212343]
 [ 0.29226794  0.61944444  0.75363636  0.36104662  0.54752227  0.19434557
   0.585       0.58818182  0.42045497  0.540893  ]
 [ 0.47351433  0.69404762  0.74727273  0.60997058  0.69431013  0.48961598
   0.78416667  0.78454545  0.74142087  0.7810878 ]
 [-0.00909136  0.57261905  0.61818182  0.43186376  0.56726107  0.11785433
   0.56607143  0.6         0.43389562  0.54493298]
 [ 0.08547505  0.50446429  0.61727273  0.40854141  0.49385612  0.02354839
   0.54        0.53909091  0.43777985  0.53136808]
 [ 0.24733021  0.57333333  0.57545455  0.53358075  0.56201021  0.2765526
   0.60833333  0.63909091  0.58929025  0.60401127]
 [ 0.23893904  0.61166667  0.62727273  0.43082495  0.60459873  0.15176389
   0.61994048  0.72545455  0.21959603  0.60028227]
 [ 0.17784712  0.62166667  0.62272727  0.59218146  0.61429293  0.27093978
   0.66        0.66727273  0.68435279  0.65548368]
 [ 0.36637458  0.70583333  0.70727273  0.65465706  0.69287296  0.28923497
   0.705       0.70363636  0.67683955  0.69346681]
 [ 0.22443583  0.59333333  0.6         0.57496161  0.58892413  0.47179536
   0.705       0.69636364  0.70824684  0.69171717]
 [ 0.37155396  0.71369048  0.73454545  0.50379748  0.69376347  0.41407407
   0.71666667  0.71363636  0.70714904  0.70931818]
 [ 0.3388192   0.62142857  0.63545455  0.5710238   0.6042151   0.4771211
   0.63809524  0.66        0.66937484  0.61715507]
 [ 0.34461043  0.66845238  0.67727273  0.60922597  0.65551893  0.1692506
   0.57333333  0.56909091  0.54350365  0.5614899 ]
 [ 0.42374184  0.70535714  0.71818182  0.70359534  0.69386447  0.42918445
   0.7327381   0.75272727  0.65880262  0.71780733]]
DT mean:
[0.31162239 0.65021362 0.67263636 0.58688356 0.63798746 0.28576129
 0.64460317 0.66281818 0.58390149 0.63234133]
---------------------------
---------------------------
RF performance:
[[ 0.54434189  0.78833333  0.79181818  0.79690239  0.7841514   0.25283291
   0.69107143  0.78454545  0.49219915  0.6825299 ]
 [ 0.28688624  0.72333333  0.72181818  0.67225247  0.71459596  0.37161629
   0.7202381   0.73272727  0.64233864  0.71207709]
 [ 0.2718652   0.64583333  0.66545455  0.59586399  0.62600788  0.32996944
   0.69666667  0.69545455  0.64106428  0.69143939]
 [ 0.14551707  0.64833333  0.65636364  0.60543003  0.63617965  0.1139056
   0.54047619  0.59909091  0.39021731  0.53049451]
 [ 0.33927841  0.66202381  0.68454545  0.58599891  0.65493673  0.29392438
   0.75833333  0.76636364  0.71335322  0.75698413]
 [ 0.293974    0.69666667  0.69727273  0.71286534  0.67394494  0.50969491
   0.76666667  0.76636364  0.78686299  0.76179293]
 [ 0.13585392  0.66428571  0.68727273  0.66344726  0.65133616  0.10813559
   0.63666667  0.63636364  0.60815157  0.61521645]
 [ 0.79087921  0.9327381   0.94272727  0.88807714  0.93404762  0.53725371
   0.75190476  0.75454545  0.64668536  0.74603619]
 [ 0.34571721  0.60416667  0.63909091  0.67036371  0.58931319  0.13223103
   0.62083333  0.67818182  0.4846092   0.61103022]
 [ 0.28663594  0.59166667  0.59090909  0.5703264   0.5699803   0.22020267
   0.6075      0.61545455  0.57314495  0.60487734]
 [ 0.06133279  0.6172619   0.65636364  0.57877282  0.59645869  0.40774257
   0.745       0.75727273  0.67135797  0.72899184]
 [ 0.46957382  0.7625      0.78363636  0.67592947  0.76039753  0.44479272
   0.6922619   0.71636364  0.74166951  0.68036491]
 [ 0.34276976  0.695       0.69636364  0.67071216  0.68362193  0.05166812
   0.62583333  0.63636364  0.48218486  0.61191919]
 [-0.02950194  0.52678571  0.59818182  0.39300447  0.49159823  0.2909816
   0.58833333  0.59909091  0.63506251  0.57654734]
 [ 0.27170861  0.62083333  0.66727273  0.45998373  0.6118956   0.12286733
   0.55833333  0.61909091  0.5071729   0.54617023]
 [ 0.24718867  0.58095238  0.62090909  0.58601783  0.57090063  0.21857205
   0.62559524  0.68636364  0.53335487  0.62030678]
 [ 0.56148951  0.78583333  0.78363636  0.77169531  0.77565102  0.5393265
   0.7925      0.80545455  0.74283952  0.79214563]
 [ 0.28079973  0.57916667  0.80636364  0.23716781  0.55323007  0.41644863
   0.6825      0.68636364  0.63901891  0.66566461]
 [ 0.41374609  0.69166667  0.75545455  0.56267089  0.68302004  0.15708994
   0.69166667  0.69454545  0.66721375  0.68564769]
 [ 0.36070975  0.6327381   0.67        0.66287841  0.63211497  0.17855394
   0.58571429  0.61090909  0.54269995  0.57392233]
 [-0.05080457  0.44077381  0.66727273  0.20030975  0.41645425  0.04354839
   0.43        0.43        0.37005725  0.41884615]
 [ 0.26557774  0.625       0.62363636  0.61662885  0.60532662  0.28788056
   0.66488095  0.68818182  0.53022778  0.65026557]
 [ 0.15955707  0.565       0.57818182  0.49123892  0.55967338  0.15998398
   0.56190476  0.71636364  0.34958134  0.52797011]
 [ 0.14536703  0.62833333  0.62909091  0.6490071   0.61560939  0.40411556
   0.72916667  0.73636364  0.68662397  0.72118826]
 [ 0.34717147  0.67333333  0.67727273  0.68078973  0.65782773  0.5654023
   0.765       0.76454545  0.78825738  0.76098485]
 [ 0.12955146  0.5575      0.56181818  0.57664494  0.54836414  0.37017891
   0.7075      0.71363636  0.67534766  0.70368659]
 [ 0.53744934  0.70595238  0.74545455  0.66300415  0.68625265  0.52898467
   0.75583333  0.75545455  0.73180129  0.7507573 ]
 [ 0.19127138  0.53690476  0.60636364  0.46501795  0.52729365  0.28317533
   0.67619048  0.70818182  0.67365304  0.65631563]
 [ 0.37693104  0.66845238  0.67636364  0.5449843   0.65429015  0.15324484
   0.695       0.68727273  0.60714338  0.67685342]
 [ 0.4642469   0.82202381  0.85272727  0.7617757   0.82895521  0.50548594
   0.79642857  0.80363636  0.7750658   0.78428891]]
RF mean:
[0.29956949 0.65577976 0.69112121 0.6003254  0.64311432 0.29999368
 0.672      0.69481818 0.61096534 0.66151052]
---------------------------
---------------------------
SVM performance:
[[-0.03636364  0.48333333  0.50909091  0.          0.33665966  0.
   0.5         0.74545455  0.          0.42669419]
 [ 0.05841897  0.53        0.55909091  0.18436382  0.42905893  0.
   0.5         0.61727273  0.          0.38145425]
 [ 0.13168831  0.56        0.62909091  0.21543204  0.46915966  0.36813559
   0.68333333  0.68545455  0.66767475  0.67764291]
 [ 0.04        0.52        0.54909091  0.10796691  0.39357358  0.
   0.5         0.69636364  0.          0.41040592]
 [ 0.27136549  0.62416667  0.69545455  0.42055877  0.5832827   0.48242931
   0.74        0.74818182  0.711673    0.73162893]
 [ 0.18424812  0.59166667  0.59818182  0.40502995  0.52858433  0.49747156
   0.75        0.74545455  0.6957707   0.72982989]
 [ 0.          0.5         0.61727273  0.          0.38145425  0.08
   0.54        0.53090909  0.42128983  0.4939899 ]
 [ 0.23552941  0.6         0.75363636  0.31462644  0.56629902  0.17459243
   0.57666667  0.65636364  0.34036491  0.52022409]
 [ 0.          0.5         0.68727273  0.          0.40718954  0.
   0.5         0.69636364  0.          0.41040592]
 [ 0.          0.5         0.51909091  0.04472136  0.35570728  0.
   0.5         0.52909091  0.          0.34558824]
 [ 0.          0.5         0.62727273  0.          0.38513072  0.
   0.5         0.57909091  0.          0.36642157]
 [ 0.          0.5         0.60727273  0.          0.37777778  0.
   0.5         0.61727273  0.          0.38145425]
 [ 0.          0.5         0.56909091  0.          0.3622549   0.
   0.5         0.55909091  0.          0.35808824]
 [ 0.          0.5         0.65727273  0.          0.39616013  0.04857143
   0.5225      0.55909091  0.09472136  0.39130252]
 [ 0.          0.5         0.66727273  0.          0.3998366   0.
   0.5         0.63727273  0.          0.38880719]
 [ 0.          0.5         0.60727273  0.          0.37777778  0.
   0.5         0.66727273  0.          0.3998366 ]
 [ 0.          0.5         0.57909091  0.          0.36642157  0.
   0.5         0.59818182  0.          0.37418301]
 [ 0.          0.5         0.83363636  0.          0.45432749  0.
   0.5         0.56909091  0.          0.3622549 ]
 [ 0.          0.5         0.67727273  0.          0.40351307  0.
   0.5         0.54909091  0.          0.35392157]
 [ 0.          0.5         0.66727273  0.          0.3998366   0.
   0.5         0.62727273  0.          0.38513072]
 [ 0.          0.5         0.77545455  0.          0.43649811 -0.1
   0.45        0.44090909  0.3241224   0.40033272]
 [ 0.14        0.57        0.57909091  0.35465447  0.50014645  0.
   0.5         0.62727273  0.          0.38513072]
 [ 0.          0.5         0.58909091  0.          0.37058824  0.
   0.5         0.76545455  0.          0.43323013]
 [ 0.          0.5         0.53909091  0.          0.3497549   0.05714286
   0.525       0.59909091  0.1         0.41142157]
 [ 0.          0.5         0.56909091  0.          0.3622549   0.33886905
   0.67166667  0.66636364  0.64153268  0.65479465]
 [ 0.          0.5         0.53909091  0.          0.3497549   0.26447205
   0.62666667  0.65727273  0.44328325  0.56700549]
 [ 0.          0.5         0.68727273  0.          0.40718954  0.50600856
   0.75416667  0.75454545  0.73742826  0.74660922]
 [ 0.          0.5         0.65727273  0.          0.39616013  0.
   0.5         0.62727273  0.          0.38513072]
 [ 0.          0.5         0.60727273  0.          0.37777778  0.
   0.5         0.56909091  0.          0.3622549 ]
 [ 0.          0.5         0.62727273  0.          0.38513072  0.
   0.5         0.62727273  0.          0.38513072]]
SVM mean:
[0.03416289 0.51597222 0.626      0.06824513 0.41030871 0.09058976
 0.54466667 0.63160606 0.17259537 0.45401019]
---------------------------
---------------------------
GBM performance:
[[ 4.87675224e-01  7.55833333e-01  7.54545455e-01  7.42340299e-01
   7.48275891e-01  8.66545606e-02  5.47023810e-01  7.46363636e-01
   1.90306709e-01  5.02554610e-01]
 [ 3.75433207e-01  6.85833333e-01  6.95454545e-01  5.93320593e-01
   6.56869797e-01  4.45372697e-01  7.02380952e-01  7.45454545e-01
   6.59863937e-01  7.00187460e-01]
 [ 4.79383150e-01  7.32500000e-01  7.64545455e-01  6.36426842e-01
   7.19752470e-01  4.27868852e-01  7.15000000e-01  7.13636364e-01
   6.87879753e-01  7.05429293e-01]
 [ 1.22487144e-01  5.70833333e-01  5.77272727e-01  5.69652163e-01
   5.65774781e-01  4.03976694e-02  4.92857143e-01  6.46363636e-01
   1.59113632e-01  4.48983271e-01]
 [ 3.71463011e-01  6.81190476e-01  7.24545455e-01  5.60431521e-01
   6.57883783e-01  3.68095238e-01  6.94166667e-01  6.97272727e-01
   6.79051273e-01  6.81654179e-01]
 [ 3.95698925e-01  6.98333333e-01  6.97272727e-01  6.84192905e-01
   6.91654179e-01  4.29213251e-01  7.03333333e-01  7.06363636e-01
   6.97637310e-01  6.94093129e-01]
 [ 1.45241168e-01  5.85714286e-01  6.47272727e-01  4.51357879e-01
   5.42851870e-01  1.88205728e-01  5.93333333e-01  5.96363636e-01
   5.57927091e-01  5.79915362e-01]
 [ 8.38498254e-01  8.99404762e-01  9.22727273e-01  8.97716598e-01
   9.07380952e-01  5.36419615e-01  7.65714286e-01  7.84545455e-01
   7.47955057e-01  7.64423077e-01]
 [ 2.91120715e-01  6.21428571e-01  7.44545455e-01  3.88485360e-01
   6.05553221e-01 -8.04887689e-02  4.66666667e-01  6.37272727e-01
   5.34522484e-02  4.06762126e-01]
 [ 2.73076923e-01  6.35833333e-01  6.35454545e-01  5.89650855e-01
   6.14484127e-01  1.18409611e-01  5.76666667e-01  5.86363636e-01
   5.18746401e-01  5.66194084e-01]
 [ 2.24331222e-01  6.00595238e-01  6.64545455e-01  4.00350052e-01
   5.84697533e-01  3.93615677e-01  7.04166667e-01  7.27272727e-01
   6.45122790e-01  6.95869131e-01]
 [ 5.28595968e-01  7.54166667e-01  7.81818182e-01  7.36201398e-01
   7.60557776e-01  3.18613617e-01  6.56547619e-01  7.15454545e-01
   5.80946152e-01  6.48213747e-01]
 [ 3.84566158e-01  7.00000000e-01  7.17272727e-01  6.35112018e-01
   7.01080586e-01  1.57568536e-01  5.61666667e-01  5.76363636e-01
   4.39140214e-01  5.29642025e-01]
 [-1.98885449e-02  4.86309524e-01  6.17272727e-01  9.97432534e-02
   4.15065359e-01  2.91117837e-01  6.44166667e-01  6.54545455e-01
   6.14085600e-01  6.33994894e-01]
 [-1.55470027e-02  4.92261905e-01  6.38181818e-01  1.07735027e-01
   4.30629085e-01 -1.31899871e-02  4.96428571e-01  6.00000000e-01
   2.32246087e-01  4.53144470e-01]
 [ 9.54669598e-02  5.45238095e-01  6.18181818e-01  3.11241792e-01
   4.96136608e-01  1.34105853e-01  5.79761905e-01  6.98181818e-01
   3.43659012e-01  5.57948179e-01]
 [ 6.18077458e-01  8.12500000e-01  8.12727273e-01  7.85952839e-01
   8.02186702e-01  2.63327722e-01  6.42500000e-01  6.84545455e-01
   4.92287449e-01  6.19148352e-01]
 [ 2.73833671e-02  5.58333333e-01  8.24545455e-01  8.81917104e-02
   5.08762040e-01  2.75333901e-01  6.30833333e-01  6.38181818e-01
   5.43975149e-01  5.99010712e-01]
 [ 2.66170016e-01  6.38690476e-01  7.24545455e-01  4.35133447e-01
   5.94741117e-01  3.72549871e-01  6.92500000e-01  7.05454545e-01
   6.62168575e-01  6.90172883e-01]
 [ 2.39744466e-01  5.88690476e-01  6.88181818e-01  4.69881996e-01
   5.83459743e-01  1.43567587e-01  5.72619048e-01  6.28181818e-01
   4.31859388e-01  5.40248330e-01]
 [ 8.03971184e-02  5.33333333e-01  7.64545455e-01  1.81613837e-01
   5.05503956e-01 -5.32889344e-02  4.63333333e-01  4.60000000e-01
   4.03907699e-01  4.35228105e-01]
 [ 3.48022599e-01  6.63333333e-01  6.65454545e-01  6.54270341e-01
   6.55033578e-01  2.74903910e-01  6.41666667e-01  6.89090909e-01
   5.74453336e-01  6.37484737e-01]
 [ 9.89576640e-02  5.42500000e-01  6.00000000e-01  3.71677234e-01
   5.14355922e-01  7.91855204e-04  5.22916667e-01  7.55454545e-01
   1.28445705e-01  4.86538958e-01]
 [ 1.24488501e-01  5.70833333e-01  5.89090909e-01  4.94834928e-01
   5.45868576e-01  3.21260142e-01  6.73333333e-01  6.84545455e-01
   6.17044279e-01  6.52014652e-01]
 [ 4.21397029e-01  7.06666667e-01  7.35454545e-01  6.19495139e-01
   6.77123155e-01  4.67849462e-01  7.45000000e-01  7.43636364e-01
   7.28287625e-01  7.36957209e-01]
 [ 3.35601993e-02  5.17500000e-01  5.30000000e-01  4.45568864e-01
   4.95035936e-01  3.98443223e-01  7.00000000e-01  7.07272727e-01
   6.55701629e-01  6.90264180e-01]
 [ 3.49045204e-01  6.55952381e-01  7.55454545e-01  4.97100505e-01
   6.34759995e-01  3.86708158e-01  6.92500000e-01  6.96363636e-01
   6.81296572e-01  6.88866411e-01]
 [ 2.38321245e-01  6.10714286e-01  7.24545455e-01  3.34054801e-01
   5.61415553e-01  3.16594805e-01  6.80952381e-01  7.26363636e-01
   5.54719692e-01  6.55998168e-01]
 [ 3.96034469e-01  6.92857143e-01  7.44545455e-01  6.21753066e-01
   6.88750000e-01  1.53243980e-01  5.54166667e-01  5.77272727e-01
   4.61343676e-01  5.27469752e-01]
 [ 5.23988288e-01  7.51785714e-01  7.88181818e-01  7.25931469e-01
   7.56034799e-01  5.75795252e-01  7.67857143e-01  8.01818182e-01
   7.40527895e-01  7.69905789e-01]]
GBM mean:
[0.29143967 0.64297222 0.70493939 0.50431396 0.62072264 0.2579687
 0.62933532 0.67766667 0.51610506 0.60994391]
---------------------------
---------------------------
BDDAE performance:
[[-1.47010393e-01  4.26818182e-01  4.28571429e-01  4.03025295e-01
   4.16813137e-01 -1.89428561e-02  4.96250000e-01  7.14285714e-01
   1.39153061e-01  4.57959674e-01]
 [ 2.94911347e-01  6.48181818e-01  6.47619048e-01  6.34468533e-01
   6.40984361e-01  4.44093869e-01  7.22115385e-01  7.33333333e-01
   7.08820547e-01  7.16616551e-01]
 [ 7.44745936e-02  5.37500000e-01  5.57142857e-01  4.77257797e-01
   5.15760622e-01  1.29750001e-01  5.64090909e-01  5.66666667e-01
   5.52005636e-01  5.59388037e-01]
 [ 1.31333327e-02  5.05909091e-01  5.09523810e-01  4.83827225e-01
   4.96735625e-01 -3.51506608e-03  4.98333333e-01  6.33333333e-01
   3.30797741e-01  4.83826550e-01]
 [ 2.48391867e-01  6.23076923e-01  6.28571429e-01  6.04711213e-01
   6.11402086e-01  3.78221990e-01  6.90909091e-01  6.85714286e-01
   6.68947399e-01  6.77865531e-01]
 [ 4.32796174e-01  7.15000000e-01  7.19047619e-01  7.02736771e-01
   7.12166085e-01  1.75216422e-01  5.87727273e-01  5.90476190e-01
   5.57323820e-01  5.73883335e-01]
 [-3.88446303e-02  4.79807692e-01  5.28571429e-01  3.96186472e-01
   4.66455492e-01  8.56023871e-02  5.43181818e-01  5.42857143e-01
   4.95116296e-01  5.19710465e-01]
 [ 2.63212581e-01  6.25000000e-01  7.00000000e-01  5.64876594e-01
   6.22331689e-01  7.21666057e-03  4.99519231e-01  5.38095238e-01
   4.57367434e-01  4.92736548e-01]
 [ 3.77714516e-01  6.71428571e-01  7.42857143e-01  6.24575192e-01
   6.79219871e-01  3.68137255e-01  6.50000000e-01  8.00000000e-01
   5.07811602e-01  6.56811816e-01]
 [-7.87722728e-02  4.60454545e-01  4.61904762e-01  4.48906288e-01
   4.55374419e-01  3.70034878e-01  6.82727273e-01  6.90476190e-01
   6.55128297e-01  6.74311583e-01]
 [-1.58499018e-02  4.92307692e-01  5.38095238e-01  4.28023717e-01
   4.79797572e-01  6.06051186e-02  5.29166667e-01  5.71428571e-01
   3.70490893e-01  4.85471056e-01]
 [-1.73048434e-04  5.00961538e-01  5.42857143e-01  4.59712226e-01
   4.95724646e-01  4.52861010e-01  7.37500000e-01  7.28571429e-01
   7.34126748e-01  7.22478323e-01]
 [ 5.13156442e-02  5.23611111e-01  5.42857143e-01  4.97718385e-01
   5.19114923e-01  3.03577357e-01  6.55555556e-01  6.52380952e-01
   6.39130824e-01  6.43242495e-01]
 [ 1.21000684e-01  5.57142857e-01  6.19047619e-01  4.76066284e-01
   5.43813606e-01  2.24016151e-01  6.11363636e-01  6.14285714e-01
   6.01274514e-01  6.08142274e-01]
 [-1.30045621e-02  4.96428571e-01  5.76190476e-01  3.70153300e-01
   4.81227313e-01  1.49603659e-01  5.67788462e-01  6.04761905e-01
   5.21389571e-01  5.57321066e-01]
 [ 1.15455084e-01  5.55769231e-01  6.04761905e-01  4.87835012e-01
   5.40382392e-01  9.61462451e-02  5.39285714e-01  6.52380952e-01
   3.80193711e-01  5.14511011e-01]
 [ 5.01819873e-01  7.55555556e-01  7.52380952e-01  7.33332705e-01
   7.41212690e-01  2.96088448e-01  6.46153846e-01  6.80952381e-01
   5.99452067e-01  6.33862211e-01]
 [ 2.12389569e-01  6.23856209e-01  7.95238095e-01  4.50117107e-01
   5.92198319e-01 -1.04432816e-01  4.45833333e-01  4.57142857e-01
   4.15958080e-01  4.34762714e-01]
 [ 1.89097019e-01  5.92857143e-01  6.47619048e-01  5.56669914e-01
   5.90009581e-01  1.09853412e-01  5.54166667e-01  5.61904762e-01
   5.44965067e-01  5.51564000e-01]
 [ 2.57249204e-01  6.17857143e-01  6.80952381e-01  5.72009249e-01
   6.16956570e-01 -1.93475674e-01  4.09134615e-01  4.85714286e-01
   1.19263632e-01  3.57465893e-01]
 [-3.00038561e-02  4.89375000e-01  7.14285714e-01  1.23977291e-01
   4.50921055e-01  4.83402952e-02  5.24090909e-01  5.23809524e-01
   5.06988639e-01  5.15311828e-01]
 [ 7.84517056e-02  5.40000000e-01  5.38095238e-01  5.18171416e-01
   5.28238265e-01  1.37944378e-02  5.05288462e-01  5.80952381e-01
   3.42488971e-01  4.74131672e-01]
 [-1.56543048e-01  4.22222222e-01  4.28571429e-01  3.94045581e-01
   4.10825142e-01  3.88106341e-01  6.61250000e-01  8.19047619e-01
   5.76453608e-01  6.83797852e-01]
 [-2.66064650e-02  4.86363636e-01  4.90476190e-01  4.63093415e-01
   4.76571393e-01  3.32719145e-01  6.65277778e-01  6.80952381e-01
   6.36644589e-01  6.58331155e-01]
 [ 1.06780390e-02  5.06944444e-01  5.14285714e-01  4.81765301e-01
   4.96300601e-01  3.24689063e-02  5.16363636e-01  5.14285714e-01
   5.02067270e-01  5.08085032e-01]
 [ 1.50602782e-01  5.73636364e-01  5.80952381e-01  5.44525517e-01
   5.63649070e-01  1.81255432e-01  5.87500000e-01  6.14285714e-01
   5.41104839e-01  5.76796913e-01]
 [ 4.28462511e-01  7.00000000e-01  7.71428571e-01  6.45604595e-01
   7.02313763e-01  4.70453762e-01  7.33181818e-01  7.38095238e-01
   7.20381716e-01  7.30746609e-01]
 [ 3.06928550e-02  5.14285714e-01  5.76190476e-01  4.74833839e-01
   5.12831596e-01  3.96231599e-01  6.85096154e-01  7.38095238e-01
   6.32009785e-01  6.85335640e-01]
 [ 1.38146499e-01  5.70673077e-01  5.90476190e-01  5.47990186e-01
   5.63156990e-01  4.85745915e-01  7.34722222e-01  7.57142857e-01
   7.11406169e-01  7.36350343e-01]
 [ 2.55973604e-01  6.21634615e-01  6.71428571e-01  5.74292484e-01
   6.18384066e-01 -5.44959135e-02  4.75000000e-01  5.52380952e-01
   2.69748268e-01  4.32546150e-01]]
BDDAE mean:
[0.12463871 0.5611553  0.60333333 0.50468363 0.55136243 0.18750928
 0.59061913 0.63412698 0.51460036 0.57744548]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.52909091 0.         0.34558824 0.
  0.5        0.74545455 0.         0.42669419]
 [0.         0.5        0.53909091 0.         0.3497549  0.
  0.5        0.61727273 0.         0.38145425]
 [0.         0.5        0.57909091 0.         0.36642157 0.
  0.5        0.50909091 0.         0.3372549 ]
 [0.         0.5        0.52909091 0.         0.34558824 0.
  0.5        0.69636364 0.         0.41040592]
 [0.         0.5        0.59818182 0.         0.37418301 0.
  0.5        0.51909091 0.         0.34142157]
 [0.         0.5        0.50909091 0.         0.3372549  0.
  0.5        0.53909091 0.         0.3497549 ]
 [0.         0.5        0.61727273 0.         0.38145425 0.
  0.5        0.49090909 0.         0.32916667]
 [0.         0.5        0.68727273 0.         0.40718954 0.
  0.5        0.59818182 0.         0.37418301]
 [0.         0.5        0.68727273 0.         0.40718954 0.
  0.5        0.69636364 0.         0.41040592]
 [0.         0.5        0.51909091 0.         0.34142157 0.
  0.5        0.52909091 0.         0.34558824]
 [0.         0.5        0.62727273 0.         0.38513072 0.
  0.5        0.57909091 0.         0.36642157]
 [0.         0.5        0.60727273 0.         0.37777778 0.
  0.5        0.61727273 0.         0.38145425]
 [0.         0.5        0.56909091 0.         0.3622549  0.
  0.5        0.55909091 0.         0.35808824]
 [0.         0.5        0.65727273 0.         0.39616013 0.
  0.5        0.53909091 0.         0.3497549 ]
 [0.         0.5        0.66727273 0.         0.3998366  0.
  0.5        0.63727273 0.         0.38880719]
 [0.         0.5        0.60727273 0.         0.37777778 0.
  0.5        0.66727273 0.         0.3998366 ]
 [0.         0.5        0.57909091 0.         0.36642157 0.
  0.5        0.59818182 0.         0.37418301]
 [0.         0.5        0.83363636 0.         0.45432749 0.
  0.5        0.56909091 0.         0.3622549 ]
 [0.         0.5        0.67727273 0.         0.40351307 0.
  0.5        0.54909091 0.         0.35392157]
 [0.         0.5        0.66727273 0.         0.3998366  0.
  0.5        0.62727273 0.         0.38513072]
 [0.         0.5        0.77545455 0.         0.43649811 0.
  0.5        0.49090909 0.         0.32916667]
 [0.         0.5        0.50909091 0.         0.3372549  0.
  0.5        0.62727273 0.         0.38513072]
 [0.         0.5        0.58909091 0.         0.37058824 0.
  0.5        0.76545455 0.         0.43323013]
 [0.         0.5        0.53909091 0.         0.3497549  0.
  0.5        0.57909091 0.         0.36642157]
 [0.         0.5        0.56909091 0.         0.3622549  0.
  0.5        0.49090909 0.         0.32916667]
 [0.         0.5        0.53909091 0.         0.3497549  0.
  0.5        0.54909091 0.         0.35392157]
 [0.         0.5        0.68727273 0.         0.40718954 0.
  0.5        0.52909091 0.         0.34558824]
 [0.         0.5        0.65727273 0.         0.39616013 0.
  0.5        0.62727273 0.         0.38513072]
 [0.         0.5        0.60727273 0.         0.37777778 0.
  0.5        0.56909091 0.         0.3622549 ]
 [0.         0.5        0.62727273 0.         0.38513072 0.
  0.5        0.62727273 0.         0.38513072]]
DUMMY mean:
[0.         0.5        0.61290909 0.         0.37838155 0.
 0.5        0.59130303 0.         0.37004415]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_32
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.294 0.646 0.69  0.562 0.633 0.321 0.659 0.69  0.595 0.647]
 [0.312 0.65  0.673 0.587 0.638 0.286 0.645 0.663 0.584 0.632]
 [0.3   0.656 0.691 0.6   0.643 0.3   0.672 0.695 0.611 0.662]
 [0.034 0.516 0.626 0.068 0.41  0.091 0.545 0.632 0.173 0.454]
 [0.291 0.643 0.705 0.504 0.621 0.258 0.629 0.678 0.516 0.61 ]
 [0.125 0.561 0.603 0.505 0.551 0.188 0.591 0.634 0.515 0.577]
 [0.    0.5   0.613 0.    0.378 0.    0.5   0.591 0.    0.37 ]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.197 0.097 0.083 0.17  0.103 0.154 0.077 0.075 0.12  0.081]
 [0.168 0.077 0.071 0.114 0.081 0.146 0.071 0.07  0.124 0.074]
 [0.177 0.097 0.085 0.144 0.102 0.157 0.084 0.077 0.119 0.087]
 [0.076 0.035 0.075 0.132 0.061 0.17  0.085 0.076 0.265 0.124]
 [0.199 0.096 0.083 0.204 0.111 0.174 0.087 0.072 0.193 0.1  ]
 [0.17  0.083 0.099 0.114 0.086 0.188 0.091 0.093 0.158 0.102]
 [0.    0.    0.076 0.    0.028 0.    0.    0.071 0.    0.027]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 67.  15.  12.  30.  16.  48.  12.  11.  20.  13.]
 [ 54.  12.  11.  19.  13.  51.  11.  11.  21.  12.]
 [ 59.  15.  12.  24.  16.  52.  13.  11.  19.  13.]
 [222.   7.  12. 193.  15. 188.  16.  12. 154.  27.]
 [ 68.  15.  12.  40.  18.  67.  14.  11.  37.  16.]
 [136.  15.  16.  23.  16. 100.  15.  15.  31.  18.]
 [  0.   0.  12.   0.   7.   0.   0.  12.   0.   7.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_32
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  12.0
step (sec):  12.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  102
Elapsed time: 378.03536025285723 minutes
Elapsed time: 6.3005893375476205 hours
