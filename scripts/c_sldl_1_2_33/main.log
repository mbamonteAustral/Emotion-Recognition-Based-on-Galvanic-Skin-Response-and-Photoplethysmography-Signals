2024-05-04 15:50:01.420734: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-04 15:50:02.278334: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-04 15:50:04.061663: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  12.0
step (sec):  9.0
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  3.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_33\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

â”â”[0m [1m1s[0m 16ms/step - loss: 0.0050 - mean_squared_error: 0.0050
[1m17/95[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 0.0160 - mean_squared_error: 0.0160 
[1m37/95[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0208 - mean_squared_error: 0.0208
[1m57/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0218 - mean_squared_error: 0.0218
[1m77/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0215 - mean_squared_error: 0.0215
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 0.0211 - mean_squared_error: 0.0211 - val_loss: 0.0180 - val_mean_squared_error: 0.0180
Epoch 4/5

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.0076 - mean_squared_error: 0.0076
[1m16/95[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 0.0112 - mean_squared_error: 0.0112 
[1m34/95[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0112 - mean_squared_error: 0.0112
[1m54/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0131 - mean_squared_error: 0.0131
[1m74/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0145 - mean_squared_error: 0.0145
[1m94/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 3ms/step - loss: 0.0155 - mean_squared_error: 0.0155
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 0.0155 - mean_squared_error: 0.0155 - val_loss: 0.0230 - val_mean_squared_error: 0.0230
Epoch 5/5

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 16ms/step - loss: 0.0012 - mean_squared_error: 0.0012
[1m17/95[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - loss: 0.0240 - mean_squared_error: 0.0240 
[1m37/95[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0208 - mean_squared_error: 0.0208
[1m57/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0190 - mean_squared_error: 0.0190
[1m77/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - loss: 0.0183 - mean_squared_error: 0.0183
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0170 - val_mean_squared_error: 0.0170
(12000, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 12000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3000, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3000, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 750, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 12000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 12000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 750, 6)    â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 750, 6)    â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 750, 12)   â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 750)   â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 9000)      â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 9000)      â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚      9,001 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 9,823 (38.37 KB)
 Trainable params: 9,823 (38.37 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2:00[0m 1s/step - binary_accuracy: 0.0000e+00 - loss: 0.8921
[1m15/95[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 0.2662 - loss: 0.8179     
[1m33/95[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 0.3913 - loss: 0.7836
[1m53/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 0.4788 - loss: 0.7399
[1m73/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.5191 - loss: 0.7248
[1m94/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.5379 - loss: 0.7163
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 6ms/step - binary_accuracy: 0.5387 - loss: 0.7158 - val_binary_accuracy: 0.6364 - val_loss: 0.6511
Epoch 2/10

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.9116
[1m19/95[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.4326 - loss: 0.6983     
[1m39/95[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.4996 - loss: 0.6866
[1m60/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.5350 - loss: 0.6757
[1m81/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.5639 - loss: 0.6633
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5812 - loss: 0.6549 - val_binary_accuracy: 0.6364 - val_loss: 0.6543
Epoch 3/10

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.5082
[1m21/95[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.5891 - loss: 0.7548 
[1m42/95[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6875 - loss: 0.6299
[1m62/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7136 - loss: 0.6019
[1m83/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7127 - loss: 0.5934
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7061 - loss: 0.5934 - val_binary_accuracy: 0.7273 - val_loss: 0.6178
Epoch 4/10

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2929
[1m20/95[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7386 - loss: 0.4730 
[1m40/95[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7362 - loss: 0.4989
[1m59/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7266 - loss: 0.5193
[1m80/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7218 - loss: 0.5347
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7215 - loss: 0.5407 - val_binary_accuracy: 0.7273 - val_loss: 0.6102
Epoch 5/10

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2923
[1m23/95[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.9137 - loss: 0.3860 
[1m43/95[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8451 - loss: 0.4439
[1m63/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7917 - loss: 0.4834
[1m82/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7697 - loss: 0.4994
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7620 - loss: 0.5056 - val_binary_accuracy: 0.7273 - val_loss: 0.6445
Epoch 6/10

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 33ms/step - binary_accuracy: 0.0000e+00 - loss: 1.1684
[1m19/95[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.5788 - loss: 0.6259     
[1m39/95[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6514 - loss: 0.5687
[1m59/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6903 - loss: 0.5372
[1m79/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7133 - loss: 0.5199
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7219 - loss: 0.5179
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7223 - loss: 0.5178 - val_binary_accuracy: 0.6364 - val_loss: 0.6233
Epoch 7/10

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4291
[1m18/95[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 4ms/step - binary_accuracy: 0.9017 - loss: 0.4985 
[1m39/95[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8715 - loss: 0.4957
[1m59/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8272 - loss: 0.5063
[1m76/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8134 - loss: 0.5014
[1m94/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8062 - loss: 0.4964
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8056 - loss: 0.4961 - val_binary_accuracy: 0.6364 - val_loss: 0.6244
Epoch 8/10

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 19ms/step - binary_accuracy: 0.0000e+00 - loss: 1.4590
[1m19/95[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8133 - loss: 0.5667     
[1m40/95[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8135 - loss: 0.5176
[1m59/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8080 - loss: 0.4986
[1m77/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8038 - loss: 0.4880
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7979 - loss: 0.4819 - val_binary_accuracy: 0.6364 - val_loss: 0.6212
Epoch 9/10

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2755
[1m22/95[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.9169 - loss: 0.2791 
[1m42/95[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8710 - loss: 0.3254
[1m59/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8537 - loss: 0.3523
[1m81/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8470 - loss: 0.3721
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8454 - loss: 0.3800 - val_binary_accuracy: 0.6364 - val_loss: 0.6113
Epoch 10/10

[1m 1/95[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.6861
[1m23/95[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7639 - loss: 0.5140 
[1m44/95[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7835 - loss: 0.4866
[1m62/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8031 - loss: 0.4652
[1m81/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8185 - loss: 0.4441
[1m95/95[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8231 - loss: 0.4390 - val_binary_accuracy: 0.6364 - val_loss: 0.6535

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.803597   0.45961967 0.9301384  0.79583293 0.30030203 0.6017638
 0.8348495  0.33046383 0.25774112 0.28867137 0.70269006 0.6117421
 0.93198997 0.3656329  0.5646069  0.717103   0.9549013  0.6240634
 0.37360626 0.8722187  0.6180843  0.9202635  0.54972345 0.61564803
 0.06491237 0.30012855 0.87886703]
predicted [1 0 1 1 0 1 1 0 0 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 0 0 1]
expected [ True  True False False  True False  True  True  True False False  True
  True  True  True  True  True False False  True False  True  True  True
 False False  True]
accuracy: 0.5925925925925926
confusion matrix: 
[[ 4  6]
 [ 5 12]]
              precision    recall  f1-score   support

       False       0.44      0.40      0.42        10
        True       0.67      0.71      0.69        17

    accuracy                           0.59        27
   macro avg       0.56      0.55      0.55        27
weighted avg       0.58      0.59      0.59        27

macro avg f1-score: 0.5533834586466165
macro avg (UAR): 0.5529411764705883
Sensitivity:  0.4
Specificity:  0.7058823529411765
g-mean:  0.5313689313240573
-------- Model Performance ----------: 
accuracy:  [0.77777778 0.62962963 0.59259259 0.66666667 0.59259259 0.74074074
 0.66666667 0.7037037  0.59259259 0.59259259]
gmean:  [0.7592566  0.64168895 0.47896948 0.57394404 0.47896948 0.70293502
 0.61834694 0.59408853 0.59408853 0.53136893]
f1_score:  [0.76176471 0.625      0.52782194 0.6136725  0.52782194 0.71578947
 0.63458647 0.64473684 0.58345021 0.55338346]
UAR:  [0.76176471 0.64411765 0.53235294 0.61176471 0.53235294 0.71176471
 0.63235294 0.64117647 0.59411765 0.55294118]
Cohen Kappa score:  [0.52352941 0.26630435 0.06896552 0.23824451 0.06896552 0.43243243
 0.27027027 0.30769231 0.17728532 0.10810811]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  12.0
step (sec):  8.992
overlap:  True
perc. of overlap:  25.066666666666666
overlap duration (sec):  3.008
Number of windows / instances:  133
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.737  0.878  0.873  0.871  0.866  0.381  0.698  0.721  0.651  0.686]
 [ 0.499  0.799  0.791  0.799  0.783  0.615  0.797  0.818  0.782  0.796]
 [ 0.615  0.813  0.827  0.819  0.809  0.575  0.712  0.726  0.724  0.707]
 [ 0.486  0.728  0.782  0.664  0.725 -0.015  0.494  0.631  0.     0.387]
 [ 0.585  0.805  0.819  0.776  0.804  0.407  0.688  0.751  0.566  0.673]
 [ 0.246  0.621  0.656  0.597  0.619 -0.     0.496  0.589  0.313  0.457]
 [ 0.     0.5    0.616  0.     0.381  0.     0.5    0.639  0.     0.39 ]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[0.47132731 0.73452381 0.73681319 0.724199   0.73154245 0.41105809
  0.70833333 0.79945055 0.62036643 0.69419572]
 [0.54799576 0.77767857 0.77362637 0.76065848 0.76639638 0.44890146
  0.72347222 0.7456044  0.69154992 0.71429845]
 [0.49786108 0.745      0.76428571 0.71438218 0.7390872  0.53485263
  0.76785714 0.76703297 0.76313349 0.76540418]
 [0.39035395 0.69285714 0.7        0.67374081 0.68804307 0.12815296
  0.56166667 0.65494505 0.47034396 0.5528279 ]
 [0.49207152 0.73694444 0.77417582 0.69852287 0.7356548  0.42353733
  0.71190476 0.71373626 0.6946266  0.70397238]
 [0.47735537 0.73928571 0.73736264 0.72645582 0.73238303 0.54119128
  0.7702381  0.77417582 0.75028004 0.76263175]
 [0.2522699  0.6225     0.65494505 0.58967801 0.61939431 0.31954854
  0.66071429 0.66153846 0.64874983 0.6548634 ]
 [0.8232899  0.90527778 0.92527473 0.89731832 0.9103744  0.57587973
  0.79       0.79725275 0.77990923 0.78457919]
 [0.26623384 0.62722222 0.70714286 0.5654811  0.62366395 0.10363913
  0.54333333 0.65604396 0.3990171  0.53403775]
 [0.3106394  0.6547619  0.66208791 0.60559244 0.64304027 0.23607397
  0.61815476 0.62307692 0.59790122 0.61084034]
 [0.19536712 0.59402778 0.63736264 0.55346903 0.58996552 0.27687188
  0.63595238 0.6532967  0.60994305 0.63040107]
 [0.52453877 0.75972222 0.78131868 0.73840053 0.75632799 0.58073942
  0.78402778 0.81318681 0.75685921 0.78353935]
 [0.34920282 0.66886905 0.69175824 0.64956342 0.66979534 0.25654145
  0.62613095 0.63901099 0.59538058 0.61612491]
 [0.11836669 0.55944444 0.6010989  0.52227521 0.55394326 0.35698073
  0.67857143 0.68241758 0.64774371 0.66626763]
 [0.30928445 0.65347222 0.70879121 0.57715479 0.63964545 0.37435165
  0.68208333 0.72252747 0.64755091 0.67860652]
 [0.38517846 0.68791667 0.71373626 0.67161524 0.68989989 0.52307254
  0.75625    0.78901099 0.74097457 0.75819843]
 [0.47389737 0.73571429 0.7532967  0.71857246 0.73283996 0.37432119
  0.67619048 0.71428571 0.63194376 0.67368821]
 [0.21434977 0.61477273 0.83516484 0.33060666 0.59610952 0.47246467
  0.73357143 0.74340659 0.72294156 0.73223969]
 [0.35171439 0.66111111 0.75164835 0.54477715 0.65956369 0.40276763
  0.70059524 0.70384615 0.68273815 0.69401043]
 [0.22772165 0.61055556 0.67472527 0.54219014 0.6070396  0.27677822
  0.64013889 0.68516484 0.5710336  0.62999354]
 [0.09239317 0.54469697 0.6978022  0.35575299 0.5332578  0.07777125
  0.53809524 0.53901099 0.53326365 0.53596029]
 [0.25205837 0.62619048 0.62912088 0.61678553 0.62264943 0.4274908
  0.70944444 0.74450549 0.6865304  0.70860571]
 [0.16046705 0.58142857 0.60769231 0.49451499 0.55655855 0.41316621
  0.68722222 0.79010989 0.64182432 0.70188312]
 [0.21329525 0.60684524 0.60879121 0.5942358  0.60081471 0.45812068
  0.72970238 0.73846154 0.6924481  0.71494246]
 [0.5383693  0.76732143 0.77637363 0.7556929  0.76538542 0.57463981
  0.78690476 0.78901099 0.77816927 0.78451245]
 [0.44144713 0.71994048 0.72197802 0.71487689 0.71868979 0.38866499
  0.69345238 0.69835165 0.67885588 0.68931528]
 [0.39946552 0.69583333 0.7521978  0.63923309 0.69450753 0.48415014
  0.74047619 0.74395604 0.72872362 0.73768562]
 [0.04916964 0.52944444 0.56263736 0.40526509 0.50339279 0.43607031
  0.71555556 0.7456044  0.6808404  0.70930113]
 [0.14965379 0.57263889 0.63186813 0.43897598 0.54669074 0.33444585
  0.66416667 0.67802198 0.65277772 0.66441601]
 [0.73719999 0.87777778 0.87252747 0.87110739 0.86642529 0.38092662
  0.69819444 0.72142857 0.65081796 0.68598039]]
KNN mean:
[0.35708462 0.67679251 0.71485348 0.62303648 0.6697694  0.38643904
 0.69108003 0.71758242 0.65824127 0.68577744]
---------------------------
---------------------------
DT performance:
[[ 0.45338505  0.7797619   0.78186813  0.71448109  0.77771062  0.25227649
   0.62583333  0.71648352  0.53971042  0.60570213]
 [ 0.3990921   0.70833333  0.70714286  0.71980953  0.70414257  0.49952773
   0.77222222  0.78241758  0.75091178  0.76497676]
 [ 0.34691378  0.68238095  0.68076923  0.67229313  0.67335581  0.42067718
   0.69285714  0.6967033   0.66291165  0.68600733]
 [ 0.23695687  0.61666667  0.61593407  0.60566135  0.61364635  0.34154054
   0.6175      0.66978022  0.61533839  0.59883652]
 [ 0.34856465  0.63958333  0.64615385  0.64559057  0.62578463  0.53594546
   0.76785714  0.76813187  0.7794322   0.76477731]
 [ 0.44737261  0.73095238  0.72967033  0.70480479  0.71457387  0.5142767
   0.75119048  0.75274725  0.7217773   0.74582054]
 [ 0.27879738  0.63625     0.66043956  0.58538045  0.6318069   0.24092821
   0.60714286  0.60769231  0.60119086  0.59887366]
 [ 0.82735824  0.92055556  0.91703297  0.91757119  0.90849799  0.49070002
   0.75486111  0.77527473  0.72614472  0.75357569]
 [ 0.26303661  0.67833333  0.70824176  0.59425154  0.6526756   0.31877001
   0.65027778  0.67692308  0.61894619  0.63652146]
 [ 0.21031379  0.59047619  0.59395604  0.57167622  0.57866445  0.13954487
   0.59434524  0.60054945  0.53954952  0.5904867 ]
 [ 0.13326692  0.53666667  0.59175824  0.40416569  0.51318433  0.1752613
   0.59130952  0.59230769  0.53695849  0.57976205]
 [ 0.48663604  0.75458333  0.75934066  0.75079898  0.74820084  0.43401943
   0.73291667  0.74340659  0.7234521   0.72788309]
 [ 0.39281639  0.69589286  0.70714286  0.69910928  0.69122387  0.23407811
   0.62452381  0.63351648  0.55046933  0.61096811]
 [ 0.02471953  0.50930556  0.53296703  0.42427778  0.49783765  0.45895671
   0.73928571  0.73626374  0.717253    0.7300129 ]
 [ 0.17379459  0.61361111  0.64230769  0.5679404   0.60521427  0.26035587
   0.67902778  0.70164835  0.58840655  0.66396016]
 [ 0.29382248  0.63541667  0.65604396  0.64145245  0.63805524  0.53006229
   0.76069444  0.78901099  0.71331992  0.76323388]
 [ 0.59925968  0.78446429  0.79065934  0.81811582  0.78378549  0.5910768
   0.77785714  0.78791209  0.77726647  0.77761714]
 [ 0.20180971  0.57234848  0.7978022   0.43454997  0.54716646  0.39266472
   0.67660714  0.68076923  0.68770907  0.66892691]
 [ 0.37091457  0.62861111  0.68406593  0.58870473  0.61290352  0.3822549
   0.66815476  0.66868132  0.70068507  0.66163805]
 [ 0.24411849  0.66013889  0.69945055  0.61093766  0.65793259  0.11416433
   0.57597222  0.62472527  0.46803277  0.56381248]
 [ 0.2310033   0.61393939  0.71263736  0.42570505  0.58387273 -0.07368708
   0.4702381   0.46648352  0.41530839  0.45352536]
 [ 0.11556005  0.61666667  0.62362637  0.5765943   0.61328557  0.12773932
   0.58555556  0.63076923  0.54187655  0.58455022]
 [ 0.1200128   0.57803571  0.57747253  0.52988575  0.56017524  0.20948787
   0.62        0.72912088  0.50789818  0.62173958]
 [ 0.21636413  0.59880952  0.6021978   0.60173337  0.58489039  0.50260538
   0.76541667  0.76868132  0.76737195  0.76052288]
 [ 0.31534029  0.67910714  0.67802198  0.63217013  0.66821019  0.42414672
   0.70952381  0.71373626  0.69808308  0.70387679]
 [ 0.3295393   0.65119048  0.65054945  0.66525759  0.64558691  0.38297373
   0.68214286  0.68296703  0.65831452  0.67585928]
 [ 0.43614566  0.69722222  0.74505495  0.66354434  0.70012761  0.49182475
   0.74047619  0.73681319  0.71819464  0.73388237]
 [ 0.18386852  0.57458333  0.59450549  0.50659203  0.56874618  0.50757715
   0.76513889  0.77747253  0.77328382  0.76332202]
 [ 0.43248209  0.71347222  0.72197802  0.66341519  0.7086506   0.22340555
   0.6497619   0.65494505  0.57859831  0.63955638]
 [ 0.49934573  0.79944444  0.79065934  0.79943877  0.78267501  0.61464573
   0.79722222  0.81813187  0.78234786  0.79645293]]
DT mean:
[0.32042038 0.66322679 0.68664835 0.62453031 0.65308612 0.35792669
 0.68153042 0.69946886 0.64869144 0.67422269]
---------------------------
---------------------------
RF performance:
[[ 0.65085887  0.81547619  0.81923077  0.82277619  0.81577423  0.34195392
   0.705       0.80494505  0.63376553  0.70000769]
 [ 0.41934668  0.79375     0.7967033   0.69882223  0.79001343  0.47779094
   0.71694444  0.72472527  0.72343481  0.69547369]
 [ 0.41227756  0.71065476  0.72032967  0.72422386  0.70682625  0.42521407
   0.69880952  0.69945055  0.68423139  0.69604312]
 [ 0.46849272  0.6797619   0.67692308  0.6472947   0.67010052  0.26183795
   0.55472222  0.64120879  0.45452507  0.555573  ]
 [ 0.51240211  0.75569444  0.76758242  0.70799517  0.74654214  0.4444342
   0.74880952  0.7521978   0.68787449  0.74521156]
 [ 0.47622349  0.7452381   0.74505495  0.75601317  0.73848874  0.47169497
   0.79285714  0.79835165  0.70848058  0.78688725]
 [ 0.28986166  0.68791667  0.71318681  0.72627093  0.68827169  0.34663462
   0.61785714  0.62307692  0.62845928  0.6082369 ]
 [ 0.85455619  0.91361111  0.91813187  0.90129481  0.90835888  0.50936524
   0.78861111  0.79835165  0.78545218  0.78439656]
 [ 0.3519744   0.70888889  0.73571429  0.70022112  0.6964613   0.30765754
   0.63277778  0.71538462  0.5459818   0.62526796]
 [ 0.2890056   0.57380952  0.57692308  0.59677039  0.55422692  0.44988744
   0.71369048  0.71373626  0.67486297  0.70520237]
 [ 0.12256123  0.53736111  0.58461538  0.52442148  0.52515381  0.38199374
   0.7025      0.71318681  0.62929885  0.69764182]
 [ 0.66686181  0.76680556  0.78351648  0.77740488  0.76070067  0.53635862
   0.72402778  0.7456044   0.76302458  0.71429917]
 [ 0.33655364  0.69630952  0.70494505  0.66045743  0.68869963  0.12519148
   0.60559524  0.61483516  0.69004526  0.58592365]
 [ 0.00634408  0.61972222  0.66813187  0.52312288  0.61904992  0.35587154
   0.66785714  0.66868132  0.63711014  0.66054585]
 [ 0.28110377  0.64694444  0.68681319  0.53517374  0.63647119  0.27231876
   0.61458333  0.66263736  0.58670348  0.6092394 ]
 [ 0.33525334  0.67958333  0.69835165  0.65741763  0.67733866  0.40605899
   0.695       0.74230769  0.70386866  0.68404339]
 [ 0.70796887  0.83321429  0.83571429  0.80798366  0.83047346  0.50637876
   0.78529762  0.7956044   0.74497777  0.78070545]
 [ 0.44477982  0.68257576  0.87967033  0.34379257  0.66748925  0.38372384
   0.73720238  0.75054945  0.74604466  0.7358947 ]
 [ 0.58343266  0.73694444  0.78186813  0.76608438  0.7217542   0.41080229
   0.7235119   0.72802198  0.7069997   0.71753095]
 [ 0.4109211   0.70708333  0.75879121  0.72446119  0.71102401  0.13928991
   0.64083333  0.69450549  0.56211959  0.62498145]
 [-0.00434468  0.61        0.78021978  0.48524471  0.62139179  0.15526355
   0.59642857  0.59395604  0.4548755   0.59075549]
 [ 0.41319469  0.6297619   0.63076923  0.6746617   0.6161057   0.24238951
   0.66902778  0.71428571  0.46872717  0.66422987]
 [ 0.2154119   0.53672619  0.55549451  0.53898614  0.52352861  0.28184725
   0.72222222  0.79120879  0.6331086   0.71713241]
 [ 0.33746511  0.63392857  0.63461538  0.54680313  0.62439935  0.4599747
   0.7189881   0.72252747  0.70311602  0.71032662]
 [ 0.47211351  0.68880952  0.69505495  0.72950213  0.67765519  0.63628362
   0.79047619  0.79505495  0.72883123  0.78820153]
 [ 0.23207263  0.63244048  0.63296703  0.632213    0.62666334  0.39550296
   0.65595238  0.66208791  0.68819703  0.65321263]
 [ 0.40380016  0.68611111  0.72912088  0.68558447  0.67980766  0.5061634
   0.7547619   0.75164835  0.76091764  0.74589921]
 [ 0.16468487  0.48694444  0.53406593  0.61082524  0.46296012  0.50460002
   0.75972222  0.77802198  0.73903442  0.75452232]
 [ 0.37485684  0.70583333  0.73791209  0.62231521  0.70522077  0.32680317
   0.65363095  0.66318681  0.61992548  0.64726307]
 [ 0.61481736  0.81333333  0.82692308  0.81869022  0.80874226  0.57497011
   0.71236111  0.72637363  0.72361829  0.70696106]]
RF mean:
[0.3948284  0.69050782 0.72031136 0.66489428 0.68332312 0.3879419
 0.69666865 0.71952381 0.66058707 0.68972034]
---------------------------
---------------------------
SVM performance:
[[ 0.08934992  0.54404762  0.56373626  0.28246604  0.45627021  0.
   0.5         0.75274725  0.          0.42934783]
 [ 0.33265207  0.66011905  0.68351648  0.59767317  0.64247789  0.22574432
   0.60208333  0.68516484  0.34342965  0.54440367]
 [ 0.4495758   0.70833333  0.7510989   0.63089049  0.69626984  0.44665142
   0.72380952  0.72362637  0.71891884  0.72119797]
 [ 0.36769517  0.68095238  0.69120879  0.64053086  0.66797097  0.
   0.5         0.70659341  0.          0.41393281]
 [ 0.54331343  0.75944444  0.7978022   0.72198837  0.76132102  0.47238075
   0.73571429  0.73846154  0.70926171  0.72458181]
 [ 0.38490502  0.69166667  0.69285714  0.64879381  0.67277979  0.47170814
   0.73690476  0.73846154  0.71490713  0.72661249]
 [ 0.          0.5         0.6021978   0.          0.37575758  0.10334465
   0.55119048  0.55604396  0.485056    0.52125893]
 [ 0.72101063  0.83638889  0.88736264  0.82139629  0.85857769  0.25014174
   0.60958333  0.69175824  0.46188627  0.58305404]
 [ 0.          0.5         0.68461538  0.          0.40629117  0.
   0.5         0.68461538  0.          0.40629117]
 [ 0.22329055  0.61071429  0.61703297  0.53519489  0.58574589  0.07863916
   0.53660714  0.55824176  0.29504555  0.45090815]
 [ 0.          0.5         0.63131868  0.          0.38687182  0.04705882
   0.52        0.58681319  0.08944272  0.40051948]
 [ 0.02352941  0.51        0.61703297  0.04472136  0.3970958   0.
   0.5         0.61648352  0.          0.38129117]
 [ 0.03036454  0.51375     0.58681319  0.06324555  0.39595808  0.
   0.5         0.55604396  0.          0.35718615]
 [ 0.          0.5         0.63131868  0.          0.38687182  0.25680769
   0.62738095  0.63296703  0.53265771  0.59242291]
 [ 0.          0.5         0.66978022  0.          0.40094109  0.
   0.5         0.6467033   0.          0.39249953]
 [ 0.          0.5         0.6021978   0.          0.37575758  0.
   0.5         0.6543956   0.          0.39531338]
 [ 0.          0.5         0.58681319  0.          0.3695671   0.
   0.5         0.59450549  0.          0.37266234]
 [ 0.          0.5         0.84945055  0.          0.45929487  0.
   0.5         0.57912088  0.          0.36647186]
 [ 0.          0.5         0.68461538  0.          0.40629117  0.
   0.5         0.54120879  0.          0.35106061]
 [ 0.          0.5         0.66978022  0.          0.40094109  0.
   0.5         0.6467033   0.          0.39249953]
 [ 0.          0.5         0.77417582  0.          0.43634783  0.
   0.5         0.52692308  0.          0.345     ]
 [ 0.22113238  0.61071429  0.60934066  0.56343604  0.58666229  0.
   0.5         0.6543956   0.          0.39531338]
 [ 0.          0.5         0.57912088  0.          0.36647186  0.
   0.5         0.73736264  0.          0.42420949]
 [ 0.          0.5         0.54120879  0.          0.35106061  0.28457082
   0.63208333  0.67747253  0.50657407  0.59324666]
 [ 0.          0.5         0.57142857  0.          0.36337662  0.43503012
   0.71785714  0.72087912  0.69288276  0.70713825]
 [ 0.          0.5         0.53406593  0.          0.3480303   0.34720164
   0.66875     0.68351648  0.63631712  0.66173075]
 [ 0.          0.5         0.68461538  0.          0.40629117  0.51377137
   0.7547619   0.75824176  0.74615649  0.75346918]
 [ 0.          0.5         0.63131868  0.          0.38687182  0.
   0.5         0.61648352  0.          0.38129117]
 [ 0.          0.5         0.61648352  0.          0.38129117  0.01860465
   0.50833333  0.57087912  0.04082483  0.3762987 ]
 [ 0.48640751  0.72791667  0.78186813  0.66439285  0.72499854 -0.01470588
   0.49375     0.63131868  0.          0.38659044]]
SVM mean:
[0.12910755 0.56180159 0.66080586 0.20715766 0.48181516 0.13123165
 0.56396032 0.64893773 0.23244536 0.48492679]
---------------------------
---------------------------
GBM performance:
[[ 0.5497435   0.77142857  0.78021978  0.74396569  0.76737884  0.16242822
   0.56166667  0.77582418  0.19415694  0.51984378]
 [ 0.42598336  0.69494048  0.70549451  0.67139649  0.68663376  0.44889534
   0.72319444  0.76758242  0.66123753  0.72098239]
 [ 0.46099636  0.70154762  0.73516484  0.64749436  0.68609077  0.26498394
   0.63095238  0.63736264  0.59868115  0.61846889]
 [ 0.35023624  0.66785714  0.67032967  0.6453103   0.65781141  0.1087301
   0.54305556  0.72197802  0.20487548  0.49774892]
 [ 0.4877093   0.72486111  0.76703297  0.68933407  0.7241502   0.45491609
   0.73571429  0.73791209  0.70437419  0.72274149]
 [ 0.41144525  0.70595238  0.7         0.66321543  0.68242459  0.52785799
   0.78571429  0.79065934  0.75440103  0.77720276]
 [ 0.31239332  0.65208333  0.70054945  0.59156131  0.65037827  0.33165944
   0.66547619  0.66813187  0.6609642   0.66029814]
 [ 0.84009163  0.90777778  0.92527473  0.90624769  0.91077729  0.5931746
   0.805       0.82087912  0.78343445  0.79884326]
 [ 0.37716612  0.65416667  0.75879121  0.52155243  0.64577742  0.10159063
   0.55166667  0.69120879  0.30557067  0.5190499 ]
 [ 0.19939175  0.60119048  0.6         0.53438959  0.57553203  0.1676058
   0.58333333  0.58681319  0.56702194  0.57685551]
 [ 0.02494814  0.51194444  0.60714286  0.30599808  0.46237944  0.2922648
   0.64428571  0.66098901  0.61819938  0.63826833]
 [ 0.60790208  0.78305556  0.81923077  0.75022011  0.78697996  0.39046963
   0.70097222  0.73131868  0.67424781  0.699745  ]
 [ 0.44343485  0.71928571  0.7489011   0.68452307  0.72296831  0.24315964
   0.60434524  0.62362637  0.57828398  0.59133887]
 [ 0.12652248  0.54833333  0.64505495  0.23521539  0.49779695  0.52037181
   0.75833333  0.76538462  0.7174318   0.74776309]
 [ 0.23375494  0.61972222  0.72967033  0.42356995  0.59811574  0.13706624
   0.57402778  0.67747253  0.32331663  0.52509843]
 [ 0.17137649  0.57375     0.6467033   0.41297962  0.5397452   0.29404728
   0.64819444  0.72142857  0.55505338  0.65234397]
 [ 0.67388943  0.83244048  0.84285714  0.82688954  0.83573769  0.35059447
   0.67660714  0.71318681  0.63703211  0.67477253]
 [ 0.15675395  0.5625      0.85054945  0.21213203  0.55810312  0.42836757
   0.71017857  0.72747253  0.69169406  0.70922736]
 [ 0.33202277  0.63388889  0.75879121  0.47483704  0.62368535  0.39952946
   0.69761905  0.70659341  0.66086713  0.68522495]
 [ 0.17381123  0.58930556  0.69725275  0.42381727  0.57759398  0.15930099
   0.56833333  0.67802198  0.35087306  0.53282001]
 [ 0.08695652  0.53333333  0.78956044  0.11547005  0.4903004  -0.14173583
   0.4297619   0.43791209  0.33789111  0.40664608]
 [ 0.33291119  0.66785714  0.66868132  0.62096411  0.64516844  0.17484286
   0.58347222  0.68516484  0.43042844  0.56886251]
 [ 0.21992538  0.6039881   0.63846154  0.50735155  0.58018024  0.10366945
   0.54277778  0.74505495  0.20964773  0.50223132]
 [ 0.18253268  0.58779762  0.6032967   0.55258794  0.57942085  0.55673546
   0.78833333  0.79065934  0.75149695  0.78301992]
 [ 0.31632059  0.64642857  0.67252747  0.57144351  0.6329362   0.49727696
   0.73809524  0.74340659  0.73681586  0.73478091]
 [ 0.39803043  0.70119048  0.71538462  0.66215345  0.69465144  0.39001994
   0.69315476  0.7         0.68217889  0.69120865]
 [ 0.34744305  0.66388889  0.77417582  0.5515668   0.66368687  0.53307674
   0.775       0.77417582  0.76202594  0.77237637]
 [ 0.14603699  0.56597222  0.6467033   0.35660902  0.52808272  0.37994187
   0.68805556  0.73076923  0.65232912  0.69005518]
 [ 0.28068481  0.64958333  0.71538462  0.54590081  0.64416372  0.28710145
   0.64511905  0.66978022  0.59782694  0.63614899]
 [ 0.58480102  0.805       0.81923077  0.77573783  0.80381295  0.40666137
   0.68763889  0.7510989   0.56577094  0.67328154]]
GBM mean:
[0.34184053 0.66270238 0.72441392 0.55414782 0.64841547 0.31882014
 0.65800265 0.70772894 0.56560429 0.64424164]
---------------------------
---------------------------
BDDAE performance:
[[-9.40030254e-02  4.53021978e-01  4.55555556e-01  4.32107795e-01
   4.43923080e-01  6.87562089e-02  5.29285714e-01  7.22222222e-01
   2.53781365e-01  5.01916117e-01]
 [ 2.07220904e-01  6.05833333e-01  6.03703704e-01  5.97704352e-01
   5.98923454e-01  5.07527358e-01  7.55965909e-01  7.62962963e-01
   7.48724368e-01  7.52096834e-01]
 [ 1.38033541e-01  5.67613636e-01  5.85185185e-01  5.45650999e-01
   5.62429557e-01  2.07228659e-01  6.02747253e-01  6.07407407e-01
   5.86284739e-01  5.97188303e-01]
 [ 2.12230183e-02  5.10164835e-01  5.14814815e-01  4.43469767e-01
   4.80320021e-01  1.28664877e-01  5.58223684e-01  6.88888889e-01
   4.24364943e-01  5.40852173e-01]
 [ 2.84575514e-01  6.38920455e-01  6.62962963e-01  6.14062679e-01
   6.35898095e-01  2.36829739e-01  6.19505495e-01  6.14814815e-01
   5.79286398e-01  5.97260703e-01]
 [ 5.59866034e-01  7.78021978e-01  7.81481481e-01  7.68540819e-01
   7.76997619e-01 -7.75557212e-02  4.61538462e-01  4.62962963e-01
   4.20892690e-01  4.42340506e-01]
 [ 6.59362264e-02  5.31534091e-01  5.59259259e-01  4.89242283e-01
   5.22390661e-01 -6.13047179e-02  4.69230769e-01  4.70370370e-01
   4.35767286e-01  4.53823861e-01]
 [ 2.53220155e-01  6.13888889e-01  7.03703704e-01  5.38778584e-01
   6.13184205e-01 -7.28892923e-02  4.63529412e-01  5.11111111e-01
   3.75355162e-01  4.45963491e-01]
 [ 4.34513949e-01  7.05555556e-01  7.77777778e-01  6.36403662e-01
   6.99825806e-01  5.17916992e-01  7.33333333e-01  8.11111111e-01
   6.84631925e-01  7.48536537e-01]
 [ 5.33367518e-02  5.26648352e-01  5.25925926e-01  5.14475877e-01
   5.20571969e-01  2.89126626e-01  6.42582418e-01  6.48148148e-01
   6.16335258e-01  6.33876625e-01]
 [ 1.16422105e-01  5.56764706e-01  6.00000000e-01  5.19426168e-01
   5.53034444e-01  1.34357424e-01  5.65000000e-01  5.85185185e-01
   5.18558557e-01  5.50710894e-01]
 [-3.50009936e-02  4.83238636e-01  5.22222222e-01  4.19965099e-01
   4.69025816e-01  4.00766206e-01  7.03529412e-01  7.14814815e-01
   6.95085949e-01  6.96964135e-01]
 [ 8.91256803e-03  5.07386364e-01  5.22222222e-01  4.83259617e-01
   4.99149384e-01  2.84963405e-01  6.40833333e-01  6.51851852e-01
   6.20077310e-01  6.35104102e-01]
 [ 8.39476947e-02  5.39705882e-01  5.88888889e-01  4.93725827e-01
   5.35467803e-01  2.68712595e-01  6.35164835e-01  6.33333333e-01
   6.22906734e-01  6.28286858e-01]
 [ 1.25607315e-03  4.97222222e-01  5.77777778e-01  4.22803623e-01
   4.91152785e-01  1.68969145e-01  5.77941176e-01  6.37037037e-01
   5.15995580e-01  5.70731406e-01]
 [ 9.67713380e-02  5.51136364e-01  5.74074074e-01  4.57878373e-01
   5.20769968e-01  6.21152471e-02  5.25000000e-01  6.62962963e-01
   2.46157840e-01  4.77061086e-01]
 [ 5.61152703e-01  7.86079545e-01  7.85185185e-01  7.77361583e-01
   7.77303835e-01  1.77726795e-01  5.80965909e-01  6.29629630e-01
   5.05835180e-01  5.67327626e-01]
 [ 3.01828544e-01  6.52717391e-01  8.48148148e-01  5.10948717e-01
   6.41647062e-01 -1.29247572e-01  4.37215909e-01  4.77777778e-01
   3.56650451e-01  4.22832547e-01]
 [ 1.97199018e-01  5.97222222e-01  6.55555556e-01  5.52138031e-01
   5.90331086e-01  2.98625242e-01  6.49166667e-01  6.55555556e-01
   6.35830126e-01  6.44759271e-01]
 [ 1.77608414e-01  5.80555556e-01  6.81481481e-01  4.49074808e-01
   5.63533333e-01 -8.21841442e-02  4.62058824e-01  5.48148148e-01
   2.76746884e-01  4.26633099e-01]
 [ 2.49069814e-01  6.13095238e-01  7.59259259e-01  5.32303402e-01
   6.16004141e-01  1.96563459e-01  5.98076923e-01  6.00000000e-01
   5.94091540e-01  5.96933386e-01]
 [-6.53103980e-02  4.67307692e-01  4.66666667e-01  4.54800081e-01
   4.60622310e-01 -3.23228012e-02  4.83333333e-01  5.77777778e-01
   3.24268561e-01  4.65967511e-01]
 [ 3.11419546e-03  5.02556818e-01  5.14814815e-01  4.81091056e-01
   4.95344239e-01  5.41939181e-01  7.48571429e-01  8.40740741e-01
   7.12733675e-01  7.66418479e-01]
 [ 7.32949007e-02  5.35833333e-01  5.44444444e-01  5.17570367e-01
   5.29847723e-01  1.90720829e-01  5.95000000e-01  6.03703704e-01
   5.82354910e-01  5.91866335e-01]
 [ 1.70034325e-01  5.85000000e-01  5.92592593e-01  5.72322464e-01
   5.81512778e-01  8.50905855e-02  5.42307692e-01  5.44444444e-01
   5.25319855e-01  5.34896150e-01]
 [ 1.32791981e-01  5.65659341e-01  5.70370370e-01  5.36641551e-01
   5.53948732e-01  1.73881499e-01  5.85833333e-01  6.00000000e-01
   5.57504631e-01  5.77515748e-01]
 [ 3.99107260e-01  6.94444444e-01  7.44444444e-01  6.70021865e-01
   6.97339604e-01  4.37333066e-01  7.16758242e-01  7.22222222e-01
   6.94026966e-01  7.09686567e-01]
 [-8.21986330e-02  4.62647059e-01  5.07407407e-01  3.94701651e-01
   4.47699514e-01  3.74513100e-01  6.74117647e-01  7.29629630e-01
   6.33802578e-01  6.79111693e-01]
 [ 1.15105622e-01  5.55000000e-01  5.92592593e-01  5.19357689e-01
   5.49040610e-01  4.29919341e-01  7.12500000e-01  7.22222222e-01
   6.96112856e-01  7.09373262e-01]
 [ 2.46179775e-01  6.21470588e-01  6.55555556e-01  5.97365651e-01
   6.18802753e-01 -4.48744772e-04  4.96470588e-01  5.88888889e-01
   3.12798087e-01  4.56634345e-01]]
BDDAE mean:
[0.15584031 0.57620822 0.61580247 0.53143981 0.56820141 0.19087649
 0.59219292 0.63419753 0.52507608 0.58075565]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.52692308 0.         0.345      0.
  0.5        0.75274725 0.         0.42934783]
 [0.         0.5        0.54835165 0.         0.35409091 0.
  0.5        0.60934066 0.         0.37852437]
 [0.         0.5        0.57912088 0.         0.36647186 0.
  0.5        0.51923077 0.         0.34157895]
 [0.         0.5        0.52692308 0.         0.345      0.
  0.5        0.70659341 0.         0.41393281]
 [0.         0.5        0.60934066 0.         0.37852437 0.
  0.5        0.51153846 0.         0.33815789]
 [0.         0.5        0.51153846 0.         0.33815789 0.
  0.5        0.51923077 0.         0.34157895]
 [0.         0.5        0.6021978  0.         0.37575758 0.
  0.5        0.51153846 0.         0.33815789]
 [0.         0.5        0.67747253 0.         0.40375494 0.
  0.5        0.61648352 0.         0.38129117]
 [0.         0.5        0.68461538 0.         0.40629117 0.
  0.5        0.68461538 0.         0.40629117]
 [0.         0.5        0.51153846 0.         0.33815789 0.
  0.5        0.53406593 0.         0.3480303 ]
 [0.         0.5        0.63131868 0.         0.38687182 0.
  0.5        0.57142857 0.         0.36337662]
 [0.         0.5        0.60934066 0.         0.37852437 0.
  0.5        0.61648352 0.         0.38129117]
 [0.         0.5        0.57912088 0.         0.36647186 0.
  0.5        0.55604396 0.         0.35718615]
 [0.         0.5        0.63131868 0.         0.38687182 0.
  0.5        0.51923077 0.         0.34157895]
 [0.         0.5        0.66978022 0.         0.40094109 0.
  0.5        0.6467033  0.         0.39249953]
 [0.         0.5        0.6021978  0.         0.37575758 0.
  0.5        0.6543956  0.         0.39531338]
 [0.         0.5        0.58681319 0.         0.3695671  0.
  0.5        0.59450549 0.         0.37266234]
 [0.         0.5        0.84945055 0.         0.45929487 0.
  0.5        0.57912088 0.         0.36647186]
 [0.         0.5        0.68461538 0.         0.40629117 0.
  0.5        0.54120879 0.         0.35106061]
 [0.         0.5        0.66978022 0.         0.40094109 0.
  0.5        0.6467033  0.         0.39249953]
 [0.         0.5        0.77417582 0.         0.43634783 0.
  0.5        0.52692308 0.         0.345     ]
 [0.         0.5        0.51153846 0.         0.33815789 0.
  0.5        0.6543956  0.         0.39531338]
 [0.         0.5        0.57912088 0.         0.36647186 0.
  0.5        0.73736264 0.         0.42420949]
 [0.         0.5        0.54120879 0.         0.35106061 0.
  0.5        0.56373626 0.         0.36028139]
 [0.         0.5        0.57142857 0.         0.36337662 0.
  0.5        0.51153846 0.         0.33815789]
 [0.         0.5        0.53406593 0.         0.3480303  0.
  0.5        0.54120879 0.         0.35106061]
 [0.         0.5        0.68461538 0.         0.40629117 0.
  0.5        0.51153846 0.         0.33815789]
 [0.         0.5        0.63131868 0.         0.38687182 0.
  0.5        0.61648352 0.         0.38129117]
 [0.         0.5        0.61648352 0.         0.38129117 0.
  0.5        0.56373626 0.         0.36028139]
 [0.         0.5        0.61648352 0.         0.38129117 0.
  0.5        0.63901099 0.         0.38968568]]
DUMMY mean:
[0.         0.5        0.61173993 0.         0.37806433 0.
 0.5        0.59190476 0.         0.37047568]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_33
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.357 0.677 0.715 0.623 0.67  0.386 0.691 0.718 0.658 0.686]
 [0.32  0.663 0.687 0.625 0.653 0.358 0.682 0.699 0.649 0.674]
 [0.395 0.691 0.72  0.665 0.683 0.388 0.697 0.72  0.661 0.69 ]
 [0.129 0.562 0.661 0.207 0.482 0.131 0.564 0.649 0.232 0.485]
 [0.342 0.663 0.724 0.554 0.648 0.319 0.658 0.708 0.566 0.644]
 [0.156 0.576 0.616 0.531 0.568 0.191 0.592 0.634 0.525 0.581]
 [0.    0.5   0.612 0.    0.378 0.    0.5   0.592 0.    0.37 ]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.18  0.09  0.081 0.134 0.094 0.132 0.066 0.062 0.085 0.067]
 [0.161 0.085 0.081 0.116 0.088 0.164 0.078 0.075 0.101 0.081]
 [0.193 0.092 0.092 0.117 0.095 0.125 0.062 0.058 0.089 0.062]
 [0.205 0.097 0.087 0.296 0.145 0.181 0.09  0.068 0.292 0.135]
 [0.184 0.091 0.078 0.182 0.103 0.169 0.088 0.072 0.178 0.099]
 [0.17  0.084 0.102 0.092 0.087 0.19  0.092 0.093 0.146 0.102]
 [0.    0.    0.077 0.    0.028 0.    0.    0.069 0.    0.027]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 50.  13.  11.  22.  14.  34.  10.   9.  13.  10.]
 [ 50.  13.  12.  19.  13.  46.  11.  11.  16.  12.]
 [ 49.  13.  13.  18.  14.  32.   9.   8.  13.   9.]
 [159.  17.  13. 143.  30. 138.  16.  10. 126.  28.]
 [ 54.  14.  11.  33.  16.  53.  13.  10.  31.  15.]
 [109.  15.  17.  17.  15. 100.  16.  15.  28.  18.]
 [  0.   0.  13.   0.   7.   0.   0.  12.   0.   7.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_33
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  12.0
step (sec):  8.992
overlap:  True
perc. of overlap:  25.066666666666666
overlap duration (sec):  3.008
Number of windows / instances:  133
Elapsed time: 475.7067307194074 minutes
Elapsed time: 7.928445511990123 hours
