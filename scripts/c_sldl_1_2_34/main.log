2024-05-04 23:45:54.731872: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-04 23:45:58.500748: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-04 23:46:08.108838: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  12.0
step (sec):  6.0
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  6.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_34\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

Specificity:  0.8
g-mean:  0.6531972647421809
-------- Model Performance ----------: 
accuracy:  [0.65  0.7   0.575 0.8   0.825 0.7   0.75  0.55  0.7   0.   ]
gmean:  [0.59553897 0.50596443 0.48989795 0.71554175 0.80332642 0.67527772
 0.68507907 0.5585696  0.65319726 0.        ]
f1_score:  [0.61538462 0.6        0.52480783 0.76190476 0.81068289 0.68
 0.71509972 0.54545455 0.67032967 0.        ]
UAR:  [0.61333333 0.61333333 0.52666667 0.74666667 0.80666667 0.68
 0.70666667 0.56       0.66666667 0.        ]
Cohen Kappa score:  [0.23287671 0.26153846 0.05555556 0.53623188 0.62162162 0.36
 0.43661972 0.11111111 0.34246575 0.        ]
Split Repetition number:  9
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [ 83   7 184 124 165  27  77 141 121  39 100  88 178 126  98 149 160  89
 127 158 132 156 183  38 118  94 140 125  76 128  10  68 175  81 142  41
 191  46  54 143   9 170  44 176  30 193  24  16 163 101 187  84  32 196
 119  61  37  55   8  25  22  12 139 114  58   3  51  28  60 173  56 131
 102 162  49  42 181 136  78 161  14 133 174 155 172 115  95 122  87  53
 192  74  72 113 166  34 179 112   4  65 164  19 186  97   5  91  29 146
  52 153 134 137 177 116  71  90  33 130  67   6  26  80 197 152  73  13
 188 145 171  66   2 108 120 109  36  59  50 105 194  69 180 189  63  23
 138 144 104 107 106  40 111 168 151  45 110  70  35   1] TEST: [185 148  99  62  75  92  18  79  43  21 135 190  15 123 147  20  57 167
 103  64  86 182  31  96 195 150 159   0  82 129  48  93  17 117  85  47
 157 154  11 169]
(DL) TRAIN number of instances:  158
(DL) TEST number of instances:  40
(DL) Total number of instances (TRAIN+TEST):  198
C:\Users\Javier\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\convolutional\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(
----- train_GSR_AE -------
Model: "sequential_1198"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 12000, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 3000, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 3000, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 750, 6)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2396              │ (None, 3000, 6)        │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2396           │ (None, 3000, 6)        │           366 │
│ (Conv1DTranspose)               │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2397              │ (None, 12000, 6)       │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2397           │ (None, 12000, 1)       │           121 │
│ (Conv1DTranspose)               │                        │               │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m2:34[0m 1s/step - loss: 71.1760 - mean_squared_error: 71.1760
[1m 12/142[0m [32m━[0m[37m━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 46.3656 - mean_squared_error: 46.3656 
[1m 27/142[0m [32m━━━[0m[37m━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 34.7940 - mean_squared_error: 34.7940
[1m 47/142[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 27.6418 - mean_squared_error: 27.6418
[1m 67/142[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 23.8910 - mean_squared_error: 23.8910
[1m 87/142[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 21.7408 - mean_squared_error: 21.7408
[1m106/142[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 3ms/step - loss: 20.2794 - mean_squared_error: 20.2794
[1m127/142[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 3ms/step - loss: 19.1210 - mean_squared_error: 19.1210
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 5ms/step - loss: 18.4564 - mean_squared_error: 18.4564 - val_loss: 7.5529 - val_mean_squared_error: 7.5529
Epoch 2/5

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m4s[0m 31ms/step - loss: 2.3856 - mean_squared_error: 2.3856
[1m 16/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 14.0260 - mean_squared_error: 14.0260
[1m 36/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 13.6771 - mean_squared_error: 13.6771
[1m 56/142[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 13.4452 - mean_squared_error: 13.4452
[1m 72/142[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 13.3195 - mean_squared_error: 13.3195
[1m 92/142[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 13.2664 - mean_squared_error: 13.2664
[1m112/142[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 3ms/step - loss: 13.1828 - mean_squared_error: 13.1828
[1m132/142[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 3ms/step - loss: 13.0380 - mean_squared_error: 13.0380
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 4ms/step - loss: 12.9752 - mean_squared_error: 12.9752 - val_loss: 7.5436 - val_mean_squared_error: 7.5436
Epoch 3/5

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m2s[0m 16ms/step - loss: 5.4466 - mean_squared_error: 5.4466
[1m 18/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 9.8990 - mean_squared_error: 9.8990 
[1m 37/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 11.4586 - mean_squared_error: 11.4586
[1m 52/142[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 12.1636 - mean_squared_error: 12.1636
[1m 72/142[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 12.6100 - mean_squared_error: 12.6100
[1m 92/142[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 12.5997 - mean_squared_error: 12.5997
[1m110/142[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 3ms/step - loss: 12.5562 - mean_squared_error: 12.5562
[1m128/142[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 3ms/step - loss: 12.5210 - mean_squared_error: 12.5210
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 4ms/step - loss: 12.4861 - mean_squared_error: 12.4861 - val_loss: 7.5415 - val_mean_squared_error: 7.5415
Epoch 4/5

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m2s[0m 16ms/step - loss: 16.4747 - mean_squared_error: 16.4747
[1m 18/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 10.7077 - mean_squared_error: 10.7077 
[1m 39/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 12.3065 - mean_squared_error: 12.3065
[1m 59/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 12.3568 - mean_squared_error: 12.3568
[1m 79/142[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 12.4365 - mean_squared_error: 12.4365
[1m 99/142[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 3ms/step - loss: 12.3008 - mean_squared_error: 12.3008
[1m120/142[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 3ms/step - loss: 12.2557 - mean_squared_error: 12.2557
[1m141/142[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 3ms/step - loss: 12.2261 - mean_squared_error: 12.2261
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 3ms/step - loss: 12.2256 - mean_squared_error: 12.2256 - val_loss: 7.5406 - val_mean_squared_error: 7.5406
Epoch 5/5

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 12ms/step - loss: 15.7587 - mean_squared_error: 15.7587
[1m 16/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 17.2412 - mean_squared_error: 17.2412 
[1m 36/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 14.9170 - mean_squared_error: 14.9170
[1m 57/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 13.2935 - mean_squared_error: 13.2935
[1m 77/142[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 12.4917 - mean_squared_error: 12.4917
[1m 97/142[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 3ms/step - loss: 12.0610 - mean_squared_error: 12.0610
[1m114/142[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 3ms/step - loss: 11.8991 - mean_squared_error: 11.8991
[1m134/142[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 3ms/step - loss: 11.9105 - mean_squared_error: 11.9105
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 4ms/step - loss: 11.9126 - mean_squared_error: 11.9126 - val_loss: 7.5406 - val_mean_squared_error: 7.5406
(158, 750, 6)
Model: "sequential_1198"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 12000, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 3000, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 3000, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 750, 6)         │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 12000, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 3000, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 3000, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 750, 6)         │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2398              │ (None, 3000, 6)        │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2398           │ (None, 3000, 6)        │           366 │
│ (Conv1DTranspose)               │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2399              │ (None, 12000, 6)       │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2399           │ (None, 12000, 1)       │           121 │
│ (Conv1DTranspose)               │                        │               │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m2:38[0m 1s/step - loss: 0.2997 - mean_squared_error: 0.2997
[1m 16/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.1066 - mean_squared_error: 0.1066 
[1m 36/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0770 - mean_squared_error: 0.0770
[1m 57/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0646 - mean_squared_error: 0.0646
[1m 71/142[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0588 - mean_squared_error: 0.0588
[1m 86/142[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0541 - mean_squared_error: 0.0541
[1m104/142[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0498 - mean_squared_error: 0.0498
[1m120/142[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - loss: 0.0469 - mean_squared_error: 0.0469
[1m137/142[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 4ms/step - loss: 0.0445 - mean_squared_error: 0.0445
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 5ms/step - loss: 0.0438 - mean_squared_error: 0.0438 - val_loss: 0.0095 - val_mean_squared_error: 0.0095
Epoch 2/5

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m2s[0m 16ms/step - loss: 0.0066 - mean_squared_error: 0.0066
[1m 18/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0183 - mean_squared_error: 0.0183 
[1m 39/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0204 - mean_squared_error: 0.0204
[1m 59/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0213 - mean_squared_error: 0.0213
[1m 79/142[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0213 - mean_squared_error: 0.0213
[1m 99/142[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0209 - mean_squared_error: 0.0209
[1m120/142[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 3ms/step - loss: 0.0204 - mean_squared_error: 0.0204
[1m139/142[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 3ms/step - loss: 0.0201 - mean_squared_error: 0.0201
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 3ms/step - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0084 - val_mean_squared_error: 0.0084
Epoch 3/5

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m4s[0m 30ms/step - loss: 0.0061 - mean_squared_error: 0.0061
[1m 19/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0093 - mean_squared_error: 0.0093 
[1m 40/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0114 - mean_squared_error: 0.0114
[1m 60/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0123 - mean_squared_error: 0.0123
[1m 81/142[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0131 - mean_squared_error: 0.0131
[1m102/142[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0138 - mean_squared_error: 0.0138
[1m121/142[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 3ms/step - loss: 0.0143 - mean_squared_error: 0.0143
[1m138/142[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 3ms/step - loss: 0.0147 - mean_squared_error: 0.0147
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 3ms/step - loss: 0.0148 - mean_squared_error: 0.0148 - val_loss: 0.0100 - val_mean_squared_error: 0.0100
Epoch 4/5

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m2s[0m 16ms/step - loss: 0.0121 - mean_squared_error: 0.0121
[1m 16/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0161 - mean_squared_error: 0.0161 
[1m 36/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0162 - mean_squared_error: 0.0162
[1m 56/142[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0164 - mean_squared_error: 0.0164
[1m 76/142[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0166 - mean_squared_error: 0.0166
[1m 95/142[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0169 - mean_squared_error: 0.0169
[1m113/142[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0170 - mean_squared_error: 0.0170
[1m130/142[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 3ms/step - loss: 0.0171 - mean_squared_error: 0.0171
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 4ms/step - loss: 0.0171 - mean_squared_error: 0.0171 - val_loss: 0.0081 - val_mean_squared_error: 0.0081
Epoch 5/5

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m4s[0m 31ms/step - loss: 0.0137 - mean_squared_error: 0.0137
[1m 19/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0170 - mean_squared_error: 0.0170 
[1m 39/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0156 - mean_squared_error: 0.0156
[1m 59/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0152 - mean_squared_error: 0.0152
[1m 78/142[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0153 - mean_squared_error: 0.0153
[1m 98/142[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 3ms/step - loss: 0.0154 - mean_squared_error: 0.0154
[1m118/142[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 3ms/step - loss: 0.0157 - mean_squared_error: 0.0157
[1m137/142[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 3ms/step - loss: 0.0159 - mean_squared_error: 0.0159
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 4ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0076 - val_mean_squared_error: 0.0076
(12000, 1, 5)
Model: "sequential_1199"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 12000, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 3000, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 3000, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 750, 6)         │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
┌─────────────────────┬───────────────────┬────────────┬───────────────────┐
│ Layer (type)        │ Output Shape      │    Param # │ Connected to      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputGSR            │ (None, 12000, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputPPG            │ (None, 12000, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1198     │ (None, 750, 6)    │        411 │ inputGSR[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1199     │ (None, 750, 6)    │        411 │ inputPPG[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_599     │ (None, 750, 12)   │          0 │ sequential_1198[… │
│ (Concatenate)       │                   │            │ sequential_1199[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ permute_599         │ (None, 12, 750)   │          0 │ concatenate_599[… │
│ (Permute)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_599         │ (None, 9000)      │          0 │ permute_599[0][0] │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_599         │ (None, 9000)      │          0 │ flatten_599[0][0] │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_599 (Dense)   │ (None, 1)         │      9,001 │ dropout_599[0][0] │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 9,823 (38.37 KB)
 Trainable params: 9,823 (38.37 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m3:01[0m 1s/step - binary_accuracy: 0.0000e+00 - loss: 1.0206
[1m 14/142[0m [32m━[0m[37m━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.4437 - loss: 0.8236     
[1m 35/142[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6296 - loss: 0.6570
[1m 55/142[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6750 - loss: 0.6171
[1m 76/142[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6880 - loss: 0.6175
[1m 98/142[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6886 - loss: 0.6210
[1m117/142[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6898 - loss: 0.6225
[1m138/142[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6899 - loss: 0.6241
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 5ms/step - binary_accuracy: 0.6892 - loss: 0.6250 - val_binary_accuracy: 0.7500 - val_loss: 0.6087
Epoch 2/10

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 14ms/step - binary_accuracy: 1.0000 - loss: 0.5205
[1m 16/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5840 - loss: 0.7075 
[1m 37/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6764 - loss: 0.6352
[1m 58/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6673 - loss: 0.6394
[1m 80/142[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6527 - loss: 0.6433
[1m102/142[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6537 - loss: 0.6371
[1m123/142[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6562 - loss: 0.6329
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6577 - loss: 0.6293 - val_binary_accuracy: 0.7500 - val_loss: 0.5879
Epoch 3/10

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m4s[0m 32ms/step - binary_accuracy: 0.0000e+00 - loss: 1.1629
[1m 19/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6999 - loss: 0.5800     
[1m 36/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7184 - loss: 0.5739
[1m 55/142[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7282 - loss: 0.5704
[1m 76/142[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7226 - loss: 0.5817
[1m 97/142[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7234 - loss: 0.5866
[1m118/142[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7224 - loss: 0.5902
[1m141/142[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7208 - loss: 0.5924
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 3ms/step - binary_accuracy: 0.7207 - loss: 0.5925 - val_binary_accuracy: 0.7500 - val_loss: 0.5895
Epoch 4/10

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m2s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.4653
[1m 17/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.9143 - loss: 0.3592 
[1m 38/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8857 - loss: 0.3718
[1m 59/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8448 - loss: 0.4173
[1m 77/142[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8200 - loss: 0.4455
[1m101/142[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8043 - loss: 0.4680
[1m121/142[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7944 - loss: 0.4823
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7857 - loss: 0.4950 - val_binary_accuracy: 0.7500 - val_loss: 0.6053
Epoch 5/10

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m4s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.3172
[1m 20/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6605 - loss: 0.5428 
[1m 42/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6231 - loss: 0.5690
[1m 63/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6111 - loss: 0.5795
[1m 84/142[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6184 - loss: 0.5802
[1m104/142[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6314 - loss: 0.5729
[1m124/142[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6408 - loss: 0.5698
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 3ms/step - binary_accuracy: 0.6497 - loss: 0.5662 - val_binary_accuracy: 0.7500 - val_loss: 0.6170
Epoch 6/10

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m2s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2585
[1m 18/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.9041 - loss: 0.3049 
[1m 38/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8909 - loss: 0.3212
[1m 58/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8693 - loss: 0.3426
[1m 80/142[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8515 - loss: 0.3618
[1m102/142[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8307 - loss: 0.3880
[1m123/142[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8106 - loss: 0.4088
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7948 - loss: 0.4256 - val_binary_accuracy: 0.6875 - val_loss: 0.6680
Epoch 7/10

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m4s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.2171
[1m 22/142[0m [32m━━━[0m[37m━━━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6817 - loss: 0.5872 
[1m 44/142[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.6923 - loss: 0.5798
[1m 66/142[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7054 - loss: 0.5670
[1m 87/142[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7169 - loss: 0.5533
[1m110/142[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7296 - loss: 0.5403
[1m132/142[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7357 - loss: 0.5369
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7382 - loss: 0.5355 - val_binary_accuracy: 0.8125 - val_loss: 0.6471
Epoch 8/10

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m4s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.9180
[1m 19/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7727 - loss: 0.4292     
[1m 41/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7989 - loss: 0.4221
[1m 63/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7793 - loss: 0.4457
[1m 86/142[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7668 - loss: 0.4598
[1m110/142[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7610 - loss: 0.4663
[1m133/142[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7582 - loss: 0.4710
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7571 - loss: 0.4737 - val_binary_accuracy: 0.7500 - val_loss: 0.6506
Epoch 9/10

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m4s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1587
[1m 19/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7431 - loss: 0.4703 
[1m 36/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7225 - loss: 0.5048
[1m 57/142[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7078 - loss: 0.5231
[1m 78/142[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7064 - loss: 0.5282
[1m100/142[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7068 - loss: 0.5288
[1m121/142[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7130 - loss: 0.5228
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7196 - loss: 0.5179 - val_binary_accuracy: 0.7500 - val_loss: 0.6647
Epoch 10/10

[1m  1/142[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m3s[0m 27ms/step - binary_accuracy: 1.0000 - loss: 0.2170
[1m 21/142[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8260 - loss: 0.3969 
[1m 42/142[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8294 - loss: 0.4039
[1m 64/142[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8302 - loss: 0.4123
[1m 86/142[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8285 - loss: 0.4187
[1m108/142[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8287 - loss: 0.4228
[1m129/142[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8260 - loss: 0.4276
[1m142/142[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8227 - loss: 0.4331 - val_binary_accuracy: 0.8125 - val_loss: 0.6830

[1m1/2[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 110ms/step
[1m2/2[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 94ms/step 
[1m2/2[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 94ms/step
predicted [0.23862655 0.83659506 0.45444867 0.72690046 0.8037519  0.5516522
 0.8152802  0.11855625 0.6521933  0.6607573  0.68942624 0.1389157
 0.74119455 0.9298421  0.83510447 0.6701287  0.5720483  0.2862661
 0.51346195 0.8570857  0.36945054 0.22403477 0.8507263  0.41877276
 0.07690148 0.67977697 0.74332666 0.50399756 0.6456686  0.569176
 0.7787133  0.84750044 0.8026172  0.7083442  0.44560653 0.8054333
 0.6458045  0.7148726  0.8380679  0.52998805]
predicted [0 1 0 1 1 1 1 0 1 1 1 0 1 1 1 1 1 0 1 1 0 0 1 0 0 1 1 1 1 1 1 1 1 1 0 1 1
 1 1 1]
expected [False  True  True  True  True  True False  True False False  True False
 False  True  True False  True False  True  True  True False False  True
 False  True  True False  True  True  True  True False  True  True  True
  True  True False False]
accuracy: 0.625
confusion matrix: 
[[ 5 10]
 [ 5 20]]
              precision    recall  f1-score   support

       False       0.50      0.33      0.40        15
        True       0.67      0.80      0.73        25

    accuracy                           0.62        40
   macro avg       0.58      0.57      0.56        40
weighted avg       0.60      0.62      0.60        40

macro avg f1-score: 0.5636363636363637
macro avg (UAR): 0.5666666666666667
Sensitivity:  0.3333333333333333
Specificity:  0.8
g-mean:  0.5163977794943222
-------- Model Performance ----------: 
accuracy:  [0.65  0.7   0.575 0.8   0.825 0.7   0.75  0.55  0.7   0.625]
gmean:  [0.59553897 0.50596443 0.48989795 0.71554175 0.80332642 0.67527772
 0.68507907 0.5585696  0.65319726 0.51639778]
f1_score:  [0.61538462 0.6        0.52480783 0.76190476 0.81068289 0.68
 0.71509972 0.54545455 0.67032967 0.56363636]
UAR:  [0.61333333 0.61333333 0.52666667 0.74666667 0.80666667 0.68
 0.70666667 0.56       0.66666667 0.56666667]
Cohen Kappa score:  [0.23287671 0.26153846 0.05555556 0.53623188 0.62162162 0.36
 0.43661972 0.11111111 0.34246575 0.14285714]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  12.0
step (sec):  6.0
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  6.0
Number of windows / instances:  198
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.773 0.894 0.89  0.891 0.886 0.564 0.783 0.798 0.777 0.781]
 [0.654 0.827 0.838 0.849 0.824 0.522 0.742 0.768 0.735 0.74 ]
 [0.683 0.869 0.879 0.859 0.869 0.58  0.771 0.798 0.839 0.773]
 [0.7   0.842 0.863 0.836 0.849 0.299 0.634 0.717 0.541 0.626]
 [0.619 0.804 0.823 0.79  0.8   0.368 0.681 0.748 0.613 0.68 ]
 [0.31  0.649 0.688 0.62  0.649 0.094 0.54  0.64  0.413 0.52 ]
 [0.    0.5   0.626 0.    0.385 0.    0.5   0.641 0.    0.391]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[0.61406403 0.80691919 0.80736842 0.80032262 0.80430713 0.4626965
  0.71785714 0.81289474 0.68336253 0.72862313]
 [0.51715334 0.76217172 0.75789474 0.75632045 0.75579051 0.64110728
  0.81643773 0.83368421 0.80632535 0.81857786]
 [0.47003233 0.73131313 0.74763158 0.71574747 0.7309862  0.47622712
  0.73939394 0.73684211 0.73354157 0.73487469]
 [0.40769936 0.70323232 0.70736842 0.69078486 0.69960487 0.39580682
  0.69119048 0.77289474 0.64695074 0.69342418]
 [0.5902365  0.78065476 0.81842105 0.75488052 0.78924445 0.60246277
  0.80025253 0.80289474 0.79194331 0.79854466]
 [0.48651703 0.74409091 0.74368421 0.73946368 0.74168774 0.61647126
  0.80888889 0.80842105 0.80224109 0.8059093 ]
 [0.34558718 0.6657967  0.70263158 0.63538142 0.66586474 0.42258051
  0.71141414 0.71210526 0.70614986 0.70901647]
 [0.87513517 0.92738095 0.94921053 0.92278437 0.93712757 0.67219879
  0.84196429 0.83815789 0.83709598 0.83354668]
 [0.33928411 0.65164835 0.74131579 0.59399193 0.66140171 0.22630901
  0.61007326 0.68710526 0.56085713 0.60820931]
 [0.36555125 0.68343434 0.68631579 0.66347506 0.67519393 0.30517986
  0.65166667 0.65631579 0.640851   0.64822835]
 [0.09616692 0.54892399 0.58       0.48912251 0.53720217 0.44136222
  0.71843434 0.73210526 0.70792344 0.71791968]
 [0.64605298 0.81641484 0.83736842 0.8065311  0.82110351 0.57903757
  0.78189103 0.80894737 0.76588422 0.78654795]
 [0.47288463 0.73503788 0.74236842 0.72522162 0.73348619 0.32099674
  0.65833333 0.67157895 0.64252892 0.65600734]
 [0.24966766 0.61456044 0.69657895 0.55429813 0.61785797 0.40040995
  0.69984848 0.70184211 0.68755691 0.69516743]
 [0.27434416 0.63452381 0.69210526 0.59638767 0.62918804 0.30340465
  0.64466575 0.68289474 0.62426392 0.64861908]
 [0.54263045 0.77037338 0.78815789 0.74991605 0.76572517 0.50241136
  0.74130037 0.79289474 0.71665119 0.7477546 ]
 [0.49848784 0.74861111 0.75736842 0.74299084 0.74778888 0.40150545
  0.69299242 0.72684211 0.65423735 0.69011815]
 [0.13567906 0.58137255 0.82368421 0.28327416 0.55588188 0.5051902
  0.74949495 0.76210526 0.73974458 0.74997426]
 [0.54777229 0.76895604 0.80842105 0.75076497 0.77036618 0.45104227
  0.72550505 0.72763158 0.7219476  0.723714  ]
 [0.31135683 0.65173993 0.71710526 0.60349203 0.64797307 0.32317343
  0.65444139 0.69657895 0.63191103 0.65862477]
 [0.16881033 0.57833333 0.70631579 0.46136903 0.57798012 0.1760885
  0.58808081 0.59105263 0.57045239 0.58077247]
 [0.35255671 0.67666667 0.67631579 0.67131129 0.67401813 0.44396639
  0.71630037 0.74763158 0.70096917 0.71719491]
 [0.21430388 0.6084596  0.62657895 0.58187143 0.60132339 0.30592298
  0.64238095 0.77315789 0.53678798 0.64357141]
 [0.21356887 0.60823232 0.60657895 0.59428553 0.59981108 0.58657707
  0.78964646 0.79868421 0.77948805 0.79036501]
 [0.52764745 0.76395202 0.76842105 0.759482   0.7626482  0.67433199
  0.83666667 0.83763158 0.83002261 0.83501728]
 [0.45446859 0.72469697 0.73184211 0.70171618 0.7180168  0.49881746
  0.75031566 0.75368421 0.73847637 0.74551749]
 [0.59040528 0.79203297 0.82842105 0.78110445 0.79373704 0.63654596
  0.81833333 0.81815789 0.81359472 0.81629788]
 [0.1144476  0.55359432 0.59131579 0.52752223 0.55310119 0.47293551
  0.73729396 0.74868421 0.72651393 0.73310261]
 [0.13770582 0.56444597 0.59973684 0.53718381 0.56343016 0.33543102
  0.66824495 0.67684211 0.65436827 0.66373111]
 [0.77323291 0.89356685 0.88973684 0.89142868 0.88571795 0.56360804
  0.7834478  0.79842105 0.77703443 0.78054001]]
KNN mean:
[0.41111502 0.70303791 0.73767544 0.6694142  0.70058553 0.45812662
 0.72622524 0.75028947 0.70765586 0.72531707]
---------------------------
---------------------------
DT performance:
[[ 0.57475169  0.78772727  0.78736842  0.79151863  0.78421715  0.41015027
   0.69345238  0.78263158  0.71598026  0.69511091]
 [ 0.43037276  0.72737374  0.72736842  0.7035618   0.72209598  0.60456595
   0.81884158  0.82263158  0.77045392  0.81479863]
 [ 0.49532032  0.72689394  0.72763158  0.72177485  0.72146087  0.40491891
   0.67353535  0.67052632  0.69574948  0.66638029]
 [ 0.25611351  0.65070707  0.65236842  0.6175338   0.64319766  0.19655487
   0.60928571  0.70157895  0.56822334  0.60558939]
 [ 0.48603604  0.76048535  0.76710526  0.74374939  0.75358806  0.51545782
   0.7759596   0.77815789  0.7632718   0.77474122]
 [ 0.50326759  0.71575758  0.71789474  0.69656003  0.71251107  0.57739699
   0.81909091  0.81921053  0.80327787  0.81730924]
 [ 0.38733021  0.67179487  0.69684211  0.64459576  0.66768905  0.39176234
   0.65606061  0.65605263  0.71650486  0.65173364]
 [ 0.74826873  0.88095238  0.90421053  0.87029619  0.88570485  0.62054318
   0.7952381   0.80236842  0.78519218  0.79129371]
 [ 0.45186712  0.75283883  0.79710526  0.7009039   0.75673399  0.39017444
   0.6728022   0.73131579  0.6205188   0.67103311]
 [ 0.47559669  0.69868687  0.69631579  0.71311201  0.69358286  0.27965061
   0.66363636  0.67131579  0.65076453  0.65924385]
 [ 0.09757636  0.54036172  0.57052632  0.53935121  0.53386215  0.50017733
   0.7322601   0.73684211  0.72774576  0.72778684]
 [ 0.61852229  0.8205815   0.83289474  0.81069875  0.81584143  0.48235651
   0.75755495  0.76315789  0.75375498  0.75010792]
 [ 0.38883815  0.71155303  0.72210526  0.70920843  0.71125343  0.26512046
   0.65271465  0.65710526  0.61629545  0.64748171]
 [ 0.28382083  0.6470696   0.67131579  0.61825922  0.64059011  0.26990701
   0.595       0.59657895  0.6005787   0.59262486]
 [ 0.27824644  0.6470696   0.68184211  0.56058051  0.63255777  0.21770419
   0.60572344  0.63184211  0.55296686  0.59818823]
 [ 0.38920882  0.69329004  0.70710526  0.69769867  0.68734844  0.56182715
   0.79184982  0.82842105  0.7488262   0.79984737]
 [ 0.60423482  0.83169192  0.83289474  0.83493303  0.82776084  0.55276461
   0.76231061  0.77631579  0.76229762  0.76577162]
 [ 0.45763131  0.75765931  0.84315789  0.59001903  0.70618658  0.43317757
   0.74867424  0.76184211  0.71772224  0.74531879]
 [ 0.45229056  0.74368132  0.77842105  0.72687752  0.74164976  0.39810829
   0.69943182  0.70315789  0.6919313   0.69444422]
 [ 0.33420465  0.66648352  0.69710526  0.62485239  0.65350349  0.2843331
   0.64535256  0.66631579  0.61945457  0.64056568]
 [ 0.17227681  0.60416667  0.71157895  0.5199267   0.59525198 -0.04927485
   0.51560606  0.51868421  0.52045506  0.50798408]
 [ 0.31397004  0.66166667  0.66263158  0.65283671  0.65881278  0.21576842
   0.59876374  0.63236842  0.60742049  0.59011213]
 [ 0.28962336  0.63996212  0.64684211  0.60318823  0.63499799  0.18576546
   0.63547619  0.73263158  0.55606229  0.63269872]
 [ 0.50423117  0.72378788  0.72657895  0.72887976  0.71960936  0.61747453
   0.81483586  0.81763158  0.79925378  0.81339183]
 [ 0.51666385  0.70183081  0.70210526  0.72467794  0.6968624   0.65554513
   0.80222222  0.80263158  0.79818577  0.8002839 ]
 [ 0.3583103   0.68555556  0.68631579  0.68492202  0.68300751  0.35400158
   0.67253788  0.67263158  0.67226422  0.66173303]
 [ 0.42504889  0.68672161  0.74736842  0.68821588  0.68440284  0.61567024
   0.80767677  0.80789474  0.78573841  0.80505425]
 [ 0.14878415  0.56879579  0.59052632  0.55039696  0.56244977  0.47099794
   0.74471154  0.75236842  0.73464066  0.7370678 ]
 [ 0.27133859  0.63189103  0.63552632  0.6472197   0.61384905  0.25498516
   0.64791667  0.65078947  0.63483187  0.64466255]
 [ 0.65407193  0.82664835  0.83815789  0.84901356  0.82422515  0.52247346
   0.74173535  0.76789474  0.73477262  0.73979614]]
DT mean:
[0.4122606  0.7054562  0.72530702 0.68551209 0.69882681 0.40666862
 0.70500857 0.72376316 0.69083786 0.70140519]
---------------------------
---------------------------
RF performance:
[[ 0.75579286  0.84777778  0.84815789  0.83478481  0.84663497  0.4480245
   0.7572619   0.81736842  0.71890256  0.75352513]
 [ 0.60967451  0.83314394  0.83315789  0.76383321  0.83132019  0.5752692
   0.80755495  0.80842105  0.75132102  0.80005486]
 [ 0.46033333  0.73541667  0.75815789  0.74243075  0.73751957  0.38635627
   0.75939394  0.75763158  0.74756593  0.75410121]
 [ 0.44420373  0.73111111  0.73289474  0.67014512  0.7283698   0.23538937
   0.62809524  0.71394737  0.40264456  0.63140949]
 [ 0.55622486  0.72264194  0.74736842  0.76701369  0.72265024  0.48228203
   0.75560606  0.75815789  0.76815899  0.75201846]
 [ 0.5332528   0.7169697   0.71815789  0.77876256  0.71273854  0.6561881
   0.82318182  0.82289474  0.74699884  0.82166909]
 [ 0.40459017  0.69585623  0.72210526  0.63862875  0.69142412  0.31770093
   0.69449495  0.69578947  0.66107892  0.69305186]
 [ 0.83118573  0.90421245  0.92894737  0.90178089  0.91408678  0.57948432
   0.79315476  0.80263158  0.78059059  0.79196724]
 [ 0.39578493  0.74716117  0.78763158  0.69335688  0.74301751  0.1572557
   0.68150183  0.72684211  0.64986761  0.67794947]
 [ 0.36740432  0.75131313  0.75210526  0.70759669  0.75039146  0.43789656
   0.72171717  0.72736842  0.68961366  0.71784659]
 [ 0.15972326  0.57339744  0.60473684  0.55498539  0.55887453  0.43235311
   0.73838384  0.75263158  0.74678598  0.73500466]
 [ 0.66374727  0.84736722  0.85315789  0.81398793  0.84335806  0.51707383
   0.75128205  0.76315789  0.75317907  0.74939715]
 [ 0.44320277  0.73364899  0.73763158  0.7241415   0.72983347  0.2933128
   0.61830808  0.62684211  0.6743396   0.61319726]
 [ 0.26890708  0.61941392  0.68710526  0.55617976  0.62067329  0.28692236
   0.63590909  0.63578947  0.63982707  0.63240211]
 [ 0.41585749  0.59340659  0.66710526  0.57216068  0.58914621  0.22614681
   0.65064103  0.67815789  0.63461914  0.65041498]
 [ 0.3727878   0.67781385  0.69236842  0.69365211  0.67528448  0.54031533
   0.78388278  0.81368421  0.69642412  0.78655805]
 [ 0.72080939  0.84633838  0.85315789  0.86228117  0.8468398   0.5605174
   0.77348485  0.78684211  0.79460127  0.77490821]
 [ 0.21462904  0.68431373  0.85868421  0.54824514  0.66474778  0.59766642
   0.79116162  0.80289474  0.77653142  0.79015276]
 [ 0.534856    0.75888278  0.78789474  0.73949361  0.75230419  0.63062563
   0.81243687  0.81868421  0.8238753   0.81123488]
 [ 0.39455255  0.66602564  0.72684211  0.58470933  0.66267976  0.34444194
   0.59171245  0.64105263  0.65247269  0.58497503]
 [ 0.28286702  0.64083333  0.74763158  0.47448883  0.63800373 -0.00356051
   0.52484848  0.53026316  0.50921647  0.51535525]
 [ 0.42659572  0.69777778  0.69763158  0.70566854  0.69519011  0.38321502
   0.68653846  0.72736842  0.62780583  0.6883361 ]
 [ 0.19014684  0.60808081  0.62263158  0.63575465  0.60491242  0.25087056
   0.62547619  0.75736842  0.61124688  0.63767619]
 [ 0.35002908  0.69050505  0.69236842  0.67173005  0.68681158  0.48488951
   0.77518939  0.77815789  0.76600925  0.77284305]
 [ 0.47299935  0.76130051  0.76815789  0.75510739  0.75977054  0.65459596
   0.84722222  0.84815789  0.85669824  0.84581096]
 [ 0.39309068  0.67994949  0.68631579  0.59858559  0.67664079  0.4969325
   0.7275      0.73157895  0.75195921  0.72238725]
 [ 0.64406522  0.74587912  0.80289474  0.75912466  0.75218751  0.6659342
   0.76479798  0.76342105  0.78078132  0.75739994]
 [ 0.18069035  0.58585165  0.64105263  0.54308397  0.5803852   0.44044078
   0.73759158  0.75315789  0.77161677  0.73659655]
 [ 0.33735608  0.70492216  0.73263158  0.58520104  0.69824333  0.37665519
   0.6582702   0.66631579  0.6980419   0.65389525]
 [ 0.68296698  0.86852106  0.87947368  0.85885662  0.86889081  0.580259
   0.77083333  0.79789474  0.8392744   0.77304388]]
RF mean:
[0.45027757 0.72232779 0.75227193 0.69119238 0.71943103 0.43451516
 0.72291444 0.74348246 0.71073495 0.72083943]
---------------------------
---------------------------
SVM performance:
[[ 0.34662633  0.66762626  0.68684211  0.61742205  0.65415933  0.
   0.5         0.75263158  0.          0.42939903]
 [ 0.38746506  0.68988636  0.70263158  0.66592367  0.68529194  0.40999086
   0.68930861  0.74684211  0.63382486  0.69101125]
 [ 0.52022044  0.74652778  0.78368421  0.69766792  0.74535213  0.40359492
   0.70191919  0.70210526  0.69181936  0.69744787]
 [ 0.40091839  0.69772727  0.70736842  0.66559723  0.68821672  0.
   0.5         0.71236842  0.          0.41594092]
 [ 0.51676927  0.74047619  0.79289474  0.69199132  0.74572598  0.51877775
   0.75848485  0.76289474  0.74142004  0.7535901 ]
 [ 0.34450584  0.67005051  0.67736842  0.64960507  0.66457045  0.56394728
   0.78388889  0.78236842  0.77558879  0.77898155]
 [ 0.          0.5         0.61131579  0.          0.37931329  0.3700551
   0.68459596  0.68631579  0.6640383   0.67571749]
 [ 0.7755929   0.87591575  0.90842105  0.86740819  0.8867175   0.44672483
   0.71547619  0.74684211  0.68685334  0.71569731]
 [ 0.          0.5         0.69184211  0.          0.40887923  0.
   0.5         0.68684211  0.          0.4070967 ]
 [ 0.35136173  0.67590909  0.67605263  0.65923008  0.66754554  0.23951471
   0.61474747  0.63605263  0.53162435  0.58577394]
 [ 0.          0.5         0.61631579  0.          0.38120723  0.24320129
   0.61073232  0.67736842  0.43279609  0.5628194 ]
 [ 0.4174599   0.69255952  0.75236842  0.61710068  0.68534096  0.10544731
   0.54555861  0.64131579  0.25045105  0.48229135]
 [ 0.29299567  0.6385101   0.67157895  0.58513471  0.62950524  0.
   0.5         0.56578947  0.          0.36123656]
 [ 0.          0.5         0.67184211  0.          0.40174911  0.3407199
   0.67        0.67105263  0.66495625  0.66813567]
 [ 0.          0.5         0.68184211  0.          0.40531417  0.
   0.5         0.63131579  0.          0.38688905]
 [ 0.          0.5         0.60105263  0.          0.37537634 -0.00958904
   0.49615385  0.66184211  0.          0.39807264]
 [ 0.08855863  0.53888889  0.61631579  0.22475469  0.45036028  0.
   0.5         0.59578947  0.          0.37333333]
 [ 0.          0.5         0.85368421  0.          0.46050408  0.32928193
   0.65164141  0.70105263  0.56832315  0.63657629]
 [ 0.          0.5         0.68184211  0.          0.40531417  0.
   0.5         0.55578947  0.          0.3572043 ]
 [ 0.          0.5         0.67684211  0.          0.40353164  0.05342466
   0.52142857  0.64631579  0.11338934  0.42808223]
 [ 0.          0.5         0.75789474  0.          0.43109244  0.
   0.5         0.51526316  0.          0.33993326]
 [ 0.32561105  0.66333333  0.66105263  0.63896188  0.650986    0.
   0.5         0.65157895  0.          0.39448619]
 [ 0.          0.5         0.58078947  0.          0.36728495  0.
   0.5         0.74736842  0.          0.42770563]
 [ 0.          0.5         0.53026316  0.          0.34638487  0.44371466
   0.71167929  0.74236842  0.64673572  0.69749307]
 [ 0.12973548  0.55902778  0.61184211  0.25956216  0.46770426  0.44325506
   0.72111111  0.72236842  0.69710672  0.71079909]
 [ 0.04769431  0.52267677  0.56052632  0.1432361   0.40023769  0.40688826
   0.70099747  0.71236842  0.68275178  0.69806719]
 [ 0.25586515  0.60595238  0.75289474  0.41796791  0.59180556  0.61456271
   0.80712121  0.80763158  0.80115801  0.80551603]
 [ 0.          0.5         0.64131579  0.          0.39067693  0.27159858
   0.61994048  0.69710526  0.47578113  0.59471027]
 [ 0.          0.5         0.62131579  0.          0.38310117  0.05910855
   0.52664141  0.58131579  0.21785113  0.43154883]
 [ 0.70007195  0.84235348  0.86342105  0.8356489   0.84913622  0.29869436
   0.6338141   0.71736842  0.54128932  0.62585333]]
SVM mean:
[0.19671507 0.59424738 0.68811404 0.30790709 0.53341285 0.21843046
 0.60550803 0.68192105 0.36059196 0.551047  ]
---------------------------
---------------------------
GBM performance:
[[ 0.58881583  0.79666667  0.80315789  0.78793644  0.79736049  0.26501349
   0.60583333  0.79815789  0.38858498  0.60109139]
 [ 0.45404232  0.73305556  0.74736842  0.70340094  0.73006049  0.56702474
   0.76733059  0.80815789  0.74314328  0.77836936]
 [ 0.47584971  0.72455808  0.76263158  0.67815742  0.72382951  0.39278196
   0.69661616  0.69657895  0.68493584  0.69128018]
 [ 0.33404452  0.67141414  0.67684211  0.6366944   0.66241294  0.07408217
   0.52809524  0.72263158  0.16719585  0.4770706 ]
 [ 0.5400698   0.75297619  0.80315789  0.71053     0.75858268  0.55156164
   0.77085859  0.77289474  0.76051434  0.76805103]
 [ 0.43733942  0.715       0.71315789  0.70553722  0.70961574  0.58252701
   0.78393939  0.78868421  0.77644405  0.78163838]
 [ 0.35749878  0.66609432  0.72236842  0.60772727  0.66470807  0.37277122
   0.68681818  0.68631579  0.66998644  0.68141054]
 [ 0.83498992  0.90238095  0.93394737  0.88675119  0.91684267  0.55154496
   0.76934524  0.79289474  0.74515636  0.76880002]
 [ 0.36789605  0.6532967   0.78263158  0.52009182  0.65695164  0.26954361
   0.62481685  0.72684211  0.44121728  0.59411623]
 [ 0.41019129  0.70555556  0.70631579  0.68551122  0.69631615  0.39938426
   0.69181818  0.70263158  0.66877887  0.68674281]
 [ 0.14402461  0.56105769  0.64131579  0.39801394  0.5220792   0.53733288
   0.75643939  0.78763158  0.72848418  0.76119271]
 [ 0.64008135  0.81520147  0.84263158  0.79592471  0.82095113  0.43617645
   0.71536172  0.74263158  0.68115161  0.71178582]
 [ 0.53326818  0.76092172  0.76815789  0.75426553  0.76009523  0.19377749
   0.59829545  0.62736842  0.50966898  0.57837861]
 [ 0.1892608   0.58250916  0.71710526  0.37892888  0.55952014  0.28590726
   0.6419697   0.64631579  0.62287251  0.63482773]
 [ 0.01189856  0.50448718  0.68210526  0.04082483  0.41901643  0.15881136
   0.56856685  0.66184211  0.40540245  0.53757119]
 [ 0.23159682  0.59642857  0.67236842  0.43704237  0.55577189  0.36925399
   0.65989011  0.76289474  0.56767372  0.66260077]
 [ 0.7462497   0.87354798  0.88368421  0.86869473  0.87730795  0.43444804
   0.7030303   0.74657895  0.65473027  0.70490435]
 [ 0.04594595  0.51666667  0.85868421  0.05773503  0.48678035  0.51929324
   0.74109848  0.76210526  0.73018188  0.74475112]
 [ 0.26600335  0.60695971  0.73210526  0.46490778  0.59652994  0.49072546
   0.73762626  0.75842105  0.69791925  0.73114803]
 [ 0.25051357  0.60934066  0.72236842  0.48034688  0.60238993  0.21481068
   0.58555403  0.67684211  0.45354837  0.55312312]
 [-0.00909091  0.5         0.75789474  0.          0.43109244 -0.03150548
   0.49555556  0.49421053  0.47389089  0.4899648 ]
 [ 0.23321214  0.61166667  0.61184211  0.60933589  0.60825982  0.2798952
   0.62614469  0.71236842  0.53679973  0.61942107]
 [ 0.36716718  0.67234848  0.70289474  0.63692354  0.66308405  0.04180333
   0.52642857  0.74763158  0.12944926  0.48012907]
 [ 0.32227826  0.65358586  0.66210526  0.64492754  0.64601259  0.63738682
   0.81041667  0.81815789  0.8113171   0.81250706]
 [ 0.4549557   0.71464646  0.73815789  0.65798395  0.7105515   0.61508115
   0.80722222  0.80789474  0.80283072  0.80615868]
 [ 0.32089129  0.67065657  0.68657895  0.61619876  0.65767143  0.4191099
   0.70794192  0.71763158  0.69569678  0.70295364]
 [ 0.42628762  0.7032967   0.80368421  0.61989686  0.70884964  0.54429711
   0.77757576  0.77710526  0.77219983  0.7752052 ]
 [ 0.10041762  0.54274267  0.65184211  0.30170005  0.49384375  0.47477419
   0.72113095  0.77289474  0.66911754  0.71985743]
 [ 0.35058287  0.65570055  0.72157895  0.57339773  0.64297685  0.36282109
   0.68106061  0.70131579  0.64838393  0.67964362]
 [ 0.61889601  0.80370879  0.82342105  0.79031039  0.80038571  0.3675957
   0.68054029  0.74763158  0.61250701  0.67955961]]
GBM mean:
[0.36817261 0.67588239 0.74440351 0.56832324 0.66266168 0.3792677
 0.68224404 0.73217544 0.60832611 0.67380847]
---------------------------
---------------------------
BDDAE performance:
[[-0.11385692  0.44318182  0.4475      0.42810297  0.43707358  0.00500583
   0.50166667  0.7275      0.13515944  0.45740687]
 [ 0.27498107  0.63787879  0.64        0.63257918  0.63502866  0.47547326
   0.734       0.7575      0.72631331  0.73687395]
 [-0.02175825  0.48925831  0.515       0.44201868  0.4769407   0.08259552
   0.54110276  0.545       0.52784337  0.53620016]
 [ 0.00742973  0.50388471  0.5075      0.48508617  0.49645106  0.13553686
   0.55714286  0.71        0.3505105   0.53436464]
 [ 0.3983232   0.696875    0.71        0.68005595  0.69220195  0.35809686
   0.68032581  0.6775      0.66799224  0.67279298]
 [ 0.58763272  0.79298246  0.795       0.78420357  0.7907889   0.07541237
   0.53784461  0.54        0.51231294  0.52598773]
 [-0.08736331  0.45833333  0.505       0.3296683   0.42509522  0.17153787
   0.58609023  0.5875      0.57941374  0.58322526]
 [ 0.44294206  0.7014245   0.78        0.65724172  0.71386918 -0.0576532
   0.47291667  0.535       0.3203127   0.4356641 ]
 [ 0.4886774   0.72261905  0.805       0.68574163  0.73869623  0.45104602
   0.69643875  0.7975      0.61860841  0.70736388]
 [ 0.05304146  0.52656642  0.53        0.51528806  0.52241172  0.20556867
   0.60151515  0.615       0.57461092  0.59431878]
 [-0.00718757  0.49533333  0.5475      0.43335549  0.48445073  0.17657696
   0.58324808  0.6125      0.52303727  0.56679506]
 [ 0.13078679  0.56145833  0.6         0.52189731  0.55677037  0.35518158
   0.68333333  0.6925      0.67238123  0.67386922]
 [ 0.1030615   0.55294118  0.56        0.54230585  0.54824649  0.24919069
   0.62455243  0.63        0.62047197  0.62261068]
 [ 0.15987826  0.57250712  0.6625      0.4970951   0.56650715  0.36670037
   0.68433584  0.6825      0.67800513  0.68018361]
 [ 0.06142121  0.52578348  0.6425      0.3984678   0.51034671  0.15549004
   0.57466667  0.625       0.52314732  0.56735033]
 [-0.03099916  0.48541667  0.5375      0.38502055  0.46631248  0.18079835
   0.57150997  0.715       0.39148207  0.54428592]
 [ 0.61289388  0.81508951  0.805       0.81094882  0.80404261  0.34406901
   0.66770833  0.7         0.61475667  0.6559646 ]
 [ 0.25132995  0.61666667  0.85        0.41662053  0.61314565  0.02632502
   0.51189258  0.5525      0.42950701  0.48946972]
 [ 0.24979848  0.62136752  0.68        0.58873007  0.62008123  0.35794975
   0.6790404   0.6825      0.66663183  0.6737792 ]
 [ 0.1159559   0.5534188   0.6475      0.46840135  0.54702376  0.03961142
   0.518       0.6025      0.35843939  0.48465028]
 [-0.01532919  0.495       0.7125      0.11934343  0.44897167  0.11940354
   0.55927318  0.5625      0.55202974  0.55717796]
 [ 0.025       0.5125      0.5125      0.5020939   0.50735626 -0.05678713
   0.47472527  0.57        0.29738059  0.44427734]
 [ 0.0044924   0.50204604  0.52        0.47307394  0.49457368  0.39977456
   0.68        0.805       0.61853968  0.69301068]
 [ 0.21726403  0.60914787  0.6075      0.60254491  0.6047342   0.29057045
   0.64053708  0.6625      0.62049365  0.64105388]
 [ 0.00603382  0.5030303   0.51        0.49209361  0.49986275  0.11
   0.555       0.555       0.54658808  0.55084023]
 [ 0.19813948  0.59621212  0.6075      0.57765332  0.5926409   0.14194138
   0.56919192  0.585       0.53963078  0.56168773]
 [ 0.53330215  0.76130952  0.8125      0.74268865  0.76497967  0.53
   0.765       0.765       0.75275535  0.76054316]
 [-0.00772232  0.49642857  0.5425      0.46137413  0.49189548  0.48417802
   0.734375    0.76        0.72030435  0.73969385]
 [ 0.11353963  0.55266667  0.6075      0.49555136  0.54550455  0.39443465
   0.69318182  0.7075      0.67351305  0.69129466]
 [ 0.3100878   0.64866667  0.6875      0.6198791   0.64873004  0.09434105
   0.54010989  0.64        0.41250938  0.51951996]]
BDDAE mean:
[0.16872654 0.58166649 0.62958333 0.52630418 0.57482445 0.22207899
 0.60729084 0.65333333 0.54082274 0.59674188]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.54026316 0.         0.35068595 0.
  0.5        0.75263158 0.         0.42939903]
 [0.         0.5        0.55052632 0.         0.35502039 0.
  0.5        0.61631579 0.         0.38120723]
 [0.         0.5        0.58078947 0.         0.36728495 0.
  0.5        0.51526316 0.         0.33993326]
 [0.         0.5        0.52526316 0.         0.34423433 0.
  0.5        0.71236842 0.         0.41594092]
 [0.         0.5        0.61131579 0.         0.37931329 0.
  0.5        0.51526316 0.         0.33993326]
 [0.         0.5        0.52026316 0.         0.3420838  0.
  0.5        0.52526316 0.         0.34423433]
 [0.         0.5        0.61131579 0.         0.37931329 0.
  0.5        0.51526316 0.         0.33993326]
 [0.         0.5        0.68684211 0.         0.4070967  0.
  0.5        0.60631579 0.         0.37741935]
 [0.         0.5        0.69184211 0.         0.40887923 0.
  0.5        0.68684211 0.         0.4070967 ]
 [0.         0.5        0.52026316 0.         0.3420838  0.
  0.5        0.54026316 0.         0.35068595]
 [0.         0.5        0.61631579 0.         0.38120723 0.
  0.5        0.58578947 0.         0.36930108]
 [0.         0.5        0.61131579 0.         0.37931329 0.
  0.5        0.61631579 0.         0.38120723]
 [0.         0.5        0.56578947 0.         0.36123656 0.
  0.5        0.56578947 0.         0.36123656]
 [0.         0.5        0.67184211 0.         0.40174911 0.
  0.5        0.52026316 0.         0.3420838 ]
 [0.         0.5        0.68184211 0.         0.40531417 0.
  0.5        0.63131579 0.         0.38688905]
 [0.         0.5        0.60105263 0.         0.37537634 0.
  0.5        0.66684211 0.         0.39996658]
 [0.         0.5        0.58578947 0.         0.36930108 0.
  0.5        0.59578947 0.         0.37333333]
 [0.         0.5        0.85368421 0.         0.46050408 0.
  0.5        0.58578947 0.         0.36930108]
 [0.         0.5        0.68184211 0.         0.40531417 0.
  0.5        0.55578947 0.         0.3572043 ]
 [0.         0.5        0.67684211 0.         0.40353164 0.
  0.5        0.63131579 0.         0.38688905]
 [0.         0.5        0.75789474 0.         0.43109244 0.
  0.5        0.51526316 0.         0.33993326]
 [0.         0.5        0.50526316 0.         0.33563218 0.
  0.5        0.65157895 0.         0.39448619]
 [0.         0.5        0.58078947 0.         0.36728495 0.
  0.5        0.74736842 0.         0.42770563]
 [0.         0.5        0.53026316 0.         0.34638487 0.
  0.5        0.56578947 0.         0.36123656]
 [0.         0.5        0.56078947 0.         0.35922043 0.
  0.5        0.50526316 0.         0.33563218]
 [0.         0.5        0.54026316 0.         0.35068595 0.
  0.5        0.55052632 0.         0.35502039]
 [0.         0.5        0.69684211 0.         0.41066176 0.
  0.5        0.51026316 0.         0.33778272]
 [0.         0.5        0.64131579 0.         0.39067693 0.
  0.5        0.61131579 0.         0.37931329]
 [0.         0.5        0.62131579 0.         0.38310117 0.
  0.5        0.56078947 0.         0.35922043]
 [0.         0.5        0.62631579 0.         0.38499511 0.
  0.5        0.64131579 0.         0.39067693]]
DUMMY mean:
[0.         0.5        0.61486842 0.         0.37928597 0.
 0.5        0.59334211 0.         0.3711401 ]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_34
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.411 0.703 0.738 0.669 0.701 0.458 0.726 0.75  0.708 0.725]
 [0.412 0.705 0.725 0.686 0.699 0.407 0.705 0.724 0.691 0.701]
 [0.45  0.722 0.752 0.691 0.719 0.435 0.723 0.743 0.711 0.721]
 [0.197 0.594 0.688 0.308 0.533 0.218 0.606 0.682 0.361 0.551]
 [0.368 0.676 0.744 0.568 0.663 0.379 0.682 0.732 0.608 0.674]
 [0.169 0.582 0.63  0.526 0.575 0.222 0.607 0.653 0.541 0.597]
 [0.    0.5   0.615 0.    0.379 0.    0.5   0.593 0.    0.371]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.194 0.097 0.086 0.132 0.099 0.132 0.067 0.061 0.079 0.067]
 [0.149 0.076 0.076 0.089 0.078 0.166 0.08  0.076 0.082 0.081]
 [0.172 0.085 0.078 0.108 0.088 0.161 0.076 0.07  0.094 0.077]
 [0.231 0.112 0.088 0.324 0.16  0.207 0.102 0.069 0.313 0.149]
 [0.203 0.102 0.074 0.226 0.12  0.172 0.085 0.066 0.17  0.096]
 [0.203 0.1   0.111 0.14  0.106 0.165 0.081 0.08  0.144 0.091]
 [0.    0.    0.078 0.    0.029 0.    0.    0.07  0.    0.027]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 47.  14.  12.  20.  14.  29.   9.   8.  11.   9.]
 [ 36.  11.  10.  13.  11.  41.  11.  11.  12.  12.]
 [ 38.  12.  10.  16.  12.  37.  11.   9.  13.  11.]
 [117.  19.  13. 105.  30.  95.  17.  10.  87.  27.]
 [ 55.  15.  10.  40.  18.  45.  12.   9.  28.  14.]
 [120.  17.  18.  27.  18.  74.  13.  12.  27.  15.]
 [  0.   0.  13.   0.   8.   0.   0.  12.   0.   7.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_34
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  12.0
step (sec):  6.0
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  6.0
Number of windows / instances:  198
Elapsed time: 678.0620612621308 minutes
Elapsed time: 11.301034354368845 hours
