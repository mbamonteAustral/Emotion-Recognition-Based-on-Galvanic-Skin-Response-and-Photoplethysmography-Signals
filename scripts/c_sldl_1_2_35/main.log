2024-05-05 11:04:20.155572: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-05 11:04:23.884985: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-05 11:04:33.129518: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  13.0
step (sec):  13.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_35\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.6712767  0.03513877 0.7034793  0.9057844  0.9360157  0.55930716
 0.29510504 0.38262853 0.7419983  0.5067913  0.28599992 0.04903287
 0.84237564 0.54161644 0.19544543 0.5625893  0.6357746  0.6646981
 0.4487534  0.38327938]
predicted [1 0 1 1 1 1 0 0 1 1 0 0 1 1 0 1 1 1 0 0]
expected [ True False False  True  True  True  True False False  True  True False
 False  True  True  True  True False False  True]
accuracy: 0.6
confusion matrix: 
[[4 4]
 [4 8]]
              precision    recall  f1-score   support

       False       0.50      0.50      0.50         8
        True       0.67      0.67      0.67        12

    accuracy                           0.60        20
   macro avg       0.58      0.58      0.58        20
weighted avg       0.60      0.60      0.60        20

macro avg f1-score: 0.5833333333333333
macro avg (UAR): 0.5833333333333333
Sensitivity:  0.5
Specificity:  0.6666666666666666
g-mean:  0.5773502691896257
-------- Model Performance ----------: 
accuracy:  [0.65 0.6  0.75 0.8  0.6  0.7  0.6  0.65 0.6  0.  ]
gmean:  [0.55901699 0.60380736 0.6770032  0.75691259 0.53033009 0.6846532
 0.45643546 0.55901699 0.57735027 0.        ]
f1_score:  [0.6011396  0.5959596  0.71509972 0.78021978 0.56043956 0.6875
 0.52380952 0.6011396  0.58333333 0.        ]
UAR:  [0.60416667 0.60416667 0.70833333 0.77083333 0.5625     0.6875
 0.54166667 0.60416667 0.58333333 0.        ]
Cohen Kappa score:  [0.22222222 0.2        0.44444444 0.56521739 0.13043478 0.375
 0.09090909 0.22222222 0.16666667 0.        ]
Split Repetition number:  9
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [72  6 27 22  9 76 93  4  0 66 42 69 70 23 44 32 81 33 49 12  5 28 39 45
 43 14 82 67 11 95 91 90 25 56 15 24 79 60 85 61 17 92 13 46  7 65 94 63
 89 64 20 59 75 84 87 47 78 48 77  3 26 10 80 62 38 71  8 16 53 68 37 88
 73 74  2 21] TEST: [54 19 83 58 35 57 29 18 30 52 41 34 86 50 36 31  1 51 40 55]
(DL) TRAIN number of instances:  76
(DL) TEST number of instances:  20
(DL) Total number of instances (TRAIN+TEST):  96
C:\Users\Javier\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\convolutional\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(
----- train_GSR_AE -------
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 12992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 812, 6)         â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2396              â”‚ (None, 3248, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2396           â”‚ (None, 3248, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2397              â”‚ (None, 12992, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2397           â”‚ (None, 12992, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:14[0m 1s/step - loss: 32.2797 - mean_squared_error: 32.2797
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 42.1563 - mean_squared_error: 42.1563
[1m13/68[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 37.3344 - mean_squared_error: 37.3344
[1m20/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 32.4668 - mean_squared_error: 32.4668
[1m26/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 29.3087 - mean_squared_error: 29.3087
[1m32/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 26.8316 - mean_squared_error: 26.8316
[1m39/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 24.6801 - mean_squared_error: 24.6801
[1m45/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 23.4693 - mean_squared_error: 23.4693
[1m52/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 22.2788 - mean_squared_error: 22.2788
[1m58/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 21.5703 - mean_squared_error: 21.5703
[1m65/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 20.8984 - mean_squared_error: 20.8984
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 13ms/step - loss: 20.5368 - mean_squared_error: 20.5368 - val_loss: 11.6111 - val_mean_squared_error: 11.6111
Epoch 2/5

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 36.8397 - mean_squared_error: 36.8397
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 16.3554 - mean_squared_error: 16.3554
[1m14/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.8296 - mean_squared_error: 13.8296
[1m20/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.9695 - mean_squared_error: 13.9695
[1m27/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 14.0618 - mean_squared_error: 14.0618
[1m33/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.9476 - mean_squared_error: 13.9476
[1m39/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.8310 - mean_squared_error: 13.8310
[1m45/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.7926 - mean_squared_error: 13.7926
[1m51/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.7728 - mean_squared_error: 13.7728
[1m57/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.7425 - mean_squared_error: 13.7425
[1m63/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 13.7019 - mean_squared_error: 13.7019
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 13.6202 - mean_squared_error: 13.6202 - val_loss: 11.5858 - val_mean_squared_error: 11.5858
Epoch 3/5

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - loss: 1.5080 - mean_squared_error: 1.5080
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 6.6475 - mean_squared_error: 6.6475
[1m13/68[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 8.1670 - mean_squared_error: 8.1670
[1m19/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 8.4078 - mean_squared_error: 8.4078
[1m26/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 9.0297 - mean_squared_error: 9.0297
[1m32/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 9.8759 - mean_squared_error: 9.8759
[1m39/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 10.5197 - mean_squared_error: 10.5197
[1m45/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 10.8417 - mean_squared_error: 10.8417
[1m50/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.0040 - mean_squared_error: 11.0040
[1m56/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.1183 - mean_squared_error: 11.1183
[1m61/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 11.1610 - mean_squared_error: 11.1610
[1m67/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 11.2200 - mean_squared_error: 11.2200
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 11.2616 - mean_squared_error: 11.2616 - val_loss: 11.5789 - val_mean_squared_error: 11.5789
Epoch 4/5

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 16.0514 - mean_squared_error: 16.0514
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 23.9491 - mean_squared_error: 23.9491
[1m12/68[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 20.8665 - mean_squared_error: 20.8665
[1m17/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 19.5247 - mean_squared_error: 19.5247
[1m22/68[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 18.6127 - mean_squared_error: 18.6127
[1m28/68[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 17.5111 - mean_squared_error: 17.5111
[1m33/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 16.6243 - mean_squared_error: 16.6243
[1m39/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 15.7711 - mean_squared_error: 15.7711
[1m44/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 15.2673 - mean_squared_error: 15.2673
[1m50/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 14.7460 - mean_squared_error: 14.7460
[1m56/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 14.4393 - mean_squared_error: 14.4393
[1m61/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 14.2133 - mean_squared_error: 14.2133
[1m67/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 13.9789 - mean_squared_error: 13.9789
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 13.9404 - mean_squared_error: 13.9404 - val_loss: 11.5756 - val_mean_squared_error: 11.5756
Epoch 5/5

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 30ms/step - loss: 40.4869 - mean_squared_error: 40.4869
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 31.4525 - mean_squared_error: 31.4525
[1m13/68[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 26.3741 - mean_squared_error: 26.3741
[1m19/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 23.6524 - mean_squared_error: 23.6525
[1m25/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 21.7477 - mean_squared_error: 21.7477
[1m30/68[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 20.6368 - mean_squared_error: 20.6368
[1m36/68[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 19.6578 - mean_squared_error: 19.6578
[1m42/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 18.9750 - mean_squared_error: 18.9750
[1m48/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 18.5061 - mean_squared_error: 18.5061
[1m53/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 18.1871 - mean_squared_error: 18.1871
[1m58/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 17.8380 - mean_squared_error: 17.8380
[1m64/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 17.4057 - mean_squared_error: 17.4057
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 17.0611 - mean_squared_error: 17.0611 - val_loss: 11.5737 - val_mean_squared_error: 11.5737
(76, 812, 6)
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 12992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 812, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 12992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 812, 6)         â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2398              â”‚ (None, 3248, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2398           â”‚ (None, 3248, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2399              â”‚ (None, 12992, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2399           â”‚ (None, 12992, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:14[0m 1s/step - loss: 0.0616 - mean_squared_error: 0.0616
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0682 - mean_squared_error: 0.0682
[1m13/68[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0628 - mean_squared_error: 0.0628
[1m19/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0590 - mean_squared_error: 0.0590
[1m26/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0549 - mean_squared_error: 0.0549
[1m32/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0520 - mean_squared_error: 0.0520
[1m38/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0494 - mean_squared_error: 0.0494
[1m44/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0472 - mean_squared_error: 0.0472
[1m51/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0457 - mean_squared_error: 0.0457
[1m57/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0445 - mean_squared_error: 0.0445
[1m64/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0434 - mean_squared_error: 0.0434
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 13ms/step - loss: 0.0427 - mean_squared_error: 0.0427 - val_loss: 0.0197 - val_mean_squared_error: 0.0197
Epoch 2/5

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - loss: 0.0274 - mean_squared_error: 0.0274
[1m 6/68[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0281 - mean_squared_error: 0.0281
[1m13/68[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0285 - mean_squared_error: 0.0285
[1m19/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0282 - mean_squared_error: 0.0282
[1m25/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0284 - mean_squared_error: 0.0284
[1m32/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0278 - mean_squared_error: 0.0278
[1m38/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0273 - mean_squared_error: 0.0273
[1m45/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0268 - mean_squared_error: 0.0268
[1m51/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0263 - mean_squared_error: 0.0263
[1m58/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0257 - mean_squared_error: 0.0257
[1m65/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 0.0251 - mean_squared_error: 0.0251
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 0.0248 - mean_squared_error: 0.0248 - val_loss: 0.0171 - val_mean_squared_error: 0.0171
Epoch 3/5

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 28ms/step - loss: 0.0088 - mean_squared_error: 0.0088
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0103 - mean_squared_error: 0.0103
[1m13/68[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0128 - mean_squared_error: 0.0128
[1m19/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0141 - mean_squared_error: 0.0141
[1m26/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0148 - mean_squared_error: 0.0148
[1m32/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0153 - mean_squared_error: 0.0153
[1m38/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0156 - mean_squared_error: 0.0156
[1m45/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0161 - mean_squared_error: 0.0161
[1m51/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0166 - mean_squared_error: 0.0166
[1m57/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0170 - mean_squared_error: 0.0170
[1m64/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0174 - mean_squared_error: 0.0174
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0175 - mean_squared_error: 0.0175 - val_loss: 0.0187 - val_mean_squared_error: 0.0187
Epoch 4/5

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.0234 - mean_squared_error: 0.0234
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0230 - mean_squared_error: 0.0230
[1m14/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0222 - mean_squared_error: 0.0222
[1m20/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0217 - mean_squared_error: 0.0217
[1m27/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0211 - mean_squared_error: 0.0211
[1m33/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0206 - mean_squared_error: 0.0206
[1m39/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0203 - mean_squared_error: 0.0203
[1m45/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0200 - mean_squared_error: 0.0200
[1m52/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0201 - mean_squared_error: 0.0201
[1m59/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0202 - mean_squared_error: 0.0202
[1m65/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 0.0201 - mean_squared_error: 0.0201
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0161 - val_mean_squared_error: 0.0161
Epoch 5/5

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.0048 - mean_squared_error: 0.0048
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0171 - mean_squared_error: 0.0171
[1m13/68[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0165 - mean_squared_error: 0.0165
[1m20/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0153 - mean_squared_error: 0.0153
[1m26/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0148 - mean_squared_error: 0.0148
[1m32/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0144 - mean_squared_error: 0.0144
[1m39/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0149 - mean_squared_error: 0.0149
[1m45/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0154 - mean_squared_error: 0.0154
[1m51/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0157 - mean_squared_error: 0.0157
[1m57/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0159 - mean_squared_error: 0.0159
[1m63/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0161 - mean_squared_error: 0.0161
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0163 - mean_squared_error: 0.0163 - val_loss: 0.0156 - val_mean_squared_error: 0.0156
(12992, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 12992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 812, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 12992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 12992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 812, 6)    â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 812, 6)    â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 812, 12)   â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 812)   â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 9744)      â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 9744)      â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚      9,745 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 10,567 (41.28 KB)
 Trainable params: 10,567 (41.28 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:45[0m 2s/step - binary_accuracy: 1.0000 - loss: 0.6058
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6422 - loss: 0.7262
[1m14/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5247 - loss: 0.7553
[1m21/68[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5392 - loss: 0.7401 
[1m27/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5479 - loss: 0.7411
[1m34/68[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5630 - loss: 0.7367
[1m40/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5746 - loss: 0.7341 
[1m46/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5851 - loss: 0.7299
[1m53/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5965 - loss: 0.7243
[1m59/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6033 - loss: 0.7223
[1m66/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6120 - loss: 0.7169
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 12ms/step - binary_accuracy: 0.6153 - loss: 0.7145 - val_binary_accuracy: 0.6250 - val_loss: 0.6745
Epoch 2/10

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 0.0000e+00 - loss: 1.3422
[1m 8/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5266 - loss: 0.8255     
[1m14/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5994 - loss: 0.7652
[1m22/68[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6100 - loss: 0.7415 
[1m28/68[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6136 - loss: 0.7249
[1m35/68[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6213 - loss: 0.7066
[1m41/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6279 - loss: 0.6933
[1m47/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6352 - loss: 0.6806
[1m53/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6373 - loss: 0.6744 
[1m59/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6409 - loss: 0.6680
[1m66/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6462 - loss: 0.6601
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.6483 - loss: 0.6567 - val_binary_accuracy: 0.6250 - val_loss: 0.6308
Epoch 3/10

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.1613
[1m 8/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 1.0000 - loss: 0.1701 
[1m15/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9801 - loss: 0.2184
[1m22/68[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9606 - loss: 0.2560
[1m28/68[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9440 - loss: 0.2884
[1m35/68[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9206 - loss: 0.3270
[1m42/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8998 - loss: 0.3576
[1m49/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8773 - loss: 0.3865
[1m56/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8576 - loss: 0.4110
[1m62/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8462 - loss: 0.4271
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.8356 - loss: 0.4420 - val_binary_accuracy: 0.7500 - val_loss: 0.5467
Epoch 4/10

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.9269
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.4007 - loss: 0.6938    
[1m13/68[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.4792 - loss: 0.6815
[1m19/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5321 - loss: 0.6582
[1m27/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5605 - loss: 0.6382 
[1m33/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5797 - loss: 0.6241
[1m40/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6011 - loss: 0.6107
[1m45/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6143 - loss: 0.6025
[1m52/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6313 - loss: 0.5908
[1m59/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6448 - loss: 0.5800
[1m65/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6535 - loss: 0.5737
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.6581 - loss: 0.5715 - val_binary_accuracy: 0.6250 - val_loss: 0.5460
Epoch 5/10

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0508
[1m 8/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7153 - loss: 0.5774 
[1m14/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7064 - loss: 0.5664
[1m22/68[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6958 - loss: 0.5708 
[1m29/68[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6937 - loss: 0.5708
[1m35/68[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6961 - loss: 0.5682
[1m42/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6973 - loss: 0.5733
[1m47/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6955 - loss: 0.5776
[1m54/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6921 - loss: 0.5823
[1m61/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6909 - loss: 0.5836
[1m67/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6902 - loss: 0.5826
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.6898 - loss: 0.5821 - val_binary_accuracy: 0.6250 - val_loss: 0.5262
Epoch 6/10

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.9244
[1m 8/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5162 - loss: 0.9208     
[1m15/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6219 - loss: 0.7544
[1m21/68[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6809 - loss: 0.6711
[1m27/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7063 - loss: 0.6393
[1m34/68[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7290 - loss: 0.6147
[1m41/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7400 - loss: 0.6002 
[1m48/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7441 - loss: 0.5918
[1m56/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7507 - loss: 0.5798
[1m62/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7564 - loss: 0.5715
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7597 - loss: 0.5656 - val_binary_accuracy: 0.8750 - val_loss: 0.3983
Epoch 7/10

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3794
[1m 8/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7853 - loss: 0.4253 
[1m14/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8090 - loss: 0.3940
[1m21/68[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8038 - loss: 0.3897 
[1m28/68[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8057 - loss: 0.3908
[1m35/68[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8061 - loss: 0.3948
[1m41/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8040 - loss: 0.4008
[1m49/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8064 - loss: 0.4024
[1m55/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8057 - loss: 0.4081
[1m62/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8036 - loss: 0.4152
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8021 - loss: 0.4198
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.8020 - loss: 0.4204 - val_binary_accuracy: 0.8750 - val_loss: 0.3595
Epoch 8/10

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7592
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5854 - loss: 0.6177    
[1m14/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6704 - loss: 0.5545
[1m20/68[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6828 - loss: 0.5559
[1m27/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7106 - loss: 0.5271
[1m35/68[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7395 - loss: 0.5003 
[1m42/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7566 - loss: 0.4879
[1m48/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7628 - loss: 0.4886
[1m56/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7651 - loss: 0.4894
[1m62/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7683 - loss: 0.4880
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7714 - loss: 0.4867
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7718 - loss: 0.4866 - val_binary_accuracy: 0.8750 - val_loss: 0.3449
Epoch 9/10

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.1868
[1m 7/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 1.0000 - loss: 0.3024
[1m14/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 1.0000 - loss: 0.3083
[1m21/68[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9902 - loss: 0.3062 
[1m28/68[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9813 - loss: 0.3086
[1m34/68[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9643 - loss: 0.3277
[1m41/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9473 - loss: 0.3492 
[1m48/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9355 - loss: 0.3620
[1m54/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9297 - loss: 0.3693
[1m60/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9246 - loss: 0.3753
[1m67/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9194 - loss: 0.3809
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.9183 - loss: 0.3819 - val_binary_accuracy: 0.8750 - val_loss: 0.3417
Epoch 10/10

[1m 1/68[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.1850
[1m 8/68[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8894 - loss: 0.3233 
[1m15/68[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8623 - loss: 0.3508
[1m21/68[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8196 - loss: 0.3977
[1m27/68[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8050 - loss: 0.4157
[1m33/68[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7978 - loss: 0.4235
[1m40/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7920 - loss: 0.4260
[1m47/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7920 - loss: 0.4232
[1m53/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7913 - loss: 0.4229
[1m61/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7898 - loss: 0.4247
[1m67/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7897 - loss: 0.4252
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7898 - loss: 0.4253 - val_binary_accuracy: 0.8750 - val_loss: 0.3050

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
predicted [0.1899615  0.95853466 0.18844838 0.40644658 0.8737974  0.849996
 0.841575   0.96105635 0.5977603  0.75179523 0.4324962  0.52283716
 0.10923748 0.884116   0.52108914 0.73040986 0.53652316 0.90628606
 0.6120237  0.7860681 ]
predicted [0 1 0 0 1 1 1 1 1 1 0 1 0 1 1 1 1 1 1 1]
expected [ True False False False False  True False False  True  True  True  True
 False  True  True  True False  True  True  True]
accuracy: 0.65
confusion matrix: 
[[ 3  5]
 [ 2 10]]
              precision    recall  f1-score   support

       False       0.60      0.38      0.46         8
        True       0.67      0.83      0.74        12

    accuracy                           0.65        20
   macro avg       0.63      0.60      0.60        20
weighted avg       0.64      0.65      0.63        20

macro avg f1-score: 0.6011396011396011
macro avg (UAR): 0.6041666666666667
Sensitivity:  0.375
Specificity:  0.8333333333333334
g-mean:  0.5590169943749475
-------- Model Performance ----------: 
accuracy:  [0.65 0.6  0.75 0.8  0.6  0.7  0.6  0.65 0.6  0.65]
gmean:  [0.55901699 0.60380736 0.6770032  0.75691259 0.53033009 0.6846532
 0.45643546 0.55901699 0.57735027 0.55901699]
f1_score:  [0.6011396  0.5959596  0.71509972 0.78021978 0.56043956 0.6875
 0.52380952 0.6011396  0.58333333 0.6011396 ]
UAR:  [0.60416667 0.60416667 0.70833333 0.77083333 0.5625     0.6875
 0.54166667 0.60416667 0.58333333 0.60416667]
Cohen Kappa score:  [0.22222222 0.2        0.44444444 0.56521739 0.13043478 0.375
 0.09090909 0.22222222 0.16666667 0.22222222]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  12.992
step (sec):  12.992
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  96
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.675 0.848 0.843 0.841 0.836 0.568 0.79  0.802 0.78  0.781]
 [0.509 0.807 0.817 0.789 0.808 0.463 0.746 0.767 0.658 0.734]
 [0.695 0.851 0.863 0.736 0.846 0.54  0.775 0.781 0.694 0.756]
 [0.132 0.558 0.646 0.224 0.48  0.    0.5   0.657 0.    0.396]
 [0.619 0.832 0.843 0.803 0.827 0.388 0.708 0.783 0.616 0.705]
 [0.264 0.627 0.66  0.596 0.625 0.074 0.534 0.625 0.369 0.504]
 [0.    0.5   0.604 0.    0.376 0.    0.5   0.657 0.    0.396]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.45153846  0.725       0.72888889  0.71602737  0.7230267   0.42950056
   0.72142857  0.78444444  0.65496876  0.7055982 ]
 [ 0.37486643  0.69166667  0.68666667  0.68091453  0.6815404   0.14765294
   0.57333333  0.62333333  0.46853187  0.55889111]
 [ 0.37527887  0.68166667  0.70555556  0.64885374  0.67764569  0.39142857
   0.695       0.69444444  0.68462461  0.68969697]
 [ 0.37225411  0.68666667  0.68777778  0.6183384   0.66193196  0.09156453
   0.54761905  0.63555556  0.38294628  0.53382243]
 [ 0.52616601  0.75        0.79222222  0.72185433  0.75749084  0.35686679
   0.68        0.68        0.65093146  0.66532273]
 [ 0.23659955  0.62        0.61555556  0.59643167  0.60579809  0.61856835
   0.80833333  0.81333333  0.79725337  0.80616522]
 [ 0.3258931   0.66        0.67777778  0.62615073  0.65027556  0.10649502
   0.555       0.55111111  0.53995873  0.54541486]
 [ 0.79451436  0.88452381  0.91888889  0.86733792  0.89395147  0.53739277
   0.7725      0.77111111  0.76256236  0.76294372]
 [ 0.39147722  0.68214286  0.76333333  0.63248532  0.68803571  0.15826939
   0.57380952  0.65888889  0.49638456  0.57216117]
 [ 0.37716874  0.685       0.69444444  0.65998874  0.6792871   0.23489924
   0.6225      0.62        0.59839498  0.60811883]
 [ 0.14846863  0.57166667  0.60222222  0.52570356  0.55918581  0.37037297
   0.675       0.71444444  0.62876426  0.6692205 ]
 [ 0.45386402  0.7175      0.76111111  0.64612413  0.71383616  0.44056214
   0.715       0.74333333  0.67223123  0.706057  ]
 [ 0.26371051  0.62833333  0.64555556  0.60950367  0.62566323  0.05093284
   0.5275      0.55444444  0.43989918  0.51147991]
 [ 0.19638999  0.5952381   0.64777778  0.49571826  0.58134477  0.44218623
   0.72        0.72555556  0.6913776   0.70955628]
 [ 0.14085363  0.575       0.63111111  0.48108494  0.56057859  0.04004432
   0.51785714  0.57222222  0.40346984  0.49764624]
 [ 0.16950442  0.57583333  0.60777778  0.54553006  0.57490842  0.42227829
   0.70654762  0.74222222  0.68112115  0.70587829]
 [ 0.48587573  0.74416667  0.75222222  0.73372119  0.73973859  0.38916268
   0.68166667  0.71888889  0.64280175  0.68342352]
 [ 0.02073807  0.49375     0.76888889  0.12661238  0.47999797  0.40059153
   0.69833333  0.70777778  0.68109971  0.69412005]
 [ 0.35315495  0.67380952  0.73888889  0.5776726   0.66607143  0.32661269
   0.66083333  0.66777778  0.64458309  0.65549062]
 [ 0.30309377  0.65297619  0.70222222  0.5862062   0.64424575  0.19634199
   0.60654762  0.64888889  0.44241025  0.56341575]
 [-0.06289878  0.46845238  0.67888889  0.12416293  0.45444444  0.10590458
   0.555       0.55444444  0.4780438   0.53237346]
 [ 0.31941477  0.66        0.65666667  0.63408273  0.64632229  0.43388879
   0.71666667  0.73888889  0.69676561  0.71221362]
 [ 0.16344163  0.58166667  0.60555556  0.53321755  0.57649267  0.2513886
   0.6327381   0.77333333  0.37832182  0.59093326]
 [-0.04168998  0.48166667  0.48666667  0.3850358   0.45096404  0.4505333
   0.72        0.73222222  0.69740165  0.71618992]
 [ 0.50385649  0.75333333  0.75777778  0.74554769  0.7500086   0.57177792
   0.785       0.78777778  0.77215563  0.78099761]
 [ 0.26830917  0.62833333  0.64666667  0.53133103  0.60676684  0.28390357
   0.6425      0.64666667  0.59264527  0.62846598]
 [ 0.46128619  0.72380952  0.78222222  0.69444903  0.72617258  0.50192308
   0.75        0.75222222  0.73825083  0.74640526]
 [ 0.12299902  0.56488095  0.63666667  0.43160863  0.54031136  0.45586552
   0.72166667  0.75        0.69159844  0.71962343]
 [ 0.14295843  0.56916667  0.61444444  0.49843162  0.55903846  0.27324842
   0.63583333  0.64666667  0.60716592  0.62584971]
 [ 0.67477119  0.84833333  0.84333333  0.84069442  0.83560551  0.56837641
   0.79047619  0.80222222  0.78014533  0.78057692]]
KNN mean:
[0.31046196 0.65248611 0.69459259 0.58382737 0.64368937 0.33495113
 0.66695635 0.69374074 0.61322698 0.65593509]
---------------------------
---------------------------
DT performance:
[[ 0.53784734  0.8075      0.81        0.79549901  0.80634199  0.24002143
   0.61666667  0.71        0.43674047  0.58936084]
 [ 0.45018315  0.7775      0.78111111  0.73955572  0.77110695  0.31667992
   0.64166667  0.68888889  0.57856722  0.63429154]
 [ 0.22599587  0.62083333  0.63        0.57778692  0.61862443  0.39331194
   0.6625      0.66777778  0.6605154   0.65715729]
 [ 0.24070397  0.62416667  0.62444444  0.62488346  0.61709596  0.31022024
   0.58690476  0.66222222  0.52484648  0.57632326]
 [ 0.41469921  0.70833333  0.71888889  0.68642389  0.69831252  0.32186679
   0.6725      0.67111111  0.642654    0.65882007]
 [ 0.34363787  0.66        0.65666667  0.61602333  0.64082168  0.24977683
   0.69666667  0.69888889  0.67541188  0.69142691]
 [ 0.13816744  0.51166667  0.52888889  0.44112681  0.4975641   0.19329268
   0.5025      0.50222222  0.57891414  0.49368687]
 [ 0.73561054  0.83571429  0.86444444  0.84935995  0.841163    0.54542769
   0.79166667  0.81555556  0.73360178  0.79845627]
 [ 0.27921593  0.62261905  0.67777778  0.69376738  0.61907051  0.12529923
   0.57261905  0.65555556  0.42018758  0.57559982]
 [ 0.05584345  0.50916667  0.51666667  0.47403288  0.49230242  0.2184771
   0.6675      0.67222222  0.61831133  0.64885309]
 [ 0.19408425  0.60666667  0.61444444  0.58283447  0.59182984  0.38468971
   0.66916667  0.68555556  0.690853    0.66314019]
 [ 0.40671321  0.72583333  0.75666667  0.67348103  0.7237321   0.30317755
   0.62833333  0.62888889  0.56950271  0.61461316]
 [ 0.28394457  0.6675      0.67666667  0.6068246   0.63191281  0.13837984
   0.5475      0.56333333  0.52509133  0.52799784]
 [ 0.12479325  0.54940476  0.59333333  0.45216432  0.52696165  0.12667247
   0.5825      0.58        0.53849987  0.56999001]
 [ 0.12522326  0.57380952  0.63        0.45204302  0.54317281  0.13119446
   0.53571429  0.56555556  0.58569203  0.52468254]
 [ 0.05039234  0.54333333  0.57777778  0.45178279  0.53236819  0.3433903
   0.71607143  0.75222222  0.69342821  0.71450216]
 [ 0.43405225  0.74333333  0.73888889  0.66363967  0.72632701  0.36254318
   0.6875      0.69666667  0.65860001  0.67922078]
 [ 0.16668515  0.61875     0.81111111  0.38662208  0.57624779  0.25469908
   0.62583333  0.64777778  0.46555238  0.61606754]
 [ 0.3825788   0.72857143  0.75222222  0.68528913  0.70987096  0.29487619
   0.6125      0.61444444  0.5266597   0.59842075]
 [ 0.26220077  0.64166667  0.66666667  0.58466522  0.61465645  0.13005375
   0.57559524  0.59666667  0.52637204  0.5599531 ]
 [ 0.19449149  0.60267857  0.74        0.39997608  0.61323098 -0.03794425
   0.5325      0.53        0.49871828  0.52507576]
 [ 0.29167247  0.5625      0.56        0.58906558  0.55010823  0.24419494
   0.60119048  0.62444444  0.54605051  0.58500916]
 [ 0.18107578  0.58166667  0.59666667  0.48773119  0.56571789  0.30310629
   0.61666667  0.73444444  0.34736332  0.58602856]
 [ 0.13597406  0.5525      0.56222222  0.54888137  0.54587579  0.38586821
   0.68083333  0.68666667  0.71405051  0.67566683]
 [ 0.28313992  0.62333333  0.63        0.60273016  0.61473943  0.41177792
   0.6975      0.69888889  0.68624355  0.69409091]
 [ 0.29126706  0.675       0.67777778  0.5236399   0.65065018  0.35299584
   0.58083333  0.58333333  0.541416    0.56905567]
 [ 0.55728195  0.76309524  0.82111111  0.76959624  0.76470738  0.44829268
   0.705       0.70555556  0.72663471  0.70179293]
 [-0.01150441  0.47380952  0.51111111  0.344624    0.45080531  0.37508113
   0.69916667  0.70555556  0.67419632  0.68748224]
 [ 0.20794611  0.6525      0.66888889  0.59872918  0.6395954   0.15881223
   0.6075      0.60777778  0.54610587  0.58715729]
 [ 0.50921796  0.80666667  0.81666667  0.78939944  0.80809413  0.46338904
   0.74642857  0.76666667  0.65802733  0.7337117 ]]
DT mean:
[0.2831045  0.64567063 0.6737037  0.58973929 0.63276693 0.28298748
 0.63531746 0.6572963  0.5862936  0.62458784]
---------------------------
---------------------------
RF performance:
[[ 0.60219512  0.8175      0.82111111  0.80571153  0.81677128  0.24759176
   0.66904762  0.76333333  0.62502516  0.65643288]
 [ 0.46868981  0.73        0.73111111  0.75399095  0.72685065  0.37615957
   0.66083333  0.68555556  0.6098195   0.64978688]
 [ 0.24412756  0.645       0.66333333  0.64297975  0.64090576  0.34603239
   0.7075      0.71111111  0.66456431  0.69927156]
 [ 0.37276812  0.665       0.65777778  0.64616479  0.6429157   0.1905182
   0.62619048  0.70444444  0.42460153  0.62423993]
 [ 0.44123029  0.7375      0.75888889  0.66983286  0.73545482  0.31296435
   0.6775      0.68222222  0.6932053   0.6710101 ]
 [ 0.31620477  0.645       0.64444444  0.60488695  0.63562049  0.55632139
   0.73666667  0.74222222  0.7088537   0.73603896]
 [ 0.22772019  0.63833333  0.64222222  0.60831209  0.62946276  0.27243902
   0.525       0.53222222  0.5275003   0.5052084 ]
 [ 0.74937776  0.90238095  0.91777778  0.88021349  0.90289932  0.4741673
   0.72416667  0.74111111  0.72309908  0.71709429]
 [ 0.42287878  0.7         0.74888889  0.5659921   0.69316767  0.12783878
   0.60595238  0.67888889  0.54815392  0.6059707 ]
 [ 0.27595238  0.7         0.70666667  0.54240964  0.69098568  0.20398674
   0.61        0.62888889  0.48850964  0.58798174]
 [ 0.2611783   0.595       0.63333333  0.54852084  0.58230075  0.49275092
   0.7625      0.78111111  0.6493231   0.75921079]
 [ 0.44526633  0.73833333  0.76444444  0.66018657  0.73852731  0.30367874
   0.7275      0.74333333  0.6512931   0.7225111 ]
 [ 0.29993914  0.66416667  0.66444444  0.70895697  0.64874292  0.1530687
   0.5075      0.53111111  0.51706035  0.50193251]
 [-0.02238707  0.44761905  0.50222222  0.34750058  0.41569514  0.3472093
   0.7325      0.72888889  0.64554645  0.72661616]
 [ 0.16304989  0.64345238  0.68666667  0.45165001  0.62004219  0.16087651
   0.60238095  0.62444444  0.55160758  0.59128982]
 [ 0.33174732  0.62083333  0.63444444  0.40765171  0.61292041  0.5423142
   0.7327381   0.77222222  0.6962997   0.73431319]
 [ 0.55334129  0.83666667  0.84222222  0.76938165  0.83238817  0.36528512
   0.74916667  0.76        0.6360833   0.73803114]
 [ 0.16307692  0.60625     0.82333333  0.29354143  0.57542981  0.42687933
   0.74        0.74888889  0.73140343  0.72830087]
 [ 0.45500283  0.65238095  0.72222222  0.63864064  0.64154277  0.34576459
   0.67        0.65888889  0.65646272  0.65143051]
 [ 0.35978017  0.73511905  0.77333333  0.66312706  0.7335169   0.21448551
   0.63154762  0.65666667  0.42917369  0.59662005]
 [ 0.14473606  0.52202381  0.67888889  0.42913602  0.50447946  0.1854878
   0.4625      0.47        0.39590572  0.4441367 ]
 [ 0.35101313  0.675       0.67777778  0.61118088  0.66885725  0.33095803
   0.69702381  0.73        0.58515613  0.6813259 ]
 [ 0.17904685  0.56583333  0.58555556  0.47927816  0.54584332  0.28966677
   0.6327381   0.73111111  0.47651521  0.61969641]
 [ 0.31434647  0.71083333  0.71777778  0.59889596  0.70751665  0.41769335
   0.73333333  0.73888889  0.75147497  0.72832973]
 [ 0.34890682  0.72333333  0.73111111  0.66867226  0.72070901  0.52235862
   0.7825      0.78333333  0.71159503  0.77963564]
 [ 0.30339836  0.6         0.60444444  0.66392804  0.57966533  0.26609391
   0.72583333  0.73111111  0.70488325  0.71975025]
 [ 0.35614444  0.74761905  0.80222222  0.68323788  0.7375      0.49230769
   0.7175      0.71888889  0.65331728  0.7088789 ]
 [ 0.08349445  0.50238095  0.58666667  0.37931699  0.46793194  0.29041841
   0.6975      0.70666667  0.65466132  0.69415779]
 [ 0.21332237  0.62333333  0.67        0.62621575  0.6181685   0.25402886
   0.63666667  0.63888889  0.60735366  0.62992424]
 [ 0.6953805   0.85083333  0.86333333  0.73633296  0.84640665  0.53978901
   0.775       0.78111111  0.69377041  0.75618715]]
RF mean:
[0.33736431 0.67472421 0.70855556 0.60286155 0.66377395 0.33497116
 0.67530952 0.69685185 0.61374063 0.66551048]
---------------------------
---------------------------
SVM performance:
[[-0.02        0.49        0.51222222  0.          0.33809524  0.
   0.5         0.74111111  0.          0.4253268 ]
 [ 0.03405405  0.515       0.56444444  0.13464102  0.40501832  0.
   0.5         0.61555556  0.          0.38071429]
 [ 0.15976834  0.5725      0.63444444  0.29472136  0.4957326   0.37289599
   0.685       0.68555556  0.66507387  0.67641775]
 [ 0.          0.5         0.53222222  0.          0.34702381  0.
   0.5         0.69777778  0.          0.41080882]
 [ 0.44007905  0.7         0.76333333  0.62314749  0.70655678  0.56331194
   0.78        0.78444444  0.74303235  0.76633311]
 [ 0.07229018  0.535       0.54222222  0.18416408  0.41304029  0.45122807
   0.72416667  0.73        0.70763453  0.71906926]
 [ 0.          0.5         0.59333333  0.          0.37214286  0.18492176
   0.59        0.59444444  0.4127332   0.52137363]
 [ 0.42718105  0.67916667  0.79222222  0.59352987  0.69347527  0.10977444
   0.55        0.64666667  0.17071068  0.46071429]
 [ 0.          0.5         0.69777778  0.          0.41080882  0.
   0.5         0.69777778  0.          0.41080882]
 [ 0.00540541  0.5025      0.53222222  0.05        0.36482601  0.
   0.5         0.54222222  0.          0.35119048]
 [ 0.          0.5         0.60444444  0.          0.37642857  0.
   0.5         0.61555556  0.          0.38071429]
 [ 0.          0.5         0.61555556  0.          0.38071429  0.
   0.5         0.60444444  0.          0.37642857]
 [ 0.          0.5         0.59333333  0.          0.37214286  0.
   0.5         0.58222222  0.          0.36785714]
 [ 0.          0.5         0.63666667  0.          0.38867647  0.18668563
   0.59        0.60666667  0.29720178  0.49503663]
 [ 0.          0.5         0.67666667  0.          0.40338235  0.
   0.5         0.63666667  0.          0.38867647]
 [ 0.          0.5         0.60444444  0.          0.37642857  0.
   0.5         0.65666667  0.          0.39602941]
 [ 0.          0.5         0.59333333  0.          0.37214286  0.
   0.5         0.59333333  0.          0.37214286]
 [ 0.          0.5         0.84555556  0.          0.45782594  0.
   0.5         0.58222222  0.          0.36785714]
 [ 0.          0.5         0.68666667  0.          0.40705882  0.
   0.5         0.55222222  0.          0.35535714]
 [ 0.          0.5         0.65666667  0.          0.39602941  0.
   0.5         0.63666667  0.          0.38867647]
 [ 0.          0.5         0.77111111  0.          0.43513072  0.
   0.5         0.51111111  0.          0.33791209]
 [ 0.          0.5         0.52222222  0.          0.34285714  0.
   0.5         0.63666667  0.          0.38867647]
 [ 0.          0.5         0.58222222  0.          0.36785714  0.
   0.5         0.75111111  0.          0.42859477]
 [ 0.          0.5         0.54222222  0.          0.35119048  0.
   0.5         0.56222222  0.          0.35952381]
 [ 0.          0.5         0.57222222  0.          0.36369048  0.38577425
   0.6975      0.68666667  0.66861698  0.67796176]
 [ 0.          0.5         0.55222222  0.          0.35535714  0.13957529
   0.565       0.59333333  0.36273917  0.50401099]
 [ 0.          0.5         0.69777778  0.          0.41080882  0.39532389
   0.695       0.70111111  0.65978601  0.68262266]
 [ 0.          0.5         0.64666667  0.          0.39235294  0.
   0.5         0.59333333  0.          0.37214286]
 [ 0.          0.5         0.61555556  0.          0.38071429  0.
   0.5         0.55222222  0.          0.35535714]
 [ 0.13220779  0.55833333  0.64555556  0.22408925  0.48035714  0.
   0.5         0.65666667  0.          0.39602941]]
SVM mean:
[0.04169953 0.51841667 0.62751852 0.0701431  0.41192888 0.09298304
 0.54588889 0.63488889 0.15625095 0.45047884]
---------------------------
---------------------------
GBM performance:
[[ 0.57910931  0.805       0.81333333  0.76025357  0.80231796  0.19405323
   0.59285714  0.78222222  0.28080604  0.56714052]
 [ 0.44768985  0.70833333  0.72111111  0.66594509  0.69540404  0.29157487
   0.67083333  0.73        0.55485429  0.66001499]
 [ 0.51313181  0.7425      0.78        0.68350746  0.73684066  0.33218623
   0.665       0.67        0.64260098  0.65706183]
 [ 0.37145403  0.69833333  0.69666667  0.63112037  0.67887279  0.19839522
   0.60238095  0.75        0.35998525  0.5755077 ]
 [ 0.50118012  0.73333333  0.78333333  0.69486025  0.7414652   0.33373358
   0.6775      0.67888889  0.60596095  0.6592785 ]
 [ 0.29550199  0.65        0.64555556  0.61831078  0.63249001  0.42748524
   0.68833333  0.70777778  0.71532622  0.67855062]
 [ 0.25364955  0.59666667  0.64666667  0.46000311  0.56896437  0.26694988
   0.635       0.63444444  0.60702384  0.62691198]
 [ 0.72701913  0.85238095  0.88444444  0.84935995  0.861163    0.61491089
   0.8         0.83333333  0.74307214  0.79945055]
 [ 0.23381649  0.60238095  0.73888889  0.36747265  0.58463801  0.0918288
   0.53928571  0.68555556  0.27116955  0.50677171]
 [ 0.07393121  0.56916667  0.58111111  0.46390695  0.54622405  0.17987944
   0.59583333  0.61        0.53478719  0.58797175]
 [ 0.20943977  0.60666667  0.64555556  0.53399935  0.59284632  0.47372961
   0.73333333  0.76        0.69477904  0.72805694]
 [ 0.50693625  0.74833333  0.78888889  0.65194889  0.73714868  0.37079676
   0.68166667  0.72222222  0.6006423   0.67306471]
 [ 0.30800438  0.65833333  0.68        0.53199967  0.61739011 -0.06009842
   0.47416667  0.52333333  0.30911167  0.44152847]
 [ 0.08025974  0.53988095  0.64666667  0.17724071  0.47191392  0.27129555
   0.6475      0.65555556  0.6172917   0.62158369]
 [-0.05907563  0.48035714  0.64333333  0.05        0.4086894   0.11356476
   0.54285714  0.61888889  0.3544732   0.49505411]
 [ 0.17158407  0.57916667  0.66777778  0.2852604   0.51299451  0.43511754
   0.70238095  0.76333333  0.65307259  0.71009935]
 [ 0.51447966  0.7575      0.76888889  0.73676892  0.74160728  0.21763158
   0.60833333  0.65555556  0.37821332  0.54695887]
 [ 0.10533137  0.575       0.84222222  0.16425211  0.56157594  0.27664669
   0.64916667  0.68222222  0.49512206  0.62391775]
 [ 0.23109753  0.61309524  0.72888889  0.42095897  0.58855257  0.22238051
   0.6125      0.62555556  0.52325496  0.5804021 ]
 [ 0.3379001   0.65357143  0.74111111  0.49065406  0.64776099  0.26991798
   0.6125      0.69888889  0.41729886  0.5753663 ]
 [ 0.06702341  0.5125      0.73222222  0.07071068  0.48269294  0.32853659
   0.645       0.64333333  0.65402257  0.63897741]
 [ 0.25535513  0.6475      0.64777778  0.6009311   0.63458153  0.18505325
   0.59166667  0.66888889  0.3923192   0.55784008]
 [ 0.18765977  0.615       0.65777778  0.49550708  0.58599484  0.15719064
   0.56904762  0.77222222  0.21042791  0.52664898]
 [ 0.22635011  0.63333333  0.64555556  0.60490632  0.6284324   0.3785349
   0.68833333  0.69666667  0.67291037  0.68373155]
 [ 0.37247897  0.67916667  0.70888889  0.62795383  0.66897769  0.49892078
   0.76        0.76        0.71715583  0.75101454]
 [ 0.43806948  0.7125      0.73        0.62985978  0.69100233  0.40916323
   0.70166667  0.71111111  0.66603909  0.69636919]
 [ 0.24335173  0.60238095  0.74777778  0.37032479  0.58702381  0.47291209
   0.7275      0.72666667  0.71355068  0.71594156]
 [-0.07305831  0.47202381  0.58222222  0.12071068  0.41089953  0.48358246
   0.73        0.77111111  0.68122603  0.72799728]
 [ 0.32691786  0.65416667  0.71        0.57797068  0.65025641  0.22006748
   0.59916667  0.60777778  0.56592012  0.58617965]
 [ 0.6185563   0.83166667  0.84333333  0.80264759  0.82688811  0.38755317
   0.7077381   0.78333333  0.61569905  0.70480284]]
GBM mean:
[0.30217151 0.65100794 0.715      0.50464486 0.62985365 0.30144982
 0.64838492 0.69762963 0.5416039  0.63013985]
---------------------------
---------------------------
BDDAE performance:
[[ 0.12        0.56        0.56        0.55397349  0.5571107   0.11491758
   0.55333333  0.71        0.39717979  0.54405239]
 [ 0.08794298  0.54393939  0.545       0.52632071  0.53470557  0.37607408
   0.69166667  0.695       0.68111097  0.68288941]
 [ 0.10698113  0.55353535  0.56        0.50222017  0.53074219  0.08
   0.54        0.54        0.47395017  0.50888135]
 [ 0.07938945  0.53888889  0.545       0.5227052   0.53336883  0.06007159
   0.52738095  0.645       0.37490917  0.51272087]
 [ 0.42232428  0.7146978   0.72        0.70934668  0.70854345  0.33
   0.665       0.665       0.62787388  0.64810447]
 [ 0.42        0.71        0.71        0.6929633   0.70233876 -0.07691423
   0.46313131  0.465       0.38686121  0.43559013]
 [-0.04599043  0.47708333  0.5         0.43992287  0.46773574  0.22
   0.61        0.61        0.58440199  0.59767626]
 [ 0.3957022   0.70238095  0.75        0.66397729  0.68913538  0.29760434
   0.64791667  0.665       0.62969739  0.64346126]
 [ 0.35657695  0.65119048  0.765       0.57690301  0.66622289  0.55861391
   0.74166667  0.845       0.68408657  0.76818845]
 [-0.14111968  0.42929293  0.43        0.41300438  0.42161315  0.22679931
   0.61060606  0.625       0.5897321   0.60784866]
 [-0.06021567  0.46875     0.5         0.42132083  0.45724433  0.26200625
   0.62083333  0.675       0.53968438  0.60663115]
 [ 0.08413617  0.54166667  0.575       0.49528917  0.53177993  0.35164966
   0.68333333  0.675       0.67491328  0.66918002]
 [-0.06152315  0.47083333  0.485       0.41097598  0.453138    0.17511525
   0.58958333  0.595       0.57960339  0.58243201]
 [ 0.16097609  0.57967033  0.625       0.54916214  0.5767427   0.4
   0.7         0.7         0.68975598  0.69568931]
 [ 0.04138949  0.51309524  0.625       0.41580275  0.50450113  0.11574611
   0.56538462  0.585       0.48161251  0.53126148]
 [-0.09649001  0.45416667  0.49        0.23198095  0.39223379  0.10497761
   0.54450549  0.665       0.27197263  0.49259012]
 [ 0.56371304  0.79375     0.78        0.78598285  0.77655672  0.18816152
   0.58333333  0.645       0.46208894  0.56194483]
 [ 0.11285148  0.56470588  0.82        0.26103617  0.53877264  0.11904834
   0.55625     0.58        0.53531291  0.55447273]
 [ 0.04479657  0.51904762  0.6         0.42641131  0.50643662  0.16179598
   0.57979798  0.58        0.54545444  0.56263681]
 [ 0.15889154  0.57362637  0.66        0.47739318  0.56097658 -0.15739346
   0.42582418  0.515       0.22129549  0.39575734]
 [ 0.0968698   0.54        0.73        0.31636432  0.52562145  0.05
   0.525       0.525       0.51333964  0.51929777]
 [-0.1         0.45        0.45        0.44140242  0.44585643 -0.11635002
   0.4456044   0.545       0.2047588   0.40180863]
 [ 0.22750245  0.61458333  0.62        0.60412642  0.60761766  0.25439893
   0.60666667  0.77        0.4751755   0.60994441]
 [-0.00295327  0.49848485  0.505       0.48898451  0.49602436  0.21961903
   0.60808081  0.62        0.587053    0.60375915]
 [-0.14702298  0.42828283  0.44        0.39444706  0.41855665 -0.03
   0.485       0.485       0.46363911  0.47527524]
 [ 0.18858511  0.59242424  0.605       0.56523808  0.58514672  0.10041932
   0.54848485  0.56        0.51747129  0.53795498]
 [ 0.43095822  0.70119048  0.775       0.66690987  0.71029701  0.55
   0.775       0.775       0.76420239  0.77086688]
 [ 0.15783685  0.58186813  0.615       0.55737103  0.57450609  0.49553321
   0.74583333  0.765       0.72459392  0.74216902]
 [ 0.09993808  0.55        0.565       0.53551325  0.54579466  0.38290174
   0.68888889  0.7         0.67068651  0.68569943]
 [ 0.2639339   0.62708333  0.66        0.59635432  0.62497803  0.07410462
   0.53351648  0.625       0.36915933  0.50406612]]
BDDAE mean:
[0.13219935 0.56480795 0.607      0.50811346 0.55480994 0.19629669
 0.59538742 0.635      0.52405256 0.58176169]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.52222222 0.         0.34285714 0.
  0.5        0.74111111 0.         0.4253268 ]
 [0.         0.5        0.55222222 0.         0.35535714 0.
  0.5        0.61555556 0.         0.38071429]
 [0.         0.5        0.57222222 0.         0.36369048 0.
  0.5        0.51111111 0.         0.33791209]
 [0.         0.5        0.53222222 0.         0.34702381 0.
  0.5        0.69777778 0.         0.41080882]
 [0.         0.5        0.62666667 0.         0.385      0.
  0.5        0.52222222 0.         0.34285714]
 [0.         0.5        0.51111111 0.         0.33791209 0.
  0.5        0.54222222 0.         0.35119048]
 [0.         0.5        0.59333333 0.         0.37214286 0.
  0.5        0.51111111 0.         0.33791209]
 [0.         0.5        0.67666667 0.         0.40338235 0.
  0.5        0.60444444 0.         0.37642857]
 [0.         0.5        0.69777778 0.         0.41080882 0.
  0.5        0.69777778 0.         0.41080882]
 [0.         0.5        0.53222222 0.         0.34702381 0.
  0.5        0.54222222 0.         0.35119048]
 [0.         0.5        0.60444444 0.         0.37642857 0.
  0.5        0.61555556 0.         0.38071429]
 [0.         0.5        0.61555556 0.         0.38071429 0.
  0.5        0.60444444 0.         0.37642857]
 [0.         0.5        0.59333333 0.         0.37214286 0.
  0.5        0.58222222 0.         0.36785714]
 [0.         0.5        0.63666667 0.         0.38867647 0.
  0.5        0.52222222 0.         0.34285714]
 [0.         0.5        0.67666667 0.         0.40338235 0.
  0.5        0.63666667 0.         0.38867647]
 [0.         0.5        0.60444444 0.         0.37642857 0.
  0.5        0.65666667 0.         0.39602941]
 [0.         0.5        0.59333333 0.         0.37214286 0.
  0.5        0.59333333 0.         0.37214286]
 [0.         0.5        0.84555556 0.         0.45782594 0.
  0.5        0.58222222 0.         0.36785714]
 [0.         0.5        0.68666667 0.         0.40705882 0.
  0.5        0.55222222 0.         0.35535714]
 [0.         0.5        0.65666667 0.         0.39602941 0.
  0.5        0.63666667 0.         0.38867647]
 [0.         0.5        0.77111111 0.         0.43513072 0.
  0.5        0.51111111 0.         0.33791209]
 [0.         0.5        0.52222222 0.         0.34285714 0.
  0.5        0.63666667 0.         0.38867647]
 [0.         0.5        0.58222222 0.         0.36785714 0.
  0.5        0.75111111 0.         0.42859477]
 [0.         0.5        0.54222222 0.         0.35119048 0.
  0.5        0.56222222 0.         0.35952381]
 [0.         0.5        0.57222222 0.         0.36369048 0.
  0.5        0.47777778 0.         0.32307692]
 [0.         0.5        0.55222222 0.         0.35535714 0.
  0.5        0.54222222 0.         0.35119048]
 [0.         0.5        0.69777778 0.         0.41080882 0.
  0.5        0.52222222 0.         0.34285714]
 [0.         0.5        0.64666667 0.         0.39235294 0.
  0.5        0.59333333 0.         0.37214286]
 [0.         0.5        0.61555556 0.         0.38071429 0.
  0.5        0.55222222 0.         0.35535714]
 [0.         0.5        0.60444444 0.         0.37642857 0.
  0.5        0.65666667 0.         0.39602941]]
DUMMY mean:
[0.         0.5        0.61455556 0.         0.37908055 0.
 0.5        0.59244444 0.         0.37057024]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_35
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.31  0.652 0.695 0.584 0.644 0.335 0.667 0.694 0.613 0.656]
 [0.283 0.646 0.674 0.59  0.633 0.283 0.635 0.657 0.586 0.625]
 [0.337 0.675 0.709 0.603 0.664 0.335 0.675 0.697 0.614 0.666]
 [0.042 0.518 0.628 0.07  0.412 0.093 0.546 0.635 0.156 0.45 ]
 [0.302 0.651 0.715 0.505 0.63  0.301 0.648 0.698 0.542 0.63 ]
 [0.132 0.565 0.607 0.508 0.555 0.196 0.595 0.635 0.524 0.582]
 [0.    0.5   0.615 0.    0.379 0.    0.5   0.592 0.    0.371]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.191 0.095 0.084 0.163 0.101 0.162 0.08  0.075 0.122 0.086]
 [0.166 0.094 0.097 0.128 0.099 0.124 0.067 0.07  0.096 0.07 ]
 [0.168 0.1   0.091 0.136 0.107 0.125 0.078 0.076 0.097 0.081]
 [0.112 0.049 0.083 0.162 0.085 0.164 0.082 0.066 0.262 0.123]
 [0.192 0.094 0.075 0.215 0.11  0.142 0.07  0.066 0.153 0.082]
 [0.183 0.091 0.109 0.126 0.095 0.18  0.087 0.088 0.143 0.097]
 [0.    0.    0.075 0.    0.028 0.    0.    0.069 0.    0.027]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 62.  15.  12.  28.  16.  48.  12.  11.  20.  13.]
 [ 59.  15.  14.  22.  16.  44.  11.  11.  16.  11.]
 [ 50.  15.  13.  23.  16.  37.  12.  11.  16.  12.]
 [269.   9.  13. 231.  21. 176.  15.  10. 168.  27.]
 [ 64.  14.  10.  43.  17.  47.  11.   9.  28.  13.]
 [138.  16.  18.  25.  17.  92.  15.  14.  27.  17.]
 [  0.   0.  12.   0.   7.   0.   0.  12.   0.   7.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_35
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  12.992
step (sec):  12.992
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  96
Elapsed time: 493.90431704521177 minutes
Elapsed time: 8.231738617420197 hours
