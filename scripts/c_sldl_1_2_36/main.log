2024-05-05 19:18:35.692244: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-05 19:18:39.369291: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-05 19:18:48.696588: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  13.0
step (sec):  9.75
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  3.25
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_36\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

ms/step - loss: 11.3941 - mean_squared_error: 11.3941
[1m85/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 11.4435 - mean_squared_error: 11.4435
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 11.4605 - mean_squared_error: 11.4605 - val_loss: 10.4703 - val_mean_squared_error: 10.4703
Epoch 3/5

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.1527 - mean_squared_error: 0.1527
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.3032 - mean_squared_error: 11.3032
[1m13/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 13.6282 - mean_squared_error: 13.6282
[1m20/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 12.9223 - mean_squared_error: 12.9223
[1m26/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 12.2776 - mean_squared_error: 12.2776
[1m32/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.9605 - mean_squared_error: 11.9605
[1m39/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 12.0655 - mean_squared_error: 12.0655
[1m44/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 12.1117 - mean_squared_error: 12.1117
[1m51/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 12.0340 - mean_squared_error: 12.0340
[1m57/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.9427 - mean_squared_error: 11.9427
[1m63/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.9244 - mean_squared_error: 11.9244
[1m69/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.9144 - mean_squared_error: 11.9144
[1m76/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 11.8916 - mean_squared_error: 11.8916
[1m82/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 11.8755 - mean_squared_error: 11.8755
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 10ms/step - loss: 11.8625 - mean_squared_error: 11.8625
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 11.8612 - mean_squared_error: 11.8612 - val_loss: 10.4701 - val_mean_squared_error: 10.4701
Epoch 4/5

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 16ms/step - loss: 7.4113 - mean_squared_error: 7.4113
[1m 6/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 12ms/step - loss: 6.1775 - mean_squared_error: 6.1775
[1m13/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 6.8662 - mean_squared_error: 6.8662
[1m19/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 7.6036 - mean_squared_error: 7.6036
[1m26/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 8.9001 - mean_squared_error: 8.9001
[1m32/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 9.5761 - mean_squared_error: 9.5761
[1m38/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 10.2791 - mean_squared_error: 10.2791
[1m44/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 10.6823 - mean_squared_error: 10.6823
[1m49/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 10.8721 - mean_squared_error: 10.8721
[1m55/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.0140 - mean_squared_error: 11.0140
[1m61/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.0528 - mean_squared_error: 11.0528
[1m67/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.0406 - mean_squared_error: 11.0406
[1m73/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.0044 - mean_squared_error: 11.0044
[1m79/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 11.0012 - mean_squared_error: 11.0012
[1m85/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 11.0058 - mean_squared_error: 11.0058
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 11.0341 - mean_squared_error: 11.0341 - val_loss: 10.4697 - val_mean_squared_error: 10.4697
Epoch 5/5

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 4.5385 - mean_squared_error: 4.5385
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 14.8242 - mean_squared_error: 14.8242
[1m13/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 14.7576 - mean_squared_error: 14.7576
[1m19/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 14.7398 - mean_squared_error: 14.7398
[1m24/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 14.3827 - mean_squared_error: 14.3827
[1m30/88[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.9111 - mean_squared_error: 13.9111
[1m36/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.5509 - mean_squared_error: 13.5509
[1m43/88[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.2560 - mean_squared_error: 13.2560
[1m49/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.1388 - mean_squared_error: 13.1388
[1m55/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.0450 - mean_squared_error: 13.0450
[1m62/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 12.9140 - mean_squared_error: 12.9140
[1m68/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 12.7885 - mean_squared_error: 12.7885
[1m74/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 12.6449 - mean_squared_error: 12.6449
[1m79/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 12.5344 - mean_squared_error: 12.5344
[1m84/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 12.4696 - mean_squared_error: 12.4696
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 12.4308 - mean_squared_error: 12.4308 - val_loss: 10.4686 - val_mean_squared_error: 10.4686
(98, 812, 6)
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 12992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 812, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 12992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 812, 6)         â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2398              â”‚ (None, 3248, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2398           â”‚ (None, 3248, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2399              â”‚ (None, 12992, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2399           â”‚ (None, 12992, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:39[0m 1s/step - loss: 0.1425 - mean_squared_error: 0.1425
[1m 6/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0941 - mean_squared_error: 0.0941
[1m11/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0829 - mean_squared_error: 0.0829
[1m17/88[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0770 - mean_squared_error: 0.0770
[1m22/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0727 - mean_squared_error: 0.0727
[1m27/88[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0683 - mean_squared_error: 0.0683
[1m33/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0648 - mean_squared_error: 0.0648
[1m39/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0622 - mean_squared_error: 0.0622
[1m44/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0602 - mean_squared_error: 0.0602
[1m50/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0583 - mean_squared_error: 0.0583
[1m56/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0566 - mean_squared_error: 0.0566
[1m62/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0549 - mean_squared_error: 0.0549
[1m68/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0533 - mean_squared_error: 0.0533
[1m74/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0519 - mean_squared_error: 0.0519
[1m80/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0506 - mean_squared_error: 0.0506
[1m86/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 0.0494 - mean_squared_error: 0.0494
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 12ms/step - loss: 0.0488 - mean_squared_error: 0.0488 - val_loss: 0.0265 - val_mean_squared_error: 0.0265
Epoch 2/5

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - loss: 0.0446 - mean_squared_error: 0.0446
[1m 6/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0230 - mean_squared_error: 0.0230
[1m12/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0244 - mean_squared_error: 0.0244
[1m18/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0240 - mean_squared_error: 0.0240
[1m24/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0233 - mean_squared_error: 0.0233
[1m30/88[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0230 - mean_squared_error: 0.0230
[1m36/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0228 - mean_squared_error: 0.0228
[1m43/88[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0224 - mean_squared_error: 0.0224
[1m49/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0219 - mean_squared_error: 0.0219
[1m56/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0214 - mean_squared_error: 0.0214
[1m62/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0213 - mean_squared_error: 0.0213
[1m69/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0212 - mean_squared_error: 0.0212
[1m75/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0210 - mean_squared_error: 0.0210
[1m82/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0210 - mean_squared_error: 0.0210
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 0.0209 - mean_squared_error: 0.0209 - val_loss: 0.0283 - val_mean_squared_error: 0.0283
Epoch 3/5

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - loss: 0.0013 - mean_squared_error: 0.0013
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0449 - mean_squared_error: 0.0449
[1m13/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0420 - mean_squared_error: 0.0420
[1m19/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0387 - mean_squared_error: 0.0387
[1m26/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0353 - mean_squared_error: 0.0353
[1m32/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0333 - mean_squared_error: 0.0333
[1m39/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0318 - mean_squared_error: 0.0318
[1m45/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0308 - mean_squared_error: 0.0308
[1m51/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0298 - mean_squared_error: 0.0298
[1m58/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0288 - mean_squared_error: 0.0288
[1m64/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0281 - mean_squared_error: 0.0281
[1m70/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0275 - mean_squared_error: 0.0275
[1m74/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0272 - mean_squared_error: 0.0272
[1m80/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0266 - mean_squared_error: 0.0266
[1m87/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 0.0260 - mean_squared_error: 0.0260
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 0.0258 - mean_squared_error: 0.0258 - val_loss: 0.0232 - val_mean_squared_error: 0.0232
Epoch 4/5

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.1079 - mean_squared_error: 0.1079
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0541 - mean_squared_error: 0.0541
[1m13/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0408 - mean_squared_error: 0.0408
[1m19/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0346 - mean_squared_error: 0.0346
[1m25/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0313 - mean_squared_error: 0.0313
[1m32/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0289 - mean_squared_error: 0.0289
[1m38/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0272 - mean_squared_error: 0.0272
[1m44/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0260 - mean_squared_error: 0.0260
[1m50/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0250 - mean_squared_error: 0.0250
[1m57/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0240 - mean_squared_error: 0.0240
[1m63/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0233 - mean_squared_error: 0.0233
[1m69/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0228 - mean_squared_error: 0.0228
[1m75/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0224 - mean_squared_error: 0.0224
[1m81/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0220 - mean_squared_error: 0.0220
[1m87/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 0.0217 - mean_squared_error: 0.0217
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0227 - val_mean_squared_error: 0.0227
Epoch 5/5

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 37ms/step - loss: 0.0046 - mean_squared_error: 0.0046
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0152 - mean_squared_error: 0.0152
[1m14/88[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0150 - mean_squared_error: 0.0150 
[1m20/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0142 - mean_squared_error: 0.0142
[1m27/88[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0141 - mean_squared_error: 0.0141 
[1m33/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0144 - mean_squared_error: 0.0144
[1m38/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0144 - mean_squared_error: 0.0144
[1m44/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0145 - mean_squared_error: 0.0145
[1m51/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0144 - mean_squared_error: 0.0144
[1m57/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0144 - mean_squared_error: 0.0144
[1m64/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0147 - mean_squared_error: 0.0147
[1m71/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0149 - mean_squared_error: 0.0149
[1m77/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0152 - mean_squared_error: 0.0152
[1m84/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 0.0154 - mean_squared_error: 0.0154
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 0.0155 - mean_squared_error: 0.0155 - val_loss: 0.0222 - val_mean_squared_error: 0.0222
(12992, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 12992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 812, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 12992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 12992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 812, 6)    â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 812, 6)    â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 812, 12)   â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 812)   â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 9744)      â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 9744)      â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚      9,745 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 10,567 (41.28 KB)
 Trainable params: 10,567 (41.28 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:53[0m 1s/step - binary_accuracy: 0.0000e+00 - loss: 0.8733
[1m 5/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 13ms/step - binary_accuracy: 0.1300 - loss: 0.9033    
[1m11/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.3262 - loss: 0.8505
[1m18/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.3986 - loss: 0.8200
[1m25/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.4327 - loss: 0.7956
[1m32/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.4628 - loss: 0.7729
[1m38/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.4816 - loss: 0.7591
[1m44/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.4992 - loss: 0.7476 
[1m51/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5145 - loss: 0.7397
[1m58/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5229 - loss: 0.7373
[1m63/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5308 - loss: 0.7335
[1m70/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5387 - loss: 0.7304 
[1m76/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5450 - loss: 0.7270
[1m83/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5533 - loss: 0.7222
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 12ms/step - binary_accuracy: 0.5597 - loss: 0.7178 - val_binary_accuracy: 0.8000 - val_loss: 0.5204
Epoch 2/10

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.6883
[1m 8/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 1.0000 - loss: 0.2879 
[1m14/88[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9317 - loss: 0.3632
[1m19/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9029 - loss: 0.3835
[1m24/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8777 - loss: 0.4087
[1m31/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8558 - loss: 0.4385
[1m38/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8389 - loss: 0.4644
[1m45/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8254 - loss: 0.4840
[1m51/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8148 - loss: 0.4967
[1m58/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8046 - loss: 0.5086
[1m65/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7960 - loss: 0.5176
[1m71/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7899 - loss: 0.5245
[1m78/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7844 - loss: 0.5307
[1m85/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7792 - loss: 0.5366
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7768 - loss: 0.5392 - val_binary_accuracy: 0.8000 - val_loss: 0.5079
Epoch 3/10

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.1811
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7150 - loss: 0.5566
[1m14/88[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6782 - loss: 0.6024
[1m20/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6976 - loss: 0.6014 
[1m26/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7082 - loss: 0.5999
[1m33/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7274 - loss: 0.5898
[1m39/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7420 - loss: 0.5790
[1m46/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7529 - loss: 0.5719
[1m52/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7597 - loss: 0.5671
[1m59/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7661 - loss: 0.5617 
[1m65/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7680 - loss: 0.5614
[1m71/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7679 - loss: 0.5622
[1m77/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7682 - loss: 0.5627
[1m84/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7687 - loss: 0.5628
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7682 - loss: 0.5629 - val_binary_accuracy: 0.7000 - val_loss: 0.5617
Epoch 4/10

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 29ms/step - binary_accuracy: 0.0000e+00 - loss: 0.8146
[1m 8/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5810 - loss: 0.6217     
[1m15/88[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6721 - loss: 0.5889
[1m21/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6966 - loss: 0.5849
[1m28/88[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6935 - loss: 0.6031
[1m35/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6985 - loss: 0.6065
[1m42/88[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7024 - loss: 0.6076
[1m49/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7050 - loss: 0.6088
[1m56/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7108 - loss: 0.6065
[1m63/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7157 - loss: 0.6051
[1m70/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7194 - loss: 0.6030
[1m77/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7223 - loss: 0.6008
[1m83/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7245 - loss: 0.5985
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7268 - loss: 0.5962 - val_binary_accuracy: 0.8000 - val_loss: 0.4904
Epoch 5/10

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 42ms/step - binary_accuracy: 1.0000 - loss: 0.4808
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9796 - loss: 0.3569 
[1m14/88[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9197 - loss: 0.4132
[1m21/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9020 - loss: 0.4203
[1m27/88[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8831 - loss: 0.4248
[1m32/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8692 - loss: 0.4298
[1m38/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8586 - loss: 0.4356
[1m44/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8549 - loss: 0.4374
[1m50/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8538 - loss: 0.4364
[1m56/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8518 - loss: 0.4375
[1m62/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8498 - loss: 0.4396
[1m68/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8478 - loss: 0.4425
[1m74/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8434 - loss: 0.4496
[1m80/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8388 - loss: 0.4558
[1m86/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8338 - loss: 0.4622
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.8317 - loss: 0.4651 - val_binary_accuracy: 0.7000 - val_loss: 0.5571
Epoch 6/10

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.1471
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7793 - loss: 0.3706 
[1m13/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7127 - loss: 0.4263
[1m19/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6762 - loss: 0.4597
[1m25/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6681 - loss: 0.4666
[1m31/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6689 - loss: 0.4755
[1m37/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6755 - loss: 0.4795
[1m43/88[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6851 - loss: 0.4781
[1m49/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6921 - loss: 0.4783
[1m57/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6989 - loss: 0.4813
[1m63/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7039 - loss: 0.4817
[1m70/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7101 - loss: 0.4801
[1m77/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7159 - loss: 0.4784
[1m83/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7194 - loss: 0.4790
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7218 - loss: 0.4810 - val_binary_accuracy: 0.9000 - val_loss: 0.4864
Epoch 7/10

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 25ms/step - binary_accuracy: 1.0000 - loss: 0.0669
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8187 - loss: 0.2752
[1m14/88[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8047 - loss: 0.3159
[1m18/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8075 - loss: 0.3292
[1m25/88[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8122 - loss: 0.3484
[1m32/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8178 - loss: 0.3534
[1m38/88[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8170 - loss: 0.3629
[1m45/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8195 - loss: 0.3691
[1m50/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8196 - loss: 0.3742
[1m57/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8190 - loss: 0.3789
[1m64/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8178 - loss: 0.3833 
[1m70/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8163 - loss: 0.3880
[1m76/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8156 - loss: 0.3915
[1m82/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8130 - loss: 0.3959
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.8099 - loss: 0.4009 - val_binary_accuracy: 0.6000 - val_loss: 0.5283
Epoch 8/10

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.3201
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7793 - loss: 0.5335
[1m14/88[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6802 - loss: 0.6071
[1m21/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6930 - loss: 0.5897 
[1m28/88[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7040 - loss: 0.5694
[1m35/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7184 - loss: 0.5478
[1m42/88[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7245 - loss: 0.5439
[1m49/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7245 - loss: 0.5438
[1m56/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7235 - loss: 0.5436
[1m62/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7234 - loss: 0.5418
[1m69/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7236 - loss: 0.5391
[1m76/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7240 - loss: 0.5355
[1m82/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7249 - loss: 0.5321
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7251 - loss: 0.5296 - val_binary_accuracy: 0.9000 - val_loss: 0.4787
Epoch 9/10

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.9662
[1m 7/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5568 - loss: 0.6290    
[1m13/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5988 - loss: 0.6341
[1m21/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6325 - loss: 0.6214 
[1m28/88[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6348 - loss: 0.6226
[1m35/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6388 - loss: 0.6203
[1m41/88[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6498 - loss: 0.6112
[1m47/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6605 - loss: 0.6019
[1m54/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6729 - loss: 0.5916 
[1m59/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6813 - loss: 0.5836
[1m65/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6900 - loss: 0.5746
[1m72/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7004 - loss: 0.5635
[1m79/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7087 - loss: 0.5553
[1m86/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7158 - loss: 0.5481
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7189 - loss: 0.5451 - val_binary_accuracy: 0.8000 - val_loss: 0.4931
Epoch 10/10

[1m 1/88[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2692
[1m 8/88[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8716 - loss: 0.3366 
[1m13/88[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7787 - loss: 0.4616
[1m20/88[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7445 - loss: 0.5027 
[1m27/88[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7303 - loss: 0.5125
[1m34/88[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7325 - loss: 0.5060
[1m40/88[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7366 - loss: 0.4977
[1m46/88[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7380 - loss: 0.4908
[1m52/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7383 - loss: 0.4873
[1m59/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7418 - loss: 0.4811
[1m66/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7449 - loss: 0.4749
[1m73/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7482 - loss: 0.4687
[1m79/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7514 - loss: 0.4644
[1m85/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7553 - loss: 0.4600
[1m88/88[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7578 - loss: 0.4576 - val_binary_accuracy: 0.7000 - val_loss: 0.5166

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 125ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 125ms/step
predicted [0.26107377 0.25026092 0.53635454 0.8518903  0.6483643  0.7874134
 0.8480321  0.24680702 0.49032846 0.525437   0.3194832  0.3319267
 0.0478299  0.8869547  0.34184465 0.6847408  0.15681429 0.27397883
 0.4750895  0.43096367 0.33257878 0.5370922  0.4940272  0.5902035
 0.31709686]
predicted [0 0 1 1 1 1 1 0 0 1 0 0 0 1 0 1 0 0 0 0 0 1 0 1 0]
expected [ True  True  True False False False  True  True False  True  True False
 False  True False  True False  True  True  True False False  True  True
  True]
accuracy: 0.48
confusion matrix: 
[[6 4]
 [9 6]]
              precision    recall  f1-score   support

       False       0.40      0.60      0.48        10
        True       0.60      0.40      0.48        15

    accuracy                           0.48        25
   macro avg       0.50      0.50      0.48        25
weighted avg       0.52      0.48      0.48        25

macro avg f1-score: 0.48
macro avg (UAR): 0.5
Sensitivity:  0.6
Specificity:  0.4
g-mean:  0.4898979485566356
-------- Model Performance ----------: 
accuracy:  [0.64 0.72 0.52 0.48 0.6  0.76 0.6  0.64 0.56 0.48]
gmean:  [0.56568542 0.61101009 0.51639778 0.48304589 0.48989795 0.74833148
 0.57735027 0.64807407 0.54772256 0.48989795]
f1_score:  [0.59893048 0.66666667 0.51298701 0.47665056 0.54044118 0.75
 0.58333333 0.63768116 0.54844007 0.48      ]
UAR:  [0.6        0.66666667 0.51666667 0.48333333 0.55       0.75
 0.58333333 0.65       0.55       0.5       ]
Cohen Kappa score:  [ 0.21052632  0.36363636  0.03225806 -0.03174603  0.10714286  0.5
  0.16666667  0.28571429  0.09836066  0.        ]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  12.992
step (sec):  9.744
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  3.248
Number of windows / instances:  123
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.675 0.844 0.846 0.833 0.834 0.558 0.779 0.797 0.766 0.774]
 [0.7   0.846 0.845 0.807 0.837 0.567 0.784 0.804 0.788 0.777]
 [0.679 0.863 0.876 0.8   0.864 0.53  0.778 0.806 0.844 0.779]
 [0.268 0.622 0.699 0.411 0.577 0.    0.5   0.651 0.    0.394]
 [0.534 0.756 0.77  0.753 0.753 0.475 0.715 0.787 0.663 0.729]
 [0.173 0.585 0.6   0.568 0.58  0.2   0.593 0.656 0.522 0.583]
 [0.    0.5   0.618 0.    0.382 0.    0.5   0.651 0.    0.394]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[0.5876666  0.79380952 0.79487179 0.78255644 0.78982739 0.44013657
  0.71305556 0.78782051 0.65851922 0.70338122]
 [0.43240171 0.72119048 0.71410256 0.71434038 0.71162504 0.40642271
  0.69392857 0.73910256 0.64564211 0.69012589]
 [0.50055204 0.74184524 0.77307692 0.69447395 0.73598936 0.42795083
  0.71309524 0.71602564 0.68655643 0.70318126]
 [0.25406336 0.625      0.64102564 0.56078616 0.60895098 0.10049789
  0.55347222 0.67307692 0.3597405  0.53438719]
 [0.54249566 0.76910714 0.79230769 0.75220449 0.76863679 0.57771084
  0.78809524 0.79038462 0.77150471 0.78303696]
 [0.48001107 0.74047619 0.73974359 0.73649363 0.73835331 0.53220439
  0.76714286 0.76538462 0.7497377  0.75722496]
 [0.38592526 0.69196429 0.70833333 0.67643408 0.68912723 0.15941176
  0.5797619  0.57884615 0.57181362 0.57589744]
 [0.85964286 0.91875    0.94166667 0.91152756 0.92877568 0.60237054
  0.80392857 0.81217949 0.79924804 0.80026696]
 [0.31392978 0.64791667 0.73141026 0.58745745 0.64643858 0.28323151
  0.63333333 0.70641026 0.55386418 0.62599415]
 [0.2303843  0.61666667 0.61602564 0.59766058 0.60560148 0.12936671
  0.56238095 0.57692308 0.52831195 0.55341542]
 [0.14737554 0.57875    0.61730769 0.48642339 0.56277001 0.39320092
  0.69607143 0.72307692 0.62748499 0.68052171]
 [0.66576128 0.82571429 0.84423077 0.81406335 0.82975299 0.5617292
  0.77392857 0.80512821 0.73751672 0.77048231]
 [0.3092581  0.65845238 0.66474359 0.61555699 0.64575403 0.06839431
  0.53488095 0.55320513 0.50727531 0.52779901]
 [0.18020617 0.59069444 0.63269231 0.53362238 0.583345   0.35733856
  0.67880952 0.68397436 0.65609399 0.66992296]
 [0.3087315  0.65694444 0.71730769 0.55299656 0.64125891 0.22812265
  0.62232143 0.65064103 0.55397284 0.60039513]
 [0.23590766 0.61357143 0.64230769 0.58011152 0.60888429 0.4280074
  0.70555556 0.75833333 0.67011853 0.70733052]
 [0.58505357 0.79107143 0.79807692 0.78427332 0.79082345 0.34433132
  0.66696429 0.69871795 0.62636287 0.66120215]
 [0.15521563 0.59681818 0.79807692 0.27888544 0.55616813 0.38619409
  0.69392857 0.69871795 0.68512143 0.6887944 ]
 [0.22173829 0.60972222 0.69102564 0.49502769 0.59614766 0.4029952
  0.7        0.70769231 0.68277806 0.69475539]
 [0.33567109 0.65625    0.7224359  0.57793634 0.6549579  0.29945836
  0.64660714 0.7        0.58789785 0.6366725 ]
 [0.05559262 0.535      0.68269231 0.3098334  0.51078427 0.00761926
  0.50357143 0.50320513 0.4882146  0.49594184]
 [0.34072527 0.66904762 0.67115385 0.64987332 0.66254787 0.42773784
  0.70607143 0.73974359 0.68621407 0.70998674]
 [0.12105231 0.55886905 0.58012821 0.53426817 0.55530747 0.25508844
  0.61944444 0.74871795 0.50395037 0.62338346]
 [0.15714286 0.57880952 0.57948718 0.57001034 0.57453879 0.50545124
  0.75142857 0.76346154 0.72279383 0.74218629]
 [0.34308327 0.67380952 0.6775641  0.65699506 0.6651426  0.59394958
  0.79761905 0.79679487 0.78734992 0.79307859]
 [0.2499083  0.62238095 0.63589744 0.57025636 0.60361547 0.39450912
  0.69809524 0.69935897 0.68999235 0.69337496]
 [0.40327284 0.68819444 0.75833333 0.65153527 0.69461202 0.47679063
  0.73690476 0.74102564 0.72051615 0.73217075]
 [0.22625479 0.60964286 0.63397436 0.59785826 0.60963723 0.57008841
  0.78142857 0.79230769 0.76894617 0.77908263]
 [0.12022448 0.56053571 0.59935897 0.46767035 0.54527327 0.30434959
  0.65       0.65897436 0.62545332 0.64338496]
 [0.67529309 0.84357143 0.84551282 0.83275064 0.83431055 0.55769266
  0.77875    0.79679487 0.76564228 0.77423243]]
KNN mean:
[0.34748471 0.6728192  0.70816239 0.61912943 0.66496526 0.37407842
 0.68501918 0.71220085 0.6472878  0.67838701]
---------------------------
---------------------------
DT performance:
[[0.60835204 0.79380952 0.79423077 0.79832778 0.79102231 0.39170333
  0.71361111 0.78910256 0.66606678 0.70616275]
 [0.583898   0.73571429 0.73974359 0.73442126 0.72740534 0.57054095
  0.77535714 0.78846154 0.77282837 0.77663697]
 [0.43387616 0.65875    0.66666667 0.66173392 0.64341605 0.33284092
  0.65       0.65128205 0.60312116 0.63904984]
 [0.2892415  0.61857143 0.62564103 0.59724847 0.60657537 0.33790888
  0.64791667 0.70641026 0.51681691 0.62230134]
 [0.28129332 0.67214286 0.68397436 0.6499204  0.6577536  0.51812325
  0.75833333 0.75833333 0.69665203 0.75607393]
 [0.34822821 0.66785714 0.66858974 0.67299614 0.66429362 0.23543423
  0.67690476 0.675      0.64797089 0.66681513]
 [0.32515496 0.67660714 0.675      0.63306448 0.66695665 0.2410946
  0.58333333 0.5775641  0.54321579 0.56524815]
 [0.71778172 0.86041667 0.87115385 0.87926051 0.85352273 0.34719258
  0.65946429 0.68141026 0.62789798 0.65309206]
 [0.22611454 0.59166667 0.67371795 0.52106763 0.5757716  0.40580858
  0.70069444 0.73205128 0.63979011 0.66822012]
 [0.21399034 0.59333333 0.58717949 0.55187343 0.57907315 0.14239853
  0.58642857 0.59230769 0.59714777 0.5778527 ]
 [0.15890048 0.57446429 0.59038462 0.51997322 0.56923758 0.37644167
  0.70232143 0.71538462 0.67092018 0.69210384]
 [0.54148211 0.76928571 0.77179487 0.7284949  0.75715188 0.45030029
  0.72732143 0.73205128 0.71436819 0.7151852 ]
 [0.41055969 0.72619048 0.73717949 0.68653531 0.72318126 0.16071809
  0.55892857 0.56794872 0.56817829 0.54385275]
 [0.15783037 0.53819444 0.58525641 0.48185304 0.52612183 0.32745029
  0.62738095 0.63397436 0.62913031 0.62234432]
 [0.25614374 0.65347222 0.68525641 0.63532776 0.6422444  0.18285818
  0.59678571 0.61730769 0.54623475 0.57403322]
 [0.16340919 0.59017857 0.61794872 0.48979666 0.5689805  0.28048598
  0.69305556 0.73333333 0.64256682 0.68458443]
 [0.62240319 0.8322619  0.83076923 0.82406523 0.82593087 0.38822755
  0.69196429 0.70833333 0.68873971 0.69021439]
 [0.25471694 0.63772727 0.7974359  0.35007043 0.61659363 0.33843415
  0.6685119  0.68012821 0.66012572 0.65751001]
 [0.56733851 0.78958333 0.82179487 0.79348909 0.78653509 0.46129763
  0.77166667 0.77307692 0.68013203 0.76701937]
 [0.33912382 0.65194444 0.70064103 0.60161538 0.65226307 0.21582099
  0.60321429 0.63461538 0.54114708 0.58854729]
 [0.16540897 0.59055556 0.68076923 0.47009826 0.56932953 0.00691337
  0.49404762 0.49679487 0.49286266 0.48846931]
 [0.26330929 0.59761905 0.59807692 0.58594714 0.59119714 0.25501726
  0.60196429 0.61153846 0.65405681 0.58473177]
 [0.33905235 0.69922619 0.71538462 0.66140725 0.69782218 0.25370893
  0.58611111 0.66538462 0.55117512 0.56377342]
 [0.13317745 0.56571429 0.56730769 0.5505695  0.5500083  0.5489455
  0.72404762 0.73141026 0.7153542  0.72352814]
 [0.30743262 0.65095238 0.64294872 0.64888767 0.6374692  0.47020715
  0.72619048 0.72628205 0.66742486 0.72091492]
 [0.21659898 0.62190476 0.62820513 0.5333772  0.59661189 0.47310734
  0.67619048 0.68333333 0.66312183 0.66813589]
 [0.34279608 0.64236111 0.71410256 0.54552909 0.63144559 0.54444805
  0.77261905 0.77307692 0.75157056 0.76870047]
 [0.25427171 0.58625    0.61089744 0.61719535 0.58130481 0.45368974
  0.70321429 0.72564103 0.68621185 0.70315746]
 [0.21596091 0.61160714 0.62179487 0.53956362 0.60122336 0.20753931
  0.62380952 0.6275641  0.60936172 0.61412723]
 [0.6998693  0.84607143 0.84487179 0.80732384 0.83667533 0.56728619
  0.78375    0.80448718 0.78849391 0.77687675]]
DT mean:
[0.34792388 0.66814779 0.69162393 0.62570113 0.6575706  0.34953145
 0.66950463 0.68645299 0.64108948 0.65930877]
---------------------------
---------------------------
RF performance:
[[0.56921569 0.86142857 0.86025641 0.76892126 0.85918748 0.22799867
  0.68166667 0.78141026 0.5523792  0.66650398]
 [0.47159117 0.74380952 0.74807692 0.6602531  0.73487426 0.52009553
  0.77625    0.78846154 0.77758178 0.7723416 ]
 [0.42499338 0.7727381  0.78333333 0.7204743  0.77262581 0.42417475
  0.66428571 0.66858974 0.62730849 0.64971534]
 [0.39031556 0.70952381 0.71602564 0.62404247 0.70874944 0.33583187
  0.6625     0.75384615 0.48968595 0.65362788]
 [0.56325439 0.72696429 0.75128205 0.73141551 0.72373981 0.56716501
  0.71785714 0.71923077 0.78369386 0.71348984]
 [0.44750144 0.73333333 0.73141026 0.65044725 0.72673243 0.44606315
  0.75880952 0.75833333 0.74548348 0.75109276]
 [0.3288509  0.65303571 0.675      0.60292478 0.64508545 0.24103178
  0.64285714 0.64166667 0.62271645 0.63539572]
 [0.81355242 0.88958333 0.9025641  0.90098139 0.88731793 0.52625498
  0.77767857 0.80448718 0.79124049 0.78096905]
 [0.34026847 0.69166667 0.74807692 0.50144753 0.67302896 0.10020508
  0.67152778 0.73205128 0.56563692 0.67499988]
 [0.3331739  0.64119048 0.64102564 0.6125897  0.62863845 0.28412873
  0.63547619 0.65       0.68039023 0.62808578]
 [0.23673589 0.575      0.60064103 0.5641589  0.56773734 0.50807533
  0.75339286 0.77371795 0.66758726 0.75442577]
 [0.5728482  0.77892857 0.78910256 0.76469726 0.76807568 0.50918258
  0.7675     0.78782051 0.73488308 0.77080907]
 [0.34304018 0.72875    0.73782051 0.63150129 0.72553516 0.23360599
  0.59083333 0.60705128 0.57858774 0.58265801]
 [0.12374067 0.65763889 0.71538462 0.55953917 0.63983039 0.39133696
  0.72380952 0.72564103 0.64123376 0.71591395]
 [0.20945185 0.64166667 0.70128205 0.54121799 0.6419071  0.17487254
  0.61678571 0.64294872 0.46164716 0.597733  ]
 [0.22176801 0.56339286 0.58717949 0.64741276 0.56346945 0.38708479
  0.70486111 0.75833333 0.66455336 0.71012114]
 [0.66643109 0.87422619 0.87820513 0.84099088 0.87313561 0.49184938
  0.81517857 0.81923077 0.75756422 0.8119814 ]
 [0.06559356 0.66181818 0.83910256 0.35298221 0.63334497 0.44610844
  0.75107143 0.76346154 0.6972137  0.74867467]
 [0.36576575 0.69375    0.78076923 0.6393176  0.69402378 0.49052565
  0.70880952 0.72435897 0.7986495  0.70285839]
 [0.40249293 0.67083333 0.70064103 0.69228086 0.66016106 0.19417647
  0.70392857 0.725      0.49977289 0.68959289]
 [0.04453097 0.55555556 0.69230769 0.33421325 0.53633202 0.09260194
  0.50238095 0.50384615 0.41429001 0.49013251]
 [0.23074783 0.6547619  0.65512821 0.73972059 0.64948107 0.2982339
  0.67339286 0.70705128 0.62731577 0.66569119]
 [0.23737845 0.59113095 0.62051282 0.57035608 0.5779102  0.27526069
  0.59722222 0.73269231 0.56504855 0.58566805]
 [0.43616882 0.69095238 0.69230769 0.64154041 0.68739011 0.51124532
  0.73952381 0.74679487 0.70275878 0.7308436 ]
 [0.28836996 0.65142857 0.66089744 0.65916997 0.64345082 0.47954943
  0.80357143 0.80448718 0.73784525 0.80032842]
 [0.24228176 0.53761905 0.53974359 0.65476202 0.51933952 0.41906299
  0.755      0.75769231 0.70516905 0.75305472]
 [0.58035617 0.74166667 0.79038462 0.69509037 0.74252752 0.5713319
  0.75595238 0.75576923 0.73900679 0.75018565]
 [0.16362809 0.53535714 0.56858974 0.49810029 0.52231466 0.4736181
  0.75964286 0.77307692 0.72384106 0.75885259]
 [0.33919232 0.64642857 0.67628205 0.6540316  0.64339116 0.38668847
  0.61333333 0.62564103 0.60701568 0.60487395]
 [0.67896985 0.86321429 0.87628205 0.79988545 0.8642612  0.52987761
  0.7775     0.80576923 0.84405006 0.77850466]]
RF mean:
[0.37107366 0.69124645 0.72198718 0.64181554 0.68378663 0.3845746
 0.70341997 0.72794872 0.66013835 0.69763751]
---------------------------
---------------------------
SVM performance:
[[ 0.09871614  0.54619048  0.56923077  0.24971365  0.44223193  0.
   0.5         0.74807692  0.          0.42786561]
 [ 0.30562377  0.6447619   0.6775641   0.52210753  0.609588    0.16607137
   0.5725      0.66538462  0.26593383  0.50319131]
 [ 0.29782351  0.635       0.69166667  0.51190331  0.60314929  0.35022614
   0.675       0.67564103  0.65861565  0.66788628]
 [ 0.18208333  0.58690476  0.61153846  0.47177872  0.55124457  0.
   0.5         0.71602564  0.          0.41701299]
 [ 0.53184929  0.755       0.79935897  0.70757203  0.75318898  0.49634146
   0.74761905  0.75192308  0.72315074  0.73869048]
 [ 0.20580794  0.60119048  0.60897436  0.52176962  0.56767194  0.49589509
   0.74880952  0.74871795  0.73324478  0.74157592]
 [ 0.          0.5         0.59294872  0.          0.37218045  0.21012396
   0.60357143  0.60769231  0.51526702  0.56288664]
 [ 0.62768288  0.78819444  0.85320513  0.75621812  0.80764835  0.28325866
   0.62785714  0.70576923  0.47080302  0.60538098]
 [ 0.          0.5         0.69102564  0.          0.40844156  0.
   0.5         0.68269231  0.          0.40558442]
 [ 0.00645161  0.50285714  0.53653846  0.04472136  0.36406433 -0.03333333
   0.48333333  0.51987179  0.          0.34052632]
 [ 0.          0.5         0.61794872  0.          0.38165414 -0.01612903
   0.49285714  0.59294872  0.          0.37182957]
 [ 0.          0.5         0.60961538  0.          0.37849624  0.
   0.5         0.60961538  0.          0.37849624]
 [ 0.0686907   0.53        0.60192308  0.13416408  0.4219883   0.
   0.5         0.5775641   0.          0.36598997]
 [ 0.          0.5         0.65897436  0.          0.39709957  0.02903226
   0.51285714  0.54487179  0.08944272  0.38277778]
 [ 0.          0.5         0.68269231  0.          0.40558442  0.
   0.5         0.63461538  0.          0.38796992]
 [ 0.          0.5         0.60128205  0.          0.37533835  0.
   0.5         0.67435897  0.          0.40272727]
 [ 0.          0.5         0.5775641   0.          0.36598997  0.
   0.5         0.59294872  0.          0.37218045]
 [ 0.          0.5         0.84551282  0.          0.45805336  0.
   0.5         0.58525641  0.          0.36908521]
 [ 0.          0.5         0.69935897  0.          0.4112987   0.
   0.5         0.55320513  0.          0.35587719]
 [ 0.          0.5         0.65897436  0.          0.39709957  0.
   0.5         0.63461538  0.          0.38796992]
 [ 0.          0.5         0.76410256  0.          0.43303219 -0.01279601
   0.49285714  0.48846154  0.08648755  0.35460925]
 [ 0.07186279  0.53452381  0.54294872  0.3809917   0.47044732  0.
   0.5         0.62628205  0.          0.38481203]
 [ 0.          0.5         0.5775641   0.          0.36598997  0.
   0.5         0.75576923  0.          0.43043478]
 [ 0.          0.5         0.51987179  0.          0.34184211  0.29401342
   0.63619048  0.68397436  0.49133218  0.59734354]
 [ 0.          0.5         0.56153846  0.          0.35938596  0.44677786
   0.72380952  0.72435897  0.70718193  0.71699828]
 [ 0.          0.5         0.53653846  0.          0.34885965  0.29182574
   0.64        0.66794872  0.57503373  0.6224035 ]
 [ 0.          0.5         0.69102564  0.          0.40844156  0.54134727
   0.7702381   0.77179487  0.76212225  0.76775308]
 [ 0.          0.5         0.63461538  0.          0.38796992  0.
   0.5         0.61794872  0.          0.38165414]
 [ 0.          0.5         0.61794872  0.          0.38165414  0.
   0.5         0.55320513  0.          0.35587719]
 [ 0.26799415  0.62160714  0.69871795  0.41105264  0.57729508  0.
   0.5         0.65128205  0.          0.39428571]]
SVM mean:
[0.08881954 0.54154101 0.64435897 0.15706643 0.45156433 0.11808849
 0.55758333 0.64542735 0.20262051 0.47305587]
---------------------------
---------------------------
GBM performance:
[[ 0.58624211  0.79261905  0.79487179  0.7807091   0.78891442  0.21626984
   0.58944444  0.78205128  0.27409975  0.55901407]
 [ 0.3617352   0.66666667  0.6974359   0.62404276  0.65165775  0.46395239
   0.72142857  0.76346154  0.68051923  0.72235761]
 [ 0.41699446  0.7010119   0.73333333  0.65662988  0.69416717  0.26540079
   0.64047619  0.64166667  0.60307687  0.62331082]
 [ 0.26665684  0.62238095  0.63269231  0.61817281  0.61461316  0.01729323
   0.50208333  0.69871795  0.09677072  0.44564441]
 [ 0.56407086  0.76125     0.80705128  0.70471444  0.75872782  0.462652
   0.73095238  0.73333333  0.71455879  0.72548576]
 [ 0.2707536   0.62857143  0.62820513  0.6072447   0.62103341  0.46979258
   0.73238095  0.74038462  0.71050113  0.72708515]
 [ 0.29222403  0.66875     0.69935897  0.59918674  0.66077756  0.28453105
   0.66785714  0.66602564  0.63191799  0.66084749]
 [ 0.7663961   0.87708333  0.9025641   0.86379846  0.88059641  0.44023202
   0.71357143  0.75        0.69011447  0.71639122]
 [ 0.2301111   0.59097222  0.73141026  0.37089121  0.56278195  0.04635365
   0.52152778  0.675       0.15450574  0.45699534]
 [ 0.2083867   0.58428571  0.59102564  0.57301079  0.5721867   0.27665148
   0.6252381   0.64166667  0.59033035  0.61748914]
 [ 0.14292636  0.56607143  0.63205128  0.42011611  0.54196152  0.50426404
   0.74839286  0.77371795  0.71105314  0.74487782]
 [ 0.51370277  0.75517857  0.77948718  0.721759    0.74675006  0.43457197
   0.70875     0.73333333  0.6640621   0.69728606]
 [ 0.34377095  0.67404762  0.70512821  0.59922434  0.65480701  0.04433035
   0.51017857  0.56153846  0.32547028  0.46864066]
 [-0.02150881  0.50375     0.64166667  0.191833    0.44155388  0.27303658
   0.62547619  0.63461538  0.6124998   0.62073246]
 [-0.06030611  0.48402778  0.6275641   0.11215475  0.4229929   0.13700068
   0.56696429  0.63461538  0.37763375  0.52771995]
 [ 0.15086575  0.57446429  0.65064103  0.31510405  0.52300961  0.18411614
   0.58472222  0.69294872  0.39981974  0.56250688]
 [ 0.69796164  0.85678571  0.86410256  0.84536417  0.85569594  0.27334728
   0.63125     0.67307692  0.50590912  0.60614098]
 [ 0.06285714  0.525       0.85320513  0.07071068  0.49337945  0.37685875
   0.68678571  0.70641026  0.63106178  0.67683104]
 [ 0.18599772  0.57916667  0.72435897  0.34961812  0.56196172  0.50263701
   0.74738095  0.76217949  0.74439083  0.74833097]
 [ 0.3280905   0.65625     0.73205128  0.59677897  0.65834335  0.16797441
   0.58375     0.67628205  0.35479358  0.54662047]
 [ 0.          0.5         0.76410256  0.          0.43303219  0.03932612
   0.51190476  0.51410256  0.49860267  0.50238046]
 [ 0.31162465  0.6702381   0.67115385  0.62815595  0.65972805  0.16162779
   0.56535714  0.6525641   0.41906742  0.53954752]
 [ 0.26936293  0.61708333  0.65833333  0.5594968   0.59684757  0.09427224
   0.50611111  0.74807692  0.11250728  0.44711373]
 [ 0.28405869  0.6347619   0.63589744  0.60940667  0.61906843  0.46941502
   0.73380952  0.74679487  0.73266453  0.73261572]
 [ 0.27741738  0.63071429  0.66089744  0.57623823  0.62008202  0.43384968
   0.725       0.72628205  0.71135712  0.72341658]
 [ 0.2662434   0.64119048  0.6525641   0.5505424   0.61780996  0.33082457
   0.68595238  0.7         0.67318901  0.68679779]
 [ 0.27120271  0.61944444  0.74807692  0.42488208  0.60004595  0.50922153
   0.76190476  0.76474359  0.74869725  0.75816032]
 [ 0.16855304  0.57446429  0.66602564  0.41604285  0.55491609  0.49425436
   0.71660714  0.7724359   0.65927856  0.71705741]
 [ 0.24963746  0.63625     0.7         0.50200866  0.61557701  0.36825806
   0.69261905  0.69935897  0.6739949   0.6851784 ]
 [ 0.53361851  0.75607143  0.76987179  0.75322536  0.75254025  0.47524997
   0.715       0.78717949  0.66330191  0.72863863]]
GBM mean:
[0.29798826 0.64495172 0.71183761 0.52136877 0.62585198 0.30725219
 0.64842923 0.70175214 0.54552499 0.63250716]
---------------------------
---------------------------
BDDAE performance:
[[-0.26992743  0.36538462  0.368       0.32174666  0.34779815 -0.00561729
   0.49736842  0.704       0.20073462  0.46733653]
 [ 0.19386014  0.59675325  0.604       0.58802967  0.59452104  0.57059501
   0.78333333  0.796       0.77712794  0.78412407]
 [ 0.00644475  0.50292208  0.524       0.45339655  0.48896577 -0.06560688
   0.46730769  0.472       0.44496739  0.45846863]
 [ 0.09436426  0.54711538  0.548       0.53726933  0.54251388  0.09021094
   0.5484127   0.664       0.3684828   0.52035033]
 [ 0.41604269  0.71145833  0.724       0.704368    0.70503455  0.38880351
   0.69679487  0.692       0.68082915  0.68715268]
 [ 0.48320128  0.74038462  0.744       0.72453395  0.73521313  0.21415406
   0.60705128  0.608       0.59352703  0.60088209]
 [ 0.10824107  0.55666667  0.572       0.52846197  0.5457426  -0.08989668
   0.45512821  0.456       0.43749421  0.44680075]
 [ 0.44877108  0.7125      0.78        0.66940197  0.71676994  0.13756968
   0.57        0.588       0.54545983  0.56154227]
 [ 0.46928706  0.71102941  0.796       0.65270883  0.72256319  0.36906815
   0.65330882  0.776       0.54775831  0.65964728]
 [-0.1420709   0.42916667  0.432       0.41923238  0.42589759  0.11266512
   0.55576923  0.56        0.54112342  0.55068897]
 [-0.07091897  0.46166667  0.496       0.40053785  0.44503362  0.174232
   0.58166667  0.624       0.53494247  0.57709018]
 [ 0.00837646  0.50333333  0.548       0.42381538  0.48301851  0.3812918
   0.69666667  0.692       0.68887357  0.68512939]
 [ 0.06567827  0.53311688  0.536       0.50604918  0.5195909   0.25380804
   0.62662338  0.632       0.59723072  0.6136097 ]
 [ 0.07940212  0.5375      0.604       0.46060279  0.52488313  0.23529384
   0.61698718  0.62        0.60275532  0.61161979]
 [-0.09270045  0.45477941  0.56        0.2992196   0.43843884  0.06464553
   0.5375      0.576       0.475906    0.51811612]
 [-0.02079094  0.49333333  0.528       0.38213146  0.46827347  0.20418922
   0.58602941  0.716       0.42351696  0.56779607]
 [ 0.47689844  0.74220779  0.744       0.71280174  0.72784607  0.35295143
   0.66666667  0.716       0.59315642  0.65454452]
 [ 0.26648462  0.61428571  0.828       0.46390754  0.62435593  0.04555168
   0.52166667  0.552       0.48956589  0.51621405]
 [ 0.19327278  0.58970588  0.676       0.50601684  0.58755057  0.18801079
   0.59285714  0.604       0.57323653  0.58724141]
 [ 0.04915968  0.52256944  0.588       0.45124047  0.51272744 -0.14023626
   0.43645833  0.54        0.15004909  0.38297247]
 [ 0.04579844  0.51535088  0.74        0.20082688  0.48968736  0.04248827
   0.52115385  0.524       0.50922944  0.51655793]
 [ 0.01898298  0.50929487  0.508       0.49706985  0.50248295  0.00273568
   0.50138889  0.592       0.34818577  0.47174853]
 [ 0.12608355  0.56428571  0.56        0.55591872  0.55604321  0.41981065
   0.68201754  0.82        0.61461171  0.70362386]
 [ 0.16854128  0.58461538  0.584       0.58084686  0.58224333  0.2409256
   0.61883117  0.632       0.6060169   0.61774802]
 [ 0.04510803  0.5237013   0.544       0.44693596  0.50389761  0.00774272
   0.50384615  0.504       0.4954714   0.4995862 ]
 [ 0.28918413  0.64326923  0.648       0.62483052  0.63721397  0.31343439
   0.6538961   0.668       0.63480896  0.65112481]
 [ 0.45903786  0.71948529  0.776       0.69746005  0.72719116  0.54102981
   0.76955128  0.772       0.75672065  0.76578156]
 [-0.00200973  0.49756944  0.556       0.4404647   0.49120061  0.35364422
   0.66666667  0.712       0.6115708   0.66202107]
 [ 0.22040107  0.61        0.62        0.60224267  0.60690176  0.34409471
   0.66785714  0.688       0.63428567  0.66125838]
 [ 0.17325592  0.585       0.6         0.56774135  0.57951305  0.19999688
   0.59270833  0.656       0.52223002  0.58317534]]
BDDAE mean:
[0.14358198 0.56928172 0.6112     0.51399366 0.56110378 0.19825289
 0.59585046 0.63853333 0.53332897 0.58613177]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.52820513 0.         0.34535088 0.
  0.5        0.74807692 0.         0.42786561]
 [0.         0.5        0.56153846 0.         0.35938596 0.
  0.5        0.60961538 0.         0.37849624]
 [0.         0.5        0.5775641  0.         0.36598997 0.
  0.5        0.51153846 0.         0.33833333]
 [0.         0.5        0.53653846 0.         0.34885965 0.
  0.5        0.71602564 0.         0.41701299]
 [0.         0.5        0.62628205 0.         0.38481203 0.
  0.5        0.51153846 0.         0.33833333]
 [0.         0.5        0.51153846 0.         0.33833333 0.
  0.5        0.53653846 0.         0.34885965]
 [0.         0.5        0.59294872 0.         0.37218045 0.
  0.5        0.51153846 0.         0.33833333]
 [0.         0.5        0.67435897 0.         0.40272727 0.
  0.5        0.61794872 0.         0.38165414]
 [0.         0.5        0.69102564 0.         0.40844156 0.
  0.5        0.68269231 0.         0.40558442]
 [0.         0.5        0.53653846 0.         0.34885965 0.
  0.5        0.53653846 0.         0.34885965]
 [0.         0.5        0.61794872 0.         0.38165414 0.
  0.5        0.60128205 0.         0.37533835]
 [0.         0.5        0.60961538 0.         0.37849624 0.
  0.5        0.60961538 0.         0.37849624]
 [0.         0.5        0.5775641  0.         0.36598997 0.
  0.5        0.5775641  0.         0.36598997]
 [0.         0.5        0.65897436 0.         0.39709957 0.
  0.5        0.53653846 0.         0.34885965]
 [0.         0.5        0.68269231 0.         0.40558442 0.
  0.5        0.63461538 0.         0.38796992]
 [0.         0.5        0.60128205 0.         0.37533835 0.
  0.5        0.67435897 0.         0.40272727]
 [0.         0.5        0.5775641  0.         0.36598997 0.
  0.5        0.59294872 0.         0.37218045]
 [0.         0.5        0.84551282 0.         0.45805336 0.
  0.5        0.58525641 0.         0.36908521]
 [0.         0.5        0.69935897 0.         0.4112987  0.
  0.5        0.55320513 0.         0.35587719]
 [0.         0.5        0.65897436 0.         0.39709957 0.
  0.5        0.63461538 0.         0.38796992]
 [0.         0.5        0.76410256 0.         0.43303219 0.
  0.5        0.50384615 0.         0.33491228]
 [0.         0.5        0.51153846 0.         0.33833333 0.
  0.5        0.62628205 0.         0.38481203]
 [0.         0.5        0.5775641  0.         0.36598997 0.
  0.5        0.75576923 0.         0.43043478]
 [0.         0.5        0.51987179 0.         0.34184211 0.
  0.5        0.56987179 0.         0.36289474]
 [0.         0.5        0.56153846 0.         0.35938596 0.
  0.5        0.48846154 0.         0.32807018]
 [0.         0.5        0.53653846 0.         0.34885965 0.
  0.5        0.55320513 0.         0.35587719]
 [0.         0.5        0.69102564 0.         0.40844156 0.
  0.5        0.51153846 0.         0.33833333]
 [0.         0.5        0.63461538 0.         0.38796992 0.
  0.5        0.61794872 0.         0.38165414]
 [0.         0.5        0.61794872 0.         0.38165414 0.
  0.5        0.55320513 0.         0.35587719]
 [0.         0.5        0.61794872 0.         0.38165414 0.
  0.5        0.65128205 0.         0.39428571]]
DUMMY mean:
[0.         0.5        0.6132906  0.         0.3786236  0.
 0.5        0.59378205 0.         0.37116595]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_36
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.347 0.673 0.708 0.619 0.665 0.374 0.685 0.712 0.647 0.678]
 [0.348 0.668 0.692 0.626 0.658 0.35  0.67  0.686 0.641 0.659]
 [0.371 0.691 0.722 0.642 0.684 0.385 0.703 0.728 0.66  0.698]
 [0.089 0.542 0.644 0.157 0.452 0.118 0.558 0.645 0.203 0.473]
 [0.298 0.645 0.712 0.521 0.626 0.307 0.648 0.702 0.546 0.633]
 [0.144 0.569 0.611 0.514 0.561 0.198 0.596 0.639 0.533 0.586]
 [0.    0.5   0.613 0.    0.379 0.    0.5   0.594 0.    0.371]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.19  0.093 0.086 0.139 0.099 0.161 0.08  0.077 0.103 0.082]
 [0.169 0.087 0.084 0.119 0.09  0.139 0.071 0.073 0.073 0.075]
 [0.183 0.096 0.092 0.121 0.1   0.138 0.072 0.07  0.107 0.076]
 [0.162 0.076 0.086 0.242 0.116 0.184 0.091 0.074 0.288 0.137]
 [0.194 0.096 0.077 0.215 0.111 0.161 0.084 0.064 0.194 0.101]
 [0.195 0.095 0.11  0.127 0.101 0.179 0.087 0.092 0.138 0.094]
 [0.    0.    0.076 0.    0.028 0.    0.    0.071 0.    0.027]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 55.  14.  12.  22.  15.  43.  12.  11.  16.  12.]
 [ 49.  13.  12.  19.  14.  40.  11.  11.  11.  11.]
 [ 49.  14.  13.  19.  15.  36.  10.  10.  16.  11.]
 [182.  14.  13. 154.  26. 156.  16.  11. 142.  29.]
 [ 65.  15.  11.  41.  18.  52.  13.   9.  36.  16.]
 [136.  17.  18.  25.  18.  90.  15.  14.  26.  16.]
 [  0.   0.  12.   0.   7.   0.   0.  12.   0.   7.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_36
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  12.992
step (sec):  9.744
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  3.248
Number of windows / instances:  123
Elapsed time: 617.3760301589966 minutes
Elapsed time: 10.289600502649943 hours
