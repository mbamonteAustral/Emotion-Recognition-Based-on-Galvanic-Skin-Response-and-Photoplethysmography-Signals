2024-05-10 20:57:40.719117: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-10 20:57:41.483458: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-10 20:57:43.106033: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  14.0
step (sec):  14.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_37_1\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

: 0.7510 - loss: 0.4662
[1m40/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7535 - loss: 0.4624
[1m47/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7544 - loss: 0.4594
[1m54/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7525 - loss: 0.4602 
[1m60/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7523 - loss: 0.4615
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7520 - loss: 0.4628 - val_binary_accuracy: 0.7143 - val_loss: 0.4873
Epoch 7/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2759
[1m 8/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9301 - loss: 0.4063 
[1m15/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7912 - loss: 0.4939
[1m21/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7503 - loss: 0.5216
[1m28/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7396 - loss: 0.5274
[1m35/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7415 - loss: 0.5255
[1m42/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7460 - loss: 0.5212
[1m48/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7476 - loss: 0.5224
[1m55/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7497 - loss: 0.5234
[1m62/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7530 - loss: 0.5215
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7543 - loss: 0.5204 - val_binary_accuracy: 0.7143 - val_loss: 0.5640
Epoch 8/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0355
[1m 8/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9665 - loss: 0.1312 
[1m14/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9020 - loss: 0.2148
[1m20/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8769 - loss: 0.2576
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8515 - loss: 0.2950
[1m33/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8404 - loss: 0.3177
[1m40/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8399 - loss: 0.3274
[1m47/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8425 - loss: 0.3305
[1m54/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8408 - loss: 0.3353
[1m60/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8379 - loss: 0.3407
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.8370 - loss: 0.3443 - val_binary_accuracy: 0.8571 - val_loss: 0.4794
Epoch 9/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.4160
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 1.0000 - loss: 0.3065
[1m14/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 1.0000 - loss: 0.3150
[1m19/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9844 - loss: 0.3252
[1m25/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9774 - loss: 0.3247
[1m32/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9728 - loss: 0.3212
[1m39/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9607 - loss: 0.3274 
[1m45/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9498 - loss: 0.3348
[1m51/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9391 - loss: 0.3418
[1m57/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9320 - loss: 0.3465
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9257 - loss: 0.3514
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.9246 - loss: 0.3523 - val_binary_accuracy: 0.8571 - val_loss: 0.4407
Epoch 10/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 27ms/step - binary_accuracy: 1.0000 - loss: 0.0519
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9389 - loss: 0.1575
[1m11/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8874 - loss: 0.2483
[1m17/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8778 - loss: 0.3031
[1m24/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8854 - loss: 0.3172
[1m31/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8951 - loss: 0.3211
[1m37/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8988 - loss: 0.3212
[1m44/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9032 - loss: 0.3198
[1m50/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9035 - loss: 0.3202
[1m57/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9018 - loss: 0.3215
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8985 - loss: 0.3250
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.8981 - loss: 0.3255 - val_binary_accuracy: 0.7143 - val_loss: 0.4952

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
predicted [0.722851   0.4032609  0.9772877  0.85029864 0.8413381  0.54311347
 0.8725131  0.86603594 0.702234   0.6615895  0.78606147 0.8002168
 0.8200501  0.21574968 0.4040104  0.81909716 0.06680744 0.02976577]
predicted [1 0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0 0]
expected [ True False  True  True  True  True False  True  True  True False False
 False  True  True False  True False]
accuracy: 0.5555555555555556
confusion matrix: 
[[2 5]
 [3 8]]
              precision    recall  f1-score   support

       False       0.40      0.29      0.33         7
        True       0.62      0.73      0.67        11

    accuracy                           0.56        18
   macro avg       0.51      0.51      0.50        18
weighted avg       0.53      0.56      0.54        18

macro avg f1-score: 0.5
macro avg (UAR): 0.5064935064935066
Sensitivity:  0.2857142857142857
Specificity:  0.7272727272727273
g-mean:  0.4558423058385518
-------- Model Performance ----------: 
accuracy:  [0.77777778 0.61111111 0.55555556 0.66666667 0.83333333 0.72222222
 0.83333333 0.66666667 0.55555556 0.        ]
gmean:  [0.78954203 0.48349378 0.34188173 0.68376346 0.75592895 0.68376346
 0.80582296 0.59215653 0.45584231 0.        ]
f1_score:  [0.775      0.54181818 0.44615385 0.66666667 0.80363636 0.69899666
 0.81939799 0.625      0.5        0.        ]
UAR:  [0.79220779 0.55194805 0.48051948 0.7012987  0.78571429 0.69480519
 0.81168831 0.62337662 0.50649351 0.        ]
Cohen Kappa score:  [ 0.55555556  0.11267606 -0.04347826  0.36470588  0.61971831  0.4
  0.64        0.26027397  0.01369863  0.        ]
Split Repetition number:  9
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [52 42 65 69 70 39  7 67 80 57  5 82 83  1 75 10 41  3 54 77 23 16 29 76
 58 22 66 15 45 35 36 85 62 44 81 71 18  9 33 40 51 78 73 31 53 11 47 34
 25 68 87 43 49 13 74  4  6 46 17 79 59 60 19 61 12 27 48 37 72  0] TEST: [14  2 64 56 86 21 38 63 24 84 32 20 55 28  8 30 50 26]
(DL) TRAIN number of instances:  70
(DL) TEST number of instances:  18
(DL) Total number of instances (TRAIN+TEST):  88
C:\Users\Javier\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\convolutional\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(
----- train_GSR_AE -------
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 14000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3500, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3500, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 875, 6)         â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2396              â”‚ (None, 3500, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2396           â”‚ (None, 3500, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2397              â”‚ (None, 14000, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2397           â”‚ (None, 14000, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:06[0m 1s/step - loss: 12.2268 - mean_squared_error: 12.2268
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 32.3895 - mean_squared_error: 32.3895
[1m13/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 30.2967 - mean_squared_error: 30.2967
[1m19/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 28.0162 - mean_squared_error: 28.0162
[1m25/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 25.8886 - mean_squared_error: 25.8886
[1m31/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 24.2872 - mean_squared_error: 24.2872
[1m37/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 22.9659 - mean_squared_error: 22.9659
[1m43/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 21.9836 - mean_squared_error: 21.9836
[1m48/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 21.3010 - mean_squared_error: 21.3010
[1m54/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 20.6182 - mean_squared_error: 20.6182
[1m60/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 20.0557 - mean_squared_error: 20.0557
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 14ms/step - loss: 19.6869 - mean_squared_error: 19.6869 - val_loss: 7.0180 - val_mean_squared_error: 7.0180
Epoch 2/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - loss: 48.5125 - mean_squared_error: 48.5125
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 26.3615 - mean_squared_error: 26.3615
[1m13/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 21.8762 - mean_squared_error: 21.8762
[1m19/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 20.5265 - mean_squared_error: 20.5265
[1m25/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 19.2529 - mean_squared_error: 19.2529
[1m30/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 18.3886 - mean_squared_error: 18.3886
[1m36/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 17.5432 - mean_squared_error: 17.5432
[1m42/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 16.9482 - mean_squared_error: 16.9482
[1m49/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 16.3865 - mean_squared_error: 16.3865
[1m55/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 15.9446 - mean_squared_error: 15.9446
[1m60/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 15.6249 - mean_squared_error: 15.6249
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 15.4051 - mean_squared_error: 15.4051 - val_loss: 7.0015 - val_mean_squared_error: 7.0015
Epoch 3/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 44ms/step - loss: 16.5018 - mean_squared_error: 16.5018
[1m 8/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 8.6107 - mean_squared_error: 8.6107   
[1m13/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 8.3472 - mean_squared_error: 8.3472
[1m19/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 9.2016 - mean_squared_error: 9.2016
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 10.2869 - mean_squared_error: 10.2869
[1m32/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 10.9881 - mean_squared_error: 10.9881
[1m39/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.6019 - mean_squared_error: 11.6019
[1m45/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.8965 - mean_squared_error: 11.8965
[1m51/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 12.0541 - mean_squared_error: 12.0541
[1m58/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 12.1424 - mean_squared_error: 12.1424
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - loss: 12.1443 - mean_squared_error: 12.1443 - val_loss: 6.9965 - val_mean_squared_error: 6.9965
Epoch 4/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.2906 - mean_squared_error: 0.2906
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 10.6980 - mean_squared_error: 10.6980
[1m14/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 12.2461 - mean_squared_error: 12.2461
[1m19/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.1211 - mean_squared_error: 13.1211
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.7239 - mean_squared_error: 13.7239
[1m32/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 14.0155 - mean_squared_error: 14.0155
[1m38/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 14.1388 - mean_squared_error: 14.1388
[1m44/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 14.0589 - mean_squared_error: 14.0589
[1m50/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 13.9826 - mean_squared_error: 13.9826
[1m57/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 13.7960 - mean_squared_error: 13.7960
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 10ms/step - loss: 13.6483 - mean_squared_error: 13.6483
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 13.6245 - mean_squared_error: 13.6245 - val_loss: 6.9944 - val_mean_squared_error: 6.9944
Epoch 5/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 1.6966 - mean_squared_error: 1.6966
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 7.8790 - mean_squared_error: 7.8790
[1m12/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.8007 - mean_squared_error: 10.8007
[1m18/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.7347 - mean_squared_error: 11.7347
[1m24/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.5400 - mean_squared_error: 11.5400
[1m29/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 11.3171 - mean_squared_error: 11.3171
[1m35/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 11.1977 - mean_squared_error: 11.1977
[1m41/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 11.3322 - mean_squared_error: 11.3322
[1m48/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 11.3121 - mean_squared_error: 11.3121
[1m54/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 11.4277 - mean_squared_error: 11.4277
[1m61/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 11.4874 - mean_squared_error: 11.4874
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 11.5159 - mean_squared_error: 11.5159 - val_loss: 6.9932 - val_mean_squared_error: 6.9932
(70, 875, 6)
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 14000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3500, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3500, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 875, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 14000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3500, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3500, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 875, 6)         â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2398              â”‚ (None, 3500, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2398           â”‚ (None, 3500, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2399              â”‚ (None, 14000, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2399           â”‚ (None, 14000, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:07[0m 1s/step - loss: 0.1604 - mean_squared_error: 0.1604
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.1222 - mean_squared_error: 0.1222
[1m12/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.1115 - mean_squared_error: 0.1115
[1m18/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.1033 - mean_squared_error: 0.1033
[1m24/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0977 - mean_squared_error: 0.0977
[1m30/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0919 - mean_squared_error: 0.0919
[1m36/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0866 - mean_squared_error: 0.0866
[1m42/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0818 - mean_squared_error: 0.0818
[1m49/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0769 - mean_squared_error: 0.0769
[1m55/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0733 - mean_squared_error: 0.0733
[1m61/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 0.0701 - mean_squared_error: 0.0701
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 14ms/step - loss: 0.0686 - mean_squared_error: 0.0686 - val_loss: 0.0174 - val_mean_squared_error: 0.0174
Epoch 2/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - loss: 0.0362 - mean_squared_error: 0.0362
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0192 - mean_squared_error: 0.0192
[1m13/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0229 - mean_squared_error: 0.0229
[1m18/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0233 - mean_squared_error: 0.0233
[1m24/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0228 - mean_squared_error: 0.0228
[1m29/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0225 - mean_squared_error: 0.0225
[1m36/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0220 - mean_squared_error: 0.0220
[1m42/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0219 - mean_squared_error: 0.0219
[1m47/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0221 - mean_squared_error: 0.0221
[1m52/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0223 - mean_squared_error: 0.0223
[1m58/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0223 - mean_squared_error: 0.0223
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0222 - mean_squared_error: 0.0222 - val_loss: 0.0147 - val_mean_squared_error: 0.0147
Epoch 3/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 26ms/step - loss: 0.0051 - mean_squared_error: 0.0051
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0127 - mean_squared_error: 0.0127
[1m12/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0163 - mean_squared_error: 0.0163
[1m18/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m24/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0191 - mean_squared_error: 0.0191
[1m30/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m36/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m42/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m48/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0188 - mean_squared_error: 0.0188
[1m53/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0189 - mean_squared_error: 0.0189
[1m59/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - loss: 0.0190 - mean_squared_error: 0.0190
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0133 - val_mean_squared_error: 0.0133
Epoch 4/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0467 - mean_squared_error: 0.0467
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0207 - mean_squared_error: 0.0207
[1m14/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0164 - mean_squared_error: 0.0164
[1m20/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0166 - mean_squared_error: 0.0166
[1m24/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0164 - mean_squared_error: 0.0164
[1m30/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0159 - mean_squared_error: 0.0159
[1m36/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0158 - mean_squared_error: 0.0158
[1m42/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0156 - mean_squared_error: 0.0156
[1m48/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0156 - mean_squared_error: 0.0156
[1m53/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0160 - mean_squared_error: 0.0160
[1m58/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0163 - mean_squared_error: 0.0163
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0139 - val_mean_squared_error: 0.0139
Epoch 5/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0078 - mean_squared_error: 0.0078
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0097 - mean_squared_error: 0.0097
[1m12/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0096 - mean_squared_error: 0.0096
[1m19/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0112 - mean_squared_error: 0.0112
[1m25/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0121 - mean_squared_error: 0.0121
[1m31/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0129 - mean_squared_error: 0.0129
[1m37/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0135 - mean_squared_error: 0.0135
[1m43/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0141 - mean_squared_error: 0.0141
[1m49/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0146 - mean_squared_error: 0.0146
[1m55/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0151 - mean_squared_error: 0.0151
[1m61/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - loss: 0.0154 - mean_squared_error: 0.0154
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0155 - mean_squared_error: 0.0155 - val_loss: 0.0132 - val_mean_squared_error: 0.0132
(14000, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 14000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3500, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3500, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 875, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 14000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 14000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 875, 6)    â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 875, 6)    â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 875, 12)   â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 875)   â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 10500)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 10500)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     10,501 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 11,323 (44.23 KB)
 Trainable params: 11,323 (44.23 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:38[0m 2s/step - binary_accuracy: 0.0000e+00 - loss: 0.8611
[1m 8/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.2482 - loss: 0.8064     
[1m14/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.2970 - loss: 0.7916
[1m21/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.3430 - loss: 0.7798 
[1m28/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.3684 - loss: 0.7741
[1m34/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.3819 - loss: 0.7692
[1m41/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.3963 - loss: 0.7626 
[1m48/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.4129 - loss: 0.7534
[1m54/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.4289 - loss: 0.7452
[1m61/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.4446 - loss: 0.7384
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 13ms/step - binary_accuracy: 0.4512 - loss: 0.7350 - val_binary_accuracy: 0.5714 - val_loss: 0.7860
Epoch 2/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 30ms/step - binary_accuracy: 1.0000 - loss: 0.1086
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6554 - loss: 0.7257
[1m14/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6238 - loss: 0.7913
[1m20/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6303 - loss: 0.7817
[1m27/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6349 - loss: 0.7669
[1m34/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6410 - loss: 0.7511
[1m40/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6429 - loss: 0.7432
[1m45/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6412 - loss: 0.7384
[1m51/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6377 - loss: 0.7356
[1m58/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6379 - loss: 0.7311
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.6387 - loss: 0.7259 - val_binary_accuracy: 0.5714 - val_loss: 0.6237
Epoch 3/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 0.0000e+00 - loss: 2.1398
[1m 8/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.4954 - loss: 1.0692     
[1m15/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5039 - loss: 0.9251
[1m22/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.4981 - loss: 0.8638
[1m28/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5017 - loss: 0.8326
[1m35/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5182 - loss: 0.8039
[1m42/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5338 - loss: 0.7818
[1m49/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5436 - loss: 0.7664
[1m55/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5532 - loss: 0.7535
[1m62/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5650 - loss: 0.7399
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.5682 - loss: 0.7362 - val_binary_accuracy: 0.5714 - val_loss: 0.6455
Epoch 4/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1023
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 1.0000 - loss: 0.2155 
[1m13/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9730 - loss: 0.2486
[1m20/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9418 - loss: 0.2867
[1m27/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9068 - loss: 0.3401
[1m33/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8750 - loss: 0.3907
[1m39/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8528 - loss: 0.4246
[1m45/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8379 - loss: 0.4493
[1m52/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8217 - loss: 0.4717
[1m59/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8035 - loss: 0.4900
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7926 - loss: 0.5001 - val_binary_accuracy: 0.7143 - val_loss: 0.6257
Epoch 5/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2738
[1m 8/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9207 - loss: 0.3493 
[1m13/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8337 - loss: 0.4657
[1m19/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7895 - loss: 0.5372
[1m25/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7796 - loss: 0.5633
[1m32/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7763 - loss: 0.5733
[1m38/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7701 - loss: 0.5788
[1m44/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7675 - loss: 0.5813
[1m50/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7672 - loss: 0.5803
[1m57/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7681 - loss: 0.5766
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7676 - loss: 0.5748 - val_binary_accuracy: 0.7143 - val_loss: 0.6257
Epoch 6/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1160
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6997 - loss: 0.4311 
[1m13/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7163 - loss: 0.4671
[1m20/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7321 - loss: 0.4717
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7399 - loss: 0.4720
[1m33/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7386 - loss: 0.4755
[1m39/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7345 - loss: 0.4835
[1m44/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7349 - loss: 0.4848
[1m51/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7374 - loss: 0.4875
[1m57/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7396 - loss: 0.4905
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7386 - loss: 0.4960 - val_binary_accuracy: 0.5714 - val_loss: 0.6206
Epoch 7/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7349
[1m 8/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6603 - loss: 0.4993     
[1m15/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6786 - loss: 0.5201
[1m22/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6848 - loss: 0.5272
[1m28/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7004 - loss: 0.5248
[1m35/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7086 - loss: 0.5262
[1m40/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7122 - loss: 0.5262
[1m47/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7163 - loss: 0.5271
[1m53/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7182 - loss: 0.5297
[1m59/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7199 - loss: 0.5309 
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7215 - loss: 0.5314 - val_binary_accuracy: 0.5714 - val_loss: 0.6766
Epoch 8/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.8936
[1m 8/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6268 - loss: 0.8163     
[1m14/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6888 - loss: 0.6887
[1m21/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7049 - loss: 0.6434 
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7076 - loss: 0.6252
[1m32/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7021 - loss: 0.6153
[1m39/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6958 - loss: 0.6078
[1m46/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6951 - loss: 0.6007 
[1m53/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6972 - loss: 0.5929
[1m59/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6998 - loss: 0.5863
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7020 - loss: 0.5808 - val_binary_accuracy: 0.5714 - val_loss: 0.6078
Epoch 9/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.1543
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8187 - loss: 0.3766
[1m13/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7765 - loss: 0.3900
[1m20/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7921 - loss: 0.3932
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8043 - loss: 0.3958
[1m33/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8004 - loss: 0.4044
[1m39/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8002 - loss: 0.4066
[1m46/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8008 - loss: 0.4084
[1m52/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7985 - loss: 0.4129
[1m59/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7970 - loss: 0.4170
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7964 - loss: 0.4202 - val_binary_accuracy: 0.5714 - val_loss: 0.6256
Epoch 10/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2417
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 1.0000 - loss: 0.2382
[1m13/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 1.0000 - loss: 0.2221
[1m20/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9736 - loss: 0.2465
[1m27/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9104 - loss: 0.3332
[1m34/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8586 - loss: 0.3956
[1m40/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8259 - loss: 0.4357
[1m47/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7947 - loss: 0.4744
[1m54/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7755 - loss: 0.4988 
[1m61/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7634 - loss: 0.5147
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7587 - loss: 0.5205 - val_binary_accuracy: 0.5714 - val_loss: 0.6842

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.86102146 0.7944678  0.86804795 0.8816464  0.15908518 0.90607226
 0.64821696 0.81762075 0.6376927  0.14941564 0.57655925 0.8829711
 0.9334377  0.6938895  0.35743472 0.5602889  0.42889082 0.7256652 ]
predicted [1 1 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1]
expected [False False  True  True False  True  True  True  True False False  True
  True  True False  True  True False]
accuracy: 0.7222222222222222
confusion matrix: 
[[ 3  4]
 [ 1 10]]
              precision    recall  f1-score   support

       False       0.75      0.43      0.55         7
        True       0.71      0.91      0.80        11

    accuracy                           0.72        18
   macro avg       0.73      0.67      0.67        18
weighted avg       0.73      0.72      0.70        18

macro avg f1-score: 0.6727272727272727
macro avg (UAR): 0.6688311688311688
Sensitivity:  0.42857142857142855
Specificity:  0.9090909090909091
g-mean:  0.6241877839323592
-------- Model Performance ----------: 
accuracy:  [0.77777778 0.61111111 0.55555556 0.66666667 0.83333333 0.72222222
 0.83333333 0.66666667 0.55555556 0.72222222]
gmean:  [0.78954203 0.48349378 0.34188173 0.68376346 0.75592895 0.68376346
 0.80582296 0.59215653 0.45584231 0.62418778]
f1_score:  [0.775      0.54181818 0.44615385 0.66666667 0.80363636 0.69899666
 0.81939799 0.625      0.5        0.67272727]
UAR:  [0.79220779 0.55194805 0.48051948 0.7012987  0.78571429 0.69480519
 0.81168831 0.62337662 0.50649351 0.66883117]
Cohen Kappa score:  [ 0.55555556  0.11267606 -0.04347826  0.36470588  0.61971831  0.4
  0.64        0.26027397  0.01369863  0.36619718]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  14.0
step (sec):  14.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  88
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.643 0.826 0.825 0.817 0.818 0.417 0.717 0.725 0.698 0.703]
 [0.412 0.712 0.71  0.7   0.7   0.313 0.721 0.739 0.644 0.718]
 [0.532 0.753 0.772 0.773 0.75  0.41  0.738 0.751 0.705 0.722]
 [0.    0.5   0.603 0.    0.375 0.    0.5   0.614 0.    0.38 ]
 [0.407 0.677 0.69  0.614 0.649 0.339 0.662 0.703 0.627 0.659]
 [0.329 0.662 0.694 0.622 0.655 0.084 0.543 0.578 0.432 0.517]
 [0.    0.5   0.603 0.    0.375 0.    0.5   0.614 0.    0.38 ]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[0.44110701 0.72       0.725      0.69004545 0.70824675 0.31468662
  0.66309524 0.72916667 0.59910826 0.65260073]
 [0.2539608  0.62666667 0.62916667 0.55973731 0.60217255 0.43885846
  0.70916667 0.75694444 0.64170616 0.7082745 ]
 [0.47406375 0.73166667 0.75138889 0.68851185 0.7204748  0.42562845
  0.7175     0.71388889 0.68611123 0.69931457]
 [0.37940977 0.6875     0.69305556 0.67055969 0.68226745 0.06207792
  0.53452381 0.68194444 0.24541022 0.49627747]
 [0.45365967 0.71833333 0.75972222 0.65343117 0.71182984 0.58184071
  0.7925     0.78888889 0.76374486 0.77625902]
 [0.42666234 0.7175     0.70694444 0.68514397 0.69544012 0.65321228
  0.8275     0.82638889 0.81106358 0.81884199]
 [0.25393847 0.6275     0.64861111 0.54648207 0.60455461 0.18638752
  0.5925     0.59027778 0.56836621 0.57931818]
 [0.68625375 0.825      0.87361111 0.79477331 0.83690143 0.65959138
  0.82666667 0.84305556 0.81138368 0.82529887]
 [0.2678035  0.62619048 0.7375     0.45663057 0.60955045 0.23938105
  0.61666667 0.69444444 0.49267333 0.5946953 ]
 [0.25720576 0.6275     0.6375     0.5967292  0.61653152 0.10089168
  0.55       0.55833333 0.53251955 0.54474747]
 [0.34795382 0.67083333 0.69166667 0.64217164 0.6657265  0.37690502
  0.6825     0.70555556 0.65112649 0.67775558]
 [0.43296125 0.71       0.73888889 0.6697586  0.71337662 0.45240665
  0.73       0.74166667 0.70621928 0.71961427]
 [0.42173399 0.715      0.71388889 0.68732461 0.69788795 0.13900174
  0.5725     0.57777778 0.51544166 0.55140693]
 [0.14374762 0.57       0.60138889 0.4693999  0.54741064 0.4537347
  0.725      0.73055556 0.69335997 0.71412532]
 [0.19099567 0.59       0.65972222 0.49325407 0.58467865 0.18909914
  0.59333333 0.64444444 0.48969721 0.57444222]
 [0.12742678 0.5625     0.6        0.42530482 0.53453297 0.42167832
  0.7        0.76111111 0.62567831 0.69535631]
 [0.32675064 0.6575     0.68194444 0.6308265  0.6533708  0.31019363
  0.65333333 0.69027778 0.60284595 0.6509041 ]
 [0.03846154 0.53571429 0.82916667 0.08451543 0.47656863 0.40436739
  0.70416667 0.725      0.64328508 0.69116162]
 [0.37177866 0.66666667 0.76527778 0.53838615 0.66553114 0.3561396
  0.68       0.68333333 0.66211912 0.67213203]
 [0.32401344 0.66166667 0.70555556 0.58223115 0.65475191 0.0999993
  0.54583333 0.60972222 0.39809794 0.52522311]
 [0.21493506 0.61309524 0.70277778 0.47351743 0.60465035 0.09775838
  0.5475     0.54583333 0.49001784 0.5262013 ]
 [0.2106373  0.6025     0.60416667 0.5608392  0.58308997 0.24276977
  0.6275     0.66805556 0.50846163 0.60712204]
 [0.31298798 0.655      0.66944444 0.60277365 0.64401598 0.25341484
  0.62142857 0.78472222 0.35639894 0.59254038]
 [0.04360719 0.52       0.52222222 0.51050547 0.51619048 0.51361292
  0.76       0.76111111 0.73301339 0.74617438]
 [0.32081182 0.66       0.66944444 0.63573299 0.64993506 0.57066548
  0.79       0.7875     0.72695982 0.76487374]
 [0.309885   0.65       0.66111111 0.56000149 0.62038351 0.40635621
  0.7075     0.71527778 0.64477438 0.68799784]
 [0.57085958 0.78333333 0.83055556 0.71760513 0.77276723 0.5675784
  0.7875     0.78194444 0.78023999 0.7802381 ]
 [0.36278222 0.67583333 0.725      0.60589805 0.66627123 0.39144094
  0.69916667 0.70833333 0.66694262 0.68253996]
 [0.23166833 0.61416667 0.65833333 0.53079505 0.59485015 0.21798396
  0.6075     0.61388889 0.57099854 0.59279582]
 [0.64303275 0.82583333 0.825      0.81697211 0.81822511 0.41715967
  0.7175     0.725      0.69773604 0.70280331]]
KNN mean:
[0.32803651 0.66158333 0.70060185 0.58599527 0.64840615 0.35149407
 0.67607937 0.70481481 0.61051671 0.66170122]
---------------------------
---------------------------
DT performance:
[[ 0.47524658  0.7525      0.74861111  0.73415317  0.74320707  0.28582969
   0.68214286  0.71666667  0.60826048  0.65532967]
 [ 0.26817446  0.62083333  0.62222222  0.57970743  0.61388528  0.30256813
   0.68083333  0.71388889  0.58469111  0.65996337]
 [ 0.36730191  0.67833333  0.68611111  0.68622105  0.67536075  0.2226006
   0.6225      0.62361111  0.54189627  0.60609141]
 [ 0.09453803  0.575       0.57916667  0.46205313  0.55896825  0.21235186
   0.61904762  0.65833333  0.42901443  0.59458791]
 [ 0.42612436  0.70583333  0.71388889  0.65886389  0.68876096  0.26928288
   0.625       0.61666667  0.62566258  0.60550866]
 [ 0.40888312  0.7025      0.69583333  0.67268711  0.68672078  0.58735862
   0.81        0.80416667  0.73042563  0.78616883]
 [ 0.22403637  0.62416667  0.62638889  0.5616469   0.61056832  0.20032357
   0.58        0.58333333  0.47862821  0.56627539]
 [ 0.78356643  0.86333333  0.88333333  0.84144344  0.86543845  0.36487933
   0.765       0.77361111  0.70730497  0.75544372]
 [ 0.38792874  0.6952381   0.71666667  0.67838676  0.68102398  0.23488975
   0.61785714  0.69305556  0.53426151  0.59742091]
 [ 0.26136961  0.6325      0.6375      0.63492449  0.63131674  0.17932818
   0.6675      0.66944444  0.60529923  0.66483405]
 [ 0.19958091  0.61833333  0.63472222  0.60892134  0.61423521  0.38890707
   0.71        0.71805556  0.65085605  0.70710678]
 [ 0.46784885  0.7125      0.72777778  0.61935204  0.70304196  0.19151439
   0.61166667  0.59722222  0.5715624   0.58400266]
 [ 0.24601011  0.64833333  0.65        0.50254178  0.62514624  0.29324315
   0.6325      0.6375      0.59040077  0.6259632 ]
 [ 0.10428405  0.58916667  0.61527778  0.52960611  0.57554917  0.06080682
   0.505       0.50694444  0.52834622  0.49400794]
 [ 0.10424242  0.64833333  0.66944444  0.43059944  0.61839966  0.24485681
   0.62        0.64444444  0.59083784  0.59363137]
 [ 0.1885495   0.6425      0.64583333  0.57566917  0.60993173  0.30754912
   0.59166667  0.64027778  0.59192163  0.5754268 ]
 [ 0.39218164  0.7075      0.7375      0.7092897   0.69878621  0.50455283
   0.74666667  0.76111111  0.7549126   0.74369547]
 [ 0.08108696  0.48303571  0.7375      0.18612344  0.44739011  0.24940534
   0.6625      0.67222222  0.62956716  0.64499833]
 [ 0.34822887  0.64285714  0.69583333  0.58640311  0.63606227  0.3676365
   0.66        0.65972222  0.64848965  0.64485209]
 [ 0.25166912  0.64166667  0.64861111  0.55367226  0.61355644  0.14304528
   0.59416667  0.62222222  0.52092556  0.5851357 ]
 [ 0.24049501  0.57619048  0.67222222  0.45200534  0.57223443 -0.01246725
   0.46        0.46805556  0.39217966  0.44064408]
 [ 0.13749221  0.59        0.59027778  0.61921239  0.5809632   0.27724856
   0.6675      0.68333333  0.62028037  0.65617438]
 [ 0.34832061  0.62416667  0.62361111  0.55676427  0.59834776  0.17226168
   0.60357143  0.69166667  0.51646289  0.59114386]
 [ 0.11000661  0.5875      0.58888889  0.55441725  0.57886364  0.37675919
   0.6625      0.65833333  0.65958165  0.64852814]
 [ 0.22502144  0.625       0.625       0.5678392   0.61632756  0.27363787
   0.745       0.74166667  0.67885017  0.73727273]
 [ 0.20389764  0.6375      0.64444444  0.59026855  0.6217785   0.25453937
   0.60416667  0.60277778  0.60051467  0.58301671]
 [ 0.35494823  0.67857143  0.72361111  0.64578995  0.65659174  0.40612436
   0.7025      0.70416667  0.69331118  0.69762987]
 [ 0.04739426  0.53666667  0.54027778  0.55775377  0.52043651  0.51617931
   0.76833333  0.76527778  0.70207087  0.75962843]
 [ 0.198002    0.6025      0.62777778  0.50641104  0.58646853  0.08140302
   0.5425      0.55        0.54017163  0.53270757]
 [ 0.41243686  0.7125      0.70972222  0.69978947  0.70012987  0.31307692
   0.72083333  0.73888889  0.64403426  0.71774531]]
DT mean:
[0.2786289  0.64516865 0.66726852 0.58541723 0.63098304 0.27565643
 0.64936508 0.66388889 0.59902406 0.63516451]
---------------------------
---------------------------
RF performance:
[[ 0.58350802  0.805       0.80555556  0.74857852  0.79824675  0.53578553
   0.75952381  0.80694444  0.68875372  0.74801782]
 [ 0.35282862  0.71916667  0.72361111  0.55623668  0.7089899   0.41065617
   0.70666667  0.73611111  0.72705587  0.70445971]
 [ 0.43747631  0.66166667  0.67361111  0.64257122  0.64863664  0.25376032
   0.6925      0.69305556  0.58136106  0.6780303 ]
 [ 0.25892254  0.5225      0.52222222  0.55462939  0.51221861  0.1686693
   0.61666667  0.725       0.54090129  0.6043673 ]
 [ 0.40466254  0.68        0.70555556  0.678168    0.6609904   0.44245186
   0.71        0.69861111  0.70452109  0.68494949]
 [ 0.43087644  0.6775      0.67638889  0.63829891  0.65862193  0.58746985
   0.78        0.78333333  0.72640448  0.77414502]
 [ 0.28061293  0.61416667  0.63611111  0.58190317  0.60621046  0.41686679
   0.665       0.66944444  0.66207975  0.65908009]
 [ 0.80664336  0.88333333  0.90833333  0.84212245  0.88843989  0.37755832
   0.70416667  0.72777778  0.69498565  0.69306666]
 [ 0.31343656  0.69404762  0.75        0.69753866  0.69265734  0.22028305
   0.58452381  0.66111111  0.46112873  0.57920246]
 [ 0.28103239  0.675       0.68333333  0.62189119  0.66454018  0.23366707
   0.6275      0.625       0.60342626  0.62096681]
 [ 0.23384873  0.63166667  0.63888889  0.704748    0.61995504  0.37129324
   0.69166667  0.70555556  0.72943976  0.68322233]
 [ 0.60736358  0.77916667  0.79305556  0.78618121  0.77846681  0.29308256
   0.5775      0.58333333  0.57737839  0.56593795]
 [ 0.17330294  0.585       0.60277778  0.59170788  0.57444444  0.25814306
   0.55833333  0.59027778  0.47733082  0.52549617]
 [ 0.14346163  0.52666667  0.56527778  0.4586772   0.51045815  0.39616372
   0.6775      0.67916667  0.67978671  0.66816378]
 [ 0.20300366  0.665       0.70277778  0.66553304  0.64434649  0.29208115
   0.68666667  0.70416667  0.50687213  0.67302864]
 [ 0.20280579  0.5025      0.54583333  0.49527654  0.48762349  0.3751049
   0.72333333  0.77222222  0.61354886  0.71351315]
 [ 0.59765567  0.78583333  0.80694444  0.75102835  0.79079615  0.49397572
   0.80333333  0.80555556  0.71159694  0.79417749]
 [ 0.19403727  0.65803571  0.81666667  0.46425211  0.62834249  0.38853965
   0.66583333  0.69305556  0.63811993  0.63760295]
 [ 0.45486014  0.70833333  0.77361111  0.72831296  0.69434066  0.46083174
   0.7175      0.71805556  0.69489011  0.71183983]
 [ 0.29308192  0.63166667  0.68611111  0.59443278  0.62472416  0.12163242
   0.5375      0.58888889  0.41064074  0.53093656]
 [ 0.05496894  0.48333333  0.63611111  0.34751155  0.45061813 -0.01550616
   0.4875      0.49861111  0.40494603  0.46494228]
 [ 0.26770663  0.58        0.57638889  0.63779383  0.56838745  0.3654391
   0.62416667  0.63472222  0.5365246   0.61433983]
 [ 0.1075437   0.52416667  0.54305556  0.60034764  0.5112987   0.09855449
   0.6297619   0.74722222  0.33329576  0.6191163 ]
 [ 0.18661887  0.58        0.59166667  0.53626433  0.56061966  0.28672608
   0.62333333  0.625       0.67540118  0.61074509]
 [ 0.26141527  0.68166667  0.69583333  0.7025902   0.67400017  0.49648211
   0.695       0.69305556  0.70507643  0.67851732]
 [ 0.29061427  0.6         0.60277778  0.60052708  0.57059607  0.29075228
   0.7175      0.7375      0.69348426  0.706829  ]
 [ 0.40287756  0.75833333  0.79305556  0.59560451  0.75184898  0.48076655
   0.6825      0.67916667  0.61217806  0.670386  ]
 [ 0.28222443  0.535       0.58888889  0.47001254  0.51642857  0.43436968
   0.69666667  0.69722222  0.7069226   0.67258131]
 [ 0.2310814   0.62166667  0.64861111  0.54792568  0.59720446  0.1978424
   0.62        0.62638889  0.57713691  0.61328283]
 [ 0.5317313   0.7525      0.77222222  0.7734572   0.7502886   0.41039539
   0.73833333  0.75138889  0.70458396  0.72175325]]
RF mean:
[0.32900678 0.65076389 0.68217593 0.62047076 0.63814469 0.33812795
 0.66668254 0.68856481 0.61265907 0.65408992]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.53333333  0.          0.3474359   0.
   0.5         0.72777778  0.          0.42071429]
 [ 0.          0.5         0.55694444  0.          0.35750916  0.04
   0.51666667  0.63611111  0.05773503  0.41192308]
 [ 0.10810811  0.55        0.61388889  0.2         0.45362637  0.40336055
   0.7         0.70277778  0.65980155  0.68237013]
 [ 0.          0.5         0.52222222  0.          0.34249084  0.
   0.5         0.70555556  0.          0.41321429]
 [ 0.32347306  0.64416667  0.72777778  0.48122576  0.62056444  0.5136034
   0.76        0.75138889  0.73294562  0.74221501]
 [ 0.          0.5         0.52222222  0.          0.34249084  0.48883635
   0.7425      0.74583333  0.71290298  0.7312987 ]
 [ 0.          0.5         0.58055556  0.          0.36692308  0.03996832
   0.5175      0.52361111  0.19472136  0.40964202]
 [ 0.19118881  0.58333333  0.71527778  0.25485474  0.52699634  0.
   0.5         0.625       0.          0.38406593]
 [ 0.          0.5         0.69444444  0.          0.40946429  0.
   0.5         0.69444444  0.          0.40946429]
 [ 0.18104047  0.595       0.55972222  0.37718189  0.49360639 -0.02162162
   0.49        0.52222222  0.          0.34249084]
 [ 0.          0.5         0.60277778  0.          0.37549451  0.
   0.5         0.59166667  0.          0.37120879]
 [ 0.          0.5         0.59166667  0.          0.37120879  0.
   0.5         0.60277778  0.          0.37549451]
 [ 0.          0.5         0.58055556  0.          0.36692308  0.
   0.5         0.59166667  0.          0.37120879]
 [ 0.          0.5         0.625       0.          0.38406593  0.025
   0.5125      0.54583333  0.05        0.3704662 ]
 [ 0.          0.5         0.67083333  0.          0.40131868  0.
   0.5         0.65833333  0.          0.39692308]
 [ 0.          0.5         0.59166667  0.          0.37120879  0.
   0.5         0.67083333  0.          0.40131868]
 [ 0.          0.5         0.60277778  0.          0.37549451  0.
   0.5         0.60277778  0.          0.37549451]
 [ 0.          0.5         0.84166667  0.          0.45656863  0.
   0.5         0.58055556  0.          0.36692308]
 [ 0.          0.5         0.69444444  0.          0.40946429  0.
   0.5         0.54444444  0.          0.35238095]
 [ 0.          0.5         0.65833333  0.          0.39692308  0.
   0.5         0.63611111  0.          0.38835165]
 [ 0.          0.5         0.77222222  0.          0.43571429  0.075
   0.5375      0.49305556  0.125       0.37751748]
 [-0.03700624  0.4825      0.5         0.03872983  0.34338994  0.
   0.5         0.625       0.          0.38406593]
 [ 0.          0.5         0.56944444  0.          0.36263736  0.
   0.5         0.75        0.          0.42821429]
 [ 0.          0.5         0.53333333  0.          0.3474359   0.14454262
   0.56666667  0.61527778  0.25773503  0.47698635]
 [ 0.          0.5         0.58055556  0.          0.36692308  0.28019228
   0.6375      0.63611111  0.47690713  0.57576757]
 [ 0.          0.5         0.53333333  0.          0.3474359   0.11069741
   0.55        0.61527778  0.21043965  0.45908425]
 [ 0.          0.5         0.69444444  0.          0.40946429  0.43121895
   0.72        0.71527778  0.70178831  0.707886  ]
 [ 0.          0.5         0.64722222  0.          0.39263736  0.
   0.5         0.60277778  0.          0.37549451]
 [ 0.          0.5         0.63611111  0.          0.38835165  0.
   0.5         0.54444444  0.          0.35238095]
 [ 0.          0.5         0.60277778  0.          0.37549451  0.
   0.5         0.61388889  0.          0.37978022]]
SVM mean:
[0.02556014 0.51183333 0.61851852 0.04506641 0.39797541 0.08435994
 0.54169444 0.62902778 0.13933256 0.44114488]
---------------------------
---------------------------
GBM performance:
[[ 5.62719110e-01  7.72500000e-01  7.72222222e-01  7.44088338e-01
   7.63715729e-01  5.02337662e-01  7.58333333e-01  8.52777778e-01
   6.11822111e-01  7.51068099e-01]
 [ 3.73187621e-01  6.80833333e-01  7.04166667e-01  5.90790231e-01
   6.56989677e-01  2.91128082e-01  6.32500000e-01  7.02777778e-01
   5.07095466e-01  6.20861638e-01]
 [ 4.01582491e-01  6.91666667e-01  7.18055556e-01  6.45084437e-01
   6.83081086e-01  1.54794026e-01  5.77500000e-01  5.68055556e-01
   4.94084234e-01  5.38904151e-01]
 [ 2.65510164e-01  6.20000000e-01  6.23611111e-01  6.03187235e-01
   6.10000000e-01  7.90513834e-04  5.01190476e-01  6.81944444e-01
   1.10439655e-01  4.47101648e-01]
 [ 5.19019889e-01  7.42500000e-01  7.94444444e-01  6.67372654e-01
   7.41901432e-01  4.90633722e-01  7.47500000e-01  7.40277778e-01
   7.20254060e-01  7.30652958e-01]
 [ 4.00044522e-01  7.17500000e-01  7.08333333e-01  6.89089840e-01
   6.98084416e-01  4.67621775e-01  7.32500000e-01  7.36111111e-01
   6.69566976e-01  7.12478355e-01]
 [ 2.32342463e-01  6.12500000e-01  6.38888889e-01  5.66863766e-01
   6.00884116e-01  2.95288620e-01  6.57500000e-01  6.62500000e-01
   6.22707976e-01  6.47171717e-01]
 [ 7.46643357e-01  8.50000000e-01  8.86111111e-01  8.34954627e-01
   8.56297036e-01  3.22060415e-01  6.47500000e-01  6.83333333e-01
   5.75960306e-01  6.31218781e-01]
 [ 4.02077922e-01  6.84523810e-01  7.94444444e-01  5.53124622e-01
   6.81208791e-01  2.05076228e-01  5.92857143e-01  6.95833333e-01
   4.60406491e-01  5.86520147e-01]
 [ 3.16290738e-01  6.60000000e-01  6.61111111e-01  6.14891731e-01
   6.44121712e-01  2.34177995e-01  6.37500000e-01  6.45833333e-01
   5.90453670e-01  6.25171495e-01]
 [ 3.35073911e-01  6.35833333e-01  6.79166667e-01  6.00559678e-01
   6.27765568e-01  5.39211753e-01  7.54166667e-01  7.73611111e-01
   7.17456745e-01  7.55035520e-01]
 [ 4.62230012e-01  7.35000000e-01  7.72222222e-01  6.63651790e-01
   7.31781552e-01  2.49329900e-01  6.38333333e-01  6.51388889e-01
   5.95500075e-01  6.21177989e-01]
 [ 2.93208411e-01  6.43333333e-01  6.80555556e-01  5.25653690e-01
   6.20170663e-01  2.42141633e-01  6.12500000e-01  6.51388889e-01
   5.26505627e-01  5.98972694e-01]
 [ 1.95088333e-01  5.70000000e-01  6.48611111e-01  4.00795363e-01
   5.46804029e-01  3.32144673e-01  6.62500000e-01  6.68055556e-01
   6.49973534e-01  6.52521645e-01]
 [ 5.63636364e-02  5.25000000e-01  6.58333333e-01  2.20879309e-01
   4.80989011e-01  6.93906094e-02  5.30000000e-01  6.45833333e-01
   2.61924512e-01  4.89587080e-01]
 [ 1.36905257e-01  5.77500000e-01  6.36111111e-01  3.52262676e-01
   5.23496503e-01  1.72727273e-01  5.58333333e-01  6.70833333e-01
   3.54699819e-01  5.40219780e-01]
 [ 3.93756331e-01  6.92500000e-01  7.26388889e-01  6.21385726e-01
   6.77216950e-01  4.29644566e-01  6.90000000e-01  7.37500000e-01
   6.57456903e-01  6.95710123e-01]
 [-3.73913043e-02  4.78571429e-01  8.05555556e-01  0.00000000e+00
   4.44613499e-01  4.20849571e-01  7.09166667e-01  7.26388889e-01
   6.73028486e-01  6.97965368e-01]
 [ 1.16363636e-01  5.66666667e-01  7.04166667e-01  2.91589987e-01
   5.29482601e-01  3.91339713e-01  7.07500000e-01  7.16666667e-01
   6.45630236e-01  6.85128205e-01]
 [ 2.15389610e-01  6.15000000e-01  6.84722222e-01  4.88668257e-01
   5.98913309e-01  2.83243073e-02  5.14166667e-01  6.09722222e-01
   2.25871692e-01  4.81814852e-01]
 [ 0.00000000e+00  5.00000000e-01  7.72222222e-01  0.00000000e+00
   4.35714286e-01 -1.34609954e-01  4.20000000e-01  4.31944444e-01
   3.72257686e-01  3.99148629e-01]
 [ 2.29634146e-01  6.15000000e-01  6.15277778e-01  6.07246713e-01
   6.10793651e-01  2.00986647e-01  5.70833333e-01  6.36111111e-01
   3.92203896e-01  5.53992674e-01]
 [ 3.43793944e-01  6.65000000e-01  6.94444444e-01  5.74351486e-01
   6.55279720e-01  1.37453416e-01  5.34523810e-01  7.38888889e-01
   1.83316497e-01  4.94285714e-01]
 [ 6.93924990e-02  5.35000000e-01  5.45833333e-01  4.70955497e-01
   5.18666056e-01  2.87848601e-01  6.33333333e-01  6.36111111e-01
   6.20648167e-01  6.17803030e-01]
 [ 2.84176788e-01  6.34166667e-01  6.72222222e-01  5.45453640e-01
   6.17500000e-01  4.99970517e-01  7.65000000e-01  7.63888889e-01
   7.30790582e-01  7.58639971e-01]
 [ 4.82267602e-01  7.37500000e-01  7.50000000e-01  6.91797506e-01
   7.23200133e-01  3.54966074e-01  6.78333333e-01  6.94444444e-01
   6.38698274e-01  6.66962482e-01]
 [ 4.06753247e-01  7.01190476e-01  7.81944444e-01  6.09106183e-01
   6.92920413e-01  4.24173144e-01  7.02500000e-01  7.01388889e-01
   7.04614274e-01  6.97373737e-01]
 [-1.02797203e-02  5.15000000e-01  6.12500000e-01  2.20769684e-01
   4.56704962e-01  3.96104635e-01  6.91666667e-01  7.19444444e-01
   6.48868048e-01  6.83333056e-01]
 [ 2.22294462e-01  5.98333333e-01  6.93055556e-01  3.36504397e-01
   5.55347985e-01  1.53268052e-01  5.92500000e-01  6.05555556e-01
   5.19642861e-01  5.81899489e-01]
 [ 4.06567906e-01  6.76666667e-01  6.90277778e-01  6.14018797e-01
   6.48751249e-01  3.38909843e-01  6.62500000e-01  7.02777778e-01
   6.26579125e-01  6.58817016e-01]]
GBM mean:
[0.29403357 0.64164286 0.70416667 0.51150326 0.62107987 0.28326947
 0.63709127 0.68171296 0.5369486  0.62071793]
---------------------------
---------------------------
BDDAE performance:
[[ 0.06614435  0.5325      0.53333333  0.52791834  0.52922471 -0.01650579
   0.49384615  0.63333333  0.30827568  0.47810821]
 [ 0.26196989  0.63375     0.63333333  0.61995069  0.6248253   0.33582331
   0.66818182  0.68333333  0.65388516  0.66330628]
 [ 0.04580979  0.52125     0.53888889  0.48096481  0.50988569  0.16666667
   0.58333333  0.58333333  0.53501773  0.55992962]
 [ 0.1         0.55        0.55        0.52224753  0.53693111  0.04142356
   0.51384615  0.64444444  0.33756287  0.49568004]
 [ 0.45700649  0.72857143  0.74444444  0.71101656  0.72373617  0.3
   0.65        0.65        0.63122048  0.641175  ]
 [ 0.43333333  0.71666667  0.71666667  0.69144333  0.70552619  0.01631847
   0.50875     0.50555556  0.47794759  0.49068453]
 [-0.00380241  0.49875     0.52222222  0.41995464  0.47892983  0.18888889
   0.59444444  0.59444444  0.56896567  0.58192912]
 [ 0.24037832  0.60833333  0.69444444  0.5421059   0.6105767   0.11323048
   0.55584416  0.57777778  0.52497195  0.54550267]
 [ 0.58606437  0.7625      0.83888889  0.71939867  0.78410601  0.31562543
   0.63333333  0.75        0.48980555  0.6245836 ]
 [ 0.08888889  0.54444444  0.54444444  0.52648412  0.53645157  0.27146086
   0.63375     0.64444444  0.61960761  0.63179415]
 [ 0.14971629  0.57337662  0.60555556  0.52965373  0.56293604  0.12419736
   0.55909091  0.59444444  0.51342471  0.5490721 ]
 [ 0.07035268  0.53896104  0.54444444  0.52195702  0.52711937  0.38353596
   0.69220779  0.7         0.68043172  0.68634185]
 [-0.03775371  0.48        0.48888889  0.44994632  0.47000387  0.29520269
   0.6474026   0.66111111  0.63270648  0.64148431]
 [-0.01090506  0.49415584  0.55        0.34735304  0.47093834  0.3302197
   0.67        0.66111111  0.65579062  0.65682534]
 [-0.0882061   0.45833333  0.53888889  0.30341567  0.43653194  0.24489667
   0.62083333  0.66111111  0.59832474  0.61807502]
 [ 0.05151123  0.52727273  0.53333333  0.48635885  0.50526061  0.13330144
   0.55416667  0.67777778  0.36327026  0.52989046]
 [ 0.57026983  0.78181818  0.8         0.76901611  0.78225723  0.01788934
   0.50974026  0.57222222  0.35153651  0.47492155]
 [ 0.28979844  0.63666667  0.83888889  0.45460252  0.62941137  0.06329671
   0.5325      0.54444444  0.4881578   0.52372228]
 [ 0.19314888  0.6         0.65        0.5364638   0.59101728  0.17746586
   0.59        0.59444444  0.56759487  0.57953239]
 [ 0.00321742  0.5         0.58888889  0.36586172  0.4857849  -0.12136946
   0.44350649  0.51666667  0.24572604  0.40539636]
 [ 0.07073438  0.53392857  0.73333333  0.27042561  0.51400862  0.13333333
   0.56666667  0.56666667  0.53982334  0.55417453]
 [-0.1         0.45        0.45        0.41485235  0.43343276 -0.0234426
   0.48831169  0.55555556  0.34803881  0.45664767]
 [-0.12820377  0.43875     0.45        0.37618193  0.42137748  0.42111128
   0.68071429  0.83888889  0.54610751  0.70039213]
 [ 0.05769863  0.5275      0.53333333  0.49176564  0.51242173  0.20235412
   0.60125     0.60555556  0.59651     0.59956742]
 [-0.02114696  0.49125     0.50555556  0.41585801  0.46873327  0.17777778
   0.58888889  0.58888889  0.57968971  0.58437893]
 [ 0.07227077  0.535       0.53888889  0.50384859  0.52094523  0.09687394
   0.54625     0.56666667  0.48878765  0.53581193]
 [ 0.50593036  0.74583333  0.78888889  0.72237566  0.74913574  0.45555556
   0.72777778  0.72777778  0.71628015  0.7229539 ]
 [-0.12241422  0.4375      0.48888889  0.39006564  0.43072363  0.37687705
   0.67987013  0.71666667  0.64082846  0.67727356]
 [-0.12480746  0.43701299  0.46111111  0.40385109  0.43502926  0.40609962
   0.70375     0.70555556  0.69567676  0.69977695]
 [ 0.32893473  0.66168831  0.69444444  0.6216383   0.6549397   0.08369726
   0.54285714  0.57777778  0.43184398  0.51749481]]
BDDAE mean:
[0.13353131 0.56486045 0.60333333 0.50456587 0.55474006 0.19039352
 0.5927038  0.63       0.52759368 0.58088089]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.53333333 0.         0.3474359  0.
  0.5        0.72777778 0.         0.42071429]
 [0.         0.5        0.55694444 0.         0.35750916 0.
  0.5        0.625      0.         0.38406593]
 [0.         0.5        0.56944444 0.         0.36263736 0.
  0.5        0.51111111 0.         0.33754579]
 [0.         0.5        0.52222222 0.         0.34249084 0.
  0.5        0.70555556 0.         0.41321429]
 [0.         0.5        0.625      0.         0.38406593 0.
  0.5        0.51111111 0.         0.33754579]
 [0.         0.5        0.52222222 0.         0.34249084 0.
  0.5        0.53333333 0.         0.3474359 ]
 [0.         0.5        0.58055556 0.         0.36692308 0.
  0.5        0.51111111 0.         0.33754579]
 [0.         0.5        0.65833333 0.         0.39692308 0.
  0.5        0.625      0.         0.38406593]
 [0.         0.5        0.69444444 0.         0.40946429 0.
  0.5        0.69444444 0.         0.40946429]
 [0.         0.5        0.45555556 0.         0.31282051 0.
  0.5        0.53333333 0.         0.3474359 ]
 [0.         0.5        0.60277778 0.         0.37549451 0.
  0.5        0.59166667 0.         0.37120879]
 [0.         0.5        0.59166667 0.         0.37120879 0.
  0.5        0.60277778 0.         0.37549451]
 [0.         0.5        0.58055556 0.         0.36692308 0.
  0.5        0.59166667 0.         0.37120879]
 [0.         0.5        0.625      0.         0.38406593 0.
  0.5        0.53333333 0.         0.3474359 ]
 [0.         0.5        0.67083333 0.         0.40131868 0.
  0.5        0.65833333 0.         0.39692308]
 [0.         0.5        0.59166667 0.         0.37120879 0.
  0.5        0.67083333 0.         0.40131868]
 [0.         0.5        0.60277778 0.         0.37549451 0.
  0.5        0.60277778 0.         0.37549451]
 [0.         0.5        0.84166667 0.         0.45656863 0.
  0.5        0.58055556 0.         0.36692308]
 [0.         0.5        0.69444444 0.         0.40946429 0.
  0.5        0.54444444 0.         0.35238095]
 [0.         0.5        0.65833333 0.         0.39692308 0.
  0.5        0.63611111 0.         0.38835165]
 [0.         0.5        0.77222222 0.         0.43571429 0.
  0.5        0.45555556 0.         0.31282051]
 [0.         0.5        0.52222222 0.         0.34249084 0.
  0.5        0.625      0.         0.38406593]
 [0.         0.5        0.56944444 0.         0.36263736 0.
  0.5        0.75       0.         0.42821429]
 [0.         0.5        0.53333333 0.         0.3474359  0.
  0.5        0.55694444 0.         0.35750916]
 [0.         0.5        0.58055556 0.         0.36692308 0.
  0.5        0.51111111 0.         0.33754579]
 [0.         0.5        0.53333333 0.         0.3474359  0.
  0.5        0.58055556 0.         0.36692308]
 [0.         0.5        0.69444444 0.         0.40946429 0.
  0.5        0.51111111 0.         0.33754579]
 [0.         0.5        0.64722222 0.         0.39263736 0.
  0.5        0.60277778 0.         0.37549451]
 [0.         0.5        0.63611111 0.         0.38835165 0.
  0.5        0.54444444 0.         0.35238095]
 [0.         0.5        0.60277778 0.         0.37549451 0.
  0.5        0.61388889 0.         0.37978022]]
DUMMY mean:
[0.         0.5        0.60898148 0.         0.37666721 0.
 0.5        0.59138889 0.         0.36993513]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_37_1
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.328 0.662 0.701 0.586 0.648 0.351 0.676 0.705 0.611 0.662]
 [0.279 0.645 0.667 0.585 0.631 0.276 0.649 0.664 0.599 0.635]
 [0.329 0.651 0.682 0.62  0.638 0.338 0.667 0.689 0.613 0.654]
 [0.026 0.512 0.619 0.045 0.398 0.084 0.542 0.629 0.139 0.441]
 [0.294 0.642 0.704 0.512 0.621 0.283 0.637 0.682 0.537 0.621]
 [0.134 0.565 0.603 0.505 0.555 0.19  0.593 0.63  0.528 0.581]
 [0.    0.5   0.609 0.    0.377 0.    0.5   0.591 0.    0.37 ]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.151 0.073 0.076 0.132 0.08  0.167 0.084 0.076 0.131 0.088]
 [0.153 0.07  0.066 0.115 0.073 0.128 0.077 0.076 0.083 0.079]
 [0.166 0.096 0.096 0.11  0.102 0.136 0.07  0.069 0.108 0.072]
 [0.075 0.034 0.079 0.118 0.06  0.159 0.08  0.07  0.244 0.117]
 [0.178 0.085 0.071 0.199 0.098 0.162 0.084 0.073 0.167 0.093]
 [0.206 0.1   0.116 0.127 0.106 0.148 0.072 0.073 0.122 0.08 ]
 [0.    0.    0.079 0.    0.03  0.    0.    0.071 0.    0.028]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 46.  11.  11.  23.  12.  48.  12.  11.  21.  13.]
 [ 55.  11.  10.  20.  12.  46.  12.  11.  14.  12.]
 [ 50.  15.  14.  18.  16.  40.  10.  10.  18.  11.]
 [293.   7.  13. 262.  15. 188.  15.  11. 175.  27.]
 [ 61.  13.  10.  39.  16.  57.  13.  11.  31.  15.]
 [154.  18.  19.  25.  19.  78.  12.  12.  23.  14.]
 [  0.   0.  13.   0.   8.   0.   0.  12.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_37_1
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  14.0
step (sec):  14.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  88
Elapsed time: 493.8723788976669 minutes
Elapsed time: 8.231206314961115 hours
