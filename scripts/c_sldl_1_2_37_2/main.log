2024-05-11 05:11:43.643028: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-11 05:11:47.243668: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-11 05:11:56.014071: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  14.0
step (sec):  10.5
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  3.5
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_37_2\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:


[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.0102 - mean_squared_error: 0.0102
[1m 6/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0217 - mean_squared_error: 0.0217
[1m12/81[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0243 - mean_squared_error: 0.0243
[1m19/81[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0236 - mean_squared_error: 0.0236
[1m25/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0230 - mean_squared_error: 0.0230
[1m31/81[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0220 - mean_squared_error: 0.0220
[1m37/81[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0211 - mean_squared_error: 0.0211
[1m44/81[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0204 - mean_squared_error: 0.0204
[1m50/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0201 - mean_squared_error: 0.0201
[1m56/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0199 - mean_squared_error: 0.0199
[1m62/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0199 - mean_squared_error: 0.0199
[1m68/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0199 - mean_squared_error: 0.0199
[1m75/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0199 - mean_squared_error: 0.0199
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 10ms/step - loss: 0.0199 - mean_squared_error: 0.0199
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0199 - mean_squared_error: 0.0199 - val_loss: 0.0054 - val_mean_squared_error: 0.0054
Epoch 5/5

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.0090 - mean_squared_error: 0.0090
[1m 6/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0092 - mean_squared_error: 0.0092
[1m12/81[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0091 - mean_squared_error: 0.0091
[1m18/81[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0102 - mean_squared_error: 0.0102
[1m24/81[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0111 - mean_squared_error: 0.0111
[1m30/81[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0116 - mean_squared_error: 0.0116
[1m37/81[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0120 - mean_squared_error: 0.0120
[1m43/81[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0125 - mean_squared_error: 0.0125
[1m50/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0133 - mean_squared_error: 0.0133
[1m56/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0138 - mean_squared_error: 0.0138
[1m62/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0141 - mean_squared_error: 0.0141
[1m68/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0145 - mean_squared_error: 0.0145
[1m74/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - loss: 0.0148 - mean_squared_error: 0.0148
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 10ms/step - loss: 0.0151 - mean_squared_error: 0.0151
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0151 - mean_squared_error: 0.0151 - val_loss: 0.0052 - val_mean_squared_error: 0.0052
(14000, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 14000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3500, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3500, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 875, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 14000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 14000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 875, 6)    â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 875, 6)    â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 875, 12)   â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 875)   â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 10500)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 10500)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     10,501 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 11,323 (44.23 KB)
 Trainable params: 11,323 (44.23 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:41[0m 1s/step - binary_accuracy: 1.0000 - loss: 0.6637
[1m 7/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7711 - loss: 0.6705
[1m14/81[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7533 - loss: 0.6497
[1m20/81[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7538 - loss: 0.6371
[1m26/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7324 - loss: 0.6435
[1m32/81[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7147 - loss: 0.6502
[1m39/81[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6997 - loss: 0.6556
[1m46/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6868 - loss: 0.6598
[1m50/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6821 - loss: 0.6611
[1m56/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6788 - loss: 0.6617
[1m63/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6790 - loss: 0.6586
[1m70/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6785 - loss: 0.6585
[1m76/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6781 - loss: 0.6582
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 13ms/step - binary_accuracy: 0.6770 - loss: 0.6586 - val_binary_accuracy: 0.7000 - val_loss: 0.6141
Epoch 2/10

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 38ms/step - binary_accuracy: 1.0000 - loss: 0.2472
[1m 7/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9796 - loss: 0.3049 
[1m13/81[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8631 - loss: 0.3848
[1m20/81[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7829 - loss: 0.4571
[1m27/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7420 - loss: 0.4961 
[1m33/81[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7215 - loss: 0.5175
[1m39/81[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7038 - loss: 0.5382
[1m46/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6957 - loss: 0.5520
[1m52/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6916 - loss: 0.5623
[1m59/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6900 - loss: 0.5702
[1m66/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6876 - loss: 0.5765
[1m73/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6854 - loss: 0.5808
[1m80/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6850 - loss: 0.5831
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.6849 - loss: 0.5841 - val_binary_accuracy: 0.7000 - val_loss: 0.6010
Epoch 3/10

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.4040
[1m 7/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6639 - loss: 0.7546
[1m14/81[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6284 - loss: 0.7166
[1m20/81[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6057 - loss: 0.7023
[1m26/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5985 - loss: 0.6994
[1m33/81[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5988 - loss: 0.6911
[1m40/81[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6026 - loss: 0.6829
[1m46/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6089 - loss: 0.6751
[1m52/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6148 - loss: 0.6677
[1m58/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6165 - loss: 0.6658
[1m64/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6194 - loss: 0.6629
[1m71/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6234 - loss: 0.6588
[1m78/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6269 - loss: 0.6548
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.6288 - loss: 0.6526 - val_binary_accuracy: 0.7000 - val_loss: 0.5919
Epoch 4/10

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4630
[1m 7/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9354 - loss: 0.3824
[1m14/81[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7733 - loss: 0.5822
[1m21/81[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7528 - loss: 0.6037 
[1m27/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7503 - loss: 0.6045
[1m34/81[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7448 - loss: 0.6077
[1m41/81[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7433 - loss: 0.6067 
[1m48/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7461 - loss: 0.6009
[1m54/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7444 - loss: 0.5983
[1m61/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7441 - loss: 0.5950
[1m68/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7397 - loss: 0.5947
[1m75/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7338 - loss: 0.5952
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7281 - loss: 0.5954 - val_binary_accuracy: 0.7000 - val_loss: 0.6085
Epoch 5/10

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m3s[0m 39ms/step - binary_accuracy: 1.0000 - loss: 0.1356
[1m 7/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8711 - loss: 0.5284 
[1m13/81[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7951 - loss: 0.6436
[1m19/81[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7897 - loss: 0.6463
[1m25/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7972 - loss: 0.6269
[1m31/81[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7992 - loss: 0.6138
[1m37/81[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7958 - loss: 0.6120
[1m43/81[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7910 - loss: 0.6118
[1m49/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7885 - loss: 0.6108
[1m55/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7870 - loss: 0.6085
[1m61/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7850 - loss: 0.6064
[1m67/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7832 - loss: 0.6047
[1m73/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7810 - loss: 0.6031
[1m79/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7782 - loss: 0.6014
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7765 - loss: 0.6009 - val_binary_accuracy: 0.7000 - val_loss: 0.5900
Epoch 6/10

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7051
[1m 7/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6092 - loss: 0.5015     
[1m13/81[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6320 - loss: 0.4950
[1m19/81[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6359 - loss: 0.5208
[1m26/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6499 - loss: 0.5237
[1m33/81[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6585 - loss: 0.5241
[1m40/81[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6628 - loss: 0.5251
[1m46/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6673 - loss: 0.5256
[1m53/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6679 - loss: 0.5286
[1m60/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6697 - loss: 0.5302
[1m66/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6714 - loss: 0.5314
[1m73/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6732 - loss: 0.5329
[1m79/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6743 - loss: 0.5342
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.6746 - loss: 0.5351 - val_binary_accuracy: 0.8000 - val_loss: 0.5759
Epoch 7/10

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.6728
[1m 7/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6163 - loss: 0.6856
[1m13/81[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6971 - loss: 0.5733
[1m20/81[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7265 - loss: 0.5416
[1m27/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7313 - loss: 0.5395
[1m33/81[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7383 - loss: 0.5322
[1m40/81[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7503 - loss: 0.5200
[1m46/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7588 - loss: 0.5116
[1m53/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7677 - loss: 0.5060
[1m60/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7691 - loss: 0.5048
[1m66/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7688 - loss: 0.5043
[1m72/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7665 - loss: 0.5053
[1m79/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7632 - loss: 0.5075 
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7612 - loss: 0.5091 - val_binary_accuracy: 0.5000 - val_loss: 0.6388
Epoch 8/10

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.2993
[1m 8/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8894 - loss: 0.4249 
[1m15/81[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8306 - loss: 0.4636
[1m21/81[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8167 - loss: 0.4612
[1m28/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8083 - loss: 0.4704
[1m35/81[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7912 - loss: 0.4952
[1m42/81[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7838 - loss: 0.5070
[1m48/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7805 - loss: 0.5129
[1m54/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7805 - loss: 0.5147
[1m60/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7801 - loss: 0.5160
[1m67/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7800 - loss: 0.5183
[1m74/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7791 - loss: 0.5200
[1m80/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7783 - loss: 0.5212
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7780 - loss: 0.5217 - val_binary_accuracy: 0.5000 - val_loss: 0.7069
Epoch 9/10

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.1696
[1m 7/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7997 - loss: 0.3396
[1m13/81[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6940 - loss: 0.4636
[1m18/81[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6651 - loss: 0.5006
[1m25/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6314 - loss: 0.5340
[1m31/81[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6211 - loss: 0.5436
[1m38/81[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6099 - loss: 0.5526
[1m45/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6038 - loss: 0.5599
[1m50/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6022 - loss: 0.5632
[1m56/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6014 - loss: 0.5657
[1m63/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6044 - loss: 0.5656
[1m70/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6094 - loss: 0.5640
[1m77/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6147 - loss: 0.5621 
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.6177 - loss: 0.5620 - val_binary_accuracy: 0.7000 - val_loss: 0.6092
Epoch 10/10

[1m 1/81[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4516
[1m 7/81[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 1.0000 - loss: 0.2679
[1m13/81[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9085 - loss: 0.3302
[1m20/81[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8545 - loss: 0.3777
[1m27/81[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8322 - loss: 0.4018
[1m34/81[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8118 - loss: 0.4253
[1m41/81[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7901 - loss: 0.4430 
[1m48/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7698 - loss: 0.4604
[1m54/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7572 - loss: 0.4714
[1m61/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7475 - loss: 0.4792
[1m67/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7437 - loss: 0.4824
[1m74/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7409 - loss: 0.4838 
[1m80/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7376 - loss: 0.4865
[1m81/81[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.7365 - loss: 0.4877 - val_binary_accuracy: 0.7000 - val_loss: 0.6002

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 118ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 118ms/step
predicted [0.15328802 0.50254357 0.7063376  0.15459777 0.6823935  0.52631986
 0.05229483 0.06113546 0.60781825 0.67399496 0.76880455 0.3448231
 0.18758781 0.7171514  0.93034774 0.6044283  0.7914957  0.80468404
 0.7513581  0.8606124  0.7754322  0.5042108  0.6554352 ]
predicted [0 1 1 0 1 1 0 0 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1]
expected [False False  True  True  True  True False False  True  True False  True
 False  True  True False  True  True False  True False  True  True]
accuracy: 0.6956521739130435
confusion matrix: 
[[ 4  5]
 [ 2 12]]
              precision    recall  f1-score   support

       False       0.67      0.44      0.53         9
        True       0.71      0.86      0.77        14

    accuracy                           0.70        23
   macro avg       0.69      0.65      0.65        23
weighted avg       0.69      0.70      0.68        23

macro avg f1-score: 0.6537634408602151
macro avg (UAR): 0.6507936507936507
Sensitivity:  0.4444444444444444
Specificity:  0.8571428571428571
g-mean:  0.6172133998483676
-------- Model Performance ----------: 
accuracy:  [0.60869565 0.65217391 0.69565217 0.73913043 0.65217391 0.52173913
 0.65217391 0.65217391 0.7826087  0.69565217]
gmean:  [0.56343617 0.65465367 0.6172134  0.72374686 0.62994079 0.50395263
 0.62994079 0.53452248 0.75592895 0.6172134 ]
f1_score:  [0.5801217  0.64615385 0.65376344 0.72619048 0.63492063 0.50682261
 0.63492063 0.58928571 0.76673428 0.65376344]
UAR:  [0.57936508 0.6547619  0.65079365 0.72619048 0.63492063 0.50793651
 0.63492063 0.5952381  0.76190476 0.65079365]
Cohen Kappa score:  [0.16194332 0.29770992 0.32067511 0.45238095 0.26984127 0.0155642
 0.26984127 0.20689655 0.53441296 0.32067511]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  14.0
step (sec):  10.496
overlap:  True
perc. of overlap:  25.02857142857143
overlap duration (sec):  3.504
Number of windows / instances:  114
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.763 0.884 0.885 0.879 0.88  0.545 0.768 0.787 0.757 0.768]
 [0.665 0.825 0.832 0.822 0.816 0.496 0.73  0.761 0.713 0.719]
 [0.735 0.865 0.858 0.82  0.848 0.488 0.729 0.759 0.736 0.73 ]
 [0.293 0.636 0.701 0.444 0.596 0.    0.5   0.648 0.    0.393]
 [0.674 0.829 0.842 0.801 0.823 0.33  0.654 0.729 0.524 0.642]
 [0.285 0.64  0.665 0.623 0.639 0.042 0.517 0.591 0.442 0.507]
 [0.    0.5   0.606 0.    0.377 0.    0.5   0.648 0.    0.393]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.47696529  0.73666667  0.74393939  0.71035405  0.72890027  0.35697104
   0.67569444  0.78939394  0.56542314  0.66815949]
 [ 0.31381025  0.66071429  0.65757576  0.63820654  0.64591353  0.39941297
   0.69571429  0.73636364  0.64612636  0.68712535]
 [ 0.53239084  0.76261905  0.77954545  0.72895333  0.75408842  0.44815838
   0.72333333  0.72575758  0.70439554  0.71572677]
 [ 0.27001542  0.63357143  0.64242424  0.58151795  0.62019814  0.09085657
   0.54583333  0.69318182  0.30770406  0.52443462]
 [ 0.52453105  0.75285714  0.78939394  0.72530364  0.75592394  0.59615819
   0.79666667  0.8         0.78650839  0.79466422]
 [ 0.40115223  0.70166667  0.70151515  0.69547009  0.6984632   0.57367377
   0.78738095  0.78863636  0.77687305  0.78352065]
 [ 0.30840348  0.65357143  0.67575758  0.63236169  0.64916196  0.18553903
   0.59333333  0.59772727  0.57550987  0.58629815]
 [ 0.83537099  0.91458333  0.93257576  0.90162546  0.91538749  0.65100622
   0.83107143  0.83257576  0.82203391  0.82185162]
 [ 0.12219065  0.55        0.68333333  0.34269103  0.53633729 -0.01727057
   0.4875      0.58181818  0.30960644  0.47044946]
 [ 0.39400621  0.69666667  0.70075758  0.67917143  0.69126901  0.12920784
   0.5647619   0.56060606  0.52558522  0.54283453]
 [ 0.10485542  0.55446429  0.60681818  0.44434777  0.53736229  0.34210576
   0.66369048  0.70984848  0.61064215  0.65836827]
 [ 0.55630481  0.77416667  0.7969697   0.74811915  0.77116579  0.53690966
   0.76035714  0.78939394  0.74080643  0.76401216]
 [ 0.4377408   0.71642857  0.725       0.70572914  0.71477814  0.09021504
   0.54285714  0.56893939  0.48845477  0.53431232]
 [ 0.05010979  0.52589286  0.57954545  0.42222582  0.51069094  0.34361376
   0.67071429  0.67651515  0.64453233  0.66084457]
 [ 0.28952737  0.66011905  0.69545455  0.57159072  0.63569292  0.23120208
   0.60952381  0.65833333  0.58330794  0.61082142]
 [ 0.3514041   0.67047619  0.69393939  0.64583538  0.66769189  0.50749879
   0.74553571  0.79090909  0.70397897  0.74378984]
 [ 0.45963783  0.72071429  0.74242424  0.70880045  0.72589286  0.1471628
   0.56892857  0.61515152  0.46620067  0.54537387]
 [ 0.06677586  0.53833333  0.8         0.15890239  0.50811774  0.34094513
   0.67357143  0.68484848  0.61226159  0.65459909]
 [ 0.42018795  0.69821429  0.76515152  0.65634517  0.70190593  0.35381579
   0.67619048  0.67575758  0.65825255  0.66650558]
 [ 0.35434698  0.67529762  0.73484848  0.59559634  0.66103875  0.22737788
   0.61267857  0.66742424  0.51692781  0.59131619]
 [ 0.23048405  0.61458333  0.72727273  0.4763643   0.60791906  0.15744323
   0.58        0.57651515  0.56588089  0.57108281]
 [ 0.34046398  0.67166667  0.66742424  0.6240884   0.64568293  0.32540923
   0.65803571  0.71212121  0.59595718  0.65623599]
 [ 0.27893558  0.63797619  0.66515152  0.5668774   0.62125375  0.12286919
   0.55347222  0.70378788  0.34760289  0.54292693]
 [ 0.02483647  0.51333333  0.51136364  0.46786418  0.49691947  0.42227494
   0.70571429  0.72878788  0.66341211  0.69681694]
 [ 0.41443645  0.70619048  0.70984848  0.68868251  0.69892552  0.60271288
   0.8         0.80530303  0.77588051  0.79328422]
 [ 0.24192508  0.61952381  0.63181818  0.58607647  0.60911949  0.49202389
   0.74583333  0.75454545  0.72904944  0.74055528]
 [ 0.43023601  0.70208333  0.76666667  0.66672438  0.70748266  0.51904594
   0.76        0.76136364  0.74239238  0.75249584]
 [ 0.22049584  0.61053571  0.63106061  0.58926301  0.60134519  0.50025458
   0.74654762  0.76287879  0.73009915  0.74558442]
 [ 0.20723432  0.60285714  0.64772727  0.48701644  0.58173057  0.20350005
   0.60214286  0.6219697   0.54244262  0.58920635]
 [ 0.76274533  0.88380952  0.88484848  0.87915473  0.87996032  0.54507382
   0.76785714  0.78712121  0.75677284  0.76808599]]
KNN mean:
[0.34738401 0.67198611 0.70967172 0.61084198 0.66267731 0.3475056
 0.67149802 0.70525253 0.61648737 0.66270943]
---------------------------
---------------------------
DT performance:
[[ 6.97981847e-01  8.56666667e-01  8.57575758e-01  8.22885252e-01
   8.55429015e-01  3.15181057e-01  6.88194444e-01  7.65151515e-01
   5.14068207e-01  6.65770738e-01]
 [ 4.48426279e-01  7.26190476e-01  7.28787879e-01  7.10383694e-01
   7.17233877e-01  4.36351989e-01  7.38214286e-01  7.43181818e-01
   7.01532556e-01  7.24274892e-01]
 [ 3.14616674e-01  6.87976190e-01  6.92424242e-01  7.10485404e-01
   6.80623821e-01  3.22794295e-01  6.45000000e-01  6.46969697e-01
   6.49394146e-01  6.35784632e-01]
 [ 3.59918697e-01  6.30714286e-01  6.28787879e-01  6.32397064e-01
   6.22975913e-01  1.06186591e-01  5.56250000e-01  6.75757576e-01
   4.00624153e-01  5.40963807e-01]
 [ 3.44022513e-01  6.64821429e-01  6.83333333e-01  6.77142633e-01
   6.53714064e-01  2.50457339e-01  6.53333333e-01  6.53787879e-01
   6.76047786e-01  6.49676157e-01]
 [ 5.09397725e-01  7.35000000e-01  7.37878788e-01  6.82767986e-01
   7.31679154e-01  3.49008433e-01  7.26190476e-01  7.27272727e-01
   7.30208128e-01  7.22646520e-01]
 [ 1.13306049e-01  6.12142857e-01  6.12878788e-01  5.52777614e-01
   5.92657310e-01  9.77742894e-02  5.56666667e-01  5.59848485e-01
   5.50289417e-01  5.43142691e-01]
 [ 8.18159686e-01  8.95833333e-01  9.14393939e-01  8.99209164e-01
   8.97495331e-01  4.20670631e-01  7.23928571e-01  7.36363636e-01
   6.49110081e-01  7.11613534e-01]
 [ 2.13845839e-01  6.04166667e-01  6.56818182e-01  5.34788417e-01
   5.95215919e-01  2.67172394e-01  6.17261905e-01  6.66666667e-01
   4.97205559e-01  5.93598111e-01]
 [ 3.60923326e-01  6.60000000e-01  6.66666667e-01  6.40695717e-01
   6.54166389e-01  3.29823360e-01  7.00714286e-01  7.02272727e-01
   6.41202304e-01  6.88329448e-01]
 [-3.00697886e-02  4.77500000e-01  5.26515152e-01  2.91607295e-01
   4.54232434e-01  5.21771272e-01  7.15476190e-01  7.27272727e-01
   6.99062417e-01  7.04918701e-01]
 [ 6.46247135e-01  8.23214286e-01  8.31818182e-01  7.68323539e-01
   8.20842491e-01  4.06269831e-01  6.90892857e-01  6.93181818e-01
   7.22891770e-01  6.78741128e-01]
 [ 2.73855613e-01  6.52380952e-01  6.56818182e-01  6.08369629e-01
   6.44165834e-01  3.08122792e-01  6.76785714e-01  6.84848485e-01
   5.93088054e-01  6.61826923e-01]
 [-1.85213900e-02  4.99107143e-01  5.43939394e-01  4.43046089e-01
   4.90362660e-01  1.75005197e-01  6.04047619e-01  6.05303030e-01
   5.76140992e-01  5.96448274e-01]
 [ 2.87364353e-01  6.40476190e-01  6.85606061e-01  6.20437317e-01
   6.41818484e-01  2.01476388e-01  6.26488095e-01  6.65151515e-01
   5.74239624e-01  6.15832501e-01]
 [ 2.49920064e-01  6.31547619e-01  6.43939394e-01  6.19836473e-01
   6.22639550e-01  4.45993701e-01  7.12500000e-01  7.47727273e-01
   6.96312015e-01  7.04822098e-01]
 [ 6.77650424e-01  8.21547619e-01  8.23484848e-01  8.03186656e-01
   8.11885761e-01  5.34340162e-01  7.59880952e-01  7.71969697e-01
   7.23376454e-01  7.52121914e-01]
 [ 2.90057644e-01  6.87777778e-01  8.06818182e-01  3.85980862e-01
   6.63242051e-01  3.96971830e-01  6.41547619e-01  6.57575758e-01
   6.22130361e-01  6.23076646e-01]
 [ 3.64575705e-01  7.16964286e-01  7.55303030e-01  6.56935265e-01
   7.11048086e-01  2.87130784e-01  6.88214286e-01  6.92424242e-01
   6.87294392e-01  6.76339771e-01]
 [ 3.31037179e-01  6.17261905e-01  6.48484848e-01  5.00725207e-01
   6.00752042e-01  2.93246205e-01  6.70892857e-01  6.86363636e-01
   6.02049549e-01  6.59529466e-01]
 [ 7.89508378e-02  5.45138889e-01  6.50000000e-01  4.18907511e-01
   5.29063099e-01  2.72532068e-04  5.15000000e-01  5.17424242e-01
   3.79772690e-01  4.90780470e-01]
 [ 3.17669036e-01  6.68333333e-01  6.68181818e-01  6.09795275e-01
   6.57560495e-01  2.85299676e-01  6.31250000e-01  6.60606061e-01
   5.86816826e-01  6.21432881e-01]
 [ 1.74130429e-01  5.70238095e-01  5.87878788e-01  5.15352201e-01
   5.62675519e-01  2.77458799e-01  5.95833333e-01  6.87121212e-01
   5.76112018e-01  5.84176925e-01]
 [ 2.01279108e-01  5.46666667e-01  5.52272727e-01  5.02805266e-01
   5.36914613e-01  3.61353787e-01  6.86428571e-01  7.01515152e-01
   6.93250049e-01  6.80287735e-01]
 [ 2.24734128e-01  6.04523810e-01  6.11363636e-01  5.37799183e-01
   5.97183372e-01  4.03364756e-01  6.58333333e-01  6.58333333e-01
   6.86640016e-01  6.49631896e-01]
 [ 3.58003359e-01  6.80238095e-01  6.74242424e-01  6.72703608e-01
   6.67660118e-01  5.02918969e-01  7.10119048e-01  7.15151515e-01
   7.03235478e-01  7.06012876e-01]
 [ 4.52431526e-01  7.18750000e-01  7.78030303e-01  7.16576679e-01
   7.14551641e-01  5.84629386e-01  7.73333333e-01  7.77272727e-01
   7.38392334e-01  7.70115163e-01]
 [ 3.33835444e-02  4.98392857e-01  5.34090909e-01  3.19140736e-01
   4.75744876e-01  3.48541826e-01  6.77500000e-01  6.94696970e-01
   6.25250566e-01  6.74201215e-01]
 [ 1.81151042e-01  5.95000000e-01  6.17424242e-01  5.22956846e-01
   5.92153272e-01  1.95812151e-01  5.70119048e-01  5.74242424e-01
   5.03642615e-01  5.54032357e-01]
 [ 6.65459366e-01  8.24523810e-01  8.31818182e-01  8.22464431e-01
   8.16147749e-01  4.96153927e-01  7.30357143e-01  7.60606061e-01
   7.13118194e-01  7.19343540e-01]]
DT mean:
[0.33133026 0.66310317 0.68691919 0.60668277 0.65372783 0.33071848
 0.6646918  0.68520202 0.62374996 0.65331577]
---------------------------
---------------------------
RF performance:
[[0.61146893 0.885      0.88560606 0.80060922 0.88430458 0.42521579
  0.68541667 0.80075758 0.55327725 0.67414025]
 [0.41196276 0.78119048 0.78712121 0.73744842 0.77717019 0.52332104
  0.79714286 0.8        0.6891208  0.78416195]
 [0.57529273 0.75440476 0.78106061 0.73854185 0.75772436 0.43506707
  0.68666667 0.68484848 0.69001473 0.66834246]
 [0.37733875 0.68357143 0.68106061 0.59951428 0.66860348 0.16549429
  0.52847222 0.68409091 0.36478718 0.50879656]
 [0.47060212 0.76821429 0.79848485 0.68094703 0.77010587 0.54880184
  0.695      0.69469697 0.79475703 0.68187198]
 [0.37492816 0.71833333 0.71818182 0.737698   0.70409757 0.52159679
  0.7852381  0.78939394 0.74315458 0.78209318]
 [0.29176009 0.6575     0.69015152 0.6331555  0.65596154 0.29065221
  0.605      0.60530303 0.59632264 0.58942641]
 [0.84151961 0.93958333 0.94848485 0.95274632 0.94102708 0.47508586
  0.77571429 0.78863636 0.75981277 0.76873515]
 [0.29856383 0.69375    0.74621212 0.53582069 0.67488304 0.11175939
  0.5827381  0.65       0.49988419 0.57500126]
 [0.39480226 0.635      0.63712121 0.68068414 0.63119658 0.21392033
  0.6252381  0.63106061 0.65100611 0.61849428]
 [0.1711188  0.50803571 0.55530303 0.53626905 0.50037941 0.5807885
  0.72559524 0.74545455 0.77536778 0.71751232]
 [0.7319076  0.83607143 0.84090909 0.81971386 0.83113678 0.35406245
  0.72142857 0.7469697  0.66784047 0.71129354]
 [0.49944913 0.69738095 0.7        0.67736086 0.68776723 0.18266104
  0.61547619 0.62878788 0.55391499 0.61223457]
 [0.11159177 0.56428571 0.61363636 0.57935329 0.55881428 0.31732563
  0.66690476 0.66515152 0.63783747 0.65883006]
 [0.35340089 0.67708333 0.73939394 0.61106369 0.66926858 0.23973983
  0.56309524 0.61590909 0.56430218 0.55112805]
 [0.16632357 0.63333333 0.66590909 0.57963123 0.61446193 0.40422136
  0.79464286 0.82424242 0.70278295 0.79593407]
 [0.69359605 0.88916667 0.88636364 0.8571406  0.88135656 0.51061204
  0.77380952 0.78939394 0.72934402 0.77104064]
 [0.32374319 0.61833333 0.83409091 0.41847034 0.59526309 0.50905372
  0.67702381 0.69090909 0.71121113 0.66390263]
 [0.36341623 0.70238095 0.76666667 0.56682413 0.69606885 0.40627687
  0.71404762 0.71515152 0.74884771 0.7072694 ]
 [0.37751788 0.70654762 0.74469697 0.65421711 0.69190009 0.05173733
  0.63857143 0.66969697 0.49082972 0.6235255 ]
 [0.03952269 0.49513889 0.62272727 0.22688371 0.4719203  0.07240582
  0.465      0.46893939 0.49673621 0.44415612]
 [0.39364915 0.59       0.58257576 0.60925334 0.57206349 0.23114497
  0.70089286 0.73712121 0.59345972 0.70223739]
 [0.15708499 0.62964286 0.65151515 0.39408136 0.617529   0.17691743
  0.59513889 0.70075758 0.61583923 0.57611154]
 [0.06420078 0.59833333 0.5969697  0.51242895 0.59157842 0.46237819
  0.67904762 0.68560606 0.64312395 0.66554307]
 [0.31402888 0.58904762 0.59393939 0.63179205 0.57597223 0.50903426
  0.78       0.78787879 0.76590084 0.77396437]
 [0.33239802 0.65833333 0.66363636 0.63178972 0.64545635 0.43134279
  0.81440476 0.825      0.72199589 0.81239539]
 [0.40373477 0.75416667 0.78181818 0.73423348 0.73764589 0.55660554
  0.77833333 0.78106061 0.79396946 0.77421134]
 [0.24841992 0.55982143 0.59545455 0.41676052 0.5518219  0.59658008
  0.73797619 0.74621212 0.69286435 0.73080836]
 [0.22101469 0.61035714 0.64848485 0.56436645 0.59760154 0.12672048
  0.60178571 0.62272727 0.54429601 0.59096719]
 [0.73467795 0.86452381 0.85757576 0.81962625 0.8482967  0.48846501
  0.72857143 0.75909091 0.73646376 0.73018908]]
RF mean:
[0.37830121 0.68995106 0.72050505 0.63128085 0.6800459  0.36396627
 0.68461243 0.71116162 0.65096884 0.67547727]
---------------------------
---------------------------
SVM performance:
[[ 0.10513784  0.55        0.5719697   0.23433793  0.44469188  0.
   0.5         0.76363636  0.          0.43274436]
 [ 0.09944468  0.54619048  0.59621212  0.27963839  0.46988753  0.13452299
   0.5575      0.65909091  0.23944272  0.4849914 ]
 [ 0.38838862  0.68        0.72727273  0.55648714  0.65360528  0.41386594
   0.705       0.70909091  0.6938914   0.70277917]
 [ 0.21492686  0.60285714  0.625       0.47136767  0.55797619  0.
   0.5         0.73636364  0.          0.42406015]
 [ 0.4784843   0.72107143  0.78106061  0.66537589  0.72432515  0.48278807
   0.74333333  0.74090909  0.72111074  0.73194666]
 [ 0.08847118  0.54166667  0.56363636  0.1935131   0.42844538  0.52143731
   0.76071429  0.76287879  0.75356351  0.75836747]
 [ 0.          0.5         0.61515152  0.          0.38070175  0.11894499
   0.56166667  0.56969697  0.39708363  0.50029686]
 [ 0.5766128   0.76458333  0.84545455  0.68316638  0.77514835  0.01276596
   0.50535714  0.63181818  0.05        0.40571035]
 [ 0.          0.5         0.7030303   0.          0.41263158  0.
   0.5         0.68484848  0.          0.40619883]
 [ 0.43629207  0.71666667  0.71818182  0.68212453  0.70228632  0.
   0.5         0.53560606  0.          0.34860681]
 [ 0.          0.5         0.62348485  0.          0.38385965 -0.01702128
   0.49285714  0.5969697   0.          0.37351221]
 [ 0.          0.5         0.60606061  0.          0.37710698  0.
   0.5         0.62348485  0.          0.38385965]
 [ 0.04210526  0.52        0.58787879  0.06324555  0.39600521  0.
   0.5         0.58787879  0.          0.36991744]
 [ 0.          0.5         0.64848485  0.          0.39333333  0.16877018
   0.58        0.60606061  0.30894514  0.4908775 ]
 [ 0.          0.5         0.67575758  0.          0.40298246  0.
   0.5         0.65757576  0.          0.39654971]
 [ 0.          0.5         0.5969697   0.          0.37351221  0.
   0.5         0.64848485  0.          0.39333333]
 [ 0.          0.5         0.60606061  0.          0.37710698  0.
   0.5         0.5969697   0.          0.37351221]
 [ 0.          0.5         0.83333333  0.          0.45443723  0.
   0.5         0.58787879  0.          0.36991744]
 [ 0.          0.5         0.68484848  0.          0.40619883  0.
   0.5         0.56969697  0.          0.3627279 ]
 [ 0.          0.5         0.67575758  0.          0.40298246  0.
   0.5         0.64015152  0.          0.39017544]
 [ 0.          0.5         0.75454545  0.          0.42984962  0.
   0.5         0.51818182  0.          0.34105392]
 [ 0.24984661  0.62833333  0.61287879  0.55510203  0.58475497  0.
   0.5         0.64848485  0.          0.39333333]
 [ 0.          0.5         0.5969697   0.          0.37351221  0.
   0.5         0.73636364  0.          0.42406015]
 [ 0.          0.5         0.52727273  0.          0.34509804  0.20205499
   0.59333333  0.63787879  0.40903338  0.5414881 ]
 [ 0.          0.5         0.56060606  0.          0.35913313  0.44548773
   0.72166667  0.72651515  0.68540943  0.70710206]
 [ 0.          0.5         0.53560606  0.          0.34860681  0.42782522
   0.70821429  0.73787879  0.61513804  0.68514491]
 [ 0.          0.5         0.7030303   0.          0.41263158  0.56089224
   0.78166667  0.78030303  0.76977946  0.77621934]
 [ 0.          0.5         0.63181818  0.          0.38701754  0.
   0.5         0.5969697   0.          0.37351221]
 [ 0.          0.5         0.61515152  0.          0.38070175  0.
   0.5         0.57878788  0.          0.36632267]
 [ 0.29299139  0.63607143  0.70075758  0.44414496  0.59599225  0.
   0.5         0.64848485  0.          0.39333333]]
SVM mean:
[0.09909005 0.54691468 0.65080808 0.16095012 0.45781742 0.11574448
 0.55704365 0.65063131 0.18811325 0.47005516]
---------------------------
---------------------------
GBM performance:
[[ 0.56633831  0.78166667  0.78863636  0.76093698  0.77586802  0.19038113
   0.60555556  0.79848485  0.26783039  0.58398914]
 [ 0.35989829  0.67666667  0.69469697  0.60004358  0.6562816   0.45738328
   0.71964286  0.76363636  0.64504057  0.7115425 ]
 [ 0.53291612  0.76452381  0.7969697   0.70875875  0.76227968  0.35967465
   0.67833333  0.68409091  0.66434171  0.67463814]
 [ 0.44738121  0.73119048  0.73787879  0.71679082  0.72423854 -0.01428571
   0.49444444  0.7280303   0.          0.42120301]
 [ 0.45919624  0.70267857  0.76287879  0.67020999  0.70619631  0.44546851
   0.725       0.72121212  0.7225216   0.7139369 ]
 [ 0.52617241  0.765       0.76287879  0.74403097  0.75448468  0.39225232
   0.66571429  0.675       0.66176463  0.6556993 ]
 [ 0.25150596  0.61535714  0.67727273  0.53897767  0.60572596  0.34356931
   0.655       0.65833333  0.65434363  0.64790154]
 [ 0.81254078  0.90208333  0.92424242  0.88642834  0.9027752   0.44287975
   0.71660714  0.75227273  0.68999855  0.71581933]
 [ 0.11156015  0.53958333  0.69242424  0.24741798  0.51029971  0.02978723
   0.5         0.68484848  0.          0.40619883]
 [ 0.34715215  0.67333333  0.67575758  0.66165973  0.66882395  0.18717304
   0.58571429  0.58636364  0.54310768  0.56479881]
 [-0.04945889  0.47803571  0.56212121  0.27551519  0.43770499  0.68007479
   0.82035714  0.83257576  0.80420894  0.81514015]
 [ 0.57411002  0.80678571  0.83257576  0.77722126  0.81212607  0.21563386
   0.59928571  0.6530303   0.44958209  0.57221504]
 [ 0.40624946  0.70095238  0.71893939  0.67713698  0.69226756 -0.00576554
   0.50821429  0.55151515  0.32289467  0.47275911]
 [-0.06482675  0.48035714  0.57878788  0.17740535  0.43381403  0.16738283
   0.59785714  0.60530303  0.55688756  0.58220821]
 [ 0.14217094  0.56041667  0.70227273  0.29754761  0.5198452  -0.02715335
   0.48363095  0.61515152  0.10400617  0.4177365 ]
 [ 0.0451308   0.52059524  0.59621212  0.22831573  0.44929266  0.28481975
   0.62946429  0.72045455  0.47675987  0.60942227]
 [ 0.67025285  0.84297619  0.85        0.82030005  0.84229201  0.36950292
   0.67452381  0.71212121  0.62708836  0.66769981]
 [ 0.07191571  0.52388889  0.80681818  0.14142136  0.51145212  0.23973959
   0.61857143  0.63863636  0.57454967  0.60257659]
 [ 0.12403008  0.54702381  0.69318182  0.26827814  0.50851258  0.3425082
   0.66        0.68333333  0.59236084  0.63829612]
 [ 0.26352589  0.61041667  0.71666667  0.39422418  0.57861437  0.1616479
   0.57410714  0.6780303   0.29690549  0.5229921 ]
 [ 0.05916199  0.52361111  0.74545455  0.12514378  0.47798835 -0.09757761
   0.46833333  0.4719697   0.35222905  0.43409688]
 [ 0.22362175  0.58666667  0.58712121  0.56806821  0.57394272  0.28892418
   0.63482143  0.71818182  0.47807639  0.61761708]
 [ 0.25332423  0.61678571  0.66742424  0.51898686  0.60432288  0.02706767
   0.50416667  0.72651515  0.05773503  0.44477026]
 [ 0.07324884  0.52666667  0.53560606  0.52770703  0.51898934  0.29396245
   0.65833333  0.66590909  0.59610925  0.6372405 ]
 [ 0.29822286  0.64119048  0.6719697   0.54259468  0.61789916  0.47823761
   0.73        0.73636364  0.70341601  0.72032884]
 [ 0.32707551  0.66285714  0.66666667  0.63511601  0.64489927  0.49958947
   0.74678571  0.76666667  0.71327373  0.73777442]
 [ 0.3463325   0.65416667  0.76363636  0.54976789  0.65377893  0.53330277
   0.76666667  0.76818182  0.76100017  0.76511988]
 [ 0.11356207  0.55232143  0.64924242  0.2862263   0.50446619  0.45462434
   0.71309524  0.75606061  0.67327603  0.71682306]
 [ 0.19999586  0.6225      0.68560606  0.42351336  0.59286298  0.02513339
   0.5222619   0.56287879  0.37267274  0.49523496]
 [ 0.67422051  0.82880952  0.84166667  0.80079122  0.82284688  0.32983375
   0.65357143  0.72878788  0.5235138   0.64151126]]
GBM mean:
[0.30555093 0.64797024 0.71285354 0.51901787 0.62882973 0.26985908
 0.63033532 0.68813131 0.49618315 0.60690968]
---------------------------
---------------------------
BDDAE performance:
[[ 0.09641101  0.54810606  0.55217391  0.53494945  0.54347951  0.05826857
   0.52722222  0.73478261  0.2576381   0.51109469]
 [ 0.22795484  0.61615385  0.61304348  0.60530269  0.6070255   0.45296953
   0.72301587  0.75217391  0.68445024  0.71527334]
 [ 0.03348265  0.51653846  0.54347826  0.42940191  0.4937772   0.06835611
   0.5344697   0.53913043  0.5048657   0.52197969]
 [ 0.14149178  0.57045455  0.57391304  0.54375181  0.55920945  0.22910483
   0.6004902   0.75217391  0.46828432  0.59896396]
 [ 0.455034    0.73253968  0.73478261  0.72668465  0.72489963  0.33167049
   0.66515152  0.66956522  0.63158206  0.65099926]
 [ 0.54611984  0.77272727  0.77391304  0.76118606  0.76913841  0.10484106
   0.55265152  0.55217391  0.5502091   0.55099953]
 [-0.06109981  0.47103175  0.51304348  0.39930274  0.46058056  0.05346603
   0.52651515  0.53043478  0.47998841  0.50648661]
 [ 0.37772676  0.67857143  0.76521739  0.59813448  0.68006708  0.1670331
   0.58166667  0.62173913  0.56156913  0.58079112]
 [ 0.47995867  0.715625    0.8         0.67363297  0.73310169  0.44800089
   0.68571429  0.80869565  0.6047156   0.70718303]
 [ 0.27549802  0.6375      0.63913043  0.63038394  0.63485861  0.25506735
   0.62651515  0.63043478  0.61185137  0.62186499]
 [-0.01140471  0.49325397  0.53043478  0.44574617  0.48386792  0.27045684
   0.63134921  0.66956522  0.58140645  0.62175105]
 [ 0.04340127  0.52222222  0.56086957  0.46937411  0.51020183  0.27899216
   0.64087302  0.65217391  0.62605457  0.63322326]
 [ 0.06807685  0.53384615  0.54347826  0.51967013  0.53015729  0.26660022
   0.63531746  0.64782609  0.62150873  0.62857535]
 [ 0.05833975  0.52375     0.59565217  0.45800027  0.51799657  0.29740248
   0.64924242  0.64782609  0.64257481  0.64523621]
 [-0.13588281  0.42857143  0.53478261  0.28728001  0.42153724  0.27043112
   0.63333333  0.67391304  0.5993717   0.62585746]
 [ 0.08995573  0.55039683  0.55652174  0.49669214  0.52159933  0.16668679
   0.57083333  0.66086957  0.46104487  0.55514834]
 [ 0.48981481  0.74880952  0.75217391  0.7194991   0.73193297  0.14514954
   0.56904762  0.61304348  0.51004964  0.55762492]
 [ 0.29716086  0.64078947  0.83043478  0.49431807  0.63967246  0.10213611
   0.55        0.5826087   0.52047353  0.54607123]
 [ 0.15829671  0.57723214  0.65217391  0.531475    0.57514148  0.25277692
   0.62230769  0.64347826  0.59450046  0.61830024]
 [ 0.18194317  0.58526786  0.69130435  0.46664616  0.57265328  0.14296432
   0.56625     0.64347826  0.49409399  0.56071834]
 [ 0.08712681  0.53921569  0.7173913   0.29608192  0.51866415  0.2387699
   0.61893939  0.62173913  0.60665586  0.61413788]
 [ 0.08171635  0.5405303   0.54347826  0.53221554  0.53773599  0.05459165
   0.5225      0.61304348  0.41459797  0.50572619]
 [ 0.11288485  0.55753968  0.56521739  0.5423976   0.54813346  0.51243085
   0.72941176  0.83913043  0.67737995  0.75017955]
 [ 0.12240172  0.56136364  0.56086957  0.55055624  0.55554967  0.15519725
   0.57615385  0.59130435  0.56108507  0.57450762]
 [-0.00655934  0.49730769  0.50869565  0.47843579  0.49270935  0.07333758
   0.53674242  0.53478261  0.53335489  0.53385865]
 [ 0.17073933  0.58409091  0.59130435  0.54471406  0.56857437  0.2322257
   0.61307692  0.63043478  0.58735314  0.60787173]
 [ 0.29804035  0.63660714  0.73478261  0.56211905  0.63703792  0.47103644
   0.73295455  0.73913043  0.71356918  0.7286251 ]
 [ 0.03938283  0.52125     0.56956522  0.47737092  0.51486307  0.47737911
   0.7297619   0.76521739  0.69793191  0.73110541]
 [ 0.12904507  0.56349206  0.58695652  0.53883004  0.55758348  0.34397781
   0.66961538  0.6826087   0.6512461   0.66599324]
 [ 0.28499407  0.63968254  0.66521739  0.62305491  0.63926768  0.04189672
   0.5175      0.59130435  0.44206474  0.50703292]]
BDDAE mean:
[0.17106838 0.58348224 0.62666667 0.53124026 0.5760339  0.23210725
 0.61128742 0.65449275 0.56304905 0.60590603]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.52727273 0.         0.34509804 0.
  0.5        0.76363636 0.         0.43274436]
 [0.         0.5        0.56060606 0.         0.35913313 0.
  0.5        0.61515152 0.         0.38070175]
 [0.         0.5        0.57878788 0.         0.36632267 0.
  0.5        0.52727273 0.         0.34509804]
 [0.         0.5        0.53560606 0.         0.34860681 0.
  0.5        0.73636364 0.         0.42406015]
 [0.         0.5        0.62348485 0.         0.38385965 0.
  0.5        0.47272727 0.         0.32083333]
 [0.         0.5        0.52727273 0.         0.34509804 0.
  0.5        0.53560606 0.         0.34860681]
 [0.         0.5        0.61515152 0.         0.38070175 0.
  0.5        0.50909091 0.         0.3370098 ]
 [0.         0.5        0.68484848 0.         0.40619883 0.
  0.5        0.63181818 0.         0.38701754]
 [0.         0.5        0.7030303  0.         0.41263158 0.
  0.5        0.68484848 0.         0.40619883]
 [0.         0.5        0.50909091 0.         0.3370098  0.
  0.5        0.53560606 0.         0.34860681]
 [0.         0.5        0.62348485 0.         0.38385965 0.
  0.5        0.60606061 0.         0.37710698]
 [0.         0.5        0.60606061 0.         0.37710698 0.
  0.5        0.62348485 0.         0.38385965]
 [0.         0.5        0.56969697 0.         0.3627279  0.
  0.5        0.58787879 0.         0.36991744]
 [0.         0.5        0.64848485 0.         0.39333333 0.
  0.5        0.53560606 0.         0.34860681]
 [0.         0.5        0.67575758 0.         0.40298246 0.
  0.5        0.65757576 0.         0.39654971]
 [0.         0.5        0.5969697  0.         0.37351221 0.
  0.5        0.64848485 0.         0.39333333]
 [0.         0.5        0.60606061 0.         0.37710698 0.
  0.5        0.5969697  0.         0.37351221]
 [0.         0.5        0.83333333 0.         0.45443723 0.
  0.5        0.58787879 0.         0.36991744]
 [0.         0.5        0.68484848 0.         0.40619883 0.
  0.5        0.56969697 0.         0.3627279 ]
 [0.         0.5        0.67575758 0.         0.40298246 0.
  0.5        0.64015152 0.         0.39017544]
 [0.         0.5        0.75454545 0.         0.42984962 0.
  0.5        0.51818182 0.         0.34105392]
 [0.         0.5        0.47272727 0.         0.32083333 0.
  0.5        0.64848485 0.         0.39333333]
 [0.         0.5        0.5969697  0.         0.37351221 0.
  0.5        0.73636364 0.         0.42406015]
 [0.         0.5        0.52727273 0.         0.34509804 0.
  0.5        0.56060606 0.         0.35913313]
 [0.         0.5        0.56060606 0.         0.35913313 0.
  0.5        0.50909091 0.         0.3370098 ]
 [0.         0.5        0.53560606 0.         0.34860681 0.
  0.5        0.56969697 0.         0.3627279 ]
 [0.         0.5        0.7030303  0.         0.41263158 0.
  0.5        0.51818182 0.         0.34105392]
 [0.         0.5        0.63181818 0.         0.38701754 0.
  0.5        0.5969697  0.         0.37351221]
 [0.         0.5        0.61515152 0.         0.38070175 0.
  0.5        0.57878788 0.         0.36632267]
 [0.         0.5        0.60606061 0.         0.37710698 0.
  0.5        0.64848485 0.         0.39333333]]
DUMMY mean:
[0.         0.5        0.6129798  0.         0.37844664 0.
 0.5        0.59835859 0.         0.37293749]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_37_2
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.347 0.672 0.71  0.611 0.663 0.348 0.671 0.705 0.616 0.663]
 [0.331 0.663 0.687 0.607 0.654 0.331 0.665 0.685 0.624 0.653]
 [0.378 0.69  0.721 0.631 0.68  0.364 0.685 0.711 0.651 0.675]
 [0.099 0.547 0.651 0.161 0.458 0.116 0.557 0.651 0.188 0.47 ]
 [0.306 0.648 0.713 0.519 0.629 0.27  0.63  0.688 0.496 0.607]
 [0.171 0.583 0.627 0.531 0.576 0.232 0.611 0.654 0.563 0.606]
 [0.    0.5   0.613 0.    0.378 0.    0.5   0.598 0.    0.373]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.186 0.092 0.084 0.148 0.098 0.181 0.091 0.079 0.136 0.095]
 [0.21  0.105 0.1   0.146 0.108 0.134 0.063 0.061 0.093 0.066]
 [0.198 0.111 0.102 0.149 0.114 0.165 0.087 0.079 0.105 0.091]
 [0.167 0.078 0.08  0.246 0.119 0.189 0.094 0.071 0.285 0.137]
 [0.223 0.113 0.09  0.221 0.128 0.192 0.092 0.077 0.226 0.111]
 [0.168 0.082 0.094 0.108 0.085 0.138 0.065 0.077 0.096 0.071]
 [0.    0.    0.077 0.    0.029 0.    0.    0.071 0.    0.028]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 54.  14.  12.  24.  15.  52.  14.  11.  22.  14.]
 [ 63.  16.  15.  24.  17.  41.   9.   9.  15.  10.]
 [ 52.  16.  14.  24.  17.  45.  13.  11.  16.  13.]
 [169.  14.  12. 153.  26. 163.  17.  11. 152.  29.]
 [ 73.  17.  13.  43.  20.  71.  15.  11.  46.  18.]
 [ 98.  14.  15.  20.  15.  59.  11.  12.  17.  12.]
 [  0.   0.  13.   0.   8.   0.   0.  12.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_37_2
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  14.0
step (sec):  10.496
overlap:  True
perc. of overlap:  25.02857142857143
overlap duration (sec):  3.504
Number of windows / instances:  114
Elapsed time: 621.1762248953183 minutes
Elapsed time: 10.352937081588639 hours
