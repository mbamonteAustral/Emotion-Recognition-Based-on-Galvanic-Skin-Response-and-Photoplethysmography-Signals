2024-05-06 20:30:36.861842: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-06 20:30:40.693354: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-06 20:30:50.230846: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  15.0
step (sec):  15.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_38\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:


[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0240 - val_mean_squared_error: 0.0240
Epoch 3/5

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0163 - mean_squared_error: 0.0163
[1m 6/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0135 - mean_squared_error: 0.0135
[1m12/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0143 - mean_squared_error: 0.0143
[1m18/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0150 - mean_squared_error: 0.0150
[1m23/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0155 - mean_squared_error: 0.0155
[1m28/57[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0159 - mean_squared_error: 0.0159
[1m33/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0161 - mean_squared_error: 0.0161
[1m39/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0163 - mean_squared_error: 0.0163
[1m44/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0164 - mean_squared_error: 0.0164
[1m50/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0163 - mean_squared_error: 0.0163
[1m55/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - loss: 0.0162 - mean_squared_error: 0.0162
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - loss: 0.0162 - mean_squared_error: 0.0162 - val_loss: 0.0206 - val_mean_squared_error: 0.0206
Epoch 4/5

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0030 - mean_squared_error: 0.0030
[1m 5/57[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - loss: 0.0188 - mean_squared_error: 0.0188
[1m10/57[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0207 - mean_squared_error: 0.0207
[1m16/57[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0199 - mean_squared_error: 0.0199
[1m22/57[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0190 - mean_squared_error: 0.0190
[1m28/57[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0188 - mean_squared_error: 0.0188
[1m34/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0184 - mean_squared_error: 0.0184
[1m40/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0184 - mean_squared_error: 0.0184
[1m46/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0183 - mean_squared_error: 0.0183
[1m51/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0182 - mean_squared_error: 0.0182
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 11ms/step - loss: 0.0181 - mean_squared_error: 0.0181
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0196 - val_mean_squared_error: 0.0196
Epoch 5/5

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - loss: 0.0092 - mean_squared_error: 0.0092
[1m 6/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0081 - mean_squared_error: 0.0081
[1m12/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0101 - mean_squared_error: 0.0101
[1m18/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0114 - mean_squared_error: 0.0114
[1m25/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0117 - mean_squared_error: 0.0117
[1m31/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0119 - mean_squared_error: 0.0119
[1m36/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0125 - mean_squared_error: 0.0125
[1m42/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0129 - mean_squared_error: 0.0129
[1m48/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0133 - mean_squared_error: 0.0133
[1m54/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - loss: 0.0136 - mean_squared_error: 0.0136
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0138 - mean_squared_error: 0.0138 - val_loss: 0.0194 - val_mean_squared_error: 0.0194
(15008, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 15008, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3752, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3752, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 938, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 15008, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 15008, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 938, 6)    â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 938, 6)    â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 938, 12)   â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 938)   â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 11256)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 11256)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     11,257 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 12,079 (47.18 KB)
 Trainable params: 12,079 (47.18 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:30[0m 2s/step - binary_accuracy: 0.0000e+00 - loss: 0.9215
[1m 7/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.3578 - loss: 0.8353    
[1m12/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.4359 - loss: 0.8291
[1m18/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.4758 - loss: 0.8186
[1m25/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5170 - loss: 0.7917
[1m31/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5425 - loss: 0.7725
[1m36/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5611 - loss: 0.7573
[1m42/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5762 - loss: 0.7463
[1m48/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5866 - loss: 0.7405
[1m54/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5979 - loss: 0.7327
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 14ms/step - binary_accuracy: 0.6061 - loss: 0.7261 - val_binary_accuracy: 0.4286 - val_loss: 0.9987
Epoch 2/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2964
[1m 6/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9389 - loss: 0.3594
[1m12/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8180 - loss: 0.6104
[1m18/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8070 - loss: 0.6367
[1m24/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8049 - loss: 0.6318
[1m30/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7980 - loss: 0.6370
[1m36/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7879 - loss: 0.6429
[1m43/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7753 - loss: 0.6485
[1m49/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7677 - loss: 0.6505
[1m55/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7640 - loss: 0.6497
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.7634 - loss: 0.6483 - val_binary_accuracy: 0.4286 - val_loss: 0.9663
Epoch 3/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4390
[1m 7/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7520 - loss: 0.5537
[1m12/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7237 - loss: 0.5504
[1m19/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6891 - loss: 0.5816
[1m25/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6886 - loss: 0.5772
[1m32/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6976 - loss: 0.5666
[1m38/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7031 - loss: 0.5581
[1m43/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7042 - loss: 0.5560
[1m49/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7040 - loss: 0.5551
[1m56/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7049 - loss: 0.5520
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.7054 - loss: 0.5511 - val_binary_accuracy: 0.4286 - val_loss: 0.9514
Epoch 4/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 34ms/step - binary_accuracy: 1.0000 - loss: 0.3219
[1m 6/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9722 - loss: 0.2778
[1m13/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8503 - loss: 0.3959
[1m19/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8274 - loss: 0.4157
[1m25/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8142 - loss: 0.4242
[1m30/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8000 - loss: 0.4355
[1m36/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7869 - loss: 0.4470
[1m43/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7768 - loss: 0.4564
[1m49/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7699 - loss: 0.4626
[1m55/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7659 - loss: 0.4654
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.7643 - loss: 0.4663 - val_binary_accuracy: 0.4286 - val_loss: 1.1976
Epoch 5/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2568
[1m 8/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8894 - loss: 0.3166 
[1m14/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8757 - loss: 0.3177
[1m20/57[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8707 - loss: 0.3273
[1m27/57[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8549 - loss: 0.3438
[1m33/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8485 - loss: 0.3514
[1m39/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8430 - loss: 0.3543
[1m45/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8420 - loss: 0.3538
[1m51/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8435 - loss: 0.3518
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.8408 - loss: 0.3634 - val_binary_accuracy: 0.4286 - val_loss: 1.3756
Epoch 6/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0501
[1m 8/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5537 - loss: 0.8565 
[1m14/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6141 - loss: 0.7702
[1m20/57[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6606 - loss: 0.7226
[1m26/57[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6885 - loss: 0.6912
[1m32/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7087 - loss: 0.6594
[1m38/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7200 - loss: 0.6378
[1m44/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7285 - loss: 0.6205
[1m51/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7396 - loss: 0.5993
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7451 - loss: 0.5844
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.7459 - loss: 0.5822 - val_binary_accuracy: 0.4286 - val_loss: 1.1675
Epoch 7/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 40ms/step - binary_accuracy: 1.0000 - loss: 0.0453
[1m 6/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9722 - loss: 0.1288
[1m12/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8570 - loss: 0.2564
[1m19/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8395 - loss: 0.2897
[1m24/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8427 - loss: 0.3055
[1m30/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8438 - loss: 0.3204
[1m37/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8424 - loss: 0.3394
[1m43/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8414 - loss: 0.3512
[1m49/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8420 - loss: 0.3571
[1m55/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8423 - loss: 0.3617
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.8422 - loss: 0.3643 - val_binary_accuracy: 0.4286 - val_loss: 1.1750
Epoch 8/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1626
[1m 7/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 1.0000 - loss: 0.2529
[1m13/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 1.0000 - loss: 0.2235
[1m20/57[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 1.0000 - loss: 0.2196
[1m26/57[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9970 - loss: 0.2236
[1m32/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9881 - loss: 0.2398
[1m38/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9776 - loss: 0.2555
[1m43/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9688 - loss: 0.2688
[1m49/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9617 - loss: 0.2814
[1m55/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9554 - loss: 0.2909
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.9532 - loss: 0.2949 - val_binary_accuracy: 0.4286 - val_loss: 1.0128
Epoch 9/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 2.0036
[1m 7/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6092 - loss: 0.9424    
[1m13/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6992 - loss: 0.7462
[1m19/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7527 - loss: 0.6332
[1m25/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7799 - loss: 0.5622
[1m31/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7952 - loss: 0.5135
[1m35/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8008 - loss: 0.4940
[1m41/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8059 - loss: 0.4747
[1m47/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8073 - loss: 0.4660
[1m53/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8089 - loss: 0.4589
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.8109 - loss: 0.4541 - val_binary_accuracy: 0.4286 - val_loss: 1.0250
Epoch 10/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.4880
[1m 7/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9796 - loss: 0.3553
[1m12/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9386 - loss: 0.3477
[1m18/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8989 - loss: 0.3454
[1m24/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8550 - loss: 0.3651
[1m30/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8318 - loss: 0.3774
[1m35/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8211 - loss: 0.3837
[1m40/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8171 - loss: 0.3837
[1m45/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8168 - loss: 0.3806
[1m51/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8189 - loss: 0.3758
[1m56/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8215 - loss: 0.3719
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.8222 - loss: 0.3708 - val_binary_accuracy: 0.4286 - val_loss: 1.4016

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 111ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 111ms/step
predicted [0.95863605 0.746245   0.33657584 0.9713034  0.9117855  0.96379346
 0.0030799  0.9795581  0.79615355 0.2733956  0.75986654 0.83904934
 0.937156   0.6387502  0.6717621  0.90220606]
predicted [1 1 0 1 1 1 0 1 1 0 1 1 1 1 1 1]
expected [False False  True False  True False False  True  True  True  True False
  True False  True  True]
accuracy: 0.5
confusion matrix: 
[[1 6]
 [2 7]]
              precision    recall  f1-score   support

       False       0.33      0.14      0.20         7
        True       0.54      0.78      0.64         9

    accuracy                           0.50        16
   macro avg       0.44      0.46      0.42        16
weighted avg       0.45      0.50      0.45        16

macro avg f1-score: 0.4181818181818182
macro avg (UAR): 0.46031746031746035
Sensitivity:  0.14285714285714285
Specificity:  0.7777777777777778
g-mean:  0.3333333333333333
-------- Model Performance ----------: 
accuracy:  [0.625  0.75   0.6875 0.75   0.5625 0.625  0.4375 0.625  0.625  0.5   ]
gmean:  [0.37796447 0.74535599 0.66666667 0.74535599 0.53452248 0.50395263
 0.43643578 0.62994079 0.6172134  0.33333333]
f1_score:  [0.5        0.74603175 0.67611336 0.74603175 0.5465587  0.56363636
 0.43529412 0.625      0.61904762 0.41818182]
UAR:  [0.57142857 0.74603175 0.67460317 0.74603175 0.54761905 0.58730159
 0.45238095 0.63492063 0.65079365 0.46031746]
Cohen Kappa score:  [ 0.15789474  0.49206349  0.35483871  0.49206349  0.09677419  0.18644068
 -0.09090909  0.26153846  0.28358209 -0.08474576]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  15.008
step (sec):  15.008
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  80
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.793 0.899 0.9   0.893 0.895 0.41  0.712 0.762 0.615 0.693]
 [0.434 0.716 0.725 0.728 0.705 0.179 0.6   0.675 0.506 0.596]
 [0.488 0.725 0.75  0.658 0.703 0.319 0.64  0.725 0.591 0.624]
 [0.    0.5   0.588 0.    0.369 0.    0.5   0.675 0.    0.402]
 [0.422 0.725 0.75  0.65  0.711 0.024 0.523 0.65  0.279 0.495]
 [0.215 0.607 0.619 0.559 0.588 0.007 0.504 0.588 0.352 0.471]
 [0.    0.5   0.588 0.    0.369 0.    0.5   0.675 0.    0.402]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.49666667  0.74916667  0.75        0.74571755  0.74674603  0.29809524
   0.64166667  0.8         0.43251857  0.62809191]
 [ 0.3719888   0.6925      0.6875      0.66217195  0.67300866  0.41456044
   0.69666667  0.75        0.59509362  0.68091464]
 [ 0.45954212  0.72416667  0.75        0.69082778  0.71993506  0.32794118
   0.6675      0.6625      0.64787182  0.65531746]
 [ 0.3304902   0.66916667  0.6625      0.64114107  0.64969697  0.2903663
   0.64666667  0.7375      0.49249984  0.6245954 ]
 [ 0.39501832  0.6875      0.725       0.6545614   0.68911255  0.475
   0.7375      0.7375      0.71675554  0.7281746 ]
 [ 0.375       0.68833333  0.6875      0.65929042  0.67344156  0.47436975
   0.73916667  0.7375      0.71627064  0.72665945]
 [ 0.1944721   0.59583333  0.625       0.51654659  0.58508658  0.10873016
   0.54833333  0.55        0.51437341  0.53676046]
 [ 0.75238095  0.87166667  0.9         0.8573779   0.87412587  0.40243913
   0.70666667  0.725       0.637773    0.68312243]
 [ 0.2657326   0.62        0.7         0.51436538  0.62045954  0.11311355
   0.565       0.65        0.36864482  0.53961594]
 [ 0.28333333  0.645       0.6375      0.58392512  0.61862193  0.26683473
   0.63333333  0.6375      0.56985969  0.61087302]
 [ 0.22421569  0.61333333  0.6375      0.54052545  0.5910878   0.41119048
   0.7025      0.725       0.6414597   0.69008492]
 [ 0.47763736  0.73333333  0.7625      0.69389377  0.72611833  0.2838677
   0.63666667  0.6625      0.5471949   0.61818015]
 [ 0.40094484  0.70333333  0.725       0.64434445  0.68953824  0.16160598
   0.58        0.5625      0.48301431  0.53601732]
 [ 0.2146003   0.60666667  0.675       0.4283709   0.57616717  0.42166667
   0.71083333  0.725       0.61658014  0.68660728]
 [ 0.23813187  0.62166667  0.7         0.46314481  0.59277389  0.16388602
   0.575       0.6375      0.49530023  0.56799978]
 [ 0.00239927  0.49666667  0.55        0.33976791  0.46578311  0.25534044
   0.64        0.7         0.46450859  0.60649739]
 [ 0.24501832  0.62833333  0.6625      0.49565899  0.59963758  0.11586081
   0.555       0.6         0.45316136  0.5425974 ]
 [ 0.09166667  0.56190476  0.7875      0.17559289  0.50493506  0.24485994
   0.6225      0.625       0.60994007  0.61370851]
 [ 0.42501832  0.72        0.775       0.62496718  0.70289044  0.28280543
   0.64333333  0.65        0.58380578  0.62555556]
 [ 0.36295482  0.67666667  0.7125      0.59996043  0.65557554  0.22501832
   0.61166667  0.675       0.46609706  0.59103563]
 [ 0.04        0.51785714  0.7375      0.19299543  0.49621712 -0.16111111
   0.41916667  0.4125      0.31618175  0.38035354]
 [ 0.08007519  0.54166667  0.5375      0.45997668  0.51109668  0.25683473
   0.62583333  0.65        0.60347227  0.62168831]
 [ 0.18282244  0.59        0.6         0.52769382  0.56226551  0.27274725
   0.61666667  0.775       0.39831277  0.60801199]
 [ 0.00225564  0.50083333  0.5125      0.34771516  0.46146465  0.39333333
   0.69916667  0.7375      0.53852596  0.65832112]
 [ 0.28009768  0.63583333  0.65        0.59397582  0.61922078  0.5
   0.75        0.75        0.7055183   0.73660173]
 [ 0.24627451  0.62666667  0.625       0.59286059  0.60747475  0.34506765
   0.6775      0.6875      0.60227338  0.64977633]
 [ 0.37774725  0.69166667  0.7375      0.61010683  0.68311688  0.425
   0.7125      0.7125      0.67398377  0.69556999]
 [ 0.21088235  0.60333333  0.625       0.57727989  0.59675325  0.52045033
   0.75833333  0.775       0.73525416  0.75348485]
 [ 0.01846154  0.515       0.575       0.33482598  0.47995338  0.31142857
   0.65333333  0.6625      0.58841264  0.63378788]
 [ 0.79333333  0.89916667  0.9         0.89281529  0.89460317  0.40960784
   0.71166667  0.7625      0.61455984  0.69346154]]
KNN mean:
[0.29463875 0.6475754  0.68708333 0.55541325 0.62889694 0.3003637
 0.64947222 0.6825     0.56097393 0.63078222]
---------------------------
---------------------------
DT performance:
[[ 0.5         0.71        0.725       0.70718279  0.7044733   0.25619048
   0.64285714  0.7125      0.54331233  0.61419025]
 [ 0.42858974  0.72333333  0.725       0.59349525  0.69378066  0.40798535
   0.69583333  0.7125      0.63363596  0.68183983]
 [ 0.37904762  0.69583333  0.7125      0.74996132  0.69489177  0.19444444
   0.55416667  0.55        0.55610869  0.53556999]
 [ 0.34954212  0.72916667  0.7375      0.58231957  0.71191919  0.29869963
   0.64666667  0.7         0.60024192  0.63732934]
 [ 0.41716117  0.67083333  0.6875      0.60547664  0.63883838  0.35
   0.6875      0.6875      0.62461648  0.66891775]
 [ 0.49666667  0.73166667  0.7375      0.6847653   0.72477633  0.38382353
   0.69        0.6875      0.61354177  0.65735209]
 [ 0.21335165  0.605       0.625       0.48062806  0.59293651  0.13382353
   0.65416667  0.65        0.54715735  0.62886003]
 [ 0.7002381   0.82        0.8375      0.85349686  0.81336219  0.35911096
   0.69333333  0.7         0.6561771   0.67457431]
 [ 0.26241758  0.6         0.675       0.36694701  0.58497669  0.12271673
   0.58        0.625       0.42815653  0.55462981]
 [ 0.16833333  0.59666667  0.6         0.51626945  0.58041126  0.1781746
   0.5875      0.6         0.56585972  0.56292041]
 [ 0.01547619  0.50333333  0.525       0.3857259   0.49393939  0.21668498
   0.63916667  0.65        0.57710087  0.62577201]
 [ 0.40358974  0.69166667  0.7         0.68520722  0.67468254  0.47040616
   0.65583333  0.675       0.64434189  0.65613997]
 [ 0.40953782  0.73833333  0.75        0.70515443  0.73175325 -0.03314837
   0.455       0.45        0.34229729  0.42134921]
 [ 0.26209114  0.61833333  0.675       0.52987617  0.6086347   0.38595238
   0.65083333  0.6625      0.62704775  0.63877345]
 [ 0.11130647  0.52833333  0.625       0.27864534  0.51067599  0.07464016
   0.56666667  0.5875      0.55993606  0.55541126]
 [ 0.19057854  0.5075      0.55        0.49832611  0.49937229  0.23304353
   0.64166667  0.675       0.54540628  0.60436896]
 [ 0.42835165  0.69        0.725       0.5679859   0.66334443  0.4059707
   0.63166667  0.6625      0.57420448  0.61636364]
 [ 0.22        0.61547619  0.775       0.34780811  0.5818315   0.21788515
   0.50166667  0.525       0.49255045  0.47853535]
 [ 0.37322344  0.69333333  0.75        0.62091017  0.69170163  0.36833333
   0.69666667  0.7         0.6962355   0.67940837]
 [ 0.2941453   0.685       0.7         0.59588776  0.67001443  0.04346154
   0.52333333  0.575       0.41698241  0.50952381]
 [ 0.02285714  0.52738095  0.675       0.39173839  0.5158708  -0.24807692
   0.38666667  0.4         0.41037309  0.37190476]
 [ 0.43846154  0.6675      0.675       0.63459289  0.65801587  0.04490842
   0.52916667  0.5625      0.5202993   0.51673882]
 [ 0.12960784  0.6325      0.625       0.5816113   0.60582251  0.11880952
   0.54        0.625       0.37137543  0.52252192]
 [ 0.18556022  0.5325      0.5375      0.50889607  0.51904762  0.50047619
   0.715       0.725       0.58098856  0.70924242]
 [ 0.21739029  0.55416667  0.5625      0.5409553   0.53106782  0.525
   0.75        0.75        0.7578966   0.74595238]
 [ 0.275       0.62666667  0.625       0.56707021  0.61176046  0.23222689
   0.62833333  0.6375      0.54089793  0.60556999]
 [ 0.39323745  0.70333333  0.7375      0.62238101  0.6811594   0.35
   0.675       0.675       0.65691681  0.65994949]
 [ 0.05931803  0.55333333  0.5625      0.49659095  0.52637807  0.36596639
   0.665       0.6875      0.62377173  0.64930569]
 [ 0.22391187  0.61666667  0.675       0.45902108  0.58688867  0.14254902
   0.52        0.525       0.46669639  0.49919192]
 [ 0.43371364  0.71583333  0.725       0.7278534   0.70468975  0.17941392
   0.6         0.675       0.50601128  0.59551782]]
DT mean:
[0.30009021 0.64278968 0.67458333 0.56289266 0.62690058 0.24264907
 0.61345635 0.635      0.5560046  0.59592417]
---------------------------
---------------------------
RF performance:
[[ 0.43155678  0.805       0.8125      0.73326816  0.79857143  0.31238095
   0.60833333  0.75        0.45670153  0.60281052]
 [ 0.3897619   0.78166667  0.775       0.69963266  0.77134921  0.40904762
   0.73666667  0.75        0.6617665   0.72689755]
 [ 0.29        0.70416667  0.725       0.67404314  0.7033189   0.2
   0.63416667  0.6375      0.56957414  0.62112554]
 [ 0.37016807  0.69333333  0.7         0.54683002  0.67704906  0.30822344
   0.555       0.65        0.50230453  0.51817516]
 [ 0.44012821  0.68416667  0.7125      0.67047332  0.67199134  0.45
   0.7125      0.7125      0.62799279  0.69731602]
 [ 0.52166667  0.7025      0.7         0.68286186  0.69095238  0.51722689
   0.75333333  0.7625      0.59705539  0.73777778]
 [ 0.20026611  0.63        0.65        0.49368932  0.60865079  0.22777778
   0.55833333  0.5625      0.51036492  0.53083694]
 [ 0.75357143  0.905       0.9125      0.8952215   0.90002886  0.54346154
   0.68666667  0.7         0.67715341  0.65939227]
 [ 0.10944624  0.665       0.7125      0.586684    0.63422078  0.05761905
   0.56833333  0.6625      0.39413195  0.54402264]
 [ 0.26833333  0.615       0.6125      0.58749674  0.58764791  0.09038462
   0.5775      0.575       0.67504193  0.55806638]
 [ 0.22116462  0.50083333  0.525       0.50519467  0.46879509  0.21406593
   0.655       0.675       0.51311836  0.63957265]
 [ 0.40616462  0.76083333  0.775       0.71539334  0.7576912   0.5103663
   0.65666667  0.675       0.5969947   0.65089466]
 [ 0.42194139  0.69833333  0.7         0.69132384  0.67674603  0.07262421
   0.61166667  0.6         0.47767714  0.56755411]
 [ 0.18952381  0.60833333  0.6875      0.3060795   0.57420135  0.32774725
   0.69        0.7         0.64534463  0.68531025]
 [ 0.09787546  0.58833333  0.6875      0.54652875  0.56151682  0.16968864
   0.57666667  0.65        0.4874911   0.5641847 ]
 [ 0.1707326   0.53916667  0.575       0.34213978  0.50763181  0.2338203
   0.72666667  0.75        0.56048696  0.70548341]
 [ 0.48406593  0.79        0.8         0.82192446  0.78717893  0.44516807
   0.725       0.75        0.7081134   0.71029582]
 [ 0.06417582  0.67380952  0.7625      0.36031794  0.59208791  0.33694752
   0.6625      0.675       0.6336157   0.65261905]
 [ 0.48776557  0.665       0.75        0.64758184  0.66289044  0.40294118
   0.73166667  0.725       0.74667576  0.72055556]
 [ 0.26978022  0.73166667  0.75        0.69650438  0.71972583  0.06864792
   0.59        0.65        0.53092021  0.56715729]
 [-0.02095238  0.49404762  0.6625      0.29981012  0.44903097 -0.21785714
   0.38916667  0.4         0.4308524   0.3683189 ]
 [ 0.265       0.6375      0.6375      0.59384663  0.62203463  0.17869963
   0.54666667  0.5875      0.56630903  0.5125974 ]
 [ 0.19428571  0.55        0.5625      0.48714573  0.51290043  0.225
   0.565       0.675       0.32012391  0.55353313]
 [ 0.36025641  0.68333333  0.6875      0.4565234   0.67078644  0.26166667
   0.75083333  0.7875      0.65551465  0.74042402]
 [ 0.39458199  0.72166667  0.725       0.52385473  0.71313131  0.525
   0.75        0.75        0.73558631  0.72580808]
 [ 0.23088235  0.65083333  0.65        0.66578721  0.63295094  0.4160989
   0.66583333  0.6875      0.51428677  0.65596681]
 [ 0.50642857  0.725       0.7625      0.64259826  0.71888112  0.6
   0.7625      0.7625      0.62647817  0.74423521]
 [ 0.09681319  0.53666667  0.575       0.3213757   0.53150072  0.41305753
   0.675       0.7         0.65288839  0.65566933]
 [ 0.12932234  0.59833333  0.65        0.44805271  0.57872794  0.23846154
   0.61666667  0.625       0.50709765  0.60935786]
 [ 0.48787546  0.725       0.75        0.65772443  0.70269675  0.31880952
   0.64        0.725       0.590618    0.62413808]]
RF mean:
[0.30775275 0.66881746 0.69958333 0.5766636  0.64949624 0.29523586
 0.64594444 0.67708333 0.57240934 0.62833657]
---------------------------
---------------------------
SVM performance:
[[0.         0.5        0.5375     0.         0.34871795 0.
  0.5        0.7625     0.         0.43238095]
 [0.         0.5        0.5625     0.         0.35897436 0.
  0.5        0.6        0.         0.37435897]
 [0.         0.5        0.575      0.         0.36410256 0.26388889
  0.63083333 0.625      0.57818539 0.60280664]
 [0.03846154 0.51666667 0.55       0.05773503 0.37692308 0.
  0.5        0.7125     0.         0.41538462]
 [0.4021978  0.68333333 0.75       0.56948118 0.67058275 0.4
  0.7        0.7        0.66362843 0.68374459]
 [0.025      0.51333333 0.5125     0.06324555 0.36190476 0.51346154
  0.7575     0.7625     0.7159416  0.73883838]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.525      0.         0.34358974]
 [0.07692308 0.53333333 0.6875     0.11547005 0.45421245 0.
  0.5        0.625      0.         0.38461538]
 [0.         0.5        0.675      0.         0.4021978  0.
  0.5        0.6875     0.         0.40659341]
 [0.05       0.52583333 0.525      0.11324555 0.38493506 0.
  0.5        0.5375     0.         0.34871795]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.575      0.         0.36410256]
 [0.         0.5        0.6        0.         0.37435897 0.
  0.5        0.6        0.         0.37435897]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.575      0.         0.36410256]
 [0.         0.5        0.6625     0.         0.3978022  0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.6875     0.         0.40659341 0.
  0.5        0.65       0.         0.39340659]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.6625     0.         0.3978022 ]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.5875     0.         0.36923077]
 [0.         0.5        0.825      0.         0.45142857 0.
  0.5        0.5875     0.         0.36923077]
 [0.         0.5        0.7125     0.         0.41538462 0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.65       0.         0.39340659 0.
  0.5        0.6375     0.         0.38901099]
 [0.         0.5        0.7875     0.         0.44       0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.525      0.         0.34358974 0.
  0.5        0.6125     0.         0.37948718]
 [0.         0.5        0.55       0.         0.35384615 0.
  0.5        0.7375     0.         0.42417582]
 [0.         0.5        0.5625     0.         0.35897436 0.075
  0.5375     0.6125     0.12071068 0.42713287]
 [0.         0.5        0.5875     0.         0.36923077 0.375
  0.6875     0.6875     0.62586368 0.66681097]
 [0.         0.5        0.525      0.         0.34358974 0.
  0.5        0.5875     0.         0.36923077]
 [0.         0.5        0.6875     0.         0.40659341 0.55
  0.775      0.775      0.75756241 0.76753968]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.5875     0.         0.36923077]
 [0.         0.5        0.6375     0.         0.38901099 0.
  0.5        0.55       0.         0.35384615]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.675      0.         0.4021978 ]]
SVM mean:
[0.01975275 0.50908333 0.61875    0.03063925 0.39272483 0.07257835
 0.53627778 0.62708333 0.11539641 0.43142324]
---------------------------
---------------------------
GBM performance:
[[ 0.33666667  0.71583333  0.725       0.68804486  0.71080808  0.24666667
   0.6         0.7875      0.35355339  0.60161172]
 [ 0.4156184   0.705       0.7125      0.63976766  0.68230159  0.27478022
   0.63416667  0.675       0.55738517  0.60582085]
 [ 0.44144689  0.72083333  0.7625      0.62312421  0.70879343  0.175
   0.5875      0.575       0.50327309  0.54919192]
 [ 0.40254902  0.71083333  0.7125      0.69865375  0.70492063  0.04
   0.525       0.725       0.07071068  0.45201465]
 [ 0.42849817  0.69666667  0.75        0.64453559  0.70011544  0.375
   0.6875      0.6875      0.67434906  0.67223665]
 [ 0.58928571  0.79416667  0.8         0.7388257   0.78787157  0.44960784
   0.75333333  0.7625      0.63064234  0.71727106]
 [ 0.32382784  0.665       0.7         0.53901882  0.63976024  0.22666667
   0.63333333  0.625       0.53738932  0.59785714]
 [ 0.7056044   0.84833333  0.875       0.81025352  0.84675991  0.45406593
   0.70333333  0.7375      0.6857734   0.69887446]
 [ 0.23501832  0.59833333  0.7125      0.34086006  0.57757576 -0.05879121
   0.46333333  0.6         0.07071068  0.41995338]
 [ 0.18088235  0.60333333  0.6         0.55579477  0.57625541  0.29434389
   0.6225      0.6375      0.56693722  0.59887279]
 [-0.09842491  0.45083333  0.5125      0.25597622  0.42634921  0.4206044
   0.70333333  0.7375      0.62910484  0.69144855]
 [ 0.50263736  0.74333333  0.775       0.70445105  0.73881674  0.3753663
   0.70416667  0.7375      0.65303342  0.70663059]
 [ 0.43893773  0.715       0.7375      0.68650511  0.71147908  0.09538462
   0.53        0.5625      0.4404179   0.48466644]
 [ 0.21802198  0.58        0.7         0.31947017  0.54621379  0.42774725
   0.71083333  0.725       0.65292527  0.69784271]
 [-0.08        0.46666667  0.6375      0.          0.38901099  0.02043956
   0.51        0.625       0.22484486  0.47174825]
 [-0.02554945  0.50583333  0.6         0.10773503  0.43230769  0.32644689
   0.65        0.75        0.44948005  0.63244755]
 [ 0.36962616  0.68166667  0.7125      0.60538608  0.6650272   0.29752747
   0.64416667  0.675       0.52906904  0.61838384]
 [-0.02        0.49166667  0.8125      0.          0.44703297  0.19143504
   0.62416667  0.65        0.54777559  0.6053824 ]
 [ 0.08014652  0.51333333  0.675       0.18690018  0.47301365  0.425
   0.7275      0.7375      0.60495106  0.69920468]
 [ 0.38335165  0.69        0.7375      0.61386212  0.67797092  0.07362637
   0.53333333  0.6375      0.27038939  0.48887113]
 [-0.02        0.48452381  0.7625      0.          0.43179487 -0.37071429
   0.33916667  0.35        0.21565173  0.318557  ]
 [ 0.28928571  0.645       0.65        0.59906024  0.6316811   0.01932234
   0.5075      0.575       0.23574582  0.46480186]
 [ 0.26901292  0.64166667  0.65        0.55466482  0.61070707  0.07428571
   0.54        0.725       0.19531442  0.49196803]
 [ 0.37681319  0.695       0.725       0.59207803  0.67881674  0.38806938
   0.675       0.7         0.60584149  0.65896825]
 [ 0.19520757  0.6025      0.65        0.45493778  0.56737762  0.375
   0.7125      0.7125      0.64114105  0.70190476]
 [ 0.25588235  0.63083333  0.625       0.59638681  0.60915584  0.43724521
   0.71416667  0.75        0.62727273  0.69591464]
 [ 0.39216117  0.705       0.775       0.58464377  0.69204129  0.45
   0.7375      0.7375      0.71675554  0.72517316]
 [ 0.18089744  0.5625      0.6375      0.40731113  0.53543124  0.48681319
   0.745       0.775       0.70968043  0.74323232]
 [ 0.20871795  0.6         0.6875      0.38872359  0.5646287   0.18434389
   0.5775      0.5875      0.51613779  0.54227273]
 [ 0.42238095  0.725       0.75        0.64953902  0.71139083  0.02395604
   0.52333333  0.65        0.27876562  0.49536464]]
GBM mean:
[0.27995013 0.63962302 0.70541667 0.486217   0.61584699 0.23997465
 0.62063889 0.67375    0.47983408 0.59494961]
---------------------------
---------------------------
BDDAE performance:
[[ 0.15527699  0.58015873  0.58125     0.56811234  0.57305753  0.00464646
   0.50416667  0.68125     0.2386937   0.47712769]
 [ 0.12814738  0.56349206  0.5625      0.53069082  0.54503261  0.38919136
   0.7         0.7         0.6711913   0.67962027]
 [ 0.06838757  0.53492063  0.55        0.48434177  0.52415202  0.025
   0.5125      0.5125      0.49124303  0.50221394]
 [ 0.06462107  0.53253968  0.53125     0.52188172  0.52496767  0.2581964
   0.61545455  0.71875     0.52927842  0.61360668]
 [ 0.12248029  0.56166667  0.59375     0.53137701  0.55661404  0.225
   0.6125      0.6125      0.60706112  0.60993464]
 [ 0.5125      0.75625     0.75625     0.74629067  0.75276101  0.09021592
   0.5468254   0.53125     0.51852851  0.52299826]
 [ 0.00633591  0.50396825  0.53125     0.37357871  0.46845624  0.0125
   0.50625     0.50625     0.4323641   0.47865864]
 [ 0.36244581  0.67818182  0.7375      0.63320788  0.67395685  0.08941436
   0.54166667  0.58125     0.50216949  0.53547497]
 [ 0.37030094  0.65727273  0.76875     0.57416633  0.66855041  0.40546625
   0.67        0.79375     0.54637571  0.67745136]
 [-0.125       0.4375      0.4375      0.42453683  0.43142029  0.12079701
   0.56031746  0.575       0.51733986  0.5534395 ]
 [ 0.02183761  0.51166667  0.58125     0.35465778  0.47944401  0.2099899
   0.6015873   0.625       0.55901597  0.59145868]
 [ 0.10024949  0.55333333  0.575       0.52196466  0.54132145  0.43373142
   0.72        0.725       0.71612185  0.71359951]
 [ 0.15968035  0.58095238  0.575       0.56610541  0.56871778  0.18830536
   0.5968254   0.6         0.55209404  0.57436447]
 [-0.04443758  0.47818182  0.5375      0.38841219  0.46768931  0.425
   0.7125      0.7125      0.69796519  0.70655335]
 [-0.08177619  0.46363636  0.5625      0.28334986  0.44524264 -0.00496713
   0.49666667  0.525       0.45764776  0.48598612]
 [-0.01710434  0.49        0.5         0.43865716  0.46267208  0.1187991
   0.54818182  0.69375     0.29671247  0.51921403]
 [ 0.3902846   0.69603175  0.70625     0.62561702  0.67320411  0.25059278
   0.61904762  0.65        0.51968231  0.59369083]
 [ 0.28206572  0.62564103  0.7875      0.50792648  0.63406593  0.17708358
   0.58730159  0.6         0.56903096  0.58372294]
 [ 0.13977128  0.55909091  0.61875     0.48627974  0.54860731  0.2625
   0.63125     0.63125     0.61056646  0.62116402]
 [ 0.15558632  0.57333333  0.625       0.48750016  0.5597269  -0.04666015
   0.47833333  0.54375     0.32572178  0.45638773]
 [-0.04109278  0.48076923  0.71875     0.15674396  0.46358761  0.15
   0.575       0.575       0.55533772  0.56541805]
 [-0.0125      0.49375     0.49375     0.43672939  0.47405144  0.05985713
   0.52833333  0.59375     0.42710501  0.51366397]
 [ 0.03665798  0.51666667  0.51875     0.48993634  0.50317644  0.26550117
   0.60833333  0.775       0.47333418  0.61370919]
 [ 0.17423876  0.58809524  0.59375     0.5746733   0.58253522  0.12520443
   0.56031746  0.5875      0.49234252  0.53983695]
 [-0.19469415  0.40396825  0.41875     0.36103067  0.39735732  0.2375
   0.61875     0.61875     0.59786603  0.60888556]
 [ 0.225       0.6125      0.6125      0.58584922  0.59979967  0.18785097
   0.59047619  0.6125      0.54213241  0.57696555]
 [ 0.40191121  0.69090909  0.7625      0.63137558  0.68730744  0.4125
   0.70625     0.70625     0.6857752   0.69795978]
 [ 0.0678389   0.535       0.56875     0.45740001  0.52468787  0.28904313
   0.63730159  0.66875     0.57589954  0.62402893]
 [ 0.21584703  0.60666667  0.6375      0.58031061  0.60244502  0.19421055
   0.59365079  0.6125      0.56119889  0.58597147]
 [ 0.2149541   0.60714286  0.61875     0.55907415  0.58758955  0.00688091
   0.50363636  0.5875      0.35200093  0.47137025]]
BDDAE mean:
[0.12866048 0.56244287 0.60208333 0.49605926 0.55073993 0.18544503
 0.58944745 0.62854167 0.52072655 0.57648258]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.5375     0.         0.34871795 0.
  0.5        0.7625     0.         0.43238095]
 [0.         0.5        0.5625     0.         0.35897436 0.
  0.5        0.6        0.         0.37435897]
 [0.         0.5        0.575      0.         0.36410256 0.
  0.5        0.4875     0.         0.32727273]
 [0.         0.5        0.5375     0.         0.34871795 0.
  0.5        0.7125     0.         0.41538462]
 [0.         0.5        0.6        0.         0.37435897 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.4875     0.         0.32727273 0.
  0.5        0.5375     0.         0.34871795]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.6625     0.         0.3978022  0.
  0.5        0.625      0.         0.38461538]
 [0.         0.5        0.675      0.         0.4021978  0.
  0.5        0.6875     0.         0.40659341]
 [0.         0.5        0.5125     0.         0.33846154 0.
  0.5        0.5375     0.         0.34871795]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.575      0.         0.36410256]
 [0.         0.5        0.6        0.         0.37435897 0.
  0.5        0.6        0.         0.37435897]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.575      0.         0.36410256]
 [0.         0.5        0.6625     0.         0.3978022  0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.6875     0.         0.40659341 0.
  0.5        0.65       0.         0.39340659]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.6625     0.         0.3978022 ]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.5875     0.         0.36923077]
 [0.         0.5        0.825      0.         0.45142857 0.
  0.5        0.5875     0.         0.36923077]
 [0.         0.5        0.7125     0.         0.41538462 0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.65       0.         0.39340659 0.
  0.5        0.6375     0.         0.38901099]
 [0.         0.5        0.7875     0.         0.44       0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.525      0.         0.34358974 0.
  0.5        0.6125     0.         0.37948718]
 [0.         0.5        0.55       0.         0.35384615 0.
  0.5        0.7375     0.         0.42417582]
 [0.         0.5        0.5625     0.         0.35897436 0.
  0.5        0.575      0.         0.36410256]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.525      0.         0.34358974 0.
  0.5        0.5875     0.         0.36923077]
 [0.         0.5        0.6875     0.         0.40659341 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.5875     0.         0.36923077]
 [0.         0.5        0.6375     0.         0.38901099 0.
  0.5        0.55       0.         0.35384615]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.675      0.         0.4021978 ]]
DUMMY mean:
[0.         0.5        0.61125    0.         0.37732667 0.
 0.5        0.59166667 0.         0.36986391]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_38
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.295 0.648 0.687 0.555 0.629 0.3   0.649 0.682 0.561 0.631]
 [0.3   0.643 0.675 0.563 0.627 0.243 0.613 0.635 0.556 0.596]
 [0.308 0.669 0.7   0.577 0.649 0.295 0.646 0.677 0.572 0.628]
 [0.02  0.509 0.619 0.031 0.393 0.073 0.536 0.627 0.115 0.431]
 [0.28  0.64  0.705 0.486 0.616 0.24  0.621 0.674 0.48  0.595]
 [0.129 0.562 0.602 0.496 0.551 0.185 0.589 0.629 0.521 0.576]
 [0.    0.5   0.611 0.    0.377 0.    0.5   0.592 0.    0.37 ]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.189 0.094 0.09  0.163 0.103 0.143 0.072 0.079 0.106 0.075]
 [0.155 0.079 0.075 0.128 0.081 0.168 0.082 0.081 0.095 0.084]
 [0.169 0.092 0.081 0.15  0.101 0.176 0.083 0.078 0.1   0.086]
 [0.073 0.033 0.079 0.105 0.059 0.162 0.081 0.074 0.25  0.121]
 [0.194 0.098 0.073 0.23  0.115 0.197 0.098 0.088 0.193 0.108]
 [0.165 0.08  0.095 0.116 0.084 0.136 0.066 0.074 0.112 0.072]
 [0.    0.    0.076 0.    0.028 0.    0.    0.072 0.    0.028]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 64.  15.  13.  29.  16.  48.  11.  12.  19.  12.]
 [ 52.  12.  11.  23.  13.  69.  13.  13.  17.  14.]
 [ 55.  14.  12.  26.  16.  60.  13.  12.  17.  14.]
 [370.   6.  13. 343.  15. 223.  15.  12. 217.  28.]
 [ 69.  15.  10.  47.  19.  82.  16.  13.  40.  18.]
 [128.  14.  16.  23.  15.  73.  11.  12.  22.  12.]
 [  0.   0.  12.   0.   7.   0.   0.  12.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_38
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  15.008
step (sec):  15.008
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  80
Elapsed time: 516.6992549339931 minutes
Elapsed time: 8.611654248899884 hours
