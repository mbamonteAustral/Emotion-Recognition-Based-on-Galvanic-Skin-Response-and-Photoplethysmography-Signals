2024-05-07 05:07:40.446664: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-07 05:07:44.160071: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-07 05:07:53.497916: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  15.0
step (sec):  11.25
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  3.75
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_39\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

m [1m0s[0m 10ms/step - binary_accuracy: 0.9276 - loss: 0.2114
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.9259 - loss: 0.2125 - val_binary_accuracy: 0.6667 - val_loss: 1.1198

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.08314317 0.9729419  0.08809929 0.01486496 0.766515   0.76269615
 0.93531346 0.19387977 0.11430961 0.6778861  0.05796742 0.9965847
 0.01911745 0.5271262  0.63268876 0.5530961  0.45133406 0.8293682
 0.8852198  0.54083276 0.59143364 0.41444588]
predicted [0 1 0 0 1 1 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 0]
expected [False False False False  True False  True  True False  True  True  True
  True  True  True  True False  True False  True  True  True]
accuracy: 0.6818181818181818
confusion matrix: 
[[ 5  3]
 [ 4 10]]
              precision    recall  f1-score   support

       False       0.56      0.62      0.59         8
        True       0.77      0.71      0.74        14

    accuracy                           0.68        22
   macro avg       0.66      0.67      0.66        22
weighted avg       0.69      0.68      0.69        22

macro avg f1-score: 0.6644880174291938
macro avg (UAR): 0.6696428571428572
Sensitivity:  0.625
Specificity:  0.7142857142857143
g-mean:  0.6681531047810609
-------- Model Performance ----------: 
accuracy:  [0.68181818 0.68181818 0.63636364 0.77272727 0.68181818 0.68181818
 0.54545455 0.68181818 0.68181818 0.        ]
gmean:  [0.62678317 0.56694671 0.5976143  0.68138514 0.69436507 0.6681531
 0.49099025 0.6681531  0.6681531  0.        ]
f1_score:  [0.64597701 0.617866   0.60714286 0.72704715 0.67578947 0.66448802
 0.50892857 0.66448802 0.66448802 0.        ]
UAR:  [0.64285714 0.61607143 0.60714286 0.71428571 0.69642857 0.66964286
 0.50892857 0.66964286 0.66964286 0.        ]
Cohen Kappa score:  [0.29357798 0.25242718 0.21428571 0.46601942 0.36363636 0.33043478
 0.01785714 0.33043478 0.33043478 0.        ]
Split Repetition number:  9
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [ 23  90  15  41 105  42  70  54 100   7  58  98  84  12   1  79  82  24
   6  10  43  45  75  38 103  44  50  31  13   8  94  83  80  52  85  89
  71  55  86  88   4  34  46  60  72  96  62  95   5  65  56  59   3  40
  81  87  51   9  22 101  11  68  47 102  21  92  29  91  64  30  77  48
  33   2  20  32  26  18  61  57  74  63  76 104  99] TEST: [ 17  67 106  19  66  69  97  53  78  39  49   0  37  36  35  16  28  73
  93  14  25  27]
(DL) TRAIN number of instances:  85
(DL) TEST number of instances:  22
(DL) Total number of instances (TRAIN+TEST):  107
C:\Users\Javier\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\convolutional\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(
----- train_GSR_AE -------
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 15008, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3752, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3752, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 938, 6)         â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2396              â”‚ (None, 3752, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2396           â”‚ (None, 3752, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2397              â”‚ (None, 15008, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2397           â”‚ (None, 15008, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:24[0m 1s/step - loss: 26.0246 - mean_squared_error: 26.0246
[1m 6/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 23.5694 - mean_squared_error: 23.5694
[1m12/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 24.0751 - mean_squared_error: 24.0751
[1m18/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 23.3310 - mean_squared_error: 23.3310
[1m23/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 23.1183 - mean_squared_error: 23.1183
[1m29/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 22.6278 - mean_squared_error: 22.6278
[1m35/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 22.0833 - mean_squared_error: 22.0833
[1m41/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 21.3494 - mean_squared_error: 21.3494
[1m47/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 20.7141 - mean_squared_error: 20.7141
[1m53/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 20.0803 - mean_squared_error: 20.0803
[1m59/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 19.5294 - mean_squared_error: 19.5294
[1m64/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 19.1468 - mean_squared_error: 19.1468
[1m70/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - loss: 18.6958 - mean_squared_error: 18.6958
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 11ms/step - loss: 18.2750 - mean_squared_error: 18.2750
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 14ms/step - loss: 18.2072 - mean_squared_error: 18.2072 - val_loss: 13.8119 - val_mean_squared_error: 13.8119
Epoch 2/5

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 28ms/step - loss: 8.4349 - mean_squared_error: 8.4349
[1m 6/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 14ms/step - loss: 9.0060 - mean_squared_error: 9.0060
[1m11/76[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 10.0368 - mean_squared_error: 10.0368
[1m17/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 10.3442 - mean_squared_error: 10.3442
[1m23/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.1347 - mean_squared_error: 10.1347
[1m28/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 10.1380 - mean_squared_error: 10.1380
[1m34/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.1809 - mean_squared_error: 10.1809
[1m40/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.2788 - mean_squared_error: 10.2788
[1m46/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.3054 - mean_squared_error: 10.3054
[1m52/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.3871 - mean_squared_error: 10.3871
[1m57/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.3844 - mean_squared_error: 10.3844
[1m63/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.3922 - mean_squared_error: 10.3922
[1m69/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - loss: 10.4724 - mean_squared_error: 10.4724
[1m75/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - loss: 10.5516 - mean_squared_error: 10.5516
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 10.5792 - mean_squared_error: 10.5792 - val_loss: 13.7939 - val_mean_squared_error: 13.7939
Epoch 3/5

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - loss: 11.4259 - mean_squared_error: 11.4259
[1m 6/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 18.2851 - mean_squared_error: 18.2851
[1m12/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 17.6411 - mean_squared_error: 17.6411
[1m18/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 16.2293 - mean_squared_error: 16.2293
[1m24/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 15.4045 - mean_squared_error: 15.4045
[1m30/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 15.0206 - mean_squared_error: 15.0206
[1m36/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 14.5489 - mean_squared_error: 14.5489
[1m42/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 14.1189 - mean_squared_error: 14.1189
[1m47/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 13.8470 - mean_squared_error: 13.8470
[1m53/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 13.5624 - mean_squared_error: 13.5624
[1m59/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 13.3894 - mean_squared_error: 13.3894
[1m64/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 13.2470 - mean_squared_error: 13.2470
[1m70/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - loss: 13.0586 - mean_squared_error: 13.0586
[1m75/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - loss: 12.9369 - mean_squared_error: 12.9369
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 12.9024 - mean_squared_error: 12.9024 - val_loss: 13.7878 - val_mean_squared_error: 13.7878
Epoch 4/5

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 35ms/step - loss: 2.8901 - mean_squared_error: 2.8901
[1m 7/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 20.8991 - mean_squared_error: 20.8991
[1m13/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 20.0707 - mean_squared_error: 20.0707
[1m18/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 19.6089 - mean_squared_error: 19.6089
[1m24/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 19.3954 - mean_squared_error: 19.3954
[1m28/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 19.0254 - mean_squared_error: 19.0254
[1m34/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 18.3771 - mean_squared_error: 18.3771
[1m40/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 17.8023 - mean_squared_error: 17.8023
[1m45/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 17.2936 - mean_squared_error: 17.2936
[1m51/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 16.7366 - mean_squared_error: 16.7366
[1m56/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 16.3448 - mean_squared_error: 16.3448
[1m61/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 16.0603 - mean_squared_error: 16.0603
[1m67/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - loss: 15.7381 - mean_squared_error: 15.7381
[1m73/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - loss: 15.4149 - mean_squared_error: 15.4149
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 15.2170 - mean_squared_error: 15.2170 - val_loss: 13.7850 - val_mean_squared_error: 13.7850
Epoch 5/5

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 5.4788 - mean_squared_error: 5.4788
[1m 7/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 8.7695 - mean_squared_error: 8.7695
[1m12/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.0700 - mean_squared_error: 10.0700
[1m18/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.3977 - mean_squared_error: 10.3977
[1m24/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.5727 - mean_squared_error: 10.5727
[1m30/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.6113 - mean_squared_error: 10.6113
[1m36/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.8070 - mean_squared_error: 10.8070
[1m41/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 10.9453 - mean_squared_error: 10.9453
[1m47/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 11.0807 - mean_squared_error: 11.0807
[1m53/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 11.3162 - mean_squared_error: 11.3162
[1m59/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 11.4480 - mean_squared_error: 11.4480
[1m64/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 11.4962 - mean_squared_error: 11.4962
[1m70/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - loss: 11.5615 - mean_squared_error: 11.5615
[1m75/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - loss: 11.5741 - mean_squared_error: 11.5741
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - loss: 11.5748 - mean_squared_error: 11.5748 - val_loss: 13.7838 - val_mean_squared_error: 13.7838
(85, 938, 6)
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 15008, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3752, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3752, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 938, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 15008, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3752, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3752, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 938, 6)         â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2398              â”‚ (None, 3752, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2398           â”‚ (None, 3752, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2399              â”‚ (None, 15008, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2399           â”‚ (None, 15008, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:24[0m 1s/step - loss: 0.2136 - mean_squared_error: 0.2136
[1m 6/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.1091 - mean_squared_error: 0.1091
[1m11/76[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0882 - mean_squared_error: 0.0882
[1m17/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0782 - mean_squared_error: 0.0782
[1m23/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0708 - mean_squared_error: 0.0708
[1m28/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0670 - mean_squared_error: 0.0670
[1m34/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0642 - mean_squared_error: 0.0642
[1m39/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0621 - mean_squared_error: 0.0621
[1m44/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0602 - mean_squared_error: 0.0602
[1m50/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0583 - mean_squared_error: 0.0583
[1m56/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0565 - mean_squared_error: 0.0565
[1m62/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0549 - mean_squared_error: 0.0549
[1m67/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0536 - mean_squared_error: 0.0536
[1m72/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - loss: 0.0524 - mean_squared_error: 0.0524
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 14ms/step - loss: 0.0512 - mean_squared_error: 0.0512 - val_loss: 0.0087 - val_mean_squared_error: 0.0087
Epoch 2/5

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.0043 - mean_squared_error: 0.0043
[1m 7/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0073 - mean_squared_error: 0.0073
[1m13/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0120 - mean_squared_error: 0.0120
[1m19/76[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0138 - mean_squared_error: 0.0138
[1m24/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0152 - mean_squared_error: 0.0152
[1m30/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0164 - mean_squared_error: 0.0164
[1m36/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0170 - mean_squared_error: 0.0170
[1m41/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0173 - mean_squared_error: 0.0173
[1m47/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0178 - mean_squared_error: 0.0178
[1m53/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0181 - mean_squared_error: 0.0181
[1m59/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0183 - mean_squared_error: 0.0183
[1m65/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0183 - mean_squared_error: 0.0183
[1m70/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - loss: 0.0184 - mean_squared_error: 0.0184
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 11ms/step - loss: 0.0186 - mean_squared_error: 0.0186
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0186 - mean_squared_error: 0.0186 - val_loss: 0.0082 - val_mean_squared_error: 0.0082
Epoch 3/5

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 27ms/step - loss: 0.0059 - mean_squared_error: 0.0059
[1m 6/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0121 - mean_squared_error: 0.0121
[1m12/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0125 - mean_squared_error: 0.0125
[1m17/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0138 - mean_squared_error: 0.0138
[1m23/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0157 - mean_squared_error: 0.0157
[1m29/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0166 - mean_squared_error: 0.0166
[1m34/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0170 - mean_squared_error: 0.0170
[1m40/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0176 - mean_squared_error: 0.0176
[1m46/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0179 - mean_squared_error: 0.0179
[1m52/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0182 - mean_squared_error: 0.0182
[1m57/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0183 - mean_squared_error: 0.0183
[1m63/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0183 - mean_squared_error: 0.0183
[1m68/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0182 - mean_squared_error: 0.0182
[1m74/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - loss: 0.0182 - mean_squared_error: 0.0182
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0075 - val_mean_squared_error: 0.0075
Epoch 4/5

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.0060 - mean_squared_error: 0.0060
[1m 7/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0149 - mean_squared_error: 0.0149
[1m13/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0156 - mean_squared_error: 0.0156
[1m18/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0168 - mean_squared_error: 0.0168
[1m23/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0183 - mean_squared_error: 0.0183
[1m29/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0193 - mean_squared_error: 0.0193
[1m34/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0198 - mean_squared_error: 0.0198
[1m39/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0199 - mean_squared_error: 0.0199
[1m44/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0199 - mean_squared_error: 0.0199
[1m50/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0196 - mean_squared_error: 0.0196
[1m55/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0194 - mean_squared_error: 0.0194
[1m60/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0192 - mean_squared_error: 0.0192
[1m65/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0190 - mean_squared_error: 0.0190
[1m71/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - loss: 0.0185 - mean_squared_error: 0.0185 - val_loss: 0.0145 - val_mean_squared_error: 0.0145
Epoch 5/5

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.0450 - mean_squared_error: 0.0450
[1m 6/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0263 - mean_squared_error: 0.0263
[1m11/76[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0240 - mean_squared_error: 0.0240
[1m17/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0220 - mean_squared_error: 0.0220
[1m23/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0205 - mean_squared_error: 0.0205
[1m29/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0200 - mean_squared_error: 0.0200
[1m34/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0196 - mean_squared_error: 0.0196
[1m41/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0193 - mean_squared_error: 0.0193
[1m47/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0189 - mean_squared_error: 0.0189
[1m52/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0186 - mean_squared_error: 0.0186
[1m58/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0185 - mean_squared_error: 0.0185
[1m64/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - loss: 0.0185 - mean_squared_error: 0.0185
[1m70/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - loss: 0.0185 - mean_squared_error: 0.0185
[1m75/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - loss: 0.0184 - mean_squared_error: 0.0184
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - loss: 0.0184 - mean_squared_error: 0.0184 - val_loss: 0.0104 - val_mean_squared_error: 0.0104
(15008, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 15008, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 3752, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 3752, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 938, 6)         â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 15008, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 15008, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 938, 6)    â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 938, 6)    â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 938, 12)   â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 938)   â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 11256)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 11256)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     11,257 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 12,079 (47.18 KB)
 Trainable params: 12,079 (47.18 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:58[0m 2s/step - binary_accuracy: 1.0000 - loss: 0.6678
[1m 6/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7111 - loss: 0.7162
[1m11/76[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6399 - loss: 0.7268
[1m17/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5846 - loss: 0.7416
[1m23/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5676 - loss: 0.7456
[1m29/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5708 - loss: 0.7404
[1m34/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5788 - loss: 0.7328
[1m39/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5816 - loss: 0.7303
[1m45/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5820 - loss: 0.7290
[1m52/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5795 - loss: 0.7281
[1m57/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5758 - loss: 0.7273
[1m63/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5750 - loss: 0.7252
[1m68/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5762 - loss: 0.7221
[1m74/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5784 - loss: 0.7188
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 13ms/step - binary_accuracy: 0.5790 - loss: 0.7177 - val_binary_accuracy: 0.8889 - val_loss: 0.3878
Epoch 2/10

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.5256
[1m 5/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 16ms/step - binary_accuracy: 0.8433 - loss: 0.5150
[1m11/76[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8344 - loss: 0.4895
[1m16/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8020 - loss: 0.5358
[1m22/76[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7741 - loss: 0.5770
[1m28/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7719 - loss: 0.5938
[1m34/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7713 - loss: 0.6010
[1m40/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7649 - loss: 0.6071
[1m46/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7626 - loss: 0.6085
[1m52/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7604 - loss: 0.6085
[1m58/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7601 - loss: 0.6069
[1m64/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7572 - loss: 0.6081
[1m70/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7551 - loss: 0.6088
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.7533 - loss: 0.6098 - val_binary_accuracy: 0.8889 - val_loss: 0.4846
Epoch 3/10

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1879
[1m 7/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9272 - loss: 0.3351
[1m13/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8705 - loss: 0.4123
[1m19/76[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8412 - loss: 0.4471
[1m26/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8322 - loss: 0.4631
[1m32/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8318 - loss: 0.4677
[1m38/76[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8346 - loss: 0.4638
[1m45/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8381 - loss: 0.4565
[1m51/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8378 - loss: 0.4585
[1m57/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8333 - loss: 0.4693
[1m62/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8293 - loss: 0.4765
[1m69/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8225 - loss: 0.4861
[1m75/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8170 - loss: 0.4933
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.8149 - loss: 0.4955 - val_binary_accuracy: 0.2222 - val_loss: 1.1049
Epoch 4/10

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.8106
[1m 6/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.2417 - loss: 0.8430    
[1m13/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.3336 - loss: 0.8241
[1m19/76[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.4280 - loss: 0.7662
[1m25/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.4851 - loss: 0.7274
[1m31/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5205 - loss: 0.7019
[1m37/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5442 - loss: 0.6858
[1m43/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5647 - loss: 0.6708
[1m49/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5816 - loss: 0.6572
[1m55/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5947 - loss: 0.6462
[1m62/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6060 - loss: 0.6372
[1m69/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6168 - loss: 0.6290
[1m75/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6250 - loss: 0.6228
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 10ms/step - binary_accuracy: 0.6276 - loss: 0.6208 - val_binary_accuracy: 0.8889 - val_loss: 0.4148
Epoch 5/10

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0962
[1m 7/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7507 - loss: 0.4259
[1m12/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6630 - loss: 0.5407
[1m18/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6285 - loss: 0.5809
[1m24/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6161 - loss: 0.5956
[1m30/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6092 - loss: 0.6013
[1m36/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6105 - loss: 0.6007
[1m42/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6154 - loss: 0.5972
[1m48/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6236 - loss: 0.5919
[1m55/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6341 - loss: 0.5844
[1m61/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6426 - loss: 0.5778
[1m67/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6496 - loss: 0.5731
[1m73/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6553 - loss: 0.5689
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.6584 - loss: 0.5664 - val_binary_accuracy: 0.6667 - val_loss: 0.6019
Epoch 6/10

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 0.0000e+00 - loss: 1.0369
[1m 7/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6296 - loss: 0.5710    
[1m13/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7005 - loss: 0.5436
[1m19/76[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7343 - loss: 0.5349
[1m25/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7551 - loss: 0.5208
[1m30/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7710 - loss: 0.5071
[1m36/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7842 - loss: 0.4949
[1m42/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7910 - loss: 0.4869
[1m49/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7916 - loss: 0.4819
[1m54/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7895 - loss: 0.4816
[1m60/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7894 - loss: 0.4801
[1m66/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7888 - loss: 0.4794
[1m73/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7857 - loss: 0.4804
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.7832 - loss: 0.4819 - val_binary_accuracy: 0.5556 - val_loss: 0.6672
Epoch 7/10

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3935
[1m 6/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7806 - loss: 0.3959
[1m13/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7412 - loss: 0.4414
[1m19/76[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7649 - loss: 0.4400
[1m24/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7818 - loss: 0.4304
[1m30/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7940 - loss: 0.4201
[1m36/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7960 - loss: 0.4162
[1m42/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7998 - loss: 0.4139
[1m49/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8018 - loss: 0.4124
[1m53/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8035 - loss: 0.4109
[1m59/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8020 - loss: 0.4116
[1m64/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7998 - loss: 0.4123
[1m70/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7958 - loss: 0.4144
[1m75/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7933 - loss: 0.4158
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.7926 - loss: 0.4161 - val_binary_accuracy: 0.6667 - val_loss: 0.5135
Epoch 8/10

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.0560
[1m 7/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8439 - loss: 0.2804
[1m12/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8093 - loss: 0.3439
[1m18/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7941 - loss: 0.3840
[1m23/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7972 - loss: 0.4002
[1m29/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8078 - loss: 0.4057
[1m36/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8165 - loss: 0.4055
[1m42/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8246 - loss: 0.4016
[1m48/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8274 - loss: 0.4054
[1m54/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8293 - loss: 0.4075
[1m60/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8313 - loss: 0.4101
[1m66/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8299 - loss: 0.4168
[1m72/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8289 - loss: 0.4214
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.8279 - loss: 0.4243 - val_binary_accuracy: 0.6667 - val_loss: 0.5750
Epoch 9/10

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.5935
[1m 7/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6639 - loss: 0.5467
[1m13/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7287 - loss: 0.4648
[1m19/76[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7729 - loss: 0.4152
[1m25/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7884 - loss: 0.3953
[1m31/76[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8021 - loss: 0.3836
[1m37/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8076 - loss: 0.3821
[1m43/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8098 - loss: 0.3827
[1m50/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8100 - loss: 0.3836
[1m56/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8123 - loss: 0.3815
[1m62/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8153 - loss: 0.3791
[1m69/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8185 - loss: 0.3771
[1m75/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8208 - loss: 0.3762
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.8214 - loss: 0.3769 - val_binary_accuracy: 0.6667 - val_loss: 0.5783
Epoch 10/10

[1m 1/76[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 37ms/step - binary_accuracy: 1.0000 - loss: 0.4343
[1m 7/76[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9796 - loss: 0.3073
[1m13/76[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9438 - loss: 0.3230
[1m18/76[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9419 - loss: 0.3171
[1m25/76[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9385 - loss: 0.3130
[1m30/76[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9368 - loss: 0.3114
[1m36/76[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9326 - loss: 0.3109
[1m42/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9248 - loss: 0.3175
[1m48/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9204 - loss: 0.3195
[1m54/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9171 - loss: 0.3214
[1m60/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9132 - loss: 0.3254
[1m65/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9113 - loss: 0.3277
[1m72/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9095 - loss: 0.3298
[1m76/76[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 11ms/step - binary_accuracy: 0.9084 - loss: 0.3309 - val_binary_accuracy: 0.6667 - val_loss: 0.5401

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.6193978  0.72684956 0.04971347 0.8729605  0.9467696  0.82661057
 0.05034405 0.75766647 0.78742486 0.7078674  0.65033424 0.4763918
 0.5103506  0.5832255  0.74367225 0.8691275  0.85427916 0.58876455
 0.20586224 0.7986909  0.7551794  0.09737715]
predicted [1 1 0 1 1 1 0 1 1 1 1 0 1 1 1 1 1 1 0 1 1 0]
expected [False  True False False  True  True False  True  True False  True False
  True  True  True False  True  True  True False  True  True]
accuracy: 0.6818181818181818
confusion matrix: 
[[ 3  5]
 [ 2 12]]
              precision    recall  f1-score   support

       False       0.60      0.38      0.46         8
        True       0.71      0.86      0.77        14

    accuracy                           0.68        22
   macro avg       0.65      0.62      0.62        22
weighted avg       0.67      0.68      0.66        22

macro avg f1-score: 0.6178660049627791
macro avg (UAR): 0.6160714285714286
Sensitivity:  0.375
Specificity:  0.8571428571428571
g-mean:  0.5669467095138409
-------- Model Performance ----------: 
accuracy:  [0.68181818 0.68181818 0.63636364 0.77272727 0.68181818 0.68181818
 0.54545455 0.68181818 0.68181818 0.68181818]
gmean:  [0.62678317 0.56694671 0.5976143  0.68138514 0.69436507 0.6681531
 0.49099025 0.6681531  0.6681531  0.56694671]
f1_score:  [0.64597701 0.617866   0.60714286 0.72704715 0.67578947 0.66448802
 0.50892857 0.66448802 0.66448802 0.617866  ]
UAR:  [0.64285714 0.61607143 0.60714286 0.71428571 0.69642857 0.66964286
 0.50892857 0.66964286 0.66964286 0.61607143]
Cohen Kappa score:  [0.29357798 0.25242718 0.21428571 0.46601942 0.36363636 0.33043478
 0.01785714 0.33043478 0.33043478 0.25242718]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  15.008
step (sec):  11.248
overlap:  True
perc. of overlap:  25.053304904051174
overlap duration (sec):  3.76
Number of windows / instances:  107
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.772 0.892 0.89  0.883 0.883 0.335 0.667 0.706 0.646 0.663]
 [0.53  0.783 0.797 0.762 0.776 0.506 0.771 0.792 0.742 0.762]
 [0.7   0.801 0.815 0.798 0.795 0.46  0.739 0.762 0.742 0.732]
 [0.326 0.645 0.729 0.475 0.619 0.    0.5   0.674 0.    0.402]
 [0.479 0.746 0.785 0.651 0.729 0.209 0.59  0.712 0.427 0.578]
 [0.285 0.641 0.673 0.623 0.639 0.147 0.568 0.655 0.506 0.566]
 [0.    0.5   0.616 0.    0.381 0.    0.5   0.674 0.    0.402]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.3992515   0.70166667  0.70090909  0.67085208  0.68615301  0.30960662
   0.65694444  0.76727273  0.53255951  0.63377052]
 [ 0.37776789  0.69        0.69        0.68415235  0.6859127   0.31260656
   0.64761905  0.68272727  0.60843633  0.64481727]
 [ 0.49419255  0.73833333  0.77363636  0.66344855  0.72837246  0.4098991
   0.705       0.70363636  0.67575808  0.69166556]
 [ 0.39381006  0.69833333  0.69818182  0.6418223   0.67763348  0.09252852
   0.54315476  0.65545455  0.37598622  0.52240522]
 [ 0.51682579  0.74845238  0.78636364  0.71720819  0.75164807  0.54513642
   0.77333333  0.77363636  0.76333111  0.76903208]
 [ 0.48000018  0.74333333  0.73909091  0.72888141  0.73371129  0.60702429
   0.80333333  0.80363636  0.78987588  0.79788573]
 [ 0.23600502  0.61380952  0.64        0.5517766   0.60138875  0.24835985
   0.62666667  0.62545455  0.61780946  0.62125874]
 [ 0.76356242  0.8702381   0.89727273  0.85887882  0.87970238  0.67561232
   0.84428571  0.84181818  0.83199302  0.83394078]
 [ 0.13849147  0.56934524  0.70181818  0.2847675   0.53179346  0.08600562
   0.54077381  0.65363636  0.34186291  0.52017904]
 [ 0.32255295  0.665       0.66        0.65422254  0.65684538  0.37777611
   0.68833333  0.69        0.68145102  0.6860409 ]
 [ 0.15843064  0.58654762  0.59909091  0.51531954  0.55990093  0.36879333
   0.68142857  0.71818182  0.56350138  0.65545983]
 [ 0.66818543  0.83392857  0.84727273  0.82114243  0.83197192  0.48151716
   0.73285714  0.76818182  0.70244685  0.7341514 ]
 [ 0.47549625  0.74059524  0.74818182  0.72247255  0.73326729  0.15822605
   0.57797619  0.60545455  0.53509226  0.5675648 ]
 [ 0.29199353  0.64404762  0.67272727  0.61597813  0.63555223  0.25761614
   0.62833333  0.63454545  0.60517395  0.61952214]
 [ 0.17896952  0.59136905  0.68090909  0.4427525   0.571019    0.30619954
   0.6577381   0.68454545  0.59266083  0.63770962]
 [ 0.11865007  0.55964286  0.60727273  0.45316752  0.54407273  0.45660588
   0.72083333  0.77636364  0.64906301  0.71847043]
 [ 0.40134431  0.69702381  0.71818182  0.66888162  0.69147784  0.41646615
   0.70261905  0.72818182  0.67727454  0.70086081]
 [ 0.15105364  0.57083333  0.81272727  0.23737734  0.55419591  0.42087122
   0.71345238  0.72909091  0.67915936  0.70025031]
 [ 0.4134817   0.69642857  0.75909091  0.64881248  0.69635504  0.4488238
   0.72583333  0.72818182  0.70436381  0.71525808]
 [ 0.0774098   0.53541667  0.60545455  0.42435719  0.52267175  0.2681651
   0.6297619   0.68454545  0.58117043  0.62559524]
 [-0.06986127  0.47083333  0.68272727  0.12009598  0.44821637  0.08579459
   0.54333333  0.55        0.52010076  0.53481019]
 [ 0.2191318   0.61        0.60909091  0.59047519  0.59974026  0.47149624
   0.72619048  0.76727273  0.69932114  0.73089286]
 [ 0.10731613  0.55357143  0.58        0.4907586   0.5413148   0.26510038
   0.62708333  0.74818182  0.47725326  0.62124183]
 [ 0.04378979  0.52333333  0.52727273  0.45853941  0.50567599  0.264567
   0.63035714  0.64545455  0.5999768   0.62229104]
 [ 0.36099155  0.6777381   0.68454545  0.63837185  0.66891359  0.58816732
   0.79666667  0.79363636  0.78238539  0.78856449]
 [ 0.38981382  0.69166667  0.70545455  0.65975712  0.68488345  0.40198931
   0.70416667  0.70454545  0.69247065  0.69646409]
 [ 0.47913023  0.74375     0.77545455  0.72873954  0.73623465  0.51228975
   0.75666667  0.75727273  0.73926417  0.7488259 ]
 [ 0.14701073  0.57738095  0.62181818  0.45159129  0.5566128   0.48183102
   0.74309524  0.75181818  0.72885876  0.737109  ]
 [ 0.02161324  0.51130952  0.59090909  0.29316222  0.47890981  0.25355227
   0.62416667  0.64181818  0.57519284  0.60860667]
 [ 0.77231205  0.89166667  0.89        0.88264787  0.88300755  0.33490895
   0.66666667  0.70636364  0.64613307  0.66304002]]
KNN mean:
[0.31762409 0.65818651 0.70018182 0.57734702 0.64590516 0.36358455
 0.68062235 0.71069697 0.63233089 0.67158949]
---------------------------
---------------------------
DT performance:
[[ 0.57643856  0.78        0.78181818  0.75113831  0.77290321  0.31052094
   0.70972222  0.75909091  0.6094416   0.66486904]
 [ 0.54609476  0.76916667  0.76818182  0.78535933  0.76165446  0.42030123
   0.73440476  0.75454545  0.68311597  0.73795954]
 [ 0.35020561  0.70119048  0.71545455  0.7077087   0.6938628   0.31478309
   0.67833333  0.67454545  0.64681557  0.67118881]
 [ 0.3167491   0.62        0.62363636  0.63217671  0.61195388 -0.0220671
   0.49970238  0.60909091  0.33657212  0.46125774]
 [ 0.60574448  0.78345238  0.80545455  0.78141472  0.784884    0.39599426
   0.69833333  0.70090909  0.66868869  0.69615773]
 [ 0.41694558  0.69333333  0.69        0.65895673  0.68440171  0.66360206
   0.805       0.80363636  0.81270795  0.80133061]
 [ 0.12503102  0.56464286  0.59454545  0.51686751  0.55621226  0.16983827
   0.58833333  0.59181818  0.53016989  0.57999334]
 [ 0.77194336  0.8735119   0.87818182  0.89447366  0.86409188  0.43311733
   0.72928571  0.74909091  0.72834477  0.72921884]
 [ 0.30440587  0.66934524  0.75727273  0.62750648  0.65762794  0.16938571
   0.6125      0.68363636  0.55357805  0.61505952]
 [ 0.22209839  0.60833333  0.60909091  0.54798507  0.59887446  0.29757289
   0.685       0.69        0.66251633  0.68285881]
 [ 0.05849805  0.51988095  0.54090909  0.51254043  0.51124015  0.32994135
   0.64607143  0.67181818  0.63660481  0.64218101]
 [ 0.4999862   0.79797619  0.80363636  0.75824306  0.78916805  0.34607365
   0.67940476  0.70181818  0.66984066  0.67285229]
 [ 0.2715054   0.65238095  0.66363636  0.60434286  0.64567752 -0.03783863
   0.56464286  0.58        0.48379771  0.54956238]
 [ 0.06935954  0.52738095  0.56090909  0.43146317  0.52190476  0.42344017
   0.72        0.72090909  0.68775185  0.71668193]
 [ 0.15386329  0.59940476  0.64454545  0.37572062  0.58762821  0.11566309
   0.58630952  0.62727273  0.54128804  0.57992979]
 [ 0.21970362  0.62035714  0.64545455  0.58464996  0.61739621  0.24167357
   0.68690476  0.70909091  0.54421998  0.66470613]
 [ 0.47944835  0.73190476  0.73727273  0.70520152  0.72570721  0.43140809
   0.75559524  0.76636364  0.72906983  0.74932817]
 [ 0.16342675  0.62986111  0.81545455  0.28173225  0.62287969  0.35529676
   0.69892857  0.71        0.62431438  0.67951628]
 [ 0.49977442  0.75565476  0.78727273  0.80524791  0.75375458  0.51333152
   0.76916667  0.76727273  0.74836107  0.76214952]
 [ 0.05979573  0.56666667  0.62818182  0.47104533  0.55782204  0.26859545
   0.61904762  0.63454545  0.56255246  0.60071429]
 [-0.03480318  0.51458333  0.67454545  0.25510104  0.49482702  0.07691528
   0.49        0.48363636  0.5210027   0.47222749]
 [ 0.24237116  0.615       0.61636364  0.51532335  0.5916514   0.09854145
   0.55416667  0.59090909  0.44015336  0.53716187]
 [ 0.09273576  0.53916667  0.54090909  0.5548043   0.51999445  0.11672499
   0.54821429  0.64363636  0.41450393  0.53225023]
 [ 0.13424429  0.58416667  0.59090909  0.53690107  0.57314103  0.40045057
   0.66285714  0.68090909  0.68982434  0.6497652 ]
 [ 0.3850007   0.64940476  0.66090909  0.57180226  0.63216866  0.41326077
   0.73666667  0.73636364  0.69736332  0.73245504]
 [ 0.28233471  0.65083333  0.65454545  0.63124819  0.64196748  0.3949171
   0.7075      0.71090909  0.71678015  0.70294372]
 [ 0.47488881  0.75892857  0.79363636  0.65495879  0.74718838  0.43786767
   0.73666667  0.73909091  0.77200015  0.72628094]
 [ 0.02646085  0.49821429  0.53        0.38999555  0.4869364   0.37379805
   0.6627381   0.66454545  0.65678011  0.65010212]
 [ 0.21086544  0.60178571  0.63        0.49679967  0.58857171  0.00188754
   0.47333333  0.48727273  0.41763223  0.45871823]
 [ 0.53037504  0.78285714  0.79727273  0.76232679  0.77591701  0.50555602
   0.77113095  0.79181818  0.74195052  0.76189991]]
DT mean:
[0.30184972 0.65531283 0.68466667 0.59343451 0.64573362 0.2986851
 0.66033201 0.68115152 0.61759142 0.64937735]
---------------------------
---------------------------
RF performance:
[[ 6.39714123e-01  8.16666667e-01  8.21818182e-01  8.25750954e-01
   8.13411588e-01  4.59447234e-01  7.40972222e-01  8.25454545e-01
   5.53109043e-01  7.18933916e-01]
 [ 5.22968418e-01  7.80000000e-01  7.83636364e-01  7.08943879e-01
   7.71995504e-01  4.91368446e-01  7.10476190e-01  7.29090909e-01
   6.87057014e-01  7.09957126e-01]
 [ 3.46823823e-01  6.81190476e-01  7.00000000e-01  6.61364912e-01
   6.75024004e-01  3.12722622e-01  6.80000000e-01  6.76363636e-01
   6.55843240e-01  6.71915307e-01]
 [ 2.75428017e-01  6.88333333e-01  6.92727273e-01  6.03307867e-01
   6.76012044e-01  3.73278048e-04  5.85416667e-01  6.80000000e-01
   3.39704606e-01  5.82574517e-01]
 [ 5.80526669e-01  7.71071429e-01  7.96363636e-01  7.85315488e-01
   7.75986652e-01  5.12053605e-01  7.38333333e-01  7.38181818e-01
   7.05795949e-01  7.31491286e-01]
 [ 4.15453418e-01  6.70000000e-01  6.61818182e-01  7.12188690e-01
   6.54148074e-01  6.10825349e-01  7.50000000e-01  7.47272727e-01
   7.38628753e-01  7.42117327e-01]
 [ 2.55583658e-01  5.98452381e-01  6.21818182e-01  5.57236642e-01
   5.93476801e-01  1.93608206e-01  6.26666667e-01  6.25454545e-01
   5.92979418e-01  6.16108336e-01]
 [ 7.94592499e-01  9.12797619e-01  9.25454545e-01  8.81369939e-01
   9.14642857e-01  6.31082560e-01  7.68928571e-01  7.86363636e-01
   7.64436465e-01  7.71083500e-01]
 [ 2.92991126e-01  6.55059524e-01  7.40000000e-01  7.13400031e-01
   6.36075330e-01  2.24435136e-01  5.44047619e-01  6.30000000e-01
   4.62693072e-01  5.39982493e-01]
 [ 4.59037280e-01  7.21666667e-01  7.20000000e-01  7.01811346e-01
   7.14500500e-01  3.77553695e-01  5.75000000e-01  5.77272727e-01
   6.40113467e-01  5.70728438e-01]
 [ 2.26476780e-01  6.08690476e-01  6.28181818e-01  6.25435550e-01
   6.02390110e-01  5.65860340e-01  6.25119048e-01  6.54545455e-01
   6.68159035e-01  6.02018961e-01]
 [ 6.42732576e-01  7.98452381e-01  8.10000000e-01  8.40955521e-01
   7.90673771e-01  2.42986235e-01  6.36428571e-01  6.52727273e-01
   5.74199855e-01  6.24967532e-01]
 [ 4.26237333e-01  6.94761905e-01  7.03636364e-01  6.94394694e-01
   6.85662116e-01  1.33350182e-01  5.60119048e-01  6.04545455e-01
   5.01652500e-01  5.39162088e-01]
 [ 1.92665802e-01  5.69047619e-01  6.14545455e-01  3.66489132e-01
   5.62064888e-01  3.00652959e-01  6.51666667e-01  6.53636364e-01
   6.19291686e-01  6.48818959e-01]
 [ 2.03819694e-01  6.71726190e-01  7.48181818e-01  6.34075798e-01
   6.74292538e-01  1.55226709e-01  5.88095238e-01  6.36363636e-01
   5.08389869e-01  5.88408120e-01]
 [ 3.15391514e-01  6.29166667e-01  6.29090909e-01  5.14632204e-01
   6.13304751e-01  2.36045988e-01  7.07142857e-01  7.37272727e-01
   6.85698338e-01  7.01388620e-01]
 [ 5.66711472e-01  7.52619048e-01  7.66363636e-01  7.55282776e-01
   7.51097514e-01  3.56644074e-01  7.65952381e-01  7.74545455e-01
   7.57033239e-01  7.60195221e-01]
 [ 2.23016350e-01  6.60416667e-01  8.30909091e-01  4.45203669e-01
   6.41027692e-01  4.04001705e-01  7.08571429e-01  7.21818182e-01
   6.41430025e-01  6.97858669e-01]
 [ 5.76997660e-01  7.83333333e-01  8.03636364e-01  7.15741289e-01
   7.71678877e-01  3.98105053e-01  7.05833333e-01  7.00909091e-01
   7.40622226e-01  6.91776557e-01]
 [ 2.12463498e-01  6.54166667e-01  6.93636364e-01  5.51271520e-01
   6.46510989e-01  1.64872265e-01  6.54166667e-01  6.83636364e-01
   5.06976939e-01  6.37738095e-01]
 [-7.02255904e-02  5.62500000e-01  7.20000000e-01  2.99177578e-01
   5.62530407e-01 -5.00528034e-02  5.46666667e-01  5.51818182e-01
   5.35097428e-01  5.37697580e-01]
 [ 3.04442773e-01  6.70000000e-01  6.73636364e-01  6.21232082e-01
   6.61269009e-01  1.28515923e-01  6.41666667e-01  6.99090909e-01
   5.92321921e-01  6.25507703e-01]
 [ 2.37355551e-01  5.81190476e-01  5.98181818e-01  5.89554307e-01
   5.76091964e-01  2.02114502e-01  5.74107143e-01  7.01818182e-01
   3.90980414e-01  5.74746732e-01]
 [ 2.75526354e-01  6.06666667e-01  6.10000000e-01  5.78776132e-01
   5.96395271e-01  4.10573394e-01  6.93690476e-01  7.10000000e-01
   6.42253960e-01  6.93325841e-01]
 [ 3.66061384e-01  6.88214286e-01  6.93636364e-01  7.06908119e-01
   6.83423243e-01  6.18082350e-01  8.11666667e-01  8.10909091e-01
   8.26028604e-01  8.05681541e-01]
 [ 2.80276660e-01  6.70833333e-01  6.72727273e-01  6.21154824e-01
   6.62374570e-01  5.23847470e-01  7.17500000e-01  7.18181818e-01
   7.24011280e-01  7.00720113e-01]
 [ 4.26590457e-01  7.87202381e-01  8.22727273e-01  7.65326908e-01
   7.85833333e-01  5.43362716e-01  7.63333333e-01  7.67272727e-01
   7.69859005e-01  7.61046176e-01]
 [ 9.60089483e-02  4.22023810e-01  4.73636364e-01  3.29185014e-01
   4.06025192e-01  3.42455709e-01  7.49285714e-01  7.60909091e-01
   6.58759510e-01  7.44126707e-01]
 [ 2.74098375e-01  6.94047619e-01  7.40000000e-01  5.46933077e-01
   6.89360052e-01  2.25926258e-01  6.74166667e-01  6.90909091e-01
   5.79174834e-01  6.48771368e-01]
 [ 7.00135215e-01  8.00833333e-01  8.14545455e-01  7.98035212e-01
   7.94655067e-01  4.59905386e-01  7.39285714e-01  7.61818182e-01
   7.42365361e-01  7.32256640e-01]]
RF mean:
[0.36866339 0.68670437 0.7170303  0.63834851 0.67939782 0.33919822
 0.67447685 0.70027273 0.62682224 0.66570352]
---------------------------
---------------------------
SVM performance:
[[ 0.02        0.51        0.54181818  0.04472136  0.36610644  0.
   0.5         0.76727273  0.          0.43385965]
 [ 0.30918642  0.6475      0.67454545  0.56693585  0.62235348  0.24772036
   0.6075      0.69181818  0.43944272  0.5719281 ]
 [ 0.30899248  0.6425      0.69909091  0.44440357  0.5987465   0.32737789
   0.66333333  0.66636364  0.62888345  0.64841187]
 [ 0.08767177  0.54166667  0.56818182  0.20570194  0.43635046  0.
   0.5         0.71        0.          0.41504988]
 [ 0.48987335  0.72821429  0.78545455  0.63916263  0.72361695  0.49296071
   0.74833333  0.74545455  0.72202284  0.73524559]
 [ 0.21069664  0.605       0.60818182  0.4936568   0.56084249  0.49461905
   0.74833333  0.74636364  0.73519136  0.74124292]
 [ 0.          0.5         0.60727273  0.          0.37753268 -0.04672414
   0.47666667  0.48545455  0.08944272  0.35404412]
 [ 0.53245981  0.74285714  0.82181818  0.67791913  0.75045123  0.31287814
   0.64035714  0.70272727  0.47385508  0.60664566]
 [ 0.          0.5         0.70090909  0.          0.41183351  0.
   0.5         0.69181818  0.          0.40861713]
 [ 0.19351642  0.595       0.60545455  0.4430721   0.54126814  0.
   0.5         0.53181818  0.          0.34705882]
 [ 0.          0.5         0.61636364  0.          0.38112745  0.
   0.5         0.59818182  0.          0.37393791]
 [ 0.          0.5         0.60727273  0.          0.37753268  0.
   0.5         0.61636364  0.          0.38112745]
 [ 0.          0.5         0.58        0.          0.36674837  0.
   0.5         0.59818182  0.          0.37393791]
 [ 0.          0.5         0.64545455  0.          0.39207516  0.29980275
   0.64833333  0.65454545  0.5584434   0.60762821]
 [ 0.          0.5         0.70090909  0.          0.41183351  0.
   0.5         0.65545455  0.          0.39575163]
 [ 0.          0.5         0.60727273  0.          0.37753268  0.
   0.5         0.65545455  0.          0.39575163]
 [ 0.          0.5         0.58909091  0.          0.37034314  0.
   0.5         0.59818182  0.          0.37393791]
 [ 0.          0.5         0.83272727  0.          0.45418129  0.
   0.5         0.58909091  0.          0.37034314]
 [ 0.          0.5         0.67363636  0.          0.40218438  0.
   0.5         0.55181818  0.          0.35539216]
 [ 0.          0.5         0.66454545  0.          0.39896801  0.
   0.5         0.63545455  0.          0.38839869]
 [ 0.          0.5         0.77636364  0.          0.43675439 -0.00333333
   0.49833333  0.48636364  0.19740963  0.39208333]
 [ 0.23241499  0.615       0.61727273  0.49402296  0.56480159  0.
   0.5         0.64545455  0.          0.39207516]
 [ 0.          0.5         0.58        0.          0.36674837  0.
   0.5         0.73909091  0.          0.4248022 ]
 [ 0.          0.5         0.54181818  0.          0.35122549  0.02583587
   0.51285714  0.58        0.08944272  0.39730392]
 [ 0.          0.5         0.57090909  0.          0.36315359  0.27609411
   0.63333333  0.65272727  0.48711238  0.58975733]
 [ 0.          0.5         0.55181818  0.          0.35539216  0.31129678
   0.64583333  0.68363636  0.54567589  0.61643773]
 [ 0.          0.5         0.69181818  0.          0.40861713  0.5646169
   0.78        0.78363636  0.77444509  0.78049312]
 [ 0.          0.5         0.62545455  0.          0.38472222  0.
   0.5         0.58909091  0.          0.37034314]
 [ 0.          0.5         0.64545455  0.          0.39207516  0.
   0.5         0.55181818  0.          0.35539216]
 [ 0.32634511  0.645       0.72909091  0.47537759  0.61913049  0.
   0.5         0.67363636  0.          0.40218438]]
SVM mean:
[0.0903719  0.5424246  0.64866667 0.14949913 0.45214164 0.11010484
 0.55344048 0.64257576 0.19137891 0.46663943]
---------------------------
---------------------------
GBM performance:
[[ 0.52195834  0.76        0.76363636  0.7328275   0.75358836  0.38170605
   0.66666667  0.84181818  0.43854107  0.660387  ]
 [ 0.40082893  0.695       0.71363636  0.65058404  0.68758242  0.38124437
   0.69285714  0.74636364  0.6145279   0.68693946]
 [ 0.43510268  0.70833333  0.74636364  0.60938376  0.69202381  0.36562134
   0.67333333  0.67727273  0.64834081  0.66331363]
 [ 0.42816747  0.71333333  0.71818182  0.69135378  0.70095377  0.07742248
   0.5375      0.71909091  0.13038243  0.46909916]
 [ 0.52490185  0.75142857  0.79545455  0.69620934  0.74358947  0.41549707
   0.68333333  0.68        0.6663477   0.66801504]
 [ 0.50009542  0.76166667  0.75545455  0.74406885  0.7455328   0.49081067
   0.73666667  0.73636364  0.71534476  0.72733572]
 [ 0.12275828  0.54666667  0.61090909  0.42806704  0.50808105  0.24667255
   0.62333333  0.62363636  0.60348524  0.61100566]
 [ 0.79863498  0.90654762  0.91636364  0.9081265   0.90524115  0.65353985
   0.81369048  0.84181818  0.79755842  0.82357906]
 [ 0.12236348  0.55327381  0.71272727  0.24711971  0.5212801   0.02329227
   0.50952381  0.68272727  0.10773503  0.44821637]
 [ 0.33576031  0.67166667  0.67363636  0.63259645  0.65680375  0.25192675
   0.62333333  0.63272727  0.59479475  0.61458347]
 [ 0.27244788  0.64083333  0.69272727  0.58654773  0.63729521  0.49371545
   0.72988095  0.76818182  0.70674203  0.73137017]
 [ 0.48124614  0.7272619   0.75454545  0.69659401  0.72387044  0.32772474
   0.65083333  0.69090909  0.61437502  0.64590521]
 [ 0.38169446  0.68821429  0.71        0.66493083  0.68590659  0.07601592
   0.53619048  0.58909091  0.30027428  0.47233427]
 [ 0.07317547  0.53392857  0.64272727  0.28539754  0.48237162  0.39911653
   0.68833333  0.69272727  0.67285205  0.68371323]
 [ 0.00192314  0.50565476  0.68272727  0.07559289  0.43635989  0.12274554
   0.55357143  0.67545455  0.23938469  0.50252451]
 [ 0.04992952  0.52369048  0.61636364  0.15927108  0.44046434  0.27425347
   0.62678571  0.72090909  0.4439784   0.60080766]
 [ 0.46058493  0.7177381   0.73909091  0.69843872  0.71621212  0.30902874
   0.65345238  0.68363636  0.5674831   0.63687549]
 [ 0.00946271  0.50833333  0.81454545  0.06666667  0.46746302  0.40241822
   0.7022619   0.72818182  0.6293269   0.68460703]
 [ 0.53883078  0.75684524  0.81363636  0.71650769  0.76153361  0.53411757
   0.765       0.77545455  0.73350981  0.75509213]
 [ 0.13935761  0.56458333  0.66272727  0.33962829  0.54185924  0.17657088
   0.58095238  0.67181818  0.39400246  0.55463469]
 [ 0.          0.5         0.77636364  0.          0.43675439  0.17721311
   0.58833333  0.58727273  0.57503296  0.58212121]
 [ 0.31357287  0.65666667  0.65454545  0.62301863  0.64341742  0.23781184
   0.62321429  0.72090909  0.39082588  0.58120798]
 [ 0.11950671  0.58571429  0.62363636  0.43743633  0.55449328  0.15408232
   0.55625     0.74909091  0.23618073  0.51922171]
 [ 0.26318452  0.61666667  0.62909091  0.6107436   0.61406926  0.50154773
   0.74452381  0.76727273  0.71325873  0.7431105 ]
 [ 0.42740013  0.70988095  0.73727273  0.63809443  0.69717921  0.52476494
   0.76166667  0.76636364  0.74201974  0.75580642]
 [ 0.4544907   0.72083333  0.73818182  0.68300267  0.71353702  0.43808143
   0.72833333  0.73727273  0.72349069  0.72171218]
 [ 0.40281605  0.69196429  0.79454545  0.56734735  0.70111695  0.6208656
   0.80833333  0.81363636  0.79819489  0.80468948]
 [-0.08479617  0.46190476  0.56636364  0.07071068  0.3896592   0.36828458
   0.68142857  0.71090909  0.62739641  0.66858059]
 [ 0.38989348  0.69107143  0.76727273  0.54234164  0.68859944  0.13536218
   0.57166667  0.57818182  0.47485035  0.54609099]
 [ 0.4788256   0.74595238  0.78545455  0.6506019   0.72903972  0.20892642
   0.59017857  0.71181818  0.42696968  0.57775186]]
GBM mean:
[0.31213728 0.65385516 0.72027273 0.51510699 0.63252929 0.32567935
 0.65671429 0.71069697 0.54424023 0.63802106]
---------------------------
---------------------------
BDDAE performance:
[[-3.33382269e-02  4.84166667e-01  4.95454545e-01  4.54989359e-01
   4.75565516e-01  4.93577389e-02  5.24117647e-01  7.22727273e-01
   2.81359831e-01  5.05831112e-01]
 [ 2.58729714e-01  6.30833333e-01  6.31818182e-01  6.17337993e-01
   6.23621377e-01  4.76177613e-01  7.32478632e-01  7.54545455e-01
   7.14956815e-01  7.33876713e-01]
 [ 8.29268657e-02  5.41452991e-01  5.59090909e-01  5.15362629e-01
   5.34287099e-01 -8.21853358e-02  4.60000000e-01  4.68181818e-01
   4.15915127e-01  4.49238865e-01]
 [-3.96849118e-04  4.99166667e-01  4.95454545e-01  4.79575671e-01
   4.86965920e-01  1.63071571e-01  5.73958333e-01  7.13636364e-01
   3.93920617e-01  5.62100896e-01]
 [ 3.15634781e-01  6.60714286e-01  6.63636364e-01  6.50644242e-01
   6.48964940e-01  2.63636364e-01  6.31818182e-01  6.31818182e-01
   6.17062730e-01  6.24787742e-01]
 [ 3.72727273e-01  6.86363636e-01  6.86363636e-01  6.70853823e-01
   6.79509449e-01  1.84930258e-02  5.08333333e-01  5.00000000e-01
   4.85253050e-01  4.92035983e-01]
 [-7.71293987e-02  4.64102564e-01  5.00000000e-01  3.77740312e-01
   4.45987840e-01  1.54545455e-01  5.77272727e-01  5.77272727e-01
   5.31284103e-01  5.55273360e-01]
 [ 3.62392677e-01  6.64761905e-01  7.45454545e-01  6.19679613e-01
   6.75108903e-01  1.59575686e-02  5.06837607e-01  5.18181818e-01
   4.91195550e-01  5.00894141e-01]
 [ 3.30132667e-01  6.45714286e-01  7.45454545e-01  5.39182708e-01
   6.45570562e-01  4.66140556e-01  7.08571429e-01  8.00000000e-01
   6.35929224e-01  7.15540362e-01]
 [ 7.27272727e-02  5.36363636e-01  5.36363636e-01  5.25182134e-01
   5.31062268e-01  2.44163209e-01  6.21666667e-01  6.22727273e-01
   6.09148961e-01  6.15536346e-01]
 [ 3.14978182e-02  5.15178571e-01  5.77272727e-01  4.28969765e-01
   4.97071702e-01  1.60483736e-01  5.77350427e-01  6.13636364e-01
   5.09252782e-01  5.59715074e-01]
 [ 1.02207785e-01  5.51709402e-01  5.77272727e-01  5.08600910e-01
   5.40041970e-01  2.68776624e-01  6.40178571e-01  6.54545455e-01
   6.21254407e-01  6.27757500e-01]
 [-1.47812099e-01  4.25641026e-01  4.36363636e-01  4.05812088e-01
   4.18910347e-01  1.69311136e-01  5.85897436e-01  5.95454545e-01
   5.68420921e-01  5.77915656e-01]
 [ 2.94792524e-02  5.12500000e-01  5.77272727e-01  4.21438672e-01
   5.01038798e-01  2.72727273e-01  6.36363636e-01  6.36363636e-01
   6.21579524e-01  6.29781068e-01]
 [-9.02395991e-02  4.57619048e-01  5.77272727e-01  2.41353671e-01
   4.32357800e-01  9.46577998e-02  5.47321429e-01  6.04545455e-01
   4.74279418e-01  5.32324088e-01]
 [ 1.26049192e-01  5.62393162e-01  6.00000000e-01  4.77920089e-01
   5.45456185e-01  1.43437980e-01  5.63392857e-01  6.59090909e-01
   3.97989139e-01  5.31788723e-01]
 [ 4.95490165e-01  7.61965812e-01  7.40909091e-01  7.44116911e-01
   7.37614709e-01  5.11357768e-02  5.22649573e-01  5.77272727e-01
   4.11114520e-01  4.92868567e-01]
 [ 3.03414550e-01  6.45833333e-01  8.18181818e-01  5.37638145e-01
   6.44272731e-01  2.06647480e-02  5.09829060e-01  5.31818182e-01
   4.81627315e-01  5.04539871e-01]
 [ 1.31378651e-01  5.63809524e-01  6.18181818e-01  5.33583478e-01
   5.61377364e-01  3.55966704e-01  6.75000000e-01  6.86363636e-01
   6.53132437e-01  6.70422439e-01]
 [ 3.81676378e-01  6.80476190e-01  7.40909091e-01  6.48690506e-01
   6.85237368e-01 -5.59784453e-03  4.96428571e-01  5.63636364e-01
   4.03610680e-01  4.78099327e-01]
 [-2.88309761e-02  4.90000000e-01  7.13636364e-01  1.36266181e-01
   4.55048689e-01  9.09090909e-02  5.45454545e-01  5.45454545e-01
   5.31761851e-01  5.38868666e-01]
 [ 2.72727273e-02  5.13636364e-01  5.13636364e-01  4.89063921e-01
   5.02722697e-01  5.10955691e-01  7.48214286e-01  7.81818182e-01
   7.30356644e-01  7.52162288e-01]
 [ 5.96879253e-02  5.30769231e-01  5.36363636e-01  5.20044688e-01
   5.24123389e-01  4.09486549e-01  6.82291667e-01  7.95454545e-01
   6.25261082e-01  6.97045022e-01]
 [ 2.18484007e-02  5.10833333e-01  5.13636364e-01  4.96021967e-01
   5.04075894e-01  2.87441142e-01  6.40170940e-01  6.63636364e-01
   6.19390832e-01  6.39468434e-01]
 [ 8.88158962e-02  5.43589744e-01  5.63636364e-01  5.25080891e-01
   5.40898604e-01  4.74559068e-02  5.23333333e-01  5.31818182e-01
   5.00094152e-01  5.15125503e-01]
 [ 1.70666938e-01  5.83333333e-01  5.95454545e-01  5.60469062e-01
   5.78182835e-01  2.06960240e-01  6.01666667e-01  6.13636364e-01
   5.77614112e-01  5.95314527e-01]
 [ 3.70478099e-01  6.72380952e-01  7.45454545e-01  6.31221863e-01
   6.78952442e-01  5.09090909e-01  7.54545455e-01  7.54545455e-01
   7.37787845e-01  7.47896531e-01]
 [ 4.29014335e-02  5.19642857e-01  5.59090909e-01  4.85702313e-01
   5.15703789e-01  3.92727618e-01  6.88461538e-01  7.22727273e-01
   6.47335791e-01  6.84455043e-01]
 [ 1.54479263e-02  5.04464286e-01  5.50000000e-01  4.35640375e-01
   4.95950418e-01  3.31167219e-01  6.62500000e-01  6.72727273e-01
   6.48212571e-01  6.61142502e-01]
 [ 2.85153534e-01  6.41071429e-01  6.72727273e-01  6.22949068e-01
   6.39408112e-01  1.46926178e-01  5.67619048e-01  6.54545455e-01
   5.05743892e-01  5.65604947e-01]]
BDDAE mean:
[0.13669969 0.56668292 0.60954545 0.51003777 0.55816966 0.20780141
 0.60045745 0.63893939 0.54806153 0.59191371]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.53181818 0.         0.34705882 0.
  0.5        0.76727273 0.         0.43385965]
 [0.         0.5        0.55181818 0.         0.35539216 0.
  0.5        0.60727273 0.         0.37753268]
 [0.         0.5        0.58       0.         0.36674837 0.
  0.5        0.52272727 0.         0.34301471]
 [0.         0.5        0.53181818 0.         0.34705882 0.
  0.5        0.71       0.         0.41504988]
 [0.         0.5        0.61636364 0.         0.38112745 0.
  0.5        0.50454545 0.         0.33492647]
 [0.         0.5        0.51363636 0.         0.33897059 0.
  0.5        0.52272727 0.         0.34301471]
 [0.         0.5        0.60727273 0.         0.37753268 0.
  0.5        0.51363636 0.         0.33897059]
 [0.         0.5        0.66454545 0.         0.39896801 0.
  0.5        0.59818182 0.         0.37393791]
 [0.         0.5        0.70090909 0.         0.41183351 0.
  0.5        0.69181818 0.         0.40861713]
 [0.         0.5        0.51363636 0.         0.33897059 0.
  0.5        0.53181818 0.         0.34705882]
 [0.         0.5        0.61636364 0.         0.38112745 0.
  0.5        0.59818182 0.         0.37393791]
 [0.         0.5        0.60727273 0.         0.37753268 0.
  0.5        0.61636364 0.         0.38112745]
 [0.         0.5        0.58       0.         0.36674837 0.
  0.5        0.59818182 0.         0.37393791]
 [0.         0.5        0.64545455 0.         0.39207516 0.
  0.5        0.51363636 0.         0.33897059]
 [0.         0.5        0.70090909 0.         0.41183351 0.
  0.5        0.65545455 0.         0.39575163]
 [0.         0.5        0.60727273 0.         0.37753268 0.
  0.5        0.65545455 0.         0.39575163]
 [0.         0.5        0.58909091 0.         0.37034314 0.
  0.5        0.59818182 0.         0.37393791]
 [0.         0.5        0.83272727 0.         0.45418129 0.
  0.5        0.58909091 0.         0.37034314]
 [0.         0.5        0.67363636 0.         0.40218438 0.
  0.5        0.55181818 0.         0.35539216]
 [0.         0.5        0.66454545 0.         0.39896801 0.
  0.5        0.63545455 0.         0.38839869]
 [0.         0.5        0.77636364 0.         0.43675439 0.
  0.5        0.50454545 0.         0.33492647]
 [0.         0.5        0.51363636 0.         0.33897059 0.
  0.5        0.64545455 0.         0.39207516]
 [0.         0.5        0.58       0.         0.36674837 0.
  0.5        0.73909091 0.         0.4248022 ]
 [0.         0.5        0.54181818 0.         0.35122549 0.
  0.5        0.57090909 0.         0.36315359]
 [0.         0.5        0.57090909 0.         0.36315359 0.
  0.5        0.53181818 0.         0.34705882]
 [0.         0.5        0.55181818 0.         0.35539216 0.
  0.5        0.56181818 0.         0.35955882]
 [0.         0.5        0.69181818 0.         0.40861713 0.
  0.5        0.50454545 0.         0.33492647]
 [0.         0.5        0.62545455 0.         0.38472222 0.
  0.5        0.58909091 0.         0.37034314]
 [0.         0.5        0.64545455 0.         0.39207516 0.
  0.5        0.55181818 0.         0.35539216]
 [0.         0.5        0.61636364 0.         0.38112745 0.
  0.5        0.67363636 0.         0.40218438]]
DUMMY mean:
[0.         0.5        0.61475758 0.         0.37916581 0.
 0.5        0.59515152 0.         0.37159843]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_39
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.318 0.658 0.7   0.577 0.646 0.364 0.681 0.711 0.632 0.672]
 [0.302 0.655 0.685 0.593 0.646 0.299 0.66  0.681 0.618 0.649]
 [0.369 0.687 0.717 0.638 0.679 0.339 0.674 0.7   0.627 0.666]
 [0.09  0.542 0.649 0.149 0.452 0.11  0.553 0.643 0.191 0.467]
 [0.312 0.654 0.72  0.515 0.633 0.326 0.657 0.711 0.544 0.638]
 [0.137 0.567 0.61  0.51  0.558 0.208 0.6   0.639 0.548 0.592]
 [0.    0.5   0.615 0.    0.379 0.    0.5   0.595 0.    0.372]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.21  0.104 0.09  0.18  0.111 0.15  0.076 0.065 0.112 0.078]
 [0.2   0.098 0.094 0.153 0.1   0.169 0.087 0.078 0.116 0.092]
 [0.189 0.095 0.091 0.143 0.098 0.179 0.074 0.065 0.114 0.075]
 [0.155 0.072 0.079 0.236 0.112 0.18  0.089 0.077 0.276 0.131]
 [0.208 0.103 0.074 0.24  0.123 0.164 0.081 0.066 0.191 0.098]
 [0.166 0.081 0.095 0.121 0.085 0.164 0.079 0.088 0.11  0.085]
 [0.    0.    0.075 0.    0.028 0.    0.    0.071 0.    0.027]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 66.  16.  13.  31.  17.  41.  11.   9.  18.  12.]
 [ 66.  15.  14.  26.  15.  57.  13.  11.  19.  14.]
 [ 51.  14.  13.  22.  14.  53.  11.   9.  18.  11.]
 [172.  13.  12. 158.  25. 163.  16.  12. 144.  28.]
 [ 67.  16.  10.  47.  19.  50.  12.   9.  35.  15.]
 [121.  14.  16.  24.  15.  79.  13.  14.  20.  14.]
 [  0.   0.  12.   0.   7.   0.   0.  12.   0.   7.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_39
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  15.008
step (sec):  11.248
overlap:  True
perc. of overlap:  25.053304904051174
overlap duration (sec):  3.76
Number of windows / instances:  107
Elapsed time: 671.6657442609469 minutes
Elapsed time: 11.194429071015781 hours
