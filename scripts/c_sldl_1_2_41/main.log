2024-05-08 08:22:43.438937: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-08 08:22:47.268106: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-08 08:22:56.837893: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  17.0
step (sec):  17.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_41\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

8ms/step - loss: 0.0235 - mean_squared_error: 0.0235
[1m46/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - loss: 0.0232 - mean_squared_error: 0.0232
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0072 - val_mean_squared_error: 0.0072
(16992, 1, 5)
Model: "sequential_1197"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 16992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1062, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 16992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 16992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1196     â”‚ (None, 1062, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1197     â”‚ (None, 1062, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_598     â”‚ (None, 1062, 12)  â”‚          0 â”‚ sequential_1196[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1197[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_598         â”‚ (None, 12, 1062)  â”‚          0 â”‚ concatenate_598[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_598         â”‚ (None, 12744)     â”‚          0 â”‚ permute_598[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_598         â”‚ (None, 12744)     â”‚          0 â”‚ flatten_598[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_598 (Dense)   â”‚ (None, 1)         â”‚     12,745 â”‚ dropout_598[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 13,567 (53.00 KB)
 Trainable params: 13,567 (53.00 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:04[0m 1s/step - binary_accuracy: 1.0000 - loss: 0.4626
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9844 - loss: 0.3767 
[1m15/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8873 - loss: 0.5631
[1m23/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8262 - loss: 0.6649
[1m30/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7917 - loss: 0.6980
[1m39/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7668 - loss: 0.7148
[1m48/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7594 - loss: 0.7174
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 12ms/step - binary_accuracy: 0.7570 - loss: 0.7181 - val_binary_accuracy: 0.6667 - val_loss: 0.6843
Epoch 2/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 26ms/step - binary_accuracy: 1.0000 - loss: 0.0845
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8894 - loss: 0.3224 
[1m16/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8649 - loss: 0.3958
[1m24/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8493 - loss: 0.4335
[1m32/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8323 - loss: 0.4628
[1m40/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8215 - loss: 0.4810
[1m48/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8114 - loss: 0.4993
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8083 - loss: 0.5057 - val_binary_accuracy: 0.6667 - val_loss: 0.6693
Epoch 3/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3346
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9844 - loss: 0.3393 
[1m14/50[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9071 - loss: 0.4110
[1m21/50[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8504 - loss: 0.4587
[1m30/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8261 - loss: 0.4793
[1m38/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8110 - loss: 0.5019
[1m47/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7992 - loss: 0.5193
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7958 - loss: 0.5253 - val_binary_accuracy: 0.6667 - val_loss: 0.6600
Epoch 4/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4506
[1m 9/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 1.0000 - loss: 0.4102 
[1m17/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9535 - loss: 0.4369
[1m26/50[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9137 - loss: 0.4652
[1m34/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8966 - loss: 0.4737
[1m42/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8775 - loss: 0.4872
[1m49/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8649 - loss: 0.4972
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8616 - loss: 0.4999 - val_binary_accuracy: 0.5000 - val_loss: 0.7026
Epoch 5/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2243
[1m10/50[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8425 - loss: 0.4732 
[1m18/50[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8083 - loss: 0.5101
[1m27/50[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8121 - loss: 0.5091
[1m35/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8047 - loss: 0.5292
[1m43/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.7940 - loss: 0.5480
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7875 - loss: 0.5585
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7869 - loss: 0.5594 - val_binary_accuracy: 0.6667 - val_loss: 0.6731
Epoch 6/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.0856
[1m 9/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 1.0000 - loss: 0.2378 
[1m17/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9628 - loss: 0.2753
[1m25/50[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9162 - loss: 0.3397
[1m33/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8753 - loss: 0.3990
[1m41/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8419 - loss: 0.4417
[1m48/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8227 - loss: 0.4672
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8178 - loss: 0.4744 - val_binary_accuracy: 0.6667 - val_loss: 0.6835
Epoch 7/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.0570
[1m 9/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5750 - loss: 0.6494     
[1m18/50[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.6897 - loss: 0.5757
[1m26/50[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7082 - loss: 0.5553
[1m35/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.7258 - loss: 0.5327
[1m43/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.7381 - loss: 0.5167
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7493 - loss: 0.5012 - val_binary_accuracy: 0.6667 - val_loss: 0.8012
Epoch 8/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0755
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8560 - loss: 0.3116 
[1m15/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8140 - loss: 0.3330
[1m23/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7910 - loss: 0.3741
[1m32/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7939 - loss: 0.3888
[1m41/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.7929 - loss: 0.4021
[1m49/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7910 - loss: 0.4083
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7897 - loss: 0.4100 - val_binary_accuracy: 0.6667 - val_loss: 0.6984
Epoch 9/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.6255
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 1.0000 - loss: 0.3590 
[1m17/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9929 - loss: 0.3327
[1m25/50[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9733 - loss: 0.3403
[1m34/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9541 - loss: 0.3539
[1m41/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9439 - loss: 0.3632
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9345 - loss: 0.3694
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9334 - loss: 0.3701 - val_binary_accuracy: 0.6667 - val_loss: 0.7742
Epoch 10/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2650
[1m 7/50[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 1.0000 - loss: 0.1601 
[1m15/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 1.0000 - loss: 0.1787
[1m23/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9718 - loss: 0.2273
[1m32/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9490 - loss: 0.2683
[1m40/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9278 - loss: 0.2986
[1m47/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9148 - loss: 0.3153
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9088 - loss: 0.3221 - val_binary_accuracy: 0.6667 - val_loss: 0.7456

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 112ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 112ms/step
predicted [0.64715105 0.13438801 0.17012623 0.26413807 0.6629821  0.26948223
 0.8550192  0.329513   0.6174909  0.7886922  0.8312994  0.5666823
 0.87952155 0.3443515 ]
predicted [1 0 0 0 1 0 1 0 1 1 1 1 1 0]
expected [False  True  True  True  True False  True  True  True False  True False
 False False]
accuracy: 0.42857142857142855
confusion matrix: 
[[2 4]
 [4 4]]
              precision    recall  f1-score   support

       False       0.33      0.33      0.33         6
        True       0.50      0.50      0.50         8

    accuracy                           0.43        14
   macro avg       0.42      0.42      0.42        14
weighted avg       0.43      0.43      0.43        14

macro avg f1-score: 0.41666666666666663
macro avg (UAR): 0.41666666666666663
Sensitivity:  0.3333333333333333
Specificity:  0.5
g-mean:  0.408248290463863
-------- Model Performance ----------: 
accuracy:  [0.64285714 0.64285714 0.64285714 0.42857143 0.64285714 0.71428571
 0.71428571 0.64285714 0.42857143 0.        ]
gmean:  [0.64549722 0.61237244 0.54006172 0.4330127  0.54006172 0.57735027
 0.57735027 0.61237244 0.40824829 0.        ]
f1_score:  [0.64102564 0.62566845 0.59064327 0.42857143 0.59064327 0.65
 0.65       0.62566845 0.41666667 0.        ]
UAR:  [0.64583333 0.625      0.60416667 0.4375     0.60416667 0.66666667
 0.66666667 0.625      0.41666667 0.        ]
Cohen Kappa score:  [ 0.28571429  0.25531915  0.22222222 -0.12        0.22222222  0.36363636
  0.36363636  0.25531915 -0.16666667  0.        ]
Split Repetition number:  9
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [31 51  3 29 60  8 48 34 32  6 42 62 37 43 39 18 65 45  7 49 53 38  0 15
  5  1 61 17 67 16 11  9 56 30 22 36 50  4 14 47 21 25 52 28  2 63 57 12
 64 20 35 40 23 54 68 46] TEST: [10 44 41 55 66 58 19 33 13 27 69 24 59 26]
(DL) TRAIN number of instances:  56
(DL) TEST number of instances:  14
(DL) Total number of instances (TRAIN+TEST):  70
C:\Users\Javier\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\convolutional\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(
----- train_GSR_AE -------
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 16992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1062, 6)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2396              â”‚ (None, 4248, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2396           â”‚ (None, 4248, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2397              â”‚ (None, 16992, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2397           â”‚ (None, 16992, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m55s[0m 1s/step - loss: 15.6141 - mean_squared_error: 15.6141
[1m 7/50[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 19.4659 - mean_squared_error: 19.4659
[1m14/50[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 18.9541 - mean_squared_error: 18.9541
[1m21/50[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 17.5157 - mean_squared_error: 17.5157 
[1m29/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 16.3313 - mean_squared_error: 16.3313
[1m36/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 15.5666 - mean_squared_error: 15.5666
[1m44/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - loss: 14.8898 - mean_squared_error: 14.8898
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 13ms/step - loss: 14.5076 - mean_squared_error: 14.5076 - val_loss: 14.2340 - val_mean_squared_error: 14.2340
Epoch 2/5

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 3.4254 - mean_squared_error: 3.4254
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 6.2600 - mean_squared_error: 6.2600 
[1m16/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 7.8175 - mean_squared_error: 7.8175
[1m23/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 8.1056 - mean_squared_error: 8.1056
[1m31/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 8.2631 - mean_squared_error: 8.2631
[1m38/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 8.4989 - mean_squared_error: 8.4989
[1m46/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - loss: 8.7812 - mean_squared_error: 8.7812
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 8.8839 - mean_squared_error: 8.8839 - val_loss: 14.1949 - val_mean_squared_error: 14.1949
Epoch 3/5

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 5.6124 - mean_squared_error: 5.6124
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 10.5861 - mean_squared_error: 10.5861
[1m15/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 12.2103 - mean_squared_error: 12.2103
[1m23/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 12.4643 - mean_squared_error: 12.4643
[1m29/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 12.2448 - mean_squared_error: 12.2448
[1m35/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 11.9874 - mean_squared_error: 11.9874
[1m43/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - loss: 11.7359 - mean_squared_error: 11.7359
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 11.5125 - mean_squared_error: 11.5125
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 11.4801 - mean_squared_error: 11.4801 - val_loss: 14.1855 - val_mean_squared_error: 14.1855
Epoch 4/5

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0201 - mean_squared_error: 0.0201
[1m 7/50[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.9606 - mean_squared_error: 0.9606 
[1m13/50[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 3.3766 - mean_squared_error: 3.3766
[1m21/50[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 4.9864 - mean_squared_error: 4.9864
[1m28/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 5.6645 - mean_squared_error: 5.6645
[1m36/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 6.6172 - mean_squared_error: 6.6172
[1m43/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - loss: 7.1554 - mean_squared_error: 7.1554
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 7.4637 - mean_squared_error: 7.4637
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 7.5106 - mean_squared_error: 7.5106 - val_loss: 14.1814 - val_mean_squared_error: 14.1814
Epoch 5/5

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 30ms/step - loss: 1.9006 - mean_squared_error: 1.9006
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 4.7393 - mean_squared_error: 4.7393 
[1m16/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 4.9840 - mean_squared_error: 4.9840
[1m23/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 6.2911 - mean_squared_error: 6.2911
[1m31/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 7.0415 - mean_squared_error: 7.0415
[1m38/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 7.5382 - mean_squared_error: 7.5382
[1m45/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - loss: 7.9505 - mean_squared_error: 7.9505
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 8.1693 - mean_squared_error: 8.1693 - val_loss: 14.1788 - val_mean_squared_error: 14.1788
(56, 1062, 6)
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 16992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1062, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 16992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1062, 6)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2398              â”‚ (None, 4248, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2398           â”‚ (None, 4248, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2399              â”‚ (None, 16992, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2399           â”‚ (None, 16992, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m55s[0m 1s/step - loss: 0.2492 - mean_squared_error: 0.2492
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.1316 - mean_squared_error: 0.1316
[1m15/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.1073 - mean_squared_error: 0.1073
[1m23/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0924 - mean_squared_error: 0.0924
[1m30/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0829 - mean_squared_error: 0.0829
[1m38/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0756 - mean_squared_error: 0.0756
[1m45/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - loss: 0.0709 - mean_squared_error: 0.0709
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 13ms/step - loss: 0.0674 - mean_squared_error: 0.0674 - val_loss: 0.0366 - val_mean_squared_error: 0.0366
Epoch 2/5

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0258 - mean_squared_error: 0.0258
[1m 7/50[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0217 - mean_squared_error: 0.0217 
[1m14/50[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0193 - mean_squared_error: 0.0193
[1m21/50[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0188 - mean_squared_error: 0.0188
[1m28/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0194 - mean_squared_error: 0.0194
[1m36/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0199 - mean_squared_error: 0.0199
[1m42/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0202 - mean_squared_error: 0.0202
[1m49/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - loss: 0.0202 - mean_squared_error: 0.0202
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 0.0201 - mean_squared_error: 0.0201 - val_loss: 0.0337 - val_mean_squared_error: 0.0337
Epoch 3/5

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0173 - mean_squared_error: 0.0173
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0102 - mean_squared_error: 0.0102 
[1m16/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0117 - mean_squared_error: 0.0117
[1m24/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0134 - mean_squared_error: 0.0134
[1m31/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0144 - mean_squared_error: 0.0144
[1m39/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0155 - mean_squared_error: 0.0155
[1m45/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - loss: 0.0159 - mean_squared_error: 0.0159
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 0.0161 - mean_squared_error: 0.0161 - val_loss: 0.0315 - val_mean_squared_error: 0.0315
Epoch 4/5

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - loss: 0.0457 - mean_squared_error: 0.0457
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0194 - mean_squared_error: 0.0194 
[1m16/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0148 - mean_squared_error: 0.0148
[1m23/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0137 - mean_squared_error: 0.0137
[1m31/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0136 - mean_squared_error: 0.0136
[1m38/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0139 - mean_squared_error: 0.0139
[1m45/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - loss: 0.0142 - mean_squared_error: 0.0142
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 0.0146 - mean_squared_error: 0.0146 - val_loss: 0.0298 - val_mean_squared_error: 0.0298
Epoch 5/5

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - loss: 0.0067 - mean_squared_error: 0.0067
[1m 7/50[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0076 - mean_squared_error: 0.0076
[1m15/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0093 - mean_squared_error: 0.0093 
[1m22/50[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0107 - mean_squared_error: 0.0107
[1m30/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0121 - mean_squared_error: 0.0121
[1m37/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0127 - mean_squared_error: 0.0127
[1m45/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 9ms/step - loss: 0.0133 - mean_squared_error: 0.0133
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 0.0137 - mean_squared_error: 0.0137 - val_loss: 0.0292 - val_mean_squared_error: 0.0292
(16992, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 16992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1062, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 16992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 16992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 1062, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 1062, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 1062, 12)  â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 1062)  â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 12744)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 12744)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     12,745 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 13,567 (53.00 KB)
 Trainable params: 13,567 (53.00 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:16[0m 2s/step - binary_accuracy: 0.0000e+00 - loss: 0.8336
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5810 - loss: 0.6012     
[1m16/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6692 - loss: 0.5740
[1m23/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.6858 - loss: 0.5992
[1m31/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6750 - loss: 0.6372
[1m40/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6551 - loss: 0.6623
[1m49/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6456 - loss: 0.6751
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 12ms/step - binary_accuracy: 0.6422 - loss: 0.6780 - val_binary_accuracy: 0.5000 - val_loss: 0.7348
Epoch 2/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4782
[1m 9/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7382 - loss: 0.5272 
[1m17/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6002 - loss: 0.6087
[1m25/50[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5775 - loss: 0.6324
[1m34/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5883 - loss: 0.6322
[1m42/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5918 - loss: 0.6369
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5936 - loss: 0.6405
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5933 - loss: 0.6411 - val_binary_accuracy: 1.0000 - val_loss: 0.4296
Epoch 3/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7762
[1m 9/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5380 - loss: 0.6056     
[1m17/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6167 - loss: 0.6092
[1m25/50[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6221 - loss: 0.6240
[1m31/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6318 - loss: 0.6256
[1m40/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6408 - loss: 0.6234
[1m49/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6489 - loss: 0.6219
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6493 - loss: 0.6226 - val_binary_accuracy: 0.8333 - val_loss: 0.4345
Epoch 4/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.6486
[1m 9/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7284 - loss: 0.6089 
[1m17/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7287 - loss: 0.5907
[1m25/50[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7290 - loss: 0.5789
[1m33/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7237 - loss: 0.5793
[1m40/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7201 - loss: 0.5802
[1m48/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7158 - loss: 0.5834
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7136 - loss: 0.5850 - val_binary_accuracy: 0.8333 - val_loss: 0.4640
Epoch 5/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4100
[1m 9/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7844 - loss: 0.4869 
[1m18/50[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.7909 - loss: 0.4741
[1m26/50[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7738 - loss: 0.4828
[1m34/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7632 - loss: 0.4884
[1m41/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7631 - loss: 0.4914
[1m47/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7661 - loss: 0.4924
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7653 - loss: 0.4963 - val_binary_accuracy: 0.8333 - val_loss: 0.5277
Epoch 6/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1369
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8738 - loss: 0.3836 
[1m16/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8133 - loss: 0.4402
[1m24/50[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7905 - loss: 0.4602
[1m30/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7708 - loss: 0.4709
[1m37/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7473 - loss: 0.4859
[1m44/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7354 - loss: 0.4914
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7209 - loss: 0.5032 - val_binary_accuracy: 1.0000 - val_loss: 0.2179
Epoch 7/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 30ms/step - binary_accuracy: 0.0000e+00 - loss: 0.8010
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6603 - loss: 0.4108     
[1m15/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.7589 - loss: 0.3668
[1m22/50[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7806 - loss: 0.3662
[1m29/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7809 - loss: 0.3827
[1m36/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7739 - loss: 0.4012
[1m43/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7584 - loss: 0.4232
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7484 - loss: 0.4391
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7471 - loss: 0.4413 - val_binary_accuracy: 0.6667 - val_loss: 0.4788
Epoch 8/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 30ms/step - binary_accuracy: 1.0000 - loss: 0.1601
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8101 - loss: 0.3705 
[1m14/50[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8102 - loss: 0.3848
[1m21/50[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8172 - loss: 0.3968
[1m28/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8242 - loss: 0.3952
[1m34/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8255 - loss: 0.3969
[1m40/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8231 - loss: 0.4017
[1m46/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8177 - loss: 0.4077
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8147 - loss: 0.4118 - val_binary_accuracy: 0.8333 - val_loss: 0.4240
Epoch 9/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 30ms/step - binary_accuracy: 1.0000 - loss: 0.3270
[1m 7/50[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9558 - loss: 0.2924 
[1m14/50[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8725 - loss: 0.3750
[1m21/50[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8176 - loss: 0.4244
[1m28/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7976 - loss: 0.4475
[1m35/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7782 - loss: 0.4645
[1m42/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7633 - loss: 0.4742
[1m49/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7567 - loss: 0.4784
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7560 - loss: 0.4788 - val_binary_accuracy: 0.8333 - val_loss: 0.3337
Epoch 10/10

[1m 1/50[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4240
[1m 8/50[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8894 - loss: 0.4734 
[1m15/50[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8485 - loss: 0.5141
[1m22/50[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8280 - loss: 0.5149
[1m29/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8228 - loss: 0.5026
[1m36/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8261 - loss: 0.4863
[1m42/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8263 - loss: 0.4767
[1m49/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8232 - loss: 0.4702
[1m50/50[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8223 - loss: 0.4688 - val_binary_accuracy: 0.6667 - val_loss: 0.5598

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 112ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 112ms/step
predicted [0.8625504  0.45659533 0.1702608  0.5137646  0.04614852 0.6209664
 0.7610724  0.5453906  0.70140386 0.56458974 0.5738627  0.36547267
 0.3137754  0.18979996]
predicted [1 0 0 1 0 1 1 1 1 1 1 0 0 0]
expected [False  True  True  True False  True  True  True False  True False  True
 False False]
accuracy: 0.5714285714285714
confusion matrix: 
[[3 3]
 [3 5]]
              precision    recall  f1-score   support

       False       0.50      0.50      0.50         6
        True       0.62      0.62      0.62         8

    accuracy                           0.57        14
   macro avg       0.56      0.56      0.56        14
weighted avg       0.57      0.57      0.57        14

macro avg f1-score: 0.5625
macro avg (UAR): 0.5625
Sensitivity:  0.5
Specificity:  0.625
g-mean:  0.5590169943749475
-------- Model Performance ----------: 
accuracy:  [0.64285714 0.64285714 0.64285714 0.42857143 0.64285714 0.71428571
 0.71428571 0.64285714 0.42857143 0.57142857]
gmean:  [0.64549722 0.61237244 0.54006172 0.4330127  0.54006172 0.57735027
 0.57735027 0.61237244 0.40824829 0.55901699]
f1_score:  [0.64102564 0.62566845 0.59064327 0.42857143 0.59064327 0.65
 0.65       0.62566845 0.41666667 0.5625    ]
UAR:  [0.64583333 0.625      0.60416667 0.4375     0.60416667 0.66666667
 0.66666667 0.625      0.41666667 0.5625    ]
Cohen Kappa score:  [ 0.28571429  0.25531915  0.22222222 -0.12        0.22222222  0.36363636
  0.36363636  0.25531915 -0.16666667  0.125     ]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  16.992
step (sec):  16.992
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  70
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.732 0.867 0.871 0.854 0.862 0.267 0.649 0.671 0.551 0.618]
 [0.528 0.768 0.786 0.671 0.746 0.361 0.652 0.686 0.634 0.621]
 [0.702 0.84  0.843 0.854 0.832 0.168 0.698 0.729 0.558 0.681]
 [0.    0.5   0.6   0.    0.374 0.    0.5   0.643 0.    0.39 ]
 [0.502 0.755 0.771 0.681 0.73  0.317 0.651 0.729 0.528 0.637]
 [0.181 0.585 0.607 0.551 0.578 0.031 0.511 0.571 0.419 0.497]
 [0.    0.5   0.6   0.    0.374 0.    0.5   0.643 0.    0.39 ]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.44099368  0.725       0.71428571  0.69821462  0.70555556  0.19078431
   0.58833333  0.77142857  0.30466691  0.57175991]
 [ 0.35508696  0.675       0.68571429  0.65855708  0.6725      0.42382237
   0.70916667  0.74285714  0.63646429  0.69771284]
 [ 0.39314888  0.69166667  0.71428571  0.63552671  0.685       0.40482051
   0.70833333  0.7         0.69996355  0.69702381]
 [ 0.5238495   0.7625      0.77142857  0.70099677  0.74445887  0.25826087
   0.635       0.71428571  0.49719719  0.61111111]
 [ 0.62999496  0.79583333  0.84285714  0.76161208  0.80608586  0.54400466
   0.77916667  0.77142857  0.75752386  0.76261905]
 [ 0.48237718  0.75        0.72857143  0.71263042  0.7193254   0.56049407
   0.77916667  0.78571429  0.76350996  0.77496032]
 [ 0.23741561  0.62333333  0.64285714  0.49650798  0.58098124  0.32792811
   0.6625      0.67142857  0.62399121  0.64805556]
 [ 0.63470635  0.80916667  0.85714286  0.74053118  0.80329004  0.62872674
   0.82083333  0.82857143  0.79897093  0.80573232]
 [ 0.22734403  0.60333333  0.71428571  0.40382329  0.58424242 -0.06757498
   0.46166667  0.61428571  0.12844571  0.43010101]
 [ 0.39123246  0.69583333  0.7         0.66059375  0.68043651  0.06609628
   0.53333333  0.54285714  0.46069189  0.51380952]
 [ 0.40732954  0.70083333  0.72857143  0.68136157  0.69499639  0.34407084
   0.66666667  0.68571429  0.62648561  0.65769841]
 [ 0.46639486  0.7325      0.75714286  0.6467356   0.70825397  0.35023158
   0.67416667  0.7         0.60655888  0.65777778]
 [ 0.46403922  0.73416667  0.74285714  0.71002952  0.72388889 -0.14061373
   0.4325      0.47142857  0.29391576  0.4141811 ]
 [ 0.04831322  0.51583333  0.5         0.42636608  0.46862554  0.28850431
   0.64583333  0.65714286  0.59293808  0.63242063]
 [-0.04034178  0.4825      0.61428571  0.18872849  0.44954545  0.28973009
   0.63666667  0.67142857  0.61086115  0.63073232]
 [ 0.15582756  0.56916667  0.61428571  0.46971515  0.55745671  0.44671704
   0.70333333  0.77142857  0.61633058  0.69984848]
 [ 0.45066636  0.73416667  0.74285714  0.66742651  0.70833333  0.32078275
   0.6625      0.71428571  0.49988579  0.62570346]
 [ 0.00980392  0.5         0.77142857  0.1         0.48315851  0.38355197
   0.69        0.71428571  0.63729052  0.68545094]
 [ 0.43104985  0.71166667  0.75714286  0.65133827  0.70338384  0.29073619
   0.65        0.65714286  0.57760209  0.63234127]
 [ 0.19122165  0.5925      0.65714286  0.4620694   0.57726912  0.16484059
   0.58        0.64285714  0.46476227  0.56515152]
 [ 0.12418585  0.55833333  0.72857143  0.28872849  0.53994172 -0.27020878
   0.36666667  0.38571429  0.25412415  0.34666667]
 [ 0.18380936  0.6         0.6         0.53287864  0.57992063  0.2411351
   0.62083333  0.65714286  0.50708319  0.58882395]
 [ 0.34155965  0.665       0.7         0.59948005  0.65840548 -0.00372549
   0.50166667  0.71428571  0.12649111  0.46170163]
 [-0.11110179  0.44583333  0.44285714  0.38155151  0.41904762  0.39504875
   0.69166667  0.71428571  0.64834869  0.68230159]
 [ 0.54128894  0.77583333  0.77142857  0.75394528  0.75948413  0.57251616
   0.79583333  0.78571429  0.78216909  0.78071429]
 [ 0.38041938  0.69166667  0.7         0.61554663  0.66473665  0.19907915
   0.59583333  0.58571429  0.53011137  0.56674603]
 [ 0.37162699  0.6875      0.75714286  0.56646451  0.66396465  0.63348718
   0.825       0.81428571  0.8156807   0.8125    ]
 [ 0.17539225  0.58416667  0.62857143  0.48233337  0.56691919  0.38075494
   0.68583333  0.71428571  0.6140676   0.67124459]
 [-0.16098659  0.42083333  0.48571429  0.20907703  0.3982684   0.4199322
   0.70833333  0.71428571  0.68667912  0.69980159]
 [ 0.73220926  0.86666667  0.87142857  0.85358931  0.86204185  0.2665363
   0.64916667  0.67142857  0.55064569  0.61792569]]
KNN mean:
[0.31596191 0.65669444 0.69809524 0.55854531 0.63898393 0.29701567
 0.64866667 0.68619048 0.55711523 0.63142058]
---------------------------
---------------------------
DT performance:
[[ 4.44340799e-01  7.45833333e-01  7.42857143e-01  7.08964004e-01
   7.24404762e-01 -2.88759919e-02  5.08333333e-01  6.28571429e-01
   3.33657694e-01  4.94974747e-01]
 [ 4.28554677e-01  6.87500000e-01  7.00000000e-01  6.80707898e-01
   6.74444444e-01  3.45605226e-01  6.84166667e-01  7.00000000e-01
   6.21565024e-01  6.67676768e-01]
 [ 5.00057971e-01  7.87500000e-01  8.00000000e-01  8.09441345e-01
   7.91865079e-01  4.58958954e-01  7.45833333e-01  7.42857143e-01
   7.02149170e-01  7.27698413e-01]
 [ 3.46385730e-01  6.79166667e-01  6.71428571e-01  6.74721800e-01
   6.61349206e-01  2.91125320e-01  5.90000000e-01  6.71428571e-01
   4.36392447e-01  5.59318182e-01]
 [ 4.13806807e-01  7.36666667e-01  7.71428571e-01  7.50317757e-01
   7.41839827e-01  2.93335225e-01  7.04166667e-01  7.14285714e-01
   7.00400240e-01  6.87460317e-01]
 [ 4.82837607e-01  7.54166667e-01  7.28571429e-01  6.87409709e-01
   7.18571429e-01  2.62673975e-01  6.66666667e-01  6.57142857e-01
   5.66699031e-01  6.40714286e-01]
 [ 1.40295408e-01  5.84166667e-01  6.00000000e-01  4.83503610e-01
   5.55173160e-01  2.19411202e-01  5.75000000e-01  5.85714286e-01
   4.67408582e-01  5.41482684e-01]
 [ 9.00388747e-01  9.00833333e-01  9.14285714e-01  9.17848315e-01
   8.96623377e-01  3.72589551e-01  6.31666667e-01  7.00000000e-01
   5.67368472e-01  5.96681097e-01]
 [ 3.33058758e-01  6.96666667e-01  7.42857143e-01  5.27158205e-01
   6.79520202e-01  1.57969703e-01  5.73333333e-01  6.57142857e-01
   3.80783967e-01  5.50126263e-01]
 [ 2.41783014e-01  6.12500000e-01  6.14285714e-01  5.59251746e-01
   5.88214286e-01  1.41121922e-01  5.62500000e-01  5.71428571e-01
   4.74413015e-01  5.55000000e-01]
 [ 2.74966900e-01  6.65833333e-01  7.00000000e-01  5.46278059e-01
   6.21284271e-01  4.34189723e-01  7.16666667e-01  7.28571429e-01
   6.37563385e-01  7.10277778e-01]
 [ 3.89645688e-01  6.95000000e-01  7.00000000e-01  5.92831577e-01
   6.78412698e-01  3.27437418e-01  6.33333333e-01  6.57142857e-01
   5.85674347e-01  6.20833333e-01]
 [ 1.25016733e-01  5.74166667e-01  5.71428571e-01  4.77871990e-01
   5.39007937e-01 -8.19360495e-02  4.54166667e-01  4.71428571e-01
   3.52034578e-01  4.38333333e-01]
 [ 9.83552341e-03  4.74166667e-01  5.14285714e-01  4.19929455e-01
   4.60652958e-01  3.16683794e-01  7.08333333e-01  7.14285714e-01
   4.99901857e-01  7.01746032e-01]
 [-4.45248869e-03  4.63333333e-01  5.28571429e-01  3.66303166e-01
   4.46284271e-01  2.01800982e-01  5.95000000e-01  6.42857143e-01
   4.99151786e-01  5.66388889e-01]
 [ 2.82700314e-01  6.15833333e-01  6.57142857e-01  5.06739504e-01
   5.99318182e-01  2.50168416e-01  6.42500000e-01  6.85714286e-01
   5.78399301e-01  6.21540404e-01]
 [ 6.16342252e-01  7.75833333e-01  7.85714286e-01  7.75661143e-01
   7.68611111e-01  4.47627850e-01  6.88333333e-01  6.85714286e-01
   7.03883701e-01  6.66071429e-01]
 [ 1.71526002e-01  5.58333333e-01  8.14285714e-01  2.80729812e-01
   5.45979021e-01  3.30170400e-01  6.98333333e-01  7.00000000e-01
   6.72209557e-01  6.77142857e-01]
 [ 4.58179493e-01  7.24166667e-01  7.57142857e-01  6.51570157e-01
   7.14696970e-01  3.57111685e-01  6.41666667e-01  6.42857143e-01
   6.45432478e-01  6.29126984e-01]
 [ 1.86185848e-01  5.02500000e-01  5.71428571e-01  5.25314957e-01
   4.87554113e-01  2.23257755e-01  5.90833333e-01  6.28571429e-01
   5.64820307e-01  5.82218615e-01]
 [-6.26939471e-02  5.01666667e-01  6.28571429e-01  1.36421914e-01
   4.60858586e-01  2.58980890e-02  5.70833333e-01  5.57142857e-01
   5.09673555e-01  5.26746032e-01]
 [ 2.70821628e-01  6.66666667e-01  6.57142857e-01  6.18328774e-01
   6.47142857e-01  3.25015739e-01  6.29166667e-01  6.57142857e-01
   5.80044497e-01  6.13506494e-01]
 [ 4.81809037e-01  7.24166667e-01  7.28571429e-01  7.35426391e-01
   6.94722222e-01  4.32907076e-02  5.50000000e-01  6.71428571e-01
   2.39457438e-01  5.30505051e-01]
 [ 1.99286015e-01  5.12500000e-01  5.14285714e-01  5.02262193e-01
   4.79776335e-01  3.62409750e-01  7.54166667e-01  7.57142857e-01
   7.16699031e-01  7.39285714e-01]
 [ 4.22468430e-01  7.00000000e-01  7.00000000e-01  7.39173518e-01
   6.82976190e-01  3.89216479e-01  7.58333333e-01  7.57142857e-01
   6.74721800e-01  7.47738095e-01]
 [ 2.82422767e-01  6.50000000e-01  6.42857143e-01  5.99654767e-01
   6.21944444e-01  7.79275362e-02  5.33333333e-01  5.28571429e-01
   4.47716239e-01  5.18809524e-01]
 [ 2.80812840e-01  6.76666667e-01  7.14285714e-01  5.70302198e-01
   6.42752525e-01  4.18492754e-01  6.87500000e-01  7.00000000e-01
   6.41646878e-01  6.86269841e-01]
 [ 7.18500349e-02  5.96666667e-01  6.42857143e-01  3.76392963e-01
   5.89577922e-01  5.12386305e-01  7.42500000e-01  7.42857143e-01
   6.79960331e-01  7.25079365e-01]
 [ 5.26926793e-02  4.85833333e-01  4.85714286e-01  4.20928112e-01
   4.59365079e-01  7.69230769e-04  5.83333333e-01  5.71428571e-01
   4.08367477e-01  5.53888889e-01]
 [ 5.27622276e-01  7.67500000e-01  7.85714286e-01  6.71277282e-01
   7.45793651e-01  3.61143616e-01  6.51666667e-01  6.85714286e-01
   6.34472540e-01  6.21363636e-01]]
DT mean:
[0.30895158 0.65052778 0.67952381 0.57709074 0.6306239  0.26123258
 0.63572222 0.66047619 0.55075562 0.61660017]
---------------------------
---------------------------
RF performance:
[[ 0.60267001  0.72083333  0.71428571  0.80122898  0.70174603  0.29098039
   0.64        0.77142857  0.27071068  0.61495921]
 [ 0.45934914  0.675       0.68571429  0.5124557   0.66996032  0.45801932
   0.72        0.74285714  0.69541152  0.71805556]
 [ 0.44039728  0.67916667  0.68571429  0.68232276  0.6731746   0.34148424
   0.68333333  0.67142857  0.63042016  0.66345238]
 [ 0.37682609  0.7125      0.71428571  0.66364875  0.70503968  0.05825448
   0.65        0.75714286  0.49167109  0.60209596]
 [ 0.40855072  0.685       0.71428571  0.71541387  0.68307359  0.38993088
   0.66666667  0.65714286  0.62124498  0.63488095]
 [ 0.46969342  0.75416667  0.74285714  0.74222853  0.73607143  0.58605929
   0.73333333  0.74285714  0.72591485  0.72507937]
 [ 0.23443658  0.57416667  0.6         0.44784249  0.52415945  0.05618618
   0.625       0.62857143  0.55925175  0.60257937]
 [ 0.79795396  0.93583333  0.94285714  0.92345271  0.93339105  0.40080918
   0.70833333  0.75714286  0.57153814  0.69170274]
 [ 0.24022546  0.62416667  0.68571429  0.58479271  0.60318903  0.05682953
   0.60583333  0.68571429  0.11543204  0.58795455]
 [ 0.11655204  0.60833333  0.62857143  0.50401112  0.5952381   0.09519864
   0.5125      0.54285714  0.51318629  0.475     ]
 [ 0.21263776  0.64083333  0.68571429  0.46219234  0.61712121  0.32446488
   0.74166667  0.75714286  0.68727564  0.72904762]
 [ 0.58566624  0.72583333  0.74285714  0.58467474  0.70242063  0.43999149
   0.65833333  0.68571429  0.70498848  0.65388889]
 [ 0.40841938  0.69        0.71428571  0.6967356   0.68035714  0.03985422
   0.5925      0.61428571  0.40485474  0.58989899]
 [-0.09543114  0.55        0.6         0.41450384  0.5105303   0.23385639
   0.6125      0.62857143  0.53287864  0.60511905]
 [ 0.07249616  0.52166667  0.6         0.31738204  0.48719697  0.28227616
   0.56416667  0.61428571  0.36557901  0.54305556]
 [ 0.22281023  0.495       0.52857143  0.42019073  0.47596681  0.29212702
   0.63583333  0.68571429  0.49096921  0.60050505]
 [ 0.68721615  0.84        0.84285714  0.76834792  0.83337302  0.2923711
   0.68916667  0.71428571  0.65727311  0.66431818]
 [ 0.24313725  0.625       0.84285714  0.34457196  0.60431235  0.34139898
   0.7025      0.71428571  0.62429999  0.68880952]
 [ 0.42026166  0.735       0.8         0.77266253  0.71328283  0.33793423
   0.69583333  0.71428571  0.56669903  0.6796176 ]
 [ 0.23578038  0.60583333  0.67142857  0.56216545  0.59388528  0.3114323
   0.66083333  0.7         0.41920469  0.65266955]
 [-0.09934046  0.52833333  0.74285714  0.14605935  0.51261072 -0.2962814
   0.47916667  0.48571429  0.35967355  0.47154762]
 [ 0.15834473  0.675       0.65714286  0.63232276  0.65170635  0.11033983
   0.65083333  0.7         0.43460652  0.64579004]
 [ 0.31311037  0.64666667  0.65714286  0.61332213  0.63454185  0.02039216
   0.65666667  0.77142857  0.37692342  0.62030303]
 [ 0.20697436  0.4875      0.48571429  0.43256985  0.48154762  0.50917317
   0.69583333  0.71428571  0.6993819   0.69269841]
 [ 0.32018085  0.73        0.72857143  0.70755616  0.71019841  0.52120624
   0.70833333  0.71428571  0.68639137  0.70039683]
 [ 0.36625253  0.6         0.6         0.60387706  0.57214286  0.25185416
   0.6125      0.61428571  0.67514361  0.59285714]
 [ 0.34571185  0.72666667  0.78571429  0.71735667  0.70023449  0.4528798
   0.73333333  0.72857143  0.76318629  0.71857143]
 [ 0.10765481  0.565       0.61428571  0.41476227  0.54545094  0.61416074
   0.67        0.68571429  0.68881097  0.66293651]
 [ 0.22935629  0.5025      0.54285714  0.45322677  0.47114358  0.13698079
   0.51666667  0.51428571  0.58811926  0.48075397]
 [ 0.70195396  0.84        0.84285714  0.85439282  0.83160534  0.16826133
   0.69833333  0.72857143  0.55792056  0.68104978]]
RF mean:
[0.32632827 0.65666667 0.69333333 0.58320902 0.63848907 0.27061419
 0.65066667 0.68142857 0.54929872 0.63298649]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.42857143  0.          0.3         0.
   0.5         0.75714286  0.          0.43012821]
 [ 0.          0.5         0.57142857  0.          0.36363636  0.
   0.5         0.61428571  0.          0.37954545]
 [ 0.          0.5         0.57142857  0.          0.36363636  0.
   0.5         0.52857143  0.          0.34454545]
 [ 0.13131313  0.5625      0.57142857  0.22320508  0.44742424  0.
   0.5         0.71428571  0.          0.41666667]
 [ 0.09518717  0.54166667  0.67142857  0.12844571  0.45590909  0.46145877
   0.72916667  0.72857143  0.69948005  0.7140873 ]
 [ 0.          0.5         0.51428571  0.          0.33818182  0.28988801
   0.6375      0.67142857  0.40721508  0.57381313]
 [ 0.          0.5         0.61428571  0.          0.37954545  0.
   0.5         0.42857143  0.          0.3       ]
 [ 0.          0.5         0.68571429  0.          0.40606061  0.
   0.5         0.64285714  0.          0.39015152]
 [ 0.          0.5         0.67142857  0.          0.40075758  0.
   0.5         0.7         0.          0.41136364]
 [-0.02962963  0.48333333  0.41428571  0.          0.29222222  0.
   0.5         0.55714286  0.          0.35727273]
 [ 0.          0.5         0.65714286  0.          0.39545455  0.
   0.5         0.57142857  0.          0.36363636]
 [ 0.          0.5         0.6         0.          0.37424242  0.
   0.5         0.6         0.          0.37424242]
 [ 0.          0.5         0.58571429  0.          0.36893939  0.
   0.5         0.6         0.          0.37424242]
 [ 0.          0.5         0.64285714  0.          0.39015152  0.
   0.5         0.54285714  0.          0.35090909]
 [ 0.          0.5         0.7         0.          0.41136364  0.
   0.5         0.65714286  0.          0.39545455]
 [ 0.          0.5         0.61428571  0.          0.37954545  0.
   0.5         0.67142857  0.          0.40075758]
 [ 0.          0.5         0.6         0.          0.37424242  0.
   0.5         0.62857143  0.          0.38484848]
 [ 0.          0.5         0.84285714  0.          0.45705128  0.
   0.5         0.58571429  0.          0.36893939]
 [ 0.          0.5         0.7         0.          0.41136364  0.
   0.5         0.52857143  0.          0.34454545]
 [ 0.          0.5         0.65714286  0.          0.39545455  0.
   0.5         0.62857143  0.          0.38484848]
 [ 0.          0.5         0.77142857  0.          0.43461538  0.
   0.5         0.54285714  0.          0.35090909]
 [ 0.          0.5         0.51428571  0.          0.33818182  0.
   0.5         0.62857143  0.          0.38484848]
 [ 0.          0.5         0.6         0.          0.37424242  0.
   0.5         0.75714286  0.          0.43012821]
 [ 0.          0.5         0.54285714  0.          0.35090909  0.
   0.5         0.57142857  0.          0.36363636]
 [ 0.          0.5         0.58571429  0.          0.36893939  0.15726496
   0.5875      0.52857143  0.32071068  0.4575    ]
 [ 0.          0.5         0.54285714  0.          0.35090909  0.
   0.5         0.54285714  0.          0.35090909]
 [ 0.          0.5         0.68571429  0.          0.40606061  0.39650427
   0.70833333  0.68571429  0.67675847  0.67892857]
 [ 0.          0.5         0.62857143  0.          0.38484848  0.
   0.5         0.58571429  0.          0.36893939]
 [ 0.          0.5         0.61428571  0.          0.37954545  0.
   0.5         0.54285714  0.          0.35090909]
 [ 0.          0.5         0.6         0.          0.37424242  0.
   0.5         0.64285714  0.          0.39015152]]
SVM mean:
[0.00656236 0.50291667 0.61333333 0.01172169 0.38225589 0.04350387
 0.52208333 0.61285714 0.07013881 0.4062286 ]
---------------------------
---------------------------
GBM performance:
[[ 0.46092385  0.76666667  0.75714286  0.67967468  0.74190476 -0.0872549
   0.49666667  0.72857143  0.07071068  0.45215618]
 [ 0.37881818  0.7         0.72857143  0.57193966  0.67318903  0.59604743
   0.79166667  0.81428571  0.73433366  0.78611111]
 [ 0.50704875  0.74166667  0.77142857  0.6989601   0.73944444  0.45024898
   0.73333333  0.71428571  0.70506538  0.70742063]
 [ 0.14930739  0.575       0.58571429  0.49247561  0.55059524 -0.01483376
   0.505       0.7         0.06324555  0.43469697]
 [ 0.56645354  0.72916667  0.78571429  0.72779169  0.7440404   0.39298051
   0.65833333  0.65714286  0.61462644  0.63369048]
 [ 0.36425641  0.7         0.68571429  0.66656496  0.68214286  0.47846843
   0.7375      0.74285714  0.71090253  0.73031746]
 [ 0.20277246  0.59416667  0.65714286  0.40151672  0.56242063  0.19961819
   0.59583333  0.58571429  0.53811926  0.56488095]
 [ 0.81560102  0.915       0.94285714  0.90157475  0.92080808  0.34367743
   0.6725      0.74285714  0.46796623  0.63171356]
 [ 0.30277377  0.62        0.71428571  0.41240194  0.59626263  0.11565644
   0.55583333  0.7         0.25943917  0.51265152]
 [ 0.25337945  0.64166667  0.65714286  0.5818603   0.62718254  0.12057312
   0.52916667  0.55714286  0.39846171  0.50662698]
 [ 0.27629117  0.62333333  0.68571429  0.51784983  0.5980303   0.51998814
   0.70833333  0.72857143  0.71656496  0.6972619 ]
 [ 0.3325691   0.67        0.7         0.59283158  0.65180014  0.41371542
   0.69916667  0.74285714  0.62583532  0.69277778]
 [ 0.22662917  0.6275      0.65714286  0.44809794  0.57381313  0.02768271
   0.545       0.6         0.33571236  0.51106061]
 [-0.07534101  0.465       0.55714286  0.20236034  0.42957071  0.20196585
   0.59583333  0.61428571  0.50721508  0.56878427]
 [-0.00836317  0.5         0.65714286  0.11801781  0.43575758  0.19684492
   0.56333333  0.68571429  0.32013696  0.50787879]
 [ 0.06272727  0.50333333  0.58571429  0.17871561  0.43434343  0.33122297
   0.63166667  0.7         0.47087064  0.59732323]
 [ 0.66095652  0.83416667  0.84285714  0.83423979  0.82753968  0.42198946
   0.705       0.75714286  0.62217067  0.69277778]
 [-0.01666667  0.56666667  0.85714286  0.1         0.54353147  0.40775403
   0.71916667  0.72857143  0.66887154  0.7018254 ]
 [ 0.10398977  0.525       0.67142857  0.25197404  0.47287879  0.2414437
   0.63333333  0.64285714  0.52120906  0.59414141]
 [ 0.04781818  0.55        0.62857143  0.37591739  0.5275974   0.30683934
   0.64583333  0.71428571  0.52163087  0.62870851]
 [-0.02352941  0.50666667  0.75714286  0.          0.46194639 -0.11268927
   0.4375      0.45714286  0.30236034  0.40888889]
 [ 0.42777146  0.70416667  0.7         0.6904796   0.69388889  0.16431256
   0.59416667  0.67142857  0.3385991   0.5384596 ]
 [ 0.28210416  0.67333333  0.71428571  0.42890521  0.63939394  0.01956522
   0.465       0.7         0.08944272  0.41136364]
 [ 0.10893554  0.57083333  0.57142857  0.52894409  0.55178571  0.49665481
   0.7375      0.74285714  0.69061366  0.72373016]
 [ 0.34551515  0.65833333  0.7         0.54061366  0.62988095  0.5088798
   0.75416667  0.75714286  0.71712084  0.74309524]
 [ 0.59465126  0.77916667  0.78571429  0.74882603  0.77055556  0.26270663
   0.61666667  0.61428571  0.57792575  0.59539683]
 [ 0.23578005  0.61833333  0.71428571  0.437907    0.58989899  0.41795784
   0.70833333  0.71428571  0.71393177  0.69980159]
 [ 0.14487096  0.56583333  0.64285714  0.27871561  0.53792929  0.43127924
   0.715       0.74285714  0.6462761   0.70295094]
 [ 0.16431256  0.58166667  0.65714286  0.33080604  0.55012626  0.0993986
   0.575       0.58571429  0.51875058  0.56039683]
 [ 0.50177612  0.755       0.77142857  0.6807402   0.72972222  0.31678137
   0.65083333  0.72857143  0.52800783  0.63656566]]
GBM mean:
[0.27980443 0.64205556 0.7047619  0.48069007 0.61626605 0.27564917
 0.63255556 0.68571429 0.49987056 0.60578183]
---------------------------
---------------------------
BDDAE performance:
[[ 0.05714286  0.52857143  0.52857143  0.48743594  0.5088669  -0.00184934
   0.49242424  0.67857143  0.26046154  0.48506061]
 [ 0.17464336  0.5875      0.59285714  0.58098292  0.5844313   0.43705304
   0.73444444  0.72142857  0.71203622  0.70576505]
 [ 0.04736505  0.525       0.54285714  0.45883354  0.50765423  0.2
   0.6         0.6         0.53703333  0.57734523]
 [-0.11428571  0.44285714  0.44285714  0.38092693  0.42135217  0.05261558
   0.5225      0.65        0.31817204  0.50080345]
 [ 0.48430871  0.75666667  0.75        0.73229283  0.73041096  0.38571429
   0.69285714  0.69285714  0.68327723  0.68876648]
 [ 0.44285714  0.72142857  0.72142857  0.6936165   0.71016707 -0.07554042
   0.4625      0.47142857  0.43320531  0.45058359]
 [-0.11176677  0.44444444  0.48571429  0.3445019   0.43269616 -0.1
   0.45        0.45        0.36864654  0.41879291]
 [ 0.22842381  0.6025      0.7         0.55024774  0.60884148  0.45216099
   0.72888889  0.74285714  0.71793266  0.72167235]
 [ 0.4716281   0.71666667  0.77857143  0.66821477  0.72359555  0.39629725
   0.6725      0.8         0.53170848  0.6784218 ]
 [ 0.07142857  0.53571429  0.53571429  0.47907695  0.51685125  0.18979214
   0.59583333  0.60714286  0.57106054  0.58722265]
 [-0.00552858  0.49222222  0.59285714  0.27315324  0.44906946 -0.05303117
   0.47083333  0.5         0.34390003  0.43707632]
 [ 0.15450416  0.57708333  0.59285714  0.54980939  0.56956408  0.37339919
   0.6875      0.69285714  0.67912152  0.68413533]
 [-0.14987836  0.425       0.42857143  0.39272531  0.41651481  0.30588548
   0.65625     0.65        0.63809801  0.64266484]
 [-0.07844943  0.46222222  0.51428571  0.35038717  0.44115119  0.27227667
   0.64166667  0.62857143  0.62177103  0.62285878]
 [-0.15175331  0.42        0.51428571  0.26940997  0.41248691  0.19757722
   0.60111111  0.63571429  0.54445671  0.59028808]
 [ 0.10028663  0.55777778  0.55714286  0.5340232   0.53517722  0.11337491
   0.54888889  0.67142857  0.21337617  0.48482966]
 [ 0.4545618   0.72708333  0.73571429  0.70362208  0.71788863  0.14599339
   0.56777778  0.65        0.3954167   0.54079898]
 [ 0.09689068  0.57083333  0.8         0.28808686  0.54039825  0.03059477
   0.51666667  0.52857143  0.44140137  0.49094705]
 [ 0.08287622  0.5425      0.61428571  0.44789705  0.53050323  0.11428571
   0.55714286  0.55714286  0.54100683  0.54971482]
 [ 0.06367686  0.52555556  0.57857143  0.47544628  0.52033812  0.17441949
   0.58333333  0.63571429  0.54275283  0.5802512 ]
 [-0.0656044   0.47575758  0.67142857  0.15352138  0.44084956  0.23371024
   0.61458333  0.62857143  0.59011165  0.60888898]
 [ 0.12857143  0.56428571  0.56428571  0.53854671  0.55247227  0.0493645
   0.52444444  0.6         0.35786236  0.49780982]
 [-0.29980754  0.35        0.36428571  0.22195781  0.32480646  0.39158621
   0.66212121  0.85        0.47588908  0.67647826]
 [-0.13613089  0.43333333  0.45        0.39627675  0.42304928  0.26207488
   0.62916667  0.64285714  0.61172829  0.6254104 ]
 [-0.41195815  0.29583333  0.31428571  0.21908824  0.28764095  0.05714286
   0.52857143  0.52857143  0.48252942  0.51426725]
 [ 0.11763688  0.55416667  0.57142857  0.52923245  0.5504782  -0.01566246
   0.49166667  0.5         0.45321593  0.47484432]
 [ 0.34944407  0.6575      0.75714286  0.6049779   0.66886787  0.41428571
   0.70714286  0.70714286  0.68628038  0.69770237]
 [-0.07676306  0.45888889  0.49285714  0.41097968  0.45316519  0.33972678
   0.6625      0.69285714  0.61879722  0.65703678]
 [ 0.15922258  0.57666667  0.62142857  0.50197047  0.56030335  0.34909611
   0.66875     0.69285714  0.63201657  0.66265608]
 [ 0.18064031  0.58541667  0.60714286  0.55053441  0.57813872  0.03131964
   0.51111111  0.57142857  0.4194534   0.49689987]]
BDDAE mean:
[0.07547277 0.53711586 0.58071429 0.45959254 0.52392436 0.19078879
 0.59277255 0.63261905 0.51409065 0.57833311]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.42857143 0.         0.3        0.
  0.5        0.75714286 0.         0.43012821]
 [0.         0.5        0.57142857 0.         0.36363636 0.
  0.5        0.61428571 0.         0.37954545]
 [0.         0.5        0.57142857 0.         0.36363636 0.
  0.5        0.52857143 0.         0.34454545]
 [0.         0.5        0.51428571 0.         0.33818182 0.
  0.5        0.71428571 0.         0.41666667]
 [0.         0.5        0.64285714 0.         0.39015152 0.
  0.5        0.51428571 0.         0.33818182]
 [0.         0.5        0.51428571 0.         0.33818182 0.
  0.5        0.55714286 0.         0.35727273]
 [0.         0.5        0.61428571 0.         0.37954545 0.
  0.5        0.42857143 0.         0.3       ]
 [0.         0.5        0.68571429 0.         0.40606061 0.
  0.5        0.64285714 0.         0.39015152]
 [0.         0.5        0.67142857 0.         0.40075758 0.
  0.5        0.7        0.         0.41136364]
 [0.         0.5        0.42857143 0.         0.3        0.
  0.5        0.55714286 0.         0.35727273]
 [0.         0.5        0.65714286 0.         0.39545455 0.
  0.5        0.57142857 0.         0.36363636]
 [0.         0.5        0.6        0.         0.37424242 0.
  0.5        0.6        0.         0.37424242]
 [0.         0.5        0.58571429 0.         0.36893939 0.
  0.5        0.6        0.         0.37424242]
 [0.         0.5        0.64285714 0.         0.39015152 0.
  0.5        0.54285714 0.         0.35090909]
 [0.         0.5        0.7        0.         0.41136364 0.
  0.5        0.65714286 0.         0.39545455]
 [0.         0.5        0.61428571 0.         0.37954545 0.
  0.5        0.67142857 0.         0.40075758]
 [0.         0.5        0.6        0.         0.37424242 0.
  0.5        0.62857143 0.         0.38484848]
 [0.         0.5        0.84285714 0.         0.45705128 0.
  0.5        0.58571429 0.         0.36893939]
 [0.         0.5        0.7        0.         0.41136364 0.
  0.5        0.52857143 0.         0.34454545]
 [0.         0.5        0.65714286 0.         0.39545455 0.
  0.5        0.62857143 0.         0.38484848]
 [0.         0.5        0.77142857 0.         0.43461538 0.
  0.5        0.54285714 0.         0.35090909]
 [0.         0.5        0.51428571 0.         0.33818182 0.
  0.5        0.62857143 0.         0.38484848]
 [0.         0.5        0.6        0.         0.37424242 0.
  0.5        0.75714286 0.         0.43012821]
 [0.         0.5        0.54285714 0.         0.35090909 0.
  0.5        0.57142857 0.         0.36363636]
 [0.         0.5        0.58571429 0.         0.36893939 0.
  0.5        0.42857143 0.         0.3       ]
 [0.         0.5        0.54285714 0.         0.35090909 0.
  0.5        0.54285714 0.         0.35090909]
 [0.         0.5        0.68571429 0.         0.40606061 0.
  0.5        0.42857143 0.         0.3       ]
 [0.         0.5        0.62857143 0.         0.38484848 0.
  0.5        0.58571429 0.         0.36893939]
 [0.         0.5        0.61428571 0.         0.37954545 0.
  0.5        0.54285714 0.         0.35090909]
 [0.         0.5        0.6        0.         0.37424242 0.
  0.5        0.64285714 0.         0.39015152]]
DUMMY mean:
[0.         0.5        0.61095238 0.         0.37668182 0.
 0.5        0.59       0.         0.36859946]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_41
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.316 0.657 0.698 0.559 0.639 0.297 0.649 0.686 0.557 0.631]
 [0.309 0.651 0.68  0.577 0.631 0.261 0.636 0.66  0.551 0.617]
 [0.326 0.657 0.693 0.583 0.638 0.271 0.651 0.681 0.549 0.633]
 [0.007 0.503 0.613 0.012 0.382 0.044 0.522 0.613 0.07  0.406]
 [0.28  0.642 0.705 0.481 0.616 0.276 0.633 0.686 0.5   0.606]
 [0.075 0.537 0.581 0.46  0.524 0.191 0.593 0.633 0.514 0.578]
 [0.    0.5   0.611 0.    0.377 0.    0.5   0.59  0.    0.369]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.22  0.11  0.099 0.183 0.116 0.213 0.107 0.094 0.179 0.112]
 [0.203 0.106 0.099 0.164 0.111 0.154 0.076 0.068 0.124 0.079]
 [0.211 0.106 0.1   0.173 0.111 0.195 0.066 0.072 0.15  0.07 ]
 [0.029 0.014 0.087 0.046 0.038 0.119 0.06  0.076 0.189 0.091]
 [0.216 0.105 0.085 0.224 0.118 0.189 0.091 0.074 0.197 0.104]
 [0.217 0.107 0.119 0.149 0.111 0.166 0.082 0.091 0.135 0.089]
 [0.    0.    0.087 0.    0.034 0.    0.    0.083 0.    0.033]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 70.  17.  14.  33.  18.  72.  16.  14.  32.  18.]
 [ 66.  16.  15.  28.  18.  59.  12.  10.  23.  13.]
 [ 65.  16.  14.  30.  17.  72.  10.  11.  27.  11.]
 [442.   3.  14. 392.  10. 274.  11.  12. 269.  22.]
 [ 77.  16.  12.  47.  19.  69.  14.  11.  39.  17.]
 [288.  20.  20.  32.  21.  87.  14.  14.  26.  15.]
 [  0.   0.  14.   0.   9.   0.   0.  14.   0.   9.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_41
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  16.992
step (sec):  16.992
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  70
Elapsed time: 516.8728491346042 minutes
Elapsed time: 8.614547485576736 hours
