2024-05-08 16:59:57.486770: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-08 17:00:01.197548: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-08 17:00:10.569744: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  17.0
step (sec):  12.75
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  4.25
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_42\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
0s[0m 8ms/step - loss: 0.0153 - mean_squared_error: 0.0153
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - loss: 0.0154 - mean_squared_error: 0.0154 - val_loss: 0.0128 - val_mean_squared_error: 0.0128
(16992, 1, 5)
Model: "sequential_1197"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 16992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1062, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 16992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 16992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1196     â”‚ (None, 1062, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1197     â”‚ (None, 1062, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_598     â”‚ (None, 1062, 12)  â”‚          0 â”‚ sequential_1196[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1197[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_598         â”‚ (None, 12, 1062)  â”‚          0 â”‚ concatenate_598[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_598         â”‚ (None, 12744)     â”‚          0 â”‚ permute_598[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_598         â”‚ (None, 12744)     â”‚          0 â”‚ flatten_598[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_598 (Dense)   â”‚ (None, 1)         â”‚     12,745 â”‚ dropout_598[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 13,567 (53.00 KB)
 Trainable params: 13,567 (53.00 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:27[0m 1s/step - binary_accuracy: 0.0000e+00 - loss: 1.0229
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5810 - loss: 0.7731     
[1m16/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6755 - loss: 0.7274
[1m24/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7047 - loss: 0.6960
[1m32/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7084 - loss: 0.6895
[1m40/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7067 - loss: 0.6840
[1m48/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7007 - loss: 0.6782
[1m55/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6972 - loss: 0.6736
[1m63/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6961 - loss: 0.6686
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 11ms/step - binary_accuracy: 0.6966 - loss: 0.6651 - val_binary_accuracy: 0.7500 - val_loss: 0.5025
Epoch 2/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2944
[1m 9/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6599 - loss: 0.5617 
[1m18/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.6265 - loss: 0.6068
[1m26/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6152 - loss: 0.6321
[1m35/67[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.6074 - loss: 0.6487
[1m43/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.6054 - loss: 0.6560
[1m51/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6144 - loss: 0.6540
[1m58/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6217 - loss: 0.6499
[1m66/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.6281 - loss: 0.6466
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.6293 - loss: 0.6461 - val_binary_accuracy: 0.7500 - val_loss: 0.5090
Epoch 3/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1768
[1m10/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8235 - loss: 0.3700 
[1m18/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8142 - loss: 0.4244
[1m26/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8115 - loss: 0.4444
[1m34/67[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8143 - loss: 0.4620
[1m43/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8168 - loss: 0.4749
[1m49/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8191 - loss: 0.4786
[1m57/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8134 - loss: 0.4893
[1m65/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8088 - loss: 0.4985
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.8076 - loss: 0.5013 - val_binary_accuracy: 0.5000 - val_loss: 0.6661
Epoch 4/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.5047
[1m10/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.9900 - loss: 0.4061 
[1m17/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9359 - loss: 0.4275
[1m25/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8993 - loss: 0.4323
[1m31/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8791 - loss: 0.4414
[1m39/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8570 - loss: 0.4595
[1m48/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8403 - loss: 0.4759
[1m56/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8325 - loss: 0.4835
[1m64/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8253 - loss: 0.4897
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.8223 - loss: 0.4922 - val_binary_accuracy: 0.7500 - val_loss: 0.5662
Epoch 5/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.5524
[1m 9/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9614 - loss: 0.5204 
[1m18/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8901 - loss: 0.5228
[1m26/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8522 - loss: 0.5343
[1m34/67[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8350 - loss: 0.5365
[1m41/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8271 - loss: 0.5347
[1m48/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8241 - loss: 0.5288
[1m57/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8192 - loss: 0.5283
[1m65/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8157 - loss: 0.5290
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.8145 - loss: 0.5294 - val_binary_accuracy: 0.7500 - val_loss: 0.5553
Epoch 6/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.1747
[1m 7/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.5211 - loss: 0.7589    
[1m16/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6714 - loss: 0.5899 
[1m24/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6802 - loss: 0.6027
[1m31/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6807 - loss: 0.6055
[1m38/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6870 - loss: 0.6023
[1m47/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6964 - loss: 0.5950
[1m55/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6993 - loss: 0.5923
[1m63/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7003 - loss: 0.5882
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.7011 - loss: 0.5853 - val_binary_accuracy: 0.7500 - val_loss: 0.4658
Epoch 7/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.5582
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.4704 - loss: 0.9887     
[1m17/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6082 - loss: 0.7860
[1m25/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6454 - loss: 0.7015
[1m33/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6779 - loss: 0.6424
[1m41/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7011 - loss: 0.6010
[1m49/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7198 - loss: 0.5715
[1m57/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7310 - loss: 0.5570
[1m65/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7371 - loss: 0.5480
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.7388 - loss: 0.5448 - val_binary_accuracy: 0.7500 - val_loss: 0.5178
Epoch 8/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.3062
[1m 9/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9317 - loss: 0.3618 
[1m17/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8422 - loss: 0.4329
[1m25/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8167 - loss: 0.4523
[1m32/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8042 - loss: 0.4670
[1m40/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7898 - loss: 0.4768
[1m48/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7846 - loss: 0.4795
[1m56/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7844 - loss: 0.4788
[1m64/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7847 - loss: 0.4768
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.7850 - loss: 0.4753 - val_binary_accuracy: 0.7500 - val_loss: 0.4496
Epoch 9/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0909
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5631 - loss: 0.7149 
[1m16/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6078 - loss: 0.7409
[1m25/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6619 - loss: 0.6913
[1m33/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6945 - loss: 0.6551
[1m42/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7203 - loss: 0.6261
[1m50/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7320 - loss: 0.6104
[1m58/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7427 - loss: 0.5945
[1m66/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7525 - loss: 0.5782
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.7550 - loss: 0.5744 - val_binary_accuracy: 0.7500 - val_loss: 0.4479
Epoch 10/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7064
[1m 9/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6857 - loss: 0.4298     
[1m17/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7977 - loss: 0.3326
[1m25/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8391 - loss: 0.3101
[1m33/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8464 - loss: 0.3299
[1m41/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8466 - loss: 0.3536
[1m49/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8464 - loss: 0.3654
[1m58/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8468 - loss: 0.3748
[1m66/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.8413 - loss: 0.3837
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.8399 - loss: 0.3855 - val_binary_accuracy: 0.7500 - val_loss: 0.4406

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.12008324 0.41014183 0.2743622  0.9114871  0.6261132  0.5977489
 0.8997841  0.15244852 0.91387737 0.16585417 0.49345678 0.6094149
 0.88230807 0.22156756 0.62130594 0.6293634  0.20142168 0.756855
 0.06839778]
predicted [0 0 0 1 1 1 1 0 1 0 0 1 1 0 1 1 0 1 0]
expected [False  True False  True  True  True False  True False  True  True  True
  True  True  True False False  True False]
accuracy: 0.5789473684210527
confusion matrix: 
[[4 3]
 [5 7]]
              precision    recall  f1-score   support

       False       0.44      0.57      0.50         7
        True       0.70      0.58      0.64        12

    accuracy                           0.58        19
   macro avg       0.57      0.58      0.57        19
weighted avg       0.61      0.58      0.59        19

macro avg f1-score: 0.5681818181818181
macro avg (UAR): 0.5773809523809523
Sensitivity:  0.5714285714285714
Specificity:  0.5833333333333334
g-mean:  0.5773502691896257
-------- Model Performance ----------: 
accuracy:  [0.73684211 0.68421053 0.73684211 0.73684211 0.63157895 0.36842105
 0.68421053 0.57894737 0.57894737 0.        ]
gmean:  [0.75592895 0.51176632 0.69006556 0.69006556 0.6172134  0.26726124
 0.70710678 0.53452248 0.57735027 0.        ]
f1_score:  [0.73389356 0.59285714 0.70769231 0.70769231 0.61449275 0.32142857
 0.68333333 0.54761905 0.56818182 0.        ]
UAR:  [0.76190476 0.60119048 0.70238095 0.70238095 0.61904762 0.32142857
 0.7202381  0.54761905 0.57738095 0.        ]
Cohen Kappa score:  [ 0.48087432  0.22972973  0.41717791  0.41717791  0.23121387 -0.35714286
  0.39361702  0.0952381   0.14606742  0.        ]
Split Repetition number:  9
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [38 84 46 53 74 73 14 64 48 49 12 93 13 22  1 18 56 31 78 28 92 35 52 33
  6 71 55 11 75 60 70  5 51 77 25  2 50 90 24 67 79 30 54 42 15 89 85 21
 83 88 65  0 34 27  8  4 26 39 10  7 23 72 47 37  3 82 66 80 17 32 16 36
 43 81 41] TEST: [45 59 91 86 62 57 40 29 19 76 68 20 87 44 61  9 63 69 58]
(DL) TRAIN number of instances:  75
(DL) TEST number of instances:  19
(DL) Total number of instances (TRAIN+TEST):  94
C:\Users\Javier\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\convolutional\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(
----- train_GSR_AE -------
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 16992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1062, 6)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2396              â”‚ (None, 4248, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2396           â”‚ (None, 4248, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2397              â”‚ (None, 16992, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2397           â”‚ (None, 16992, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:13[0m 1s/step - loss: 2.2788 - mean_squared_error: 2.2788
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 16.6750 - mean_squared_error: 16.6750
[1m15/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 18.0930 - mean_squared_error: 18.0930
[1m23/67[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 16.8790 - mean_squared_error: 16.8790
[1m30/67[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 16.0972 - mean_squared_error: 16.0972
[1m37/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 15.7774 - mean_squared_error: 15.7774
[1m45/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 15.4354 - mean_squared_error: 15.4354
[1m52/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 15.3830 - mean_squared_error: 15.3830
[1m60/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - loss: 15.2278 - mean_squared_error: 15.2278
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 15.0322 - mean_squared_error: 15.0322
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 12ms/step - loss: 15.0012 - mean_squared_error: 15.0012 - val_loss: 6.9192 - val_mean_squared_error: 6.9192
Epoch 2/5

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 4.9015 - mean_squared_error: 4.9015
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 8.5338 - mean_squared_error: 8.5338 
[1m16/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 12.4235 - mean_squared_error: 12.4235
[1m23/67[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 13.3011 - mean_squared_error: 13.3011
[1m31/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 13.3477 - mean_squared_error: 13.3477
[1m38/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 13.3635 - mean_squared_error: 13.3635
[1m46/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 13.1488 - mean_squared_error: 13.1488
[1m53/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 12.8622 - mean_squared_error: 12.8622
[1m61/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - loss: 12.5335 - mean_squared_error: 12.5335
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - loss: 12.3763 - mean_squared_error: 12.3763 - val_loss: 6.9127 - val_mean_squared_error: 6.9127
Epoch 3/5

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 7.9997 - mean_squared_error: 7.9997
[1m 9/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 20.6819 - mean_squared_error: 20.6819
[1m16/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 19.9874 - mean_squared_error: 19.9874
[1m24/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 18.2253 - mean_squared_error: 18.2253
[1m32/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 16.7781 - mean_squared_error: 16.7781
[1m39/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 16.0999 - mean_squared_error: 16.0999
[1m46/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 15.7302 - mean_squared_error: 15.7302
[1m54/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - loss: 15.2566 - mean_squared_error: 15.2566
[1m61/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - loss: 14.8933 - mean_squared_error: 14.8933
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - loss: 14.5594 - mean_squared_error: 14.5594 - val_loss: 6.9123 - val_mean_squared_error: 6.9123
Epoch 4/5

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 44ms/step - loss: 7.7367 - mean_squared_error: 7.7367
[1m 7/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 10.6807 - mean_squared_error: 10.6807
[1m15/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 10.7014 - mean_squared_error: 10.7014
[1m22/67[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 11.7563 - mean_squared_error: 11.7563
[1m29/67[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 11.6873 - mean_squared_error: 11.6873
[1m36/67[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 11.5347 - mean_squared_error: 11.5347
[1m44/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 11.4989 - mean_squared_error: 11.4989
[1m52/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 11.4837 - mean_squared_error: 11.4837
[1m59/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - loss: 11.4077 - mean_squared_error: 11.4077
[1m66/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - loss: 11.3465 - mean_squared_error: 11.3465
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - loss: 11.3497 - mean_squared_error: 11.3497 - val_loss: 6.9126 - val_mean_squared_error: 6.9126
Epoch 5/5

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 30ms/step - loss: 1.7032 - mean_squared_error: 1.7032
[1m 9/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 22.5244 - mean_squared_error: 22.5244
[1m16/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 21.7544 - mean_squared_error: 21.7544
[1m24/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 19.6338 - mean_squared_error: 19.6338
[1m31/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 17.9238 - mean_squared_error: 17.9238
[1m39/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 16.5465 - mean_squared_error: 16.5465
[1m46/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 15.7333 - mean_squared_error: 15.7333
[1m54/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - loss: 15.2478 - mean_squared_error: 15.2478
[1m62/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - loss: 14.8175 - mean_squared_error: 14.8175
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - loss: 14.5302 - mean_squared_error: 14.5302 - val_loss: 6.9125 - val_mean_squared_error: 6.9125
(75, 1062, 6)
Model: "sequential_1198"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 16992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1062, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 16992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1062, 6)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2398              â”‚ (None, 4248, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2398           â”‚ (None, 4248, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2399              â”‚ (None, 16992, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2399           â”‚ (None, 16992, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:12[0m 1s/step - loss: 0.2704 - mean_squared_error: 0.2704
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.1327 - mean_squared_error: 0.1327 
[1m15/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.1046 - mean_squared_error: 0.1046
[1m22/67[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0907 - mean_squared_error: 0.0907
[1m30/67[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0792 - mean_squared_error: 0.0792
[1m38/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0709 - mean_squared_error: 0.0709
[1m45/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0659 - mean_squared_error: 0.0659
[1m53/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0616 - mean_squared_error: 0.0616
[1m60/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0585 - mean_squared_error: 0.0585
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 12ms/step - loss: 0.0556 - mean_squared_error: 0.0556 - val_loss: 0.0292 - val_mean_squared_error: 0.0292
Epoch 2/5

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - loss: 0.0131 - mean_squared_error: 0.0131
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0297 - mean_squared_error: 0.0297 
[1m15/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0273 - mean_squared_error: 0.0273
[1m23/67[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0245 - mean_squared_error: 0.0245
[1m30/67[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0232 - mean_squared_error: 0.0232
[1m38/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0221 - mean_squared_error: 0.0221
[1m45/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0217 - mean_squared_error: 0.0217
[1m53/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0213 - mean_squared_error: 0.0213
[1m60/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0208 - mean_squared_error: 0.0208
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 0.0203 - mean_squared_error: 0.0203
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - loss: 0.0202 - mean_squared_error: 0.0202 - val_loss: 0.0251 - val_mean_squared_error: 0.0251
Epoch 3/5

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.0127 - mean_squared_error: 0.0127
[1m 7/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - loss: 0.0253 - mean_squared_error: 0.0253
[1m15/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0247 - mean_squared_error: 0.0247 
[1m22/67[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0231 - mean_squared_error: 0.0231
[1m30/67[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0212 - mean_squared_error: 0.0212
[1m37/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0199 - mean_squared_error: 0.0199
[1m45/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0191 - mean_squared_error: 0.0191
[1m52/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m60/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0182 - mean_squared_error: 0.0182
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 9ms/step - loss: 0.0178 - mean_squared_error: 0.0178
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - loss: 0.0178 - mean_squared_error: 0.0178 - val_loss: 0.0241 - val_mean_squared_error: 0.0241
Epoch 4/5

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - loss: 0.0134 - mean_squared_error: 0.0134
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0094 - mean_squared_error: 0.0094 
[1m15/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0103 - mean_squared_error: 0.0103
[1m23/67[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0106 - mean_squared_error: 0.0106
[1m30/67[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0110 - mean_squared_error: 0.0110
[1m37/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0113 - mean_squared_error: 0.0113
[1m45/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0119 - mean_squared_error: 0.0119
[1m52/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0122 - mean_squared_error: 0.0122
[1m58/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 9ms/step - loss: 0.0125 - mean_squared_error: 0.0125
[1m65/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 9ms/step - loss: 0.0127 - mean_squared_error: 0.0127
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - loss: 0.0127 - mean_squared_error: 0.0127 - val_loss: 0.0231 - val_mean_squared_error: 0.0231
Epoch 5/5

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 40ms/step - loss: 0.0042 - mean_squared_error: 0.0042
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0075 - mean_squared_error: 0.0075 
[1m15/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0081 - mean_squared_error: 0.0081
[1m23/67[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0096 - mean_squared_error: 0.0096
[1m30/67[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0104 - mean_squared_error: 0.0104
[1m37/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0109 - mean_squared_error: 0.0109
[1m43/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0112 - mean_squared_error: 0.0112
[1m51/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0115 - mean_squared_error: 0.0115
[1m58/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - loss: 0.0118 - mean_squared_error: 0.0118
[1m66/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - loss: 0.0120 - mean_squared_error: 0.0120
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - loss: 0.0121 - mean_squared_error: 0.0121 - val_loss: 0.0227 - val_mean_squared_error: 0.0227
(16992, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 16992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1062, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 16992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 16992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 1062, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 1062, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 1062, 12)  â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 1062)  â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 12744)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 12744)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     12,745 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 13,567 (53.00 KB)
 Trainable params: 13,567 (53.00 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:42[0m 2s/step - binary_accuracy: 0.0000e+00 - loss: 0.7907
[1m 9/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.3595 - loss: 0.7278     
[1m17/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.4864 - loss: 0.6953
[1m24/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5373 - loss: 0.6787
[1m33/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5665 - loss: 0.6689
[1m41/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5821 - loss: 0.6648
[1m49/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5960 - loss: 0.6629
[1m57/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6104 - loss: 0.6579
[1m66/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6251 - loss: 0.6519
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 11ms/step - binary_accuracy: 0.6282 - loss: 0.6503 - val_binary_accuracy: 0.6250 - val_loss: 0.8202
Epoch 2/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.8109
[1m 9/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.4825 - loss: 0.9386     
[1m17/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5768 - loss: 0.8200
[1m25/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.5965 - loss: 0.7829
[1m33/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6199 - loss: 0.7515
[1m41/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6350 - loss: 0.7295
[1m50/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6423 - loss: 0.7138
[1m58/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6495 - loss: 0.7012
[1m66/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6542 - loss: 0.6928
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.6552 - loss: 0.6911 - val_binary_accuracy: 0.7500 - val_loss: 0.6244
Epoch 3/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7141
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.4746 - loss: 0.5493     
[1m16/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 7ms/step - binary_accuracy: 0.5622 - loss: 0.5491
[1m24/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6008 - loss: 0.5399
[1m32/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6181 - loss: 0.5421
[1m40/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6307 - loss: 0.5486
[1m47/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6394 - loss: 0.5534
[1m54/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6410 - loss: 0.5612
[1m62/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6443 - loss: 0.5652
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.6487 - loss: 0.5649 - val_binary_accuracy: 0.7500 - val_loss: 0.7103
Epoch 4/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 24ms/step - binary_accuracy: 1.0000 - loss: 0.5310
[1m 9/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9172 - loss: 0.4298 
[1m16/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8701 - loss: 0.4721
[1m23/67[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8358 - loss: 0.5108
[1m31/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8089 - loss: 0.5277
[1m38/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7870 - loss: 0.5405
[1m46/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7748 - loss: 0.5491
[1m54/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7708 - loss: 0.5530
[1m62/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7678 - loss: 0.5542
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.7671 - loss: 0.5532 - val_binary_accuracy: 0.7500 - val_loss: 0.6667
Epoch 5/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.1013
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8560 - loss: 0.2399 
[1m16/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8158 - loss: 0.2830
[1m24/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7931 - loss: 0.3272
[1m31/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7787 - loss: 0.3616
[1m38/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7736 - loss: 0.3801
[1m45/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7733 - loss: 0.3916
[1m52/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7748 - loss: 0.4007
[1m60/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7739 - loss: 0.4129
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 9ms/step - binary_accuracy: 0.7744 - loss: 0.4217 - val_binary_accuracy: 0.6250 - val_loss: 0.6094
Epoch 6/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.2362
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 1.0000 - loss: 0.2780 
[1m17/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9641 - loss: 0.3688
[1m25/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9605 - loss: 0.3741
[1m33/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9498 - loss: 0.3790
[1m41/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9356 - loss: 0.3959
[1m49/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9251 - loss: 0.4074
[1m57/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9163 - loss: 0.4148
[1m65/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.9096 - loss: 0.4201
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.9065 - loss: 0.4236 - val_binary_accuracy: 0.6250 - val_loss: 0.6083
Epoch 7/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.8499
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.5903 - loss: 0.4597     
[1m17/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6115 - loss: 0.4913
[1m25/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6305 - loss: 0.4962
[1m32/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6507 - loss: 0.4858
[1m40/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6718 - loss: 0.4699
[1m47/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.6901 - loss: 0.4588
[1m55/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7063 - loss: 0.4516
[1m63/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.7148 - loss: 0.4556
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.7201 - loss: 0.4579 - val_binary_accuracy: 0.6250 - val_loss: 0.5915
Epoch 8/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2434
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9207 - loss: 0.3972 
[1m16/67[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.8964 - loss: 0.4096
[1m24/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8980 - loss: 0.3873
[1m32/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8988 - loss: 0.3679
[1m40/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8827 - loss: 0.3799
[1m48/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8664 - loss: 0.3948
[1m55/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8523 - loss: 0.4092
[1m63/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8382 - loss: 0.4225
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.8311 - loss: 0.4302 - val_binary_accuracy: 0.6250 - val_loss: 0.6122
Epoch 9/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2449
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.9665 - loss: 0.2319 
[1m17/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8712 - loss: 0.3337
[1m25/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8522 - loss: 0.3622
[1m34/67[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8531 - loss: 0.3767
[1m42/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8508 - loss: 0.3929
[1m51/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8483 - loss: 0.4068
[1m59/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8493 - loss: 0.4112
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8492 - loss: 0.4150
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.8492 - loss: 0.4154 - val_binary_accuracy: 0.6250 - val_loss: 0.6222
Epoch 10/10

[1m 1/67[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.3337
[1m 8/67[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 9ms/step - binary_accuracy: 0.7853 - loss: 0.3502 
[1m17/67[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8412 - loss: 0.3270
[1m24/67[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8595 - loss: 0.3106
[1m33/67[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8641 - loss: 0.3093
[1m42/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8626 - loss: 0.3192
[1m49/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8543 - loss: 0.3355
[1m58/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8475 - loss: 0.3503
[1m66/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 8ms/step - binary_accuracy: 0.8446 - loss: 0.3585
[1m67/67[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 8ms/step - binary_accuracy: 0.8439 - loss: 0.3604 - val_binary_accuracy: 0.6250 - val_loss: 0.6959

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.62562466 0.22331476 0.011299   0.17139308 0.86193985 0.79566056
 0.75598866 0.8019848  0.9322488  0.9177186  0.9423645  0.94730973
 0.14266513 0.17788465 0.980122   0.74376786 0.7581693  0.8927496
 0.8166213 ]
predicted [1 0 0 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1]
expected [ True  True False False  True False  True  True False  True  True False
 False  True  True False  True  True  True]
accuracy: 0.6842105263157895
confusion matrix: 
[[ 3  4]
 [ 2 10]]
              precision    recall  f1-score   support

       False       0.60      0.43      0.50         7
        True       0.71      0.83      0.77        12

    accuracy                           0.68        19
   macro avg       0.66      0.63      0.63        19
weighted avg       0.67      0.68      0.67        19

macro avg f1-score: 0.6346153846153846
macro avg (UAR): 0.6309523809523809
Sensitivity:  0.42857142857142855
Specificity:  0.8333333333333334
g-mean:  0.5976143046671968
-------- Model Performance ----------: 
accuracy:  [0.73684211 0.68421053 0.73684211 0.73684211 0.63157895 0.36842105
 0.68421053 0.57894737 0.57894737 0.68421053]
gmean:  [0.75592895 0.51176632 0.69006556 0.69006556 0.6172134  0.26726124
 0.70710678 0.53452248 0.57735027 0.5976143 ]
f1_score:  [0.73389356 0.59285714 0.70769231 0.70769231 0.61449275 0.32142857
 0.68333333 0.54761905 0.56818182 0.63461538]
UAR:  [0.76190476 0.60119048 0.70238095 0.70238095 0.61904762 0.32142857
 0.7202381  0.54761905 0.57738095 0.63095238]
Cohen Kappa score:  [ 0.48087432  0.22972973  0.41717791  0.41717791  0.23121387 -0.35714286
  0.39361702  0.0952381   0.14606742  0.27848101]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  16.992
step (sec):  12.752
overlap:  True
perc. of overlap:  24.95291902071563
overlap duration (sec):  4.24
Number of windows / instances:  94
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.726 0.869 0.871 0.854 0.859 0.491 0.743 0.763 0.734 0.742]
 [0.61  0.832 0.85  0.791 0.836 0.339 0.664 0.687 0.613 0.656]
 [0.626 0.822 0.829 0.822 0.81  0.341 0.714 0.741 0.539 0.714]
 [0.193 0.583 0.67  0.315 0.523 0.    0.5   0.65  0.    0.394]
 [0.725 0.858 0.872 0.839 0.856 0.42  0.689 0.743 0.629 0.688]
 [0.233 0.618 0.642 0.595 0.611 0.076 0.538 0.589 0.463 0.528]
 [0.    0.5   0.607 0.    0.377 0.    0.5   0.65  0.    0.394]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.46596087  0.735       0.73444444  0.71711329  0.72585692  0.20609708
   0.59583333  0.73888889  0.40918223  0.58547619]
 [ 0.38817828  0.69666667  0.69888889  0.66564843  0.68213398  0.20520988
   0.59416667  0.65        0.53781676  0.58815851]
 [ 0.3539254   0.6725      0.69333333  0.64082549  0.66524753  0.53434066
   0.77        0.76555556  0.75513282  0.76026335]
 [ 0.37849178  0.69        0.70111111  0.61923681  0.66968531  0.18914526
   0.59880952  0.66666667  0.49412989  0.57358059]
 [ 0.58735178  0.78333333  0.81888889  0.76246773  0.78989011  0.42974629
   0.715       0.71666667  0.68672722  0.70240981]
 [ 0.38924064  0.695       0.69888889  0.67930664  0.68916306  0.56427685
   0.78166667  0.78777778  0.76530049  0.77671717]
 [ 0.2589578   0.6325      0.65111111  0.59757236  0.62531219  0.17792732
   0.59        0.59555556  0.5669843   0.5806746 ]
 [ 0.84616086  0.91904762  0.93888889  0.9106948   0.92234432  0.60941052
   0.8075      0.81777778  0.79314422  0.80139694]
 [ 0.26074052  0.62261905  0.73444444  0.46268531  0.61260531  0.15281171
   0.58333333  0.66888889  0.46146609  0.5623489 ]
 [ 0.38323662  0.695       0.69        0.67069419  0.68032301  0.20653683
   0.60416667  0.61        0.58684212  0.59685981]
 [ 0.17140844  0.58333333  0.62666667  0.48444004  0.5662142   0.35044791
   0.67166667  0.71222222  0.55659398  0.64855977]
 [ 0.64840985  0.82083333  0.83888889  0.81048001  0.82182373  0.37141907
   0.68333333  0.72444444  0.57795425  0.66722527]
 [ 0.42903561  0.71583333  0.72333333  0.69007708  0.70608863  0.10729028
   0.55416667  0.56444444  0.52779298  0.54419553]
 [ 0.28385512  0.6452381   0.68777778  0.58556544  0.6334707   0.45435766
   0.7275      0.73111111  0.70554968  0.71898102]
 [ 0.02323303  0.5047619   0.60666667  0.32849904  0.49104396  0.1074812
   0.55238095  0.59333333  0.49685446  0.54286436]
 [ 0.28148632  0.64666667  0.65888889  0.61696001  0.63332334  0.54464608
   0.7702381   0.80777778  0.71891866  0.76697136]
 [ 0.32241729  0.66333333  0.66888889  0.65153309  0.65621406  0.57291044
   0.78583333  0.79444444  0.76314618  0.7774395 ]
 [ 0.05712736  0.5625      0.81111111  0.17259838  0.49793866  0.4999874
   0.74916667  0.75888889  0.73372226  0.74550144]
 [ 0.36011021  0.67261905  0.73555556  0.63600998  0.67484557  0.40568036
   0.70333333  0.71        0.68775371  0.69873377]
 [ 0.37459439  0.68630952  0.74666667  0.56655189  0.66258159  0.27527851
   0.63571429  0.66555556  0.60136734  0.62542763]
 [-0.05427153  0.48452381  0.67111111  0.16911538  0.45326896  0.17630967
   0.5875      0.58222222  0.5372005   0.56636724]
 [ 0.21161159  0.605       0.60666667  0.5743367   0.59119963  0.34861356
   0.675       0.71222222  0.60539816  0.66249084]
 [ 0.03540853  0.51666667  0.55555556  0.38930809  0.48623848  0.40339085
   0.67738095  0.82222222  0.50604311  0.67942577]
 [ 0.21376173  0.6075      0.60333333  0.59153062  0.597114    0.48394622
   0.74166667  0.74777778  0.72675521  0.73676601]
 [ 0.54681051  0.77416667  0.77777778  0.76779615  0.77170996  0.63840081
   0.815       0.82222222  0.80416238  0.81619769]
 [ 0.37969636  0.6875      0.7         0.65685713  0.67946609  0.26159731
   0.63083333  0.63444444  0.56656594  0.60715007]
 [ 0.35940761  0.68214286  0.75555556  0.60260519  0.66763251  0.5771978
   0.79        0.78777778  0.77713851  0.78296537]
 [ 0.39697804  0.7025      0.72222222  0.6841968   0.69466228  0.53857406
   0.77083333  0.78555556  0.711629    0.7551973 ]
 [ 0.04567266  0.52333333  0.55444444  0.45412712  0.50851732  0.28828737
   0.64166667  0.65222222  0.59879716  0.62531052]
 [ 0.72580437  0.86916667  0.87111111  0.85404697  0.85876041  0.49131941
   0.74285714  0.76333333  0.73391144  0.74167527]]
KNN mean:
[0.3374934  0.66985317 0.70940741 0.60042934 0.65715586 0.37242128
 0.68488492 0.713      0.6331327  0.67457772]
---------------------------
---------------------------
DT performance:
[[0.55249799 0.83       0.82777778 0.77021091 0.82580808 0.30174448
  0.63392857 0.72666667 0.48022943 0.61752289]
 [0.37223613 0.67416667 0.68333333 0.68360063 0.66387279 0.27175214
  0.63583333 0.65888889 0.59131809 0.61709429]
 [0.19671759 0.65583333 0.66       0.64751047 0.65029582 0.36477444
  0.7025     0.70555556 0.63148342 0.6994733 ]
 [0.28586755 0.6025     0.60777778 0.54623204 0.59023282 0.13783756
  0.56904762 0.63111111 0.46457681 0.55639222]
 [0.61311768 0.7875     0.79666667 0.80456833 0.77792874 0.45674927
  0.6975     0.69222222 0.69339097 0.68724026]
 [0.14429275 0.615      0.61888889 0.52242382 0.59702936 0.37353855
  0.71666667 0.72333333 0.68933812 0.71237568]
 [0.35182898 0.69666667 0.72444444 0.69927981 0.68537296 0.20883459
  0.5975     0.59555556 0.51880758 0.58124542]
 [0.84890269 0.91190476 0.92777778 0.91849194 0.90587912 0.43446276
  0.72833333 0.76444444 0.69107838 0.71656233]
 [0.44827246 0.66904762 0.72333333 0.61502277 0.65048243 0.33257647
  0.66071429 0.70222222 0.59983446 0.63139943]
 [0.30983114 0.655      0.65666667 0.60041156 0.65062771 0.1737819
  0.5975     0.60666667 0.51749059 0.57713287]
 [0.10140924 0.56190476 0.55555556 0.54730173 0.53173909 0.34974916
  0.69916667 0.72333333 0.6351815  0.68891442]
 [0.65501412 0.84583333 0.87111111 0.81407272 0.85240815 0.33283485
  0.6775     0.68333333 0.67797476 0.6588703 ]
 [0.36836243 0.63083333 0.63888889 0.70717068 0.61329365 0.01907353
  0.5225     0.53444444 0.49389705 0.5074334 ]
 [0.08306838 0.51666667 0.55111111 0.46739316 0.50197885 0.38134414
  0.6625     0.65777778 0.67060756 0.64651238]
 [0.12508001 0.5702381  0.63888889 0.42062675 0.55779221 0.15924232
  0.56071429 0.60555556 0.52894134 0.55420122]
 [0.24285685 0.6        0.59666667 0.6360999  0.58734488 0.41162329
  0.73928571 0.75666667 0.7003372  0.7292366 ]
 [0.38988303 0.74083333 0.74111111 0.72383249 0.73263653 0.55237517
  0.74666667 0.74444444 0.75205696 0.73277778]
 [0.1716388  0.575      0.79888889 0.19354143 0.55702148 0.41565768
  0.695      0.70555556 0.70618294 0.68767316]
 [0.41740419 0.68809524 0.75666667 0.62449379 0.68801116 0.41708255
  0.7025     0.7        0.67478833 0.69489177]
 [0.29583637 0.63154762 0.66222222 0.62136081 0.61658342 0.01864947
  0.50357143 0.54333333 0.48966958 0.49470779]
 [0.07188717 0.4952381  0.60888889 0.32014994 0.48234432 0.04353565
  0.5275     0.51777778 0.4924636  0.50569264]
 [0.32831638 0.665      0.67       0.6468529  0.6594031  0.27056248
  0.65416667 0.67111111 0.63586216 0.64143828]
 [0.17495214 0.5575     0.57333333 0.53334346 0.55238095 0.224817
  0.66785714 0.78777778 0.54691375 0.63009463]
 [0.27724373 0.6325     0.62666667 0.62630086 0.61835859 0.42604366
  0.68166667 0.68222222 0.67984693 0.66961233]
 [0.31918003 0.68916667 0.70333333 0.63403124 0.68139971 0.4739011
  0.7475     0.74888889 0.73472727 0.74176407]
 [0.31400831 0.67       0.67       0.67524807 0.6629798  0.33553052
  0.5675     0.57       0.51516444 0.536887  ]
 [0.34050442 0.70833333 0.74333333 0.61191855 0.67490025 0.43859689
  0.7425     0.74333333 0.6881804  0.73436147]
 [0.15728011 0.61       0.61555556 0.55857174 0.57632035 0.48166882
  0.72666667 0.73555556 0.73666982 0.71881452]
 [0.20058093 0.62583333 0.62777778 0.61969509 0.60312965 0.3216494
  0.6475     0.65       0.64752651 0.6353663 ]
 [0.61012902 0.83166667 0.85       0.79128997 0.83604035 0.33860379
  0.66369048 0.68666667 0.61277108 0.65618049]]
DT mean:
[0.32560669 0.66479365 0.69088889 0.61936825 0.65278654 0.31561979
 0.65584921 0.67514815 0.61657704 0.64206231]
---------------------------
---------------------------
RF performance:
[[0.60804878 0.84       0.83888889 0.77012168 0.83573232 0.34393715
  0.56517857 0.70333333 0.68021166 0.56678571]
 [0.54181449 0.72583333 0.73222222 0.68351288 0.72224636 0.30195044
  0.6775     0.71333333 0.57498904 0.65944833]
 [0.27868476 0.69333333 0.70555556 0.67755165 0.67702298 0.42644809
  0.6925     0.7        0.72845999 0.67971612]
 [0.30408117 0.6875     0.69111111 0.70296233 0.68065296 0.05035016
  0.48690476 0.65111111 0.45985741 0.45070944]
 [0.71269383 0.8        0.81777778 0.71022927 0.79362693 0.53287548
  0.7675     0.76555556 0.75187798 0.76378788]
 [0.51216238 0.725      0.72666667 0.68005522 0.7180636  0.64692333
  0.77166667 0.78111111 0.75287637 0.76856088]
 [0.2435829  0.63083333 0.64777778 0.69331734 0.61373959 0.10659309
  0.6        0.60111111 0.49422704 0.58828644]
 [0.86606516 0.93571429 0.94777778 0.9245487  0.93305861 0.53042571
  0.72       0.75333333 0.71160505 0.72459624]
 [0.30893504 0.80595238 0.82888889 0.57424864 0.79630037 0.18648057
  0.64285714 0.7        0.57802624 0.61815949]
 [0.24020906 0.645      0.64333333 0.53665068 0.6349531  0.38209573
  0.64166667 0.65444444 0.59199071 0.62418221]
 [0.28160629 0.58988095 0.63777778 0.61107194 0.58906538 0.65932054
  0.7275     0.75555556 0.70316751 0.72463203]
 [0.65017808 0.76083333 0.77555556 0.77000292 0.75660312 0.49044687
  0.73666667 0.74888889 0.71288792 0.72555139]
 [0.29068665 0.73083333 0.74222222 0.59821925 0.72841464 0.12942302
  0.57416667 0.59666667 0.44862765 0.56543124]
 [0.04245564 0.53690476 0.59111111 0.43099674 0.50662546 0.41796703
  0.7625     0.76222222 0.62636754 0.75083888]
 [0.18807792 0.63214286 0.69222222 0.51211572 0.60737277 0.07220356
  0.58035714 0.64888889 0.46523775 0.56878803]
 [0.3618714  0.66       0.68       0.6279147  0.64617105 0.47602187
  0.75595238 0.78888889 0.64482516 0.74981144]
 [0.59804524 0.79       0.79555556 0.79899256 0.7829576  0.48926459
  0.76333333 0.76666667 0.69766299 0.75688867]
 [0.20549693 0.74375    0.86444444 0.35779355 0.6769993  0.39620812
  0.75166667 0.75666667 0.72072438 0.74011017]
 [0.39885827 0.66428571 0.72444444 0.60495393 0.65468074 0.33693836
  0.7925     0.78555556 0.72453451 0.78188672]
 [0.34057767 0.68630952 0.73444444 0.60581289 0.66356144 0.14684547
  0.61369048 0.67222222 0.45489147 0.60474359]
 [0.15716465 0.56190476 0.72333333 0.16546537 0.5376572  0.18966098
  0.5575     0.55       0.46761167 0.52656954]
 [0.361974   0.6775     0.67888889 0.65488751 0.67004412 0.27618715
  0.67083333 0.71222222 0.63313366 0.65950438]
 [0.16907674 0.57416667 0.59555556 0.46993378 0.53676324 0.30365241
  0.65238095 0.76777778 0.3201004  0.63939857]
 [0.36260788 0.67       0.67222222 0.6071493  0.66442724 0.55089851
  0.75583333 0.75777778 0.77750981 0.75001998]
 [0.34374699 0.75416667 0.76444444 0.69550261 0.75118964 0.45961538
  0.7625     0.76888889 0.75269926 0.75851204]
 [0.23627754 0.7        0.70222222 0.61124348 0.69159452 0.46022789
  0.68083333 0.68666667 0.7165663  0.67448773]
 [0.48551521 0.70238095 0.73333333 0.56608485 0.66994603 0.38934066
  0.69       0.68777778 0.75337703 0.68358586]
 [0.19713938 0.5775     0.59333333 0.48156266 0.55538573 0.42987358
  0.77916667 0.78777778 0.73086656 0.77437951]
 [0.11194911 0.60166667 0.62888889 0.54167683 0.58346764 0.34197192
  0.70416667 0.70222222 0.65620685 0.68794456]
 [0.62552679 0.82166667 0.82888889 0.8219325  0.80977051 0.34127119
  0.71369048 0.74111111 0.53899192 0.71386447]]
RF mean:
[0.36750366 0.69750198 0.72462963 0.61621705 0.68293647 0.36218063
 0.68636706 0.71559259 0.62900373 0.67603938]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.52222222  0.          0.34267399  0.
   0.5         0.75666667  0.          0.43047386]
 [ 0.          0.5         0.56333333  0.          0.36011905  0.
   0.5         0.62888889  0.          0.38571429]
 [ 0.23655576  0.60833333  0.67111111  0.35236034  0.54450549  0.37945209
   0.69        0.69111111  0.65225536  0.67249556]
 [ 0.09783784  0.5475      0.57444444  0.3098141   0.47639194  0.
   0.5         0.72444444  0.          0.41970588]
 [ 0.46215697  0.70833333  0.78888889  0.59702157  0.70318681  0.52166937
   0.76        0.76222222  0.72806709  0.74712315]
 [-0.02162162  0.49        0.52222222  0.          0.34267399  0.54634953
   0.76916667  0.77888889  0.75717835  0.76975108]
 [ 0.          0.5         0.60666667  0.          0.37714286  0.08129555
   0.54        0.54222222  0.30065519  0.45522811]
 [ 0.31625668  0.63333333  0.77666667  0.42805982  0.62090175  0.02702703
   0.5125      0.62888889  0.05        0.40417582]
 [ 0.          0.5         0.70222222  0.          0.41220588  0.
   0.5         0.70222222  0.          0.41220588]
 [ 0.15148514  0.575       0.57333333  0.41194509  0.50969086  0.
   0.5         0.56333333  0.          0.36011905]
 [ 0.          0.5         0.65        0.          0.39367647  0.
   0.5         0.60666667  0.          0.37714286]
 [ 0.          0.5         0.62888889  0.          0.38571429  0.
   0.5         0.60666667  0.          0.37714286]
 [ 0.          0.5         0.57333333  0.          0.36428571  0.
   0.5         0.57333333  0.          0.36428571]
 [ 0.          0.5         0.67        0.          0.40102941  0.
   0.5         0.53333333  0.          0.34761905]
 [ 0.          0.5         0.69111111  0.          0.40845588  0.
   0.5         0.66        0.          0.39735294]
 [ 0.          0.5         0.59555556  0.          0.37285714  0.
   0.5         0.67        0.          0.40102941]
 [ 0.          0.5         0.58444444  0.          0.36857143  0.
   0.5         0.59555556  0.          0.37285714]
 [ 0.          0.5         0.85333333  0.          0.46013072  0.
   0.5         0.58444444  0.          0.36857143]
 [ 0.          0.5         0.68        0.          0.40470588  0.
   0.5         0.54333333  0.          0.35178571]
 [ 0.          0.5         0.67        0.          0.40102941  0.
   0.5         0.65        0.          0.39367647]
 [ 0.          0.5         0.76666667  0.          0.43374183  0.
   0.5         0.51111111  0.          0.33772894]
 [ 0.14402597  0.5725      0.54888889  0.3725377   0.49430181  0.
   0.5         0.64        0.          0.39      ]
 [ 0.          0.5         0.58444444  0.          0.36857143  0.
   0.5         0.75666667  0.          0.43047386]
 [ 0.          0.5         0.52222222  0.          0.34267399  0.26246218
   0.62416667  0.66222222  0.47650762  0.57848901]
 [ 0.          0.5         0.56333333  0.          0.36011905  0.18526427
   0.5975      0.57444444  0.49745121  0.54405206]
 [ 0.          0.5         0.53333333  0.          0.34761905  0.34090167
   0.6625      0.69111111  0.52522568  0.62854312]
 [ 0.          0.5         0.70222222  0.          0.41220588  0.51562718
   0.7625      0.75555556  0.74524261  0.74998751]
 [ 0.          0.5         0.62888889  0.          0.38571429  0.02857143
   0.5125      0.59444444  0.05        0.39107143]
 [ 0.          0.5         0.59555556  0.          0.37285714  0.
   0.5         0.55333333  0.          0.35595238]
 [ 0.19274131  0.58333333  0.67        0.31547005  0.5231044   0.
   0.5         0.65        0.          0.39367647]]
SVM mean:
[0.05264794 0.52394444 0.63377778 0.09290696 0.42302859 0.09628734
 0.54769444 0.6397037  0.15941944 0.45361437]
---------------------------
---------------------------
GBM performance:
[[ 0.65746355  0.82        0.81444444  0.80256235  0.80976912  0.19495342
   0.58660714  0.77888889  0.27303051  0.55138655]
 [ 0.24303764  0.61583333  0.64777778  0.54427932  0.59494172  0.46910503
   0.69        0.75777778  0.63310043  0.67411089]
 [ 0.4707182   0.72333333  0.75888889  0.68091482  0.72355561  0.21833977
   0.6175      0.62666667  0.57041049  0.59911006]
 [ 0.32703663  0.665       0.67222222  0.61631512  0.65739899  0.10173913
   0.54166667  0.73555556  0.14142136  0.48792017]
 [ 0.59449464  0.79166667  0.83        0.72244134  0.78239011  0.49227821
   0.735       0.73444444  0.73484715  0.73048701]
 [ 0.20813743  0.625       0.62777778  0.58483014  0.60811883  0.4897651
   0.75166667  0.76666667  0.71185897  0.74940865]
 [ 0.31445073  0.65333333  0.69333333  0.52890755  0.6259016   0.10886626
   0.5675      0.56222222  0.50259919  0.54285798]
 [ 0.79531417  0.89404762  0.92555556  0.8585587   0.89869963  0.43008662
   0.7075      0.74555556  0.65128654  0.70547897]
 [ 0.23443086  0.60119048  0.74444444  0.37377235  0.58409745  0.21167336
   0.59642857  0.73444444  0.29220695  0.54034745]
 [ 0.29251575  0.645       0.64666667  0.61192991  0.63035548  0.24328595
   0.61        0.64        0.53361774  0.58215201]
 [ 0.01818946  0.49166667  0.56333333  0.35151672  0.48944098  0.42019737
   0.68583333  0.72555556  0.65033649  0.68407426]
 [ 0.64739174  0.80416667  0.85111111  0.77321902  0.81659008  0.45724109
   0.73833333  0.76777778  0.68799931  0.73091464]
 [ 0.38100812  0.72083333  0.74333333  0.62104289  0.70821928  0.02563483
   0.52        0.56555556  0.32727848  0.47248085]
 [-0.06620321  0.48154762  0.62666667  0.10337857  0.41820028  0.27821284
   0.675       0.68        0.56995182  0.65724831]
 [ 0.0243794   0.51190476  0.67        0.13938469  0.46154304 -0.08283422
   0.45952381  0.58444444  0.10487548  0.40303571]
 [ 0.04554932  0.53416667  0.60222222  0.23614272  0.47797536  0.34401103
   0.65357143  0.74555556  0.55423201  0.65254121]
 [ 0.59917496  0.7975      0.80666667  0.7851717   0.79587912  0.39283037
   0.68333333  0.72333333  0.62775662  0.68082418]
 [-0.0125      0.54375     0.85333333  0.          0.50976307  0.2939667
   0.6675      0.69222222  0.59273669  0.64613303]
 [ 0.11818182  0.5452381   0.68111111  0.28260065  0.49175285  0.39271183
   0.695       0.70111111  0.67419197  0.68760295]
 [ 0.31047182  0.64345238  0.71333333  0.51125029  0.61885989  0.1697479
   0.57083333  0.69222222  0.28094011  0.52303571]
 [ 0.02809365  0.51160714  0.75666667  0.07071068  0.46297386  0.15154594
   0.5675      0.56333333  0.54466138  0.55348846]
 [ 0.38197802  0.71        0.71222222  0.67092326  0.7025777   0.32845893
   0.65416667  0.72666667  0.56557527  0.63867799]
 [ 0.15623118  0.60083333  0.64777778  0.47334326  0.56970613  0.16318514
   0.58214286  0.75555556  0.32544124  0.55288282]
 [ 0.35331194  0.6775      0.68111111  0.66193817  0.67108586  0.37031739
   0.68416667  0.69222222  0.67580211  0.66824037]
 [ 0.27310778  0.65416667  0.68222222  0.53937009  0.63597514  0.44509832
   0.72        0.72666667  0.67999194  0.71394134]
 [ 0.45254271  0.7125      0.72222222  0.68974049  0.70811688  0.38238392
   0.70083333  0.71        0.66271986  0.68758963]
 [ 0.30626501  0.60357143  0.72333333  0.43594782  0.59612153  0.46594289
   0.725       0.72333333  0.71647405  0.71505772]
 [ 0.08824851  0.52333333  0.60444444  0.29412598  0.45804029  0.42100291
   0.695       0.71555556  0.65994879  0.68307276]
 [ 0.3058637   0.63666667  0.68333333  0.50188052  0.61170246  0.32118528
   0.65416667  0.66222222  0.62113786  0.63778055]
 [ 0.72466106  0.85833333  0.87222222  0.83942262  0.85625624  0.41990343
   0.68869048  0.74333333  0.62925086  0.68828671]]
GBM mean:
[0.30911822 0.6532381  0.71859259 0.51018739 0.63253362 0.30402789
 0.64748214 0.6992963  0.53985606 0.62800563]
---------------------------
---------------------------
BDDAE performance:
[[ 0.00651233  0.50388889  0.50526316  0.48012578  0.49327762 -0.11186427
   0.45071429  0.62631579  0.13520777  0.4240362 ]
 [ 0.32121509  0.66136364  0.66315789  0.65624282  0.65696706  0.37108038
   0.68928571  0.70526316  0.67828787  0.68284431]
 [ 0.03155863  0.51534091  0.54736842  0.45656292  0.50221644  0.13360045
   0.56666667  0.57368421  0.50968518  0.55057236]
 [ 0.01961475  0.50944444  0.51578947  0.48060228  0.49830216 -0.06528738
   0.47428571  0.64210526  0.18279233  0.44276998]
 [ 0.31690903  0.65952381  0.67894737  0.64563621  0.65456957  0.19100826
   0.59722222  0.58947368  0.57131665  0.58072726]
 [ 0.48694678  0.74166667  0.74736842  0.71870176  0.73519211 -0.02401281
   0.48833333  0.48947368  0.41581844  0.45437009]
 [-0.01130778  0.49285714  0.52105263  0.45151062  0.47900477  0.08706627
   0.54388889  0.54736842  0.51633212  0.53247054]
 [ 0.21978241  0.60128205  0.7         0.49438324  0.59499397  0.12862507
   0.56428571  0.57368421  0.54904984  0.55350903]
 [ 0.48699366  0.71923077  0.8         0.67905193  0.7371227   0.58303757
   0.75833333  0.84736842  0.70466298  0.78025903]
 [ 0.09608449  0.54777778  0.54736842  0.53755622  0.54223822  0.1908627
   0.59602273  0.60526316  0.57432888  0.58772823]
 [ 0.00837683  0.50416667  0.58421053  0.29005204  0.46596976  0.00151749
   0.50059524  0.56842105  0.39086473  0.48151212]
 [ 0.05541476  0.52678571  0.57894737  0.44962259  0.51699702  0.39993265
   0.69880952  0.72105263  0.68200015  0.69462115]
 [ 0.05960669  0.52897727  0.53157895  0.51849641  0.52250902  0.26658115
   0.63465909  0.64210526  0.6102055   0.62324327]
 [ 0.05817013  0.52564103  0.62105263  0.42096873  0.51964267  0.28560332
   0.64388889  0.64210526  0.6306424   0.6365107 ]
 [ 0.08069887  0.54230769  0.59473684  0.50284123  0.53231035  0.02107363
   0.52051282  0.58947368  0.37394737  0.49334264]
 [ 0.01125553  0.50681818  0.53157895  0.3917108   0.47601657  0.12595655
   0.55320513  0.68947368  0.3378234   0.52738318]
 [ 0.39993848  0.70568182  0.69473684  0.65755376  0.67428894  0.37608839
   0.67613636  0.71578947  0.62006962  0.67050181]
 [ 0.31476776  0.65        0.82105263  0.56077358  0.65361145 -0.05140324
   0.47613636  0.5         0.40979438  0.46165855]
 [ 0.09051443  0.54423077  0.61578947  0.46732071  0.53805186  0.22561631
   0.61277778  0.61578947  0.59680164  0.60649261]
 [ 0.13706293  0.56730769  0.64736842  0.48763692  0.55282941 -0.09514019
   0.45654762  0.55789474  0.14022147  0.39378714]
 [ 0.08810059  0.5425      0.72631579  0.35235461  0.52755131  0.22166028
   0.61111111  0.61052632  0.60758767  0.60886849]
 [ 0.14714927  0.57388889  0.57368421  0.56123817  0.56717586  0.12747237
   0.56011905  0.62105263  0.47829484  0.55259684]
 [-0.10122084  0.45        0.45789474  0.43314728  0.44335356  0.4292835
   0.68857143  0.81578947  0.59337829  0.70203333]
 [-0.12316233  0.43888889  0.44210526  0.42890796  0.43607718  0.32229578
   0.65625     0.67894737  0.63416601  0.65598811]
 [-0.21573651  0.39090909  0.40526316  0.36460364  0.38434507  0.01772624
   0.50888889  0.50526316  0.4885847   0.49696609]
 [ 0.14864622  0.57333333  0.57894737  0.5471555   0.56343191  0.05815813
   0.52897727  0.54736842  0.49685629  0.52066844]
 [ 0.33013157  0.65448718  0.74210526  0.55893291  0.64730147  0.52509631
   0.76333333  0.76315789  0.75344657  0.75884559]
 [-0.0939525   0.45714286  0.49473684  0.37072681  0.44429518  0.48072132
   0.73352273  0.75263158  0.71855827  0.7367696 ]
 [ 0.05436842  0.52727273  0.54736842  0.49632916  0.51977025  0.47418454
   0.73011364  0.75263158  0.71001122  0.73199207]
 [ 0.23324344  0.61845238  0.64210526  0.59488949  0.61118062  0.07617743
   0.53809524  0.58947368  0.46250394  0.52783718]]
BDDAE mean:
[0.12192277 0.55937228 0.60192982 0.50185454 0.54968647 0.19242394
 0.594043   0.63596491 0.51910802 0.58236353]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.52222222 0.         0.34267399 0.
  0.5        0.75666667 0.         0.43047386]
 [0.         0.5        0.56333333 0.         0.36011905 0.
  0.5        0.62888889 0.         0.38571429]
 [0.         0.5        0.58444444 0.         0.36857143 0.
  0.5        0.52222222 0.         0.34267399]
 [0.         0.5        0.53333333 0.         0.34761905 0.
  0.5        0.72444444 0.         0.41970588]
 [0.         0.5        0.64       0.         0.39       0.
  0.5        0.52222222 0.         0.34267399]
 [0.         0.5        0.53333333 0.         0.34761905 0.
  0.5        0.54333333 0.         0.35178571]
 [0.         0.5        0.60666667 0.         0.37714286 0.
  0.5        0.51111111 0.         0.33772894]
 [0.         0.5        0.69111111 0.         0.40845588 0.
  0.5        0.61777778 0.         0.38142857]
 [0.         0.5        0.70222222 0.         0.41220588 0.
  0.5        0.70222222 0.         0.41220588]
 [0.         0.5        0.51111111 0.         0.33772894 0.
  0.5        0.56333333 0.         0.36011905]
 [0.         0.5        0.65       0.         0.39367647 0.
  0.5        0.60666667 0.         0.37714286]
 [0.         0.5        0.62888889 0.         0.38571429 0.
  0.5        0.60666667 0.         0.37714286]
 [0.         0.5        0.57333333 0.         0.36428571 0.
  0.5        0.57333333 0.         0.36428571]
 [0.         0.5        0.67       0.         0.40102941 0.
  0.5        0.53333333 0.         0.34761905]
 [0.         0.5        0.69111111 0.         0.40845588 0.
  0.5        0.66       0.         0.39735294]
 [0.         0.5        0.59555556 0.         0.37285714 0.
  0.5        0.67       0.         0.40102941]
 [0.         0.5        0.58444444 0.         0.36857143 0.
  0.5        0.59555556 0.         0.37285714]
 [0.         0.5        0.85333333 0.         0.46013072 0.
  0.5        0.58444444 0.         0.36857143]
 [0.         0.5        0.68       0.         0.40470588 0.
  0.5        0.54333333 0.         0.35178571]
 [0.         0.5        0.67       0.         0.40102941 0.
  0.5        0.65       0.         0.39367647]
 [0.         0.5        0.76666667 0.         0.43374183 0.
  0.5        0.51111111 0.         0.33772894]
 [0.         0.5        0.46666667 0.         0.31794872 0.
  0.5        0.64       0.         0.39      ]
 [0.         0.5        0.58444444 0.         0.36857143 0.
  0.5        0.75666667 0.         0.43047386]
 [0.         0.5        0.52222222 0.         0.34267399 0.
  0.5        0.55333333 0.         0.35595238]
 [0.         0.5        0.56333333 0.         0.36011905 0.
  0.5        0.46666667 0.         0.31794872]
 [0.         0.5        0.53333333 0.         0.34761905 0.
  0.5        0.55333333 0.         0.35595238]
 [0.         0.5        0.70222222 0.         0.41220588 0.
  0.5        0.46666667 0.         0.31794872]
 [0.         0.5        0.62888889 0.         0.38571429 0.
  0.5        0.58444444 0.         0.36857143]
 [0.         0.5        0.59555556 0.         0.37285714 0.
  0.5        0.55333333 0.         0.35595238]
 [0.         0.5        0.60666667 0.         0.37714286 0.
  0.5        0.65       0.         0.39367647]]
DUMMY mean:
[0.         0.5        0.61514815 0.         0.37903956 0.
 0.5        0.59503704 0.         0.3713393 ]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_42
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.337 0.67  0.709 0.6   0.657 0.372 0.685 0.713 0.633 0.675]
 [0.326 0.665 0.691 0.619 0.653 0.316 0.656 0.675 0.617 0.642]
 [0.368 0.698 0.725 0.616 0.683 0.362 0.686 0.716 0.629 0.676]
 [0.053 0.524 0.634 0.093 0.423 0.096 0.548 0.64  0.159 0.454]
 [0.309 0.653 0.719 0.51  0.633 0.304 0.647 0.699 0.54  0.628]
 [0.122 0.559 0.602 0.502 0.55  0.192 0.594 0.636 0.519 0.582]
 [0.    0.5   0.615 0.    0.379 0.    0.5   0.595 0.    0.371]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.203 0.1   0.087 0.169 0.108 0.163 0.082 0.076 0.11  0.085]
 [0.182 0.097 0.095 0.143 0.102 0.137 0.07  0.07  0.088 0.073]
 [0.193 0.09  0.084 0.145 0.096 0.16  0.078 0.061 0.12  0.084]
 [0.112 0.05  0.085 0.174 0.085 0.176 0.088 0.076 0.269 0.128]
 [0.228 0.111 0.087 0.234 0.127 0.147 0.071 0.062 0.174 0.086]
 [0.172 0.084 0.103 0.102 0.087 0.194 0.092 0.09  0.162 0.104]
 [0.    0.    0.082 0.    0.031 0.    0.    0.076 0.    0.03 ]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 60.  15.  12.  28.  16.  44.  12.  11.  17.  13.]
 [ 56.  15.  14.  23.  16.  43.  11.  10.  14.  11.]
 [ 53.  13.  12.  24.  14.  44.  11.   9.  19.  12.]
 [213.  10.  13. 187.  20. 183.  16.  12. 169.  28.]
 [ 74.  17.  12.  46.  20.  48.  11.   9.  32.  14.]
 [141.  15.  17.  20.  16. 101.  15.  14.  31.  18.]
 [  0.   0.  13.   0.   8.   0.   0.  13.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_42
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  16.992
step (sec):  12.752
overlap:  True
perc. of overlap:  24.95291902071563
overlap duration (sec):  4.24
Number of windows / instances:  94
Elapsed time: 673.0632625500361 minutes
Elapsed time: 11.217721042500601 hours
