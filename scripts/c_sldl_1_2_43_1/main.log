2024-05-12 06:24:31.172671: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-12 06:24:34.780279: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-12 06:24:43.790567: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  18.0
step (sec):  18.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_43_1\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 18000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4500, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4500, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1125, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 18000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4500, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4500, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1125, 6)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2398              â”‚ (None, 4500, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2398           â”‚ (None, 4500, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2399              â”‚ (None, 18000, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2399           â”‚ (None, 18000, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m48s[0m 1s/step - loss: 0.2599 - mean_squared_error: 0.2599
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - loss: 0.1737 - mean_squared_error: 0.1737
[1m10/46[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.1398 - mean_squared_error: 0.1398
[1m15/46[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.1172 - mean_squared_error: 0.1172
[1m20/46[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.1056 - mean_squared_error: 0.1056
[1m25/46[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0972 - mean_squared_error: 0.0972
[1m31/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0893 - mean_squared_error: 0.0893
[1m36/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0837 - mean_squared_error: 0.0837
[1m41/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0788 - mean_squared_error: 0.0788
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - loss: 0.0746 - mean_squared_error: 0.0746
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 17ms/step - loss: 0.0738 - mean_squared_error: 0.0738 - val_loss: 0.0126 - val_mean_squared_error: 0.0126
Epoch 2/5

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0043 - mean_squared_error: 0.0043
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0121 - mean_squared_error: 0.0121
[1m11/46[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0152 - mean_squared_error: 0.0152
[1m16/46[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0175 - mean_squared_error: 0.0175
[1m21/46[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0182 - mean_squared_error: 0.0182
[1m26/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0184 - mean_squared_error: 0.0184
[1m31/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0186 - mean_squared_error: 0.0186
[1m36/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0188 - mean_squared_error: 0.0188
[1m41/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0188 - mean_squared_error: 0.0188
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - loss: 0.0188 - mean_squared_error: 0.0188
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0107 - val_mean_squared_error: 0.0107
Epoch 3/5

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0607 - mean_squared_error: 0.0607
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0306 - mean_squared_error: 0.0306
[1m11/46[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0246 - mean_squared_error: 0.0246
[1m17/46[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0224 - mean_squared_error: 0.0224
[1m22/46[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0210 - mean_squared_error: 0.0210
[1m27/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0197 - mean_squared_error: 0.0197
[1m32/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m37/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0184 - mean_squared_error: 0.0184
[1m41/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0183 - mean_squared_error: 0.0183
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 12ms/step - loss: 0.0182 - mean_squared_error: 0.0182
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - loss: 0.0182 - mean_squared_error: 0.0182 - val_loss: 0.0098 - val_mean_squared_error: 0.0098
Epoch 4/5

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0155 - mean_squared_error: 0.0155
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0097 - mean_squared_error: 0.0097
[1m11/46[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0116 - mean_squared_error: 0.0116
[1m16/46[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0121 - mean_squared_error: 0.0121
[1m22/46[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0122 - mean_squared_error: 0.0122
[1m27/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0131 - mean_squared_error: 0.0131
[1m32/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0137 - mean_squared_error: 0.0137
[1m37/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0142 - mean_squared_error: 0.0142
[1m42/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - loss: 0.0145 - mean_squared_error: 0.0145
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - loss: 0.0148 - mean_squared_error: 0.0148 - val_loss: 0.0105 - val_mean_squared_error: 0.0105
Epoch 5/5

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0070 - mean_squared_error: 0.0070
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0082 - mean_squared_error: 0.0082
[1m10/46[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0111 - mean_squared_error: 0.0111
[1m15/46[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0139 - mean_squared_error: 0.0139
[1m20/46[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0154 - mean_squared_error: 0.0154
[1m26/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0164 - mean_squared_error: 0.0164
[1m31/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0170 - mean_squared_error: 0.0170
[1m36/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0171 - mean_squared_error: 0.0171
[1m41/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0171 - mean_squared_error: 0.0171
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - loss: 0.0170 - mean_squared_error: 0.0170 - val_loss: 0.0104 - val_mean_squared_error: 0.0104
(18000, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 18000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4500, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4500, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1125, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 18000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 18000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 1125, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 1125, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 1125, 12)  â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 1125)  â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 13500)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 13500)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     13,501 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 14,323 (55.95 KB)
 Trainable params: 14,323 (55.95 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m57s[0m 1s/step - binary_accuracy: 1.0000 - loss: 0.5467
[1m 7/46[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6350 - loss: 0.7583
[1m12/46[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5849 - loss: 0.7479
[1m17/46[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5666 - loss: 0.7397
[1m23/46[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5643 - loss: 0.7352
[1m28/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5775 - loss: 0.7249
[1m33/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5907 - loss: 0.7150
[1m38/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.5963 - loss: 0.7143
[1m43/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.6013 - loss: 0.7118
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 16ms/step - binary_accuracy: 0.6042 - loss: 0.7106 - val_binary_accuracy: 1.0000 - val_loss: 0.3806
Epoch 2/10

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7615
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.2889 - loss: 0.8341    
[1m11/46[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.3648 - loss: 0.8074
[1m17/46[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.4090 - loss: 0.7843
[1m22/46[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.4520 - loss: 0.7639
[1m27/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.4856 - loss: 0.7444
[1m32/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5033 - loss: 0.7335
[1m38/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5225 - loss: 0.7238
[1m44/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5372 - loss: 0.7167
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.5444 - loss: 0.7129 - val_binary_accuracy: 1.0000 - val_loss: 0.3315
Epoch 3/10

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.2696
[1m 7/46[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6296 - loss: 0.7509    
[1m13/46[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6909 - loss: 0.6741
[1m18/46[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7212 - loss: 0.6296
[1m23/46[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7344 - loss: 0.6079
[1m28/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7392 - loss: 0.5991
[1m33/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7445 - loss: 0.5934
[1m39/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7542 - loss: 0.5852
[1m44/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7584 - loss: 0.5817
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.7584 - loss: 0.5814 - val_binary_accuracy: 0.8333 - val_loss: 0.3382
Epoch 4/10

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.5084
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8972 - loss: 0.4843
[1m12/46[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8621 - loss: 0.4650
[1m17/46[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8631 - loss: 0.4373
[1m22/46[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8582 - loss: 0.4359
[1m27/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8505 - loss: 0.4379
[1m32/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8436 - loss: 0.4444
[1m37/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8354 - loss: 0.4534
[1m41/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8311 - loss: 0.4586
[1m45/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8277 - loss: 0.4625
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.8258 - loss: 0.4645 - val_binary_accuracy: 0.5000 - val_loss: 0.7208
Epoch 5/10

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.2513
[1m 7/46[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.7711 - loss: 0.4805
[1m12/46[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7745 - loss: 0.5328
[1m17/46[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7780 - loss: 0.5346
[1m22/46[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7807 - loss: 0.5331
[1m28/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7718 - loss: 0.5407
[1m33/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7663 - loss: 0.5455
[1m38/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7604 - loss: 0.5514
[1m44/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7575 - loss: 0.5561
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.7562 - loss: 0.5579 - val_binary_accuracy: 0.5000 - val_loss: 0.7238
Epoch 6/10

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.6587
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.6556 - loss: 0.7573
[1m11/46[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6567 - loss: 0.7017
[1m16/46[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.6738 - loss: 0.6700
[1m22/46[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.6999 - loss: 0.6424
[1m28/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7180 - loss: 0.6214
[1m33/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7313 - loss: 0.6048
[1m39/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7404 - loss: 0.5935
[1m45/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7473 - loss: 0.5854
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.7497 - loss: 0.5823 - val_binary_accuracy: 0.8333 - val_loss: 0.4602
Epoch 7/10

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.6118
[1m 7/46[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 1.0000 - loss: 0.4082
[1m12/46[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 1.0000 - loss: 0.3654
[1m18/46[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9647 - loss: 0.3923
[1m23/46[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9516 - loss: 0.3904
[1m29/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9345 - loss: 0.3874
[1m35/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9203 - loss: 0.3862
[1m41/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9091 - loss: 0.3863
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8978 - loss: 0.3908
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.8954 - loss: 0.3919 - val_binary_accuracy: 0.5000 - val_loss: 0.7451
Epoch 8/10

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 41ms/step - binary_accuracy: 1.0000 - loss: 0.2745
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9389 - loss: 0.3921
[1m12/46[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9150 - loss: 0.4005
[1m18/46[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.8967 - loss: 0.4060
[1m23/46[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8776 - loss: 0.4135
[1m28/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8719 - loss: 0.4131
[1m34/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8674 - loss: 0.4125
[1m39/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8671 - loss: 0.4091
[1m44/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8638 - loss: 0.4114
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.8627 - loss: 0.4122 - val_binary_accuracy: 0.8333 - val_loss: 0.4880
Epoch 9/10

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0608
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 1.0000 - loss: 0.1185
[1m12/46[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9931 - loss: 0.1757
[1m17/46[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9643 - loss: 0.2281
[1m23/46[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9480 - loss: 0.2618
[1m29/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9352 - loss: 0.2864
[1m34/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9246 - loss: 0.3034
[1m39/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9142 - loss: 0.3177
[1m44/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9066 - loss: 0.3278
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.9028 - loss: 0.3331 - val_binary_accuracy: 0.6667 - val_loss: 0.6180
Epoch 10/10

[1m 1/46[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2118
[1m 6/46[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9389 - loss: 0.2294
[1m12/46[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8600 - loss: 0.2909
[1m17/46[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8419 - loss: 0.3046
[1m23/46[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8447 - loss: 0.3074
[1m28/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8505 - loss: 0.3083
[1m32/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8527 - loss: 0.3145
[1m37/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8533 - loss: 0.3206
[1m43/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8560 - loss: 0.3239
[1m46/46[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.8569 - loss: 0.3272 - val_binary_accuracy: 0.6667 - val_loss: 0.5794

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.0181541  0.7243428  0.75405276 0.9031369  0.92605025 0.9400417
 0.67089236 0.38014743 0.48777527 0.871449   0.59599775 0.86089444
 0.8340459  0.79433966]
predicted [0 1 1 1 1 1 1 0 0 1 1 1 1 1]
expected [False  True  True  True  True False  True  True  True  True False False
  True False]
accuracy: 0.5714285714285714
confusion matrix: 
[[1 4]
 [2 7]]
              precision    recall  f1-score   support

       False       0.33      0.20      0.25         5
        True       0.64      0.78      0.70         9

    accuracy                           0.57        14
   macro avg       0.48      0.49      0.47        14
weighted avg       0.53      0.57      0.54        14

macro avg f1-score: 0.475
macro avg (UAR): 0.48888888888888893
Sensitivity:  0.2
Specificity:  0.7777777777777778
g-mean:  0.39440531887330776
-------- Model Performance ----------: 
accuracy:  [0.57142857 0.5        0.64285714 0.71428571 0.57142857 0.64285714
 0.64285714 0.57142857 0.5        0.57142857]
gmean:  [0.51639778 0.36514837 0.42163702 0.59628479 0.39440532 0.55777335
 0.63245553 0.57735027 0.47140452 0.39440532]
f1_score:  [0.53333333 0.42690058 0.52380952 0.65       0.475      0.59064327
 0.62566845 0.5625     0.47593583 0.475     ]
UAR:  [0.53333333 0.43333333 0.54444444 0.64444444 0.48888889 0.58888889
 0.63333333 0.57777778 0.47777778 0.48888889]
Cohen Kappa score:  [ 0.06666667 -0.13953488  0.1025641   0.31707317 -0.02439024  0.18604651
  0.25531915  0.14285714 -0.04255319 -0.02439024]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  18.0
step (sec):  18.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  66
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.679  0.851  0.848  0.84   0.836  0.218  0.616  0.645  0.507  0.591]
 [ 0.494  0.728  0.738  0.68   0.707  0.351  0.654  0.686  0.609  0.646]
 [ 0.536  0.773  0.79   0.762  0.755  0.274  0.703  0.719  0.604  0.702]
 [ 0.     0.5    0.624  0.     0.383  0.     0.5    0.638  0.     0.389]
 [ 0.543  0.778  0.798  0.697  0.762  0.121  0.555  0.645  0.297  0.512]
 [ 0.084  0.541  0.593  0.493  0.534 -0.071  0.468  0.521  0.341  0.451]
 [ 0.     0.5    0.624  0.     0.383  0.     0.5    0.638  0.     0.389]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.4229019   0.7125      0.71190476  0.64635623  0.68742063  0.24267446
   0.63        0.7452381   0.42030679  0.60590909]
 [ 0.357       0.67916667  0.67857143  0.62760209  0.66011905  0.45219827
   0.71666667  0.77380952  0.55501894  0.68707071]
 [ 0.47720215  0.73333333  0.75714286  0.64617795  0.71116522  0.42187291
   0.7125      0.7         0.6411156   0.67956349]
 [ 0.3768798   0.69166667  0.69285714  0.66613457  0.67904762  0.06246438
   0.5325      0.61666667  0.37717107  0.51545094]
 [ 0.62443535  0.80833333  0.85        0.71359758  0.79638889  0.27497436
   0.64166667  0.63571429  0.6239998   0.62880952]
 [ 0.3190303   0.65833333  0.67142857  0.58541725  0.63672078  0.53648441
   0.76666667  0.76904762  0.73281338  0.75230159]
 [ 0.40453604  0.6875      0.72619048  0.6124557   0.68706349  0.17815385
   0.59166667  0.58809524  0.52044285  0.56142857]
 [ 0.68431373  0.8375      0.88809524  0.7280239   0.8204329   0.53999125
   0.77416667  0.77619048  0.74962641  0.75769841]
 [ 0.24532152  0.6225      0.72857143  0.41859243  0.59219697  0.08121849
   0.53        0.66190476  0.27336928  0.51118687]
 [ 0.29284727  0.65        0.64047619  0.5643055   0.61634921  0.17887625
   0.5875      0.58571429  0.53433366  0.57253968]
 [ 0.11615998  0.55416667  0.57619048  0.48752273  0.5434127   0.17119051
   0.58416667  0.63095238  0.43587656  0.55122294]
 [ 0.52174948  0.755       0.80238095  0.688525    0.74813492  0.38578393
   0.70833333  0.70238095  0.67967468  0.68190476]
 [ 0.41456856  0.7125      0.71190476  0.69302014  0.69849206  0.25892133
   0.62916667  0.65        0.56321169  0.61448413]
 [ 0.05066926  0.515       0.58095238  0.37522914  0.49440476  0.3838274
   0.6875      0.7         0.65991982  0.68198413]
 [-0.07697662  0.4675      0.6047619   0.17925505  0.43719697  0.10477504
   0.55083333  0.59761905  0.42124136  0.5410101 ]
 [ 0.14252306  0.56666667  0.6         0.47023687  0.54989538  0.3275612
   0.65916667  0.73809524  0.51590415  0.64709596]
 [ 0.42791432  0.7125      0.71428571  0.69301821  0.70333333  0.35773393
   0.67916667  0.71190476  0.56288803  0.65416667]
 [-0.02686275  0.50166667  0.75714286  0.07745967  0.4515676   0.29164103
   0.65        0.65238095  0.63661386  0.64047619]
 [ 0.15151609  0.57166667  0.62857143  0.43672018  0.5625      0.33641603
   0.66666667  0.66904762  0.64333914  0.6568254 ]
 [ 0.34251354  0.6675      0.70952381  0.59361159  0.65573232  0.14898182
   0.57166667  0.62142857  0.43764037  0.55055556]
 [ 0.05330148  0.525       0.66904762  0.23416408  0.49444444  0.06422557
   0.53333333  0.52857143  0.42556542  0.49059524]
 [ 0.17057809  0.5875      0.59285714  0.4480399   0.54583333  0.43188771
   0.71333333  0.75714286  0.65770721  0.71045455]
 [ 0.15048309  0.575       0.60714286  0.39401663  0.52324315  0.03647059
   0.52        0.74761905  0.12649111  0.47318182]
 [-0.13430435  0.43333333  0.43809524  0.38854107  0.42345238  0.3843339
   0.68333333  0.71190476  0.62824942  0.68166667]
 [ 0.40569565  0.70416667  0.70714286  0.68240016  0.69404762  0.55814976
   0.77916667  0.77857143  0.74948005  0.76444444]
 [ 0.40605797  0.7         0.70952381  0.64341069  0.6872619   0.36826087
   0.68333333  0.68095238  0.66873592  0.67373016]
 [ 0.42133887  0.7375      0.7452381   0.67925258  0.69737374  0.42322134
   0.70416667  0.72380952  0.61326643  0.68007937]
 [ 0.02808959  0.50666667  0.55238095  0.38661483  0.49190476  0.45225598
   0.725       0.7547619   0.67153273  0.71960317]
 [ 0.02691135  0.5125      0.57142857  0.30731322  0.4802381   0.1116941
   0.55416667  0.57619048  0.47685075  0.54111111]
 [ 0.67940373  0.85083333  0.84761905  0.84014529  0.83555556  0.21808696
   0.61583333  0.6452381   0.50698069  0.59095238]]
KNN mean:
[0.28252662 0.64125    0.68238095 0.53023867 0.62016433 0.29281092
 0.64605556 0.68103175 0.55031224 0.62725012]
---------------------------
---------------------------
DT performance:
[[ 0.48415385  0.72916667  0.72380952  0.68441609  0.71178571  0.09157219
   0.5325      0.62380952  0.24350074  0.49227273]
 [ 0.13038573  0.6         0.60238095  0.42128647  0.56104618  0.36168801
   0.6775      0.69285714  0.55610549  0.65337302]
 [ 0.33026219  0.7         0.71190476  0.67854768  0.68980159  0.34930769
   0.675       0.67142857  0.71909499  0.66238095]
 [ 0.10053549  0.525       0.51428571  0.41117036  0.50583333  0.19132992
   0.69        0.7452381   0.4366445   0.65058081]
 [ 0.63493996  0.80083333  0.82142857  0.6974359   0.76310967  0.40486325
   0.71666667  0.70952381  0.70679327  0.69916667]
 [ 0.41054779  0.63333333  0.63571429  0.62204053  0.62492063  0.4747561
   0.75416667  0.74761905  0.66850705  0.7372619 ]
 [ 0.13119033  0.56083333  0.6         0.38205324  0.52669553  0.17533333
   0.57083333  0.55952381  0.4812157   0.53940476]
 [ 0.8505176   0.9275      0.93571429  0.90157475  0.92472222  0.39183108
   0.7075      0.72142857  0.67745018  0.68353175]
 [ 0.25552359  0.595       0.66904762  0.40574616  0.56263348  0.42437513
   0.63        0.70238095  0.53280654  0.61295815]
 [ 0.17028824  0.57083333  0.56190476  0.55689141  0.5431746   0.12139394
   0.52916667  0.54047619  0.3887846   0.49547619]
 [ 0.01102409  0.51666667  0.54047619  0.37287807  0.49190476  0.54327329
   0.755       0.77142857  0.74750195  0.72948413]
 [ 0.52164803  0.74        0.78571429  0.82282096  0.73970058  0.42412324
   0.65833333  0.69047619  0.65226219  0.64710317]
 [ 0.27090747  0.57916667  0.58095238  0.61943059  0.55912698  0.06465902
   0.47083333  0.48809524  0.40259172  0.45642857]
 [-0.06647157  0.50583333  0.51190476  0.3737241   0.48718254  0.14069565
   0.59166667  0.58809524  0.48305691  0.57142857]
 [ 0.09271203  0.5125      0.56666667  0.34535683  0.49755411 -0.05830406
   0.41        0.45714286  0.25085381  0.40190476]
 [-0.00715415  0.525       0.54047619  0.38951955  0.50940476  0.22211658
   0.65833333  0.70714286  0.56026621  0.61884921]
 [ 0.15503922  0.60833333  0.63809524  0.56019268  0.58968254  0.47459197
   0.72083333  0.71190476  0.71133922  0.68615079]
 [ 0.16546728  0.45833333  0.68095238  0.35634511  0.425169    0.24764615
   0.65833333  0.6547619   0.55991709  0.62813492]
 [ 0.40373432  0.6125      0.64285714  0.50136481  0.58230159  0.30033668
   0.7125      0.71190476  0.60745861  0.7022619 ]
 [ 0.21783445  0.605       0.65238095  0.56736847  0.60128788  0.36809423
   0.63916667  0.64047619  0.6339825   0.62539683]
 [-0.0926087   0.475       0.5952381   0.35024973  0.45626263 -0.06017614
   0.5375      0.5452381   0.50277     0.52107143]
 [ 0.24828205  0.6375      0.63333333  0.64103547  0.62488095  0.18326657
   0.605       0.65        0.48352268  0.57515873]
 [ 0.03364103  0.54166667  0.5452381   0.49736324  0.49928571  0.47163683
   0.68        0.76428571  0.61959256  0.66401515]
 [-0.04316809  0.54166667  0.53095238  0.40486333  0.5134127   0.60925253
   0.7375      0.72619048  0.69341655  0.70146825]
 [ 0.32074805  0.65833333  0.66428571  0.59591095  0.63480159  0.49789355
   0.7125      0.71428571  0.739482    0.69940476]
 [ 0.37423188  0.7         0.7047619   0.71765484  0.69111111  0.21126219
   0.60833333  0.60952381  0.52204053  0.595     ]
 [ 0.36840814  0.715       0.75952381  0.58549481  0.71198052  0.42852437
   0.6625      0.68095238  0.59442902  0.63489538]
 [ 0.048786    0.5525      0.58333333  0.42418331  0.51771284  0.33174071
   0.7         0.70952381  0.67131438  0.6872619 ]
 [-0.11327202  0.4375      0.4547619   0.32061253  0.40412698  0.28787291
   0.58333333  0.59761905  0.57331728  0.5590873 ]
 [ 0.49389855  0.72833333  0.73809524  0.68038214  0.70747475  0.35054098
   0.65416667  0.68571429  0.6093478   0.64632395]]
DT mean:
[0.23006776 0.60977778 0.63753968 0.52959714 0.58860292 0.30084993
 0.64130556 0.66063492 0.56764554 0.61924122]
---------------------------
---------------------------
RF performance:
[[ 0.57356522  0.77083333  0.77142857  0.70393372  0.76547619  0.10994154
   0.565       0.72619048  0.37085316  0.53671717]
 [ 0.30513043  0.65416667  0.65714286  0.6232164   0.6384127   0.32325454
   0.63583333  0.65238095  0.55439282  0.61595238]
 [ 0.46752306  0.69583333  0.71190476  0.71488322  0.66866522  0.26833333
   0.67083333  0.66904762  0.61251236  0.64099206]
 [ 0.36219088  0.63333333  0.63333333  0.56816265  0.62238095  0.10520826
   0.625       0.70952381  0.41000149  0.56930375]
 [ 0.53602568  0.8275      0.86666667  0.6631937   0.8175      0.32270163
   0.69166667  0.68095238  0.70611661  0.67059524]
 [ 0.44873062  0.66666667  0.66666667  0.72404787  0.66178571  0.427338
   0.70416667  0.69761905  0.68601733  0.6815873 ]
 [ 0.20984762  0.65833333  0.68333333  0.5722274   0.61941919  0.24111169
   0.52916667  0.52142857  0.53399785  0.50488095]
 [ 0.74798137  0.9275      0.93571429  0.80732551  0.92285714  0.38115385
   0.72666667  0.72857143  0.60486703  0.70472222]
 [ 0.28615385  0.525       0.63095238  0.53940845  0.49256133  0.36236451
   0.615       0.68333333  0.53757879  0.60284271]
 [ 0.16707027  0.55416667  0.5547619   0.46927053  0.50600649  0.38938573
   0.67083333  0.67619048  0.5462761   0.63833333]
 [ 0.18559477  0.54166667  0.55714286  0.45193853  0.52777778  0.54475716
   0.7275      0.7452381   0.61035352  0.71019841]
 [ 0.58745744  0.84833333  0.86666667  0.74959804  0.84980159  0.47418471
   0.7         0.71428571  0.6234052   0.67452381]
 [ 0.34301449  0.66666667  0.67142857  0.52899023  0.65107143  0.24402899
   0.54583333  0.56904762  0.36001796  0.51382395]
 [ 0.12852716  0.59916667  0.66904762  0.2104233   0.59329004  0.36728986
   0.67083333  0.67857143  0.6222274   0.63787157]
 [ 0.1078133   0.5625      0.68333333  0.26855046  0.54308442  0.03909736
   0.47        0.52619048  0.32637102  0.44630952]
 [-0.03478261  0.525       0.5452381   0.20487548  0.50825397  0.35438366
   0.685       0.74047619  0.58746306  0.64916667]
 [ 0.48387291  0.69583333  0.7047619   0.71631284  0.68702381  0.41891432
   0.75416667  0.76904762  0.45473119  0.74134921]
 [ 0.29903038  0.61166667  0.8         0.33761306  0.56456294  0.41807039
   0.70833333  0.72380952  0.63902813  0.70281746]
 [ 0.57757677  0.6825      0.71666667  0.66738204  0.66916667  0.46718283
   0.67916667  0.67619048  0.58343065  0.65940476]
 [ 0.23463406  0.63        0.66666667  0.47717107  0.60906566  0.13563538
   0.68666667  0.71666667  0.35409665  0.65435426]
 [-0.04490561  0.465       0.63333333  0.13223192  0.4719697  -0.01968765
   0.50833333  0.50714286  0.26539265  0.47964286]
 [ 0.11285507  0.6625      0.64761905  0.51110155  0.63285714  0.19811557
   0.51666667  0.58809524  0.39915638  0.46926407]
 [ 0.12539314  0.60416667  0.62380952  0.51251236  0.59531746  0.20764341
   0.61        0.76428571  0.26812031  0.56853535]
 [ 0.0447293   0.47916667  0.47619048  0.50223872  0.45666667  0.4715892
   0.78333333  0.78571429  0.63742459  0.77007937]
 [ 0.38614067  0.56666667  0.57380952  0.56176689  0.52678571  0.61622222
   0.71666667  0.71666667  0.68232632  0.70039683]
 [ 0.23008696  0.57916667  0.58333333  0.59730931  0.55321429  0.14288538
   0.64583333  0.65952381  0.57134313  0.63357143]
 [ 0.49511144  0.695       0.7452381   0.55959761  0.67340909  0.40562451
   0.74583333  0.75714286  0.61740858  0.73543651]
 [-0.26244589  0.49666667  0.5547619   0.39007601  0.45261905  0.51056522
   0.7625      0.78809524  0.66731044  0.7665873 ]
 [ 0.02727743  0.55        0.59285714  0.40668795  0.52162698  0.20716276
   0.55        0.57142857  0.4695168   0.5252381 ]
 [ 0.53615942  0.77333333  0.79047619  0.76199785  0.75529221  0.27401717
   0.70333333  0.71904762  0.60439491  0.70235931]]
RF mean:
[0.28891199 0.63827778 0.67380952 0.53113482 0.61859738 0.31361585
 0.65347222 0.68206349 0.53020441 0.6302286 ]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.52857143  0.          0.34515152  0.
   0.5         0.7452381   0.          0.42636364]
 [ 0.          0.5         0.54285714  0.          0.35151515  0.
   0.5         0.62380952  0.          0.38348485]
 [ 0.          0.5         0.57619048  0.          0.36484848  0.02536232
   0.5125      0.51666667  0.14855986  0.39022727]
 [ 0.          0.5         0.51428571  0.          0.33878788  0.
   0.5         0.71190476  0.          0.41545455]
 [ 0.          0.5         0.65238095  0.          0.39409091  0.36551616
   0.6875      0.68571429  0.65433479  0.66964286]
 [-0.02727273  0.4875      0.5         0.          0.33242424  0.19640504
   0.59166667  0.63571429  0.26783039  0.50184343]
 [ 0.          0.5         0.62380952  0.          0.38348485  0.
   0.5         0.51428571  0.          0.33878788]
 [ 0.          0.5         0.6952381   0.          0.41        0.
   0.5         0.62380952  0.          0.38348485]
 [ 0.          0.5         0.6952381   0.          0.41        0.
   0.5         0.6952381   0.          0.41      ]
 [ 0.          0.5         0.51428571  0.          0.33878788  0.
   0.5         0.52857143  0.          0.34515152]
 [ 0.          0.5         0.60952381  0.          0.37818182  0.
   0.5         0.63809524  0.          0.38878788]
 [ 0.          0.5         0.63809524  0.          0.38878788  0.
   0.5         0.60952381  0.          0.37818182]
 [ 0.          0.5         0.57619048  0.          0.36484848  0.
   0.5         0.57619048  0.          0.36484848]
 [ 0.          0.5         0.65238095  0.          0.39409091  0.
   0.5         0.55952381  0.          0.35818182]
 [ 0.          0.5         0.71190476  0.          0.41545455  0.
   0.5         0.63809524  0.          0.38878788]
 [ 0.          0.5         0.59285714  0.          0.37151515  0.
   0.5         0.68095238  0.          0.40469697]
 [ 0.          0.5         0.57619048  0.          0.36484848  0.
   0.5         0.59285714  0.          0.37151515]
 [ 0.          0.5         0.81904762  0.          0.4497669   0.
   0.5         0.57619048  0.          0.36484848]
 [ 0.          0.5         0.66666667  0.          0.39939394  0.
   0.5         0.51428571  0.          0.33878788]
 [ 0.          0.5         0.65238095  0.          0.39409091  0.
   0.5         0.62380952  0.          0.38348485]
 [ 0.          0.5         0.76190476  0.          0.43181818  0.
   0.5         0.52857143  0.          0.34515152]
 [ 0.          0.5         0.51428571  0.          0.33878788  0.
   0.5         0.65238095  0.          0.39409091]
 [ 0.          0.5         0.55952381  0.          0.35818182  0.
   0.5         0.76190476  0.          0.43181818]
 [ 0.          0.5         0.52857143  0.          0.34515152  0.
   0.5         0.57619048  0.          0.36484848]
 [ 0.          0.5         0.54285714  0.          0.35151515 -0.07944664
   0.4625      0.47142857  0.          0.31828283]
 [ 0.          0.5         0.52857143  0.          0.34515152  0.
   0.5         0.55952381  0.          0.35818182]
 [ 0.          0.5         0.71190476  0.          0.41545455  0.09358586
   0.54583333  0.55952381  0.3121887   0.46820346]
 [ 0.          0.5         0.62380952  0.          0.38348485  0.
   0.5         0.60952381  0.          0.37818182]
 [ 0.          0.5         0.60952381  0.          0.37818182  0.
   0.5         0.55952381  0.          0.35818182]
 [ 0.          0.5         0.62380952  0.          0.38348485  0.
   0.5         0.63809524  0.          0.38878788]]
SVM mean:
[-0.00090909  0.49958333  0.61142857  0.          0.37737607  0.02004742
  0.51        0.60690476  0.04609712  0.39374303]
---------------------------
---------------------------
GBM performance:
[[ 0.53016074  0.77916667  0.78333333  0.74652236  0.76789683  0.09411765
   0.54        0.75952381  0.14142136  0.49530303]
 [ 0.23913043  0.64583333  0.66190476  0.51826352  0.62624459  0.43330981
   0.70833333  0.75238095  0.60151672  0.69410173]
 [ 0.43672596  0.7125      0.74047619  0.63525385  0.69588745  0.25402267
   0.65833333  0.6547619   0.62827016  0.64142857]
 [ 0.03283761  0.53333333  0.52142857  0.44072669  0.50940476  0.03529412
   0.515       0.6952381   0.12071068  0.45584416]
 [ 0.65341615  0.82333333  0.86666667  0.69416339  0.80247475  0.38367001
   0.67916667  0.67857143  0.66917239  0.66916667]
 [ 0.41077835  0.6875      0.69761905  0.66969151  0.67837302  0.46123655
   0.725       0.72857143  0.66767558  0.71718254]
 [ 0.20541961  0.5875      0.6452381   0.47069245  0.57406566  0.19866667
   0.56666667  0.56904762  0.52211793  0.55154762]
 [ 0.76032152  0.84        0.8952381   0.84299611  0.85515152  0.53255515
   0.76583333  0.78333333  0.61796623  0.72987013]
 [ 0.20964925  0.61        0.72619048  0.31284167  0.56161616  0.10420168
   0.605       0.7452381   0.21213203  0.5869697 ]
 [-0.02065527  0.50416667  0.50714286  0.39056915  0.46965729  0.29462582
   0.64583333  0.6547619   0.47228406  0.59694444]
 [ 0.01878035  0.55        0.6047619   0.29216683  0.52213203  0.55888123
   0.72916667  0.78333333  0.6722274   0.71138889]
 [ 0.56666204  0.7775      0.81666667  0.6069291   0.76727994  0.45339107
   0.72916667  0.74761905  0.66114962  0.71134921]
 [ 0.28165349  0.63333333  0.66666667  0.58142797  0.61563492  0.19768933
   0.60833333  0.64047619  0.48538326  0.58402237]
 [ 0.12245425  0.52416667  0.62380952  0.25466691  0.48459596  0.2354207
   0.6125      0.61904762  0.53252564  0.59063492]
 [-0.05836317  0.485       0.65        0.12548293  0.44080808  0.03020892
   0.53583333  0.63571429  0.19717775  0.46414141]
 [ 0.08787879  0.525       0.60952381  0.23094011  0.44522727  0.22390756
   0.6075      0.71190476  0.36590415  0.57315657]
 [ 0.37370807  0.66666667  0.69761905  0.62666389  0.6518254   0.33725598
   0.67916667  0.70952381  0.6222782   0.67380952]
 [-0.04352941  0.48666667  0.72857143  0.07745967  0.44259324  0.35720083
   0.67083333  0.68571429  0.61964893  0.64472222]
 [ 0.15978257  0.55166667  0.6547619   0.25392864  0.52767677  0.21585548
   0.6125      0.60952381  0.55427812  0.58988095]
 [ 0.19481167  0.5925      0.66428571  0.43430826  0.5835101   0.17601277
   0.57666667  0.62619048  0.4335613   0.55380952]
 [-0.02352941  0.49        0.74761905  0.          0.42651515 -0.04113341
   0.4625      0.4547619   0.43347872  0.43285714]
 [ 0.31917651  0.62916667  0.61904762  0.55021064  0.59996032  0.09972106
   0.54416667  0.63571429  0.27334092  0.48630952]
 [ 0.11621313  0.54166667  0.57142857  0.31641099  0.48493506  0.05882353
   0.525       0.77619048  0.07071068  0.46893939]
 [-0.021       0.47083333  0.4547619   0.34610997  0.41996032  0.28293352
   0.64583333  0.65        0.66589103  0.61849206]
 [ 0.24824067  0.64166667  0.64761905  0.50191779  0.61242063  0.46456022
   0.73333333  0.72857143  0.67404787  0.71047619]
 [ 0.58916074  0.79166667  0.8         0.74164688  0.78115079  0.23442029
   0.59583333  0.61190476  0.51380712  0.57619048]
 [ 0.23317866  0.655       0.78571429  0.33086408  0.61479798  0.42285639
   0.7375      0.7547619   0.67051439  0.71996032]
 [-0.02299266  0.4775      0.5547619   0.22871561  0.43969697  0.3901421
   0.7         0.74285714  0.6295555   0.69441919]
 [-0.10159514  0.45416667  0.53571429  0.11123724  0.38411616  0.3958495
   0.68333333  0.70238095  0.63889406  0.66964286]
 [ 0.54309641  0.7775      0.79761905  0.6966979   0.76164141  0.12060102
   0.555       0.6452381   0.29725555  0.51173521]]
GBM mean:
[0.23471906 0.61483333 0.67587302 0.43431687 0.58490835 0.26687661
 0.63177778 0.68309524 0.48882991 0.60414322]
---------------------------
---------------------------
BDDAE performance:
[[-0.07142857  0.46428571  0.46428571  0.44110826  0.45304761  0.1901914
   0.59        0.71428571  0.41837383  0.57413679]
 [ 0.0564888   0.52708333  0.52857143  0.51787528  0.5219235   0.42810624
   0.72777778  0.70714286  0.70917357  0.69809317]
 [ 0.03742719  0.51875     0.53571429  0.47445209  0.50348524  0.32857143
   0.66428571  0.66428571  0.63811761  0.65212056]
 [-0.15714286  0.42142857  0.42142857  0.28673432  0.38011407  0.07712528
   0.5275      0.65714286  0.36287396  0.51919239]
 [ 0.08784517  0.55222222  0.57857143  0.48911921  0.5335582   0.21428571
   0.60714286  0.60714286  0.59184314  0.60011294]
 [ 0.45714286  0.72857143  0.72857143  0.71635235  0.72380189 -0.12518874
   0.43541667  0.43571429  0.3926031   0.41251607]
 [ 0.04123492  0.52111111  0.57857143  0.38724427  0.49141089  0.24285714
   0.62142857  0.62142857  0.59055975  0.60685426]
 [ 0.31970958  0.65        0.73571429  0.61020189  0.65647775 -0.02763128
   0.48777778  0.53571429  0.43427476  0.47702967]
 [ 0.41166036  0.6875      0.77857143  0.63969331  0.69832092  0.64069952
   0.7875      0.87857143  0.71585541  0.80919913]
 [-0.12857143  0.43571429  0.43571429  0.40526283  0.42157485  0.2
   0.6         0.6         0.57865943  0.58999254]
 [ 0.11582675  0.55416667  0.6         0.38982925  0.510367   -0.0830723
   0.46333333  0.57857143  0.1202184   0.40201644]
 [ 0.19380497  0.59333333  0.64285714  0.5410007   0.58388342  0.48564548
   0.74791667  0.74285714  0.73616411  0.73744896]
 [-0.12573271  0.4375      0.45        0.38657348  0.42531029  0.33326528
   0.66875     0.66428571  0.65203595  0.65614706]
 [ 0.10978305  0.55333333  0.58571429  0.51365157  0.5416984   0.14403948
   0.575       0.56428571  0.55436648  0.55696061]
 [-0.00182629  0.5         0.56428571  0.42006907  0.4921124   0.04754361
   0.51888889  0.56428571  0.44270795  0.50565371]
 [ 0.14744867  0.57083333  0.6         0.49470609  0.54801258  0.09688835
   0.5375      0.69285714  0.29599487  0.51244585]
 [ 0.42992214  0.72291667  0.72142857  0.65550947  0.69511337  0.26248758
   0.62708333  0.66428571  0.51491415  0.5973692 ]
 [ 0.47529568  0.74848485  0.81428571  0.72517458  0.73400856  0.09445172
   0.54583333  0.57142857  0.46810851  0.52830686]
 [ 0.30303832  0.64111111  0.69285714  0.60460785  0.64464553  0.25714286
   0.62857143  0.62857143  0.61385358  0.62210787]
 [ 0.25566001  0.62222222  0.68571429  0.49385117  0.60139054  0.01752347
   0.50555556  0.56428571  0.36466805  0.48651143]
 [-0.05232158  0.47424242  0.65        0.22077982  0.46083555  0.11428571
   0.55714286  0.55714286  0.52119979  0.54001928]
 [ 0.1         0.55        0.55        0.50693066  0.53012146 -0.02700973
   0.49222222  0.59285714  0.20276888  0.44094254]
 [ 0.06934212  0.53541667  0.54285714  0.50958149  0.52492451  0.57046186
   0.7530303   0.87857143  0.7049874   0.78003953]
 [-0.1         0.45        0.45        0.43640767  0.44395732  0.04441174
   0.52083333  0.54285714  0.49070211  0.51562297]
 [-0.02724231  0.4875      0.49285714  0.46039346  0.47491124  0.08571429
   0.54285714  0.54285714  0.51532918  0.52955452]
 [ 0.11428571  0.55714286  0.55714286  0.51249784  0.53580941  0.00907777
   0.50416667  0.51428571  0.48520273  0.49778381]
 [ 0.48266575  0.7675      0.76428571  0.76380549  0.736759    0.3
   0.65        0.65        0.62278273  0.63791065]
 [ 0.06392182  0.53444444  0.55        0.47401066  0.51307837  0.35310716
   0.66875     0.7         0.62087097  0.66146199]
 [-0.09179724  0.45416667  0.47142857  0.40112878  0.443509    0.23463095
   0.6125      0.64285714  0.56252121  0.60229954]
 [ 0.08396582  0.54111111  0.59285714  0.49272623  0.5338791  -0.07068462
   0.46777778  0.52142857  0.34082507  0.45083242]]
BDDAE mean:
[0.12001356 0.56006975 0.59214286 0.49904264 0.54526807 0.18129758
 0.58788474 0.62666667 0.50875189 0.57335609]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.52857143 0.         0.34515152 0.
  0.5        0.7452381  0.         0.42636364]
 [0.         0.5        0.54285714 0.         0.35151515 0.
  0.5        0.62380952 0.         0.38348485]
 [0.         0.5        0.57619048 0.         0.36484848 0.
  0.5        0.51428571 0.         0.33878788]
 [0.         0.5        0.51428571 0.         0.33878788 0.
  0.5        0.71190476 0.         0.41545455]
 [0.         0.5        0.65238095 0.         0.39409091 0.
  0.5        0.51428571 0.         0.33878788]
 [0.         0.5        0.51428571 0.         0.33878788 0.
  0.5        0.55952381 0.         0.35818182]
 [0.         0.5        0.62380952 0.         0.38348485 0.
  0.5        0.51428571 0.         0.33878788]
 [0.         0.5        0.6952381  0.         0.41       0.
  0.5        0.62380952 0.         0.38348485]
 [0.         0.5        0.6952381  0.         0.41       0.
  0.5        0.6952381  0.         0.41      ]
 [0.         0.5        0.51428571 0.         0.33878788 0.
  0.5        0.52857143 0.         0.34515152]
 [0.         0.5        0.60952381 0.         0.37818182 0.
  0.5        0.63809524 0.         0.38878788]
 [0.         0.5        0.63809524 0.         0.38878788 0.
  0.5        0.60952381 0.         0.37818182]
 [0.         0.5        0.57619048 0.         0.36484848 0.
  0.5        0.57619048 0.         0.36484848]
 [0.         0.5        0.65238095 0.         0.39409091 0.
  0.5        0.55952381 0.         0.35818182]
 [0.         0.5        0.71190476 0.         0.41545455 0.
  0.5        0.63809524 0.         0.38878788]
 [0.         0.5        0.59285714 0.         0.37151515 0.
  0.5        0.68095238 0.         0.40469697]
 [0.         0.5        0.57619048 0.         0.36484848 0.
  0.5        0.59285714 0.         0.37151515]
 [0.         0.5        0.81904762 0.         0.4497669  0.
  0.5        0.57619048 0.         0.36484848]
 [0.         0.5        0.66666667 0.         0.39939394 0.
  0.5        0.51428571 0.         0.33878788]
 [0.         0.5        0.65238095 0.         0.39409091 0.
  0.5        0.62380952 0.         0.38348485]
 [0.         0.5        0.76190476 0.         0.43181818 0.
  0.5        0.52857143 0.         0.34515152]
 [0.         0.5        0.51428571 0.         0.33878788 0.
  0.5        0.65238095 0.         0.39409091]
 [0.         0.5        0.55952381 0.         0.35818182 0.
  0.5        0.76190476 0.         0.43181818]
 [0.         0.5        0.52857143 0.         0.34515152 0.
  0.5        0.57619048 0.         0.36484848]
 [0.         0.5        0.54285714 0.         0.35151515 0.
  0.5        0.51428571 0.         0.33878788]
 [0.         0.5        0.52857143 0.         0.34515152 0.
  0.5        0.55952381 0.         0.35818182]
 [0.         0.5        0.71190476 0.         0.41545455 0.
  0.5        0.51428571 0.         0.33878788]
 [0.         0.5        0.62380952 0.         0.38348485 0.
  0.5        0.60952381 0.         0.37818182]
 [0.         0.5        0.60952381 0.         0.37818182 0.
  0.5        0.55952381 0.         0.35818182]
 [0.         0.5        0.62380952 0.         0.38348485 0.
  0.5        0.63809524 0.         0.38878788]]
DUMMY mean:
[0.         0.5        0.61190476 0.         0.37758819 0.
 0.5        0.59849206 0.         0.37258081]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_43_1
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.283  0.641  0.682  0.53   0.62   0.293  0.646  0.681  0.55   0.627]
 [ 0.23   0.61   0.638  0.53   0.589  0.301  0.641  0.661  0.568  0.619]
 [ 0.289  0.638  0.674  0.531  0.619  0.314  0.653  0.682  0.53   0.63 ]
 [-0.001  0.5    0.611  0.     0.377  0.02   0.51   0.607  0.046  0.394]
 [ 0.235  0.615  0.676  0.434  0.585  0.267  0.632  0.683  0.489  0.604]
 [ 0.12   0.56   0.592  0.499  0.545  0.181  0.588  0.627  0.509  0.573]
 [ 0.     0.5    0.612  0.     0.378  0.     0.5    0.598  0.     0.373]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.216 0.109 0.096 0.179 0.113 0.151 0.076 0.068 0.141 0.08 ]
 [0.225 0.107 0.103 0.152 0.113 0.167 0.082 0.079 0.13  0.082]
 [0.227 0.109 0.104 0.172 0.115 0.153 0.082 0.077 0.127 0.089]
 [0.005 0.002 0.079 0.    0.03  0.077 0.038 0.07  0.136 0.063]
 [0.235 0.114 0.106 0.219 0.13  0.159 0.078 0.072 0.188 0.09 ]
 [0.189 0.096 0.107 0.12  0.098 0.189 0.091 0.096 0.149 0.1  ]
 [0.    0.    0.078 0.    0.029 0.    0.    0.07  0.    0.027]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[  76.   17.   14.   34.   18.   52.   12.   10.   26.   13.]
 [  98.   18.   16.   29.   19.   56.   13.   12.   23.   13.]
 [  79.   17.   15.   32.   19.   49.   13.   11.   24.   14.]
 [-550.    0.   13.    0.    8.  384.    7.   12.  295.   16.]
 [ 100.   19.   16.   50.   22.   60.   12.   11.   38.   15.]
 [ 157.   17.   18.   24.   18.  104.   15.   15.   29.   17.]
 [   0.    0.   13.    0.    8.    0.    0.   12.    0.    7.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_43_1
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  18.0
step (sec):  18.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  66
Elapsed time: 553.7694636901219 minutes
Elapsed time: 9.229491061502033 hours
