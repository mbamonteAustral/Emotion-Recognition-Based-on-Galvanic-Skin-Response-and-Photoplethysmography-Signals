2024-05-12 15:38:34.156795: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-12 15:38:35.007085: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-12 15:38:36.746433: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  18.0
step (sec):  13.5
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  4.5
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_43_2\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:


 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:08[0m 1s/step - loss: 0.0778 - mean_squared_error: 0.0778
[1m 5/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0719 - mean_squared_error: 0.0719
[1m10/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0658 - mean_squared_error: 0.0658
[1m15/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0634 - mean_squared_error: 0.0634
[1m20/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0609 - mean_squared_error: 0.0609
[1m25/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0580 - mean_squared_error: 0.0580
[1m29/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0559 - mean_squared_error: 0.0559
[1m34/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0537 - mean_squared_error: 0.0537
[1m39/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0518 - mean_squared_error: 0.0518
[1m45/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0499 - mean_squared_error: 0.0499
[1m50/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0485 - mean_squared_error: 0.0485
[1m55/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0474 - mean_squared_error: 0.0474
[1m60/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 12ms/step - loss: 0.0464 - mean_squared_error: 0.0464
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 16ms/step - loss: 0.0456 - mean_squared_error: 0.0456 - val_loss: 0.0177 - val_mean_squared_error: 0.0177
Epoch 2/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - loss: 0.0077 - mean_squared_error: 0.0077
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0170 - mean_squared_error: 0.0170
[1m11/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0174 - mean_squared_error: 0.0174
[1m15/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0178 - mean_squared_error: 0.0178
[1m21/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0180 - mean_squared_error: 0.0180
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0178 - mean_squared_error: 0.0178
[1m31/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0177 - mean_squared_error: 0.0177
[1m37/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0177 - mean_squared_error: 0.0177
[1m41/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0179 - mean_squared_error: 0.0179
[1m46/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0182 - mean_squared_error: 0.0182
[1m51/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0184 - mean_squared_error: 0.0184
[1m57/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m62/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 12ms/step - loss: 0.0188 - mean_squared_error: 0.0188
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - loss: 0.0188 - mean_squared_error: 0.0188 - val_loss: 0.0142 - val_mean_squared_error: 0.0142
Epoch 3/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 25ms/step - loss: 0.0025 - mean_squared_error: 0.0025
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0212 - mean_squared_error: 0.0212
[1m11/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0217 - mean_squared_error: 0.0217
[1m15/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0206 - mean_squared_error: 0.0206
[1m20/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0205 - mean_squared_error: 0.0205
[1m25/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0203 - mean_squared_error: 0.0203
[1m31/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0206 - mean_squared_error: 0.0206
[1m36/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0206 - mean_squared_error: 0.0206
[1m41/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0208 - mean_squared_error: 0.0208
[1m47/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0209 - mean_squared_error: 0.0209
[1m52/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0208 - mean_squared_error: 0.0208
[1m58/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - loss: 0.0207 - mean_squared_error: 0.0207
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 12ms/step - loss: 0.0206 - mean_squared_error: 0.0206
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0148 - val_mean_squared_error: 0.0148
Epoch 4/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0077 - mean_squared_error: 0.0077
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0146 - mean_squared_error: 0.0146
[1m11/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0171 - mean_squared_error: 0.0171
[1m17/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0180 - mean_squared_error: 0.0180
[1m22/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0185 - mean_squared_error: 0.0185
[1m27/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m32/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0188 - mean_squared_error: 0.0188
[1m38/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m43/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m48/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0186 - mean_squared_error: 0.0186
[1m52/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0186 - mean_squared_error: 0.0186
[1m58/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 12ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0133 - val_mean_squared_error: 0.0133
Epoch 5/5

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0016 - mean_squared_error: 0.0016
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0061 - mean_squared_error: 0.0061
[1m11/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0105 - mean_squared_error: 0.0105
[1m16/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0138 - mean_squared_error: 0.0138
[1m21/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0150 - mean_squared_error: 0.0150
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0159 - mean_squared_error: 0.0159
[1m32/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0168 - mean_squared_error: 0.0168
[1m36/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0172 - mean_squared_error: 0.0172
[1m40/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0175 - mean_squared_error: 0.0175
[1m46/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0178 - mean_squared_error: 0.0178
[1m51/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0179 - mean_squared_error: 0.0179
[1m56/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 12ms/step - loss: 0.0180 - mean_squared_error: 0.0180
[1m61/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 12ms/step - loss: 0.0181 - mean_squared_error: 0.0181
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - loss: 0.0181 - mean_squared_error: 0.0181 - val_loss: 0.0139 - val_mean_squared_error: 0.0139
(18000, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 18000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 4500, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 4500, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1125, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 18000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 18000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 1125, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 1125, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 1125, 12)  â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 1125)  â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 13500)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 13500)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     13,501 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 14,323 (55.95 KB)
 Trainable params: 14,323 (55.95 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:17[0m 1s/step - binary_accuracy: 1.0000 - loss: 0.4997
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7806 - loss: 0.6538
[1m11/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7593 - loss: 0.6574
[1m16/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7549 - loss: 0.6472
[1m22/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7424 - loss: 0.6565
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7443 - loss: 0.6537
[1m31/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7423 - loss: 0.6541
[1m37/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7417 - loss: 0.6524
[1m41/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7410 - loss: 0.6508
[1m46/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7385 - loss: 0.6489
[1m51/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7358 - loss: 0.6471
[1m56/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7320 - loss: 0.6467
[1m62/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7260 - loss: 0.6475
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 15ms/step - binary_accuracy: 0.7236 - loss: 0.6479 - val_binary_accuracy: 0.7143 - val_loss: 0.6074
Epoch 2/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.6113
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5389 - loss: 0.7049
[1m12/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.6061 - loss: 0.6731
[1m17/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.6626 - loss: 0.6373
[1m23/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7122 - loss: 0.5964
[1m28/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7364 - loss: 0.5787
[1m34/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7485 - loss: 0.5760
[1m39/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7462 - loss: 0.5850
[1m44/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7415 - loss: 0.5926
[1m49/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.7379 - loss: 0.5980
[1m54/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7362 - loss: 0.6021
[1m59/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7359 - loss: 0.6047
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.7363 - loss: 0.6067 - val_binary_accuracy: 0.8571 - val_loss: 0.5761
Epoch 3/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4984
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 1.0000 - loss: 0.5197
[1m11/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9612 - loss: 0.5284
[1m17/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9239 - loss: 0.5490
[1m22/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9004 - loss: 0.5565
[1m28/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8729 - loss: 0.5614
[1m34/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8560 - loss: 0.5637
[1m39/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8468 - loss: 0.5638
[1m45/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8328 - loss: 0.5673
[1m50/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8245 - loss: 0.5677
[1m56/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8166 - loss: 0.5672
[1m61/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8108 - loss: 0.5670
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.8077 - loss: 0.5669 - val_binary_accuracy: 0.7143 - val_loss: 0.5474
Epoch 4/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2599
[1m 7/63[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 1.0000 - loss: 0.2903
[1m12/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9201 - loss: 0.3953
[1m17/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8842 - loss: 0.4507
[1m23/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8701 - loss: 0.4696
[1m28/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8617 - loss: 0.4796
[1m33/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8582 - loss: 0.4838
[1m38/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8579 - loss: 0.4831
[1m44/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8570 - loss: 0.4841
[1m50/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8519 - loss: 0.4852
[1m55/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8451 - loss: 0.4886
[1m60/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8399 - loss: 0.4916
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.8354 - loss: 0.4944 - val_binary_accuracy: 0.5714 - val_loss: 0.5698
Epoch 5/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.5581
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8417 - loss: 0.5261
[1m12/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8664 - loss: 0.5061
[1m18/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8861 - loss: 0.4734
[1m24/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8912 - loss: 0.4622
[1m29/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8857 - loss: 0.4611
[1m34/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8832 - loss: 0.4568
[1m40/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8728 - loss: 0.4589
[1m45/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8606 - loss: 0.4641
[1m51/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8483 - loss: 0.4695
[1m56/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8403 - loss: 0.4739
[1m61/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8354 - loss: 0.4762
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.8321 - loss: 0.4778 - val_binary_accuracy: 0.8571 - val_loss: 0.5447
Epoch 6/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3179
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 1.0000 - loss: 0.2776
[1m12/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9771 - loss: 0.3042
[1m17/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9530 - loss: 0.3097
[1m22/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9294 - loss: 0.3297
[1m28/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9099 - loss: 0.3440
[1m32/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8965 - loss: 0.3527
[1m38/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8861 - loss: 0.3589
[1m43/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8823 - loss: 0.3602
[1m49/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8774 - loss: 0.3638
[1m54/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8735 - loss: 0.3668
[1m59/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8699 - loss: 0.3698
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.8665 - loss: 0.3741 - val_binary_accuracy: 0.8571 - val_loss: 0.5493
Epoch 7/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0853
[1m 5/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1395
[1m10/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.8539 - loss: 0.3180
[1m15/63[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8248 - loss: 0.3702
[1m21/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8183 - loss: 0.3958
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8116 - loss: 0.4110
[1m32/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8034 - loss: 0.4238
[1m37/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8022 - loss: 0.4295
[1m42/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7995 - loss: 0.4335
[1m48/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7980 - loss: 0.4379
[1m53/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7986 - loss: 0.4384
[1m59/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8002 - loss: 0.4369
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.8019 - loss: 0.4350 - val_binary_accuracy: 0.8571 - val_loss: 0.5788
Epoch 8/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 33ms/step - binary_accuracy: 1.0000 - loss: 0.2105
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9722 - loss: 0.2095
[1m11/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9330 - loss: 0.2334
[1m16/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9314 - loss: 0.2289
[1m21/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9127 - loss: 0.2441
[1m26/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9008 - loss: 0.2621
[1m31/63[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8945 - loss: 0.2758
[1m35/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8929 - loss: 0.2809
[1m40/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8905 - loss: 0.2877
[1m44/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8877 - loss: 0.2941
[1m48/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8863 - loss: 0.2995
[1m53/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8840 - loss: 0.3052
[1m58/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8832 - loss: 0.3087
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8816 - loss: 0.3124
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.8812 - loss: 0.3131 - val_binary_accuracy: 0.7143 - val_loss: 0.6228
Epoch 9/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 34ms/step - binary_accuracy: 1.0000 - loss: 0.0593
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 1.0000 - loss: 0.0978
[1m11/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 1.0000 - loss: 0.1464
[1m17/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 1.0000 - loss: 0.1824
[1m23/63[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 1.0000 - loss: 0.1975
[1m28/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9933 - loss: 0.2119
[1m33/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9827 - loss: 0.2282
[1m38/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9711 - loss: 0.2458
[1m44/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9619 - loss: 0.2600
[1m49/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9545 - loss: 0.2691
[1m55/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9470 - loss: 0.2778
[1m61/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.9410 - loss: 0.2839
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.9385 - loss: 0.2867 - val_binary_accuracy: 0.7143 - val_loss: 0.6358
Epoch 10/10

[1m 1/63[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3179
[1m 6/63[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 1.0000 - loss: 0.1964
[1m11/63[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9612 - loss: 0.2249
[1m16/63[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9334 - loss: 0.2617
[1m22/63[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9234 - loss: 0.2726
[1m27/63[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9227 - loss: 0.2730
[1m32/63[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9243 - loss: 0.2714
[1m38/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9274 - loss: 0.2688
[1m44/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9307 - loss: 0.2670
[1m48/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9329 - loss: 0.2659
[1m53/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9348 - loss: 0.2646
[1m59/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9327 - loss: 0.2662
[1m63/63[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 12ms/step - binary_accuracy: 0.9317 - loss: 0.2673 - val_binary_accuracy: 0.8571 - val_loss: 0.6617

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.8440618  0.8034397  0.78471386 0.33280307 0.05563242 0.23307782
 0.94718546 0.706367   0.8788811  0.04414741 0.11396181 0.7635049
 0.68509895 0.7543416  0.60029566 0.48289067 0.750151   0.81349957]
predicted [1 1 1 0 0 0 1 1 1 0 0 1 1 1 1 0 1 1]
expected [False False  True  True False  True  True  True  True False False  True
  True  True False  True  True False]
accuracy: 0.6111111111111112
confusion matrix: 
[[3 4]
 [3 8]]
              precision    recall  f1-score   support

       False       0.50      0.43      0.46         7
        True       0.67      0.73      0.70        11

    accuracy                           0.61        18
   macro avg       0.58      0.58      0.58        18
weighted avg       0.60      0.61      0.60        18

macro avg f1-score: 0.5785953177257526
macro avg (UAR): 0.577922077922078
Sensitivity:  0.42857142857142855
Specificity:  0.7272727272727273
g-mean:  0.5582905262390825
-------- Model Performance ----------: 
accuracy:  [0.66666667 0.61111111 0.55555556 0.66666667 0.77777778 0.66666667
 0.61111111 0.66666667 0.61111111 0.61111111]
gmean:  [0.67419986 0.48349378 0.55829053 0.64465837 0.76447079 0.64465837
 0.60302269 0.59215653 0.55829053 0.55829053]
f1_score:  [0.6625     0.54181818 0.55       0.64935065 0.76623377 0.64935065
 0.6        0.625      0.57859532 0.57859532]
UAR:  [0.67532468 0.55194805 0.55844156 0.64935065 0.76623377 0.64935065
 0.6038961  0.62337662 0.57792208 0.57792208]
Cohen Kappa score:  [0.33333333 0.11267606 0.11111111 0.2987013  0.53246753 0.2987013
 0.20253165 0.26027397 0.16       0.16      ]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  18.0
step (sec):  13.504
overlap:  True
perc. of overlap:  24.977777777777778
overlap duration (sec):  4.496
Number of windows / instances:  88
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.763 0.884 0.882 0.876 0.878 0.275 0.639 0.657 0.592 0.626]
 [0.592 0.786 0.806 0.79  0.782 0.513 0.782 0.803 0.786 0.78 ]
 [0.612 0.841 0.854 0.766 0.84  0.558 0.762 0.786 0.691 0.758]
 [0.118 0.554 0.649 0.178 0.466 0.    0.5   0.647 0.    0.393]
 [0.576 0.78  0.782 0.776 0.766 0.524 0.778 0.829 0.7   0.778]
 [0.247 0.623 0.644 0.608 0.62  0.102 0.558 0.606 0.452 0.534]
 [0.    0.5   0.603 0.    0.375 0.    0.5   0.647 0.    0.393]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.45894448  0.725       0.73472222  0.70124497  0.72094156  0.29093168
   0.62738095  0.76944444  0.45986803  0.6239881 ]
 [ 0.42789411  0.715       0.71527778  0.70250368  0.70877345  0.5264411
   0.7625      0.78472222  0.73707031  0.75634921]
 [ 0.49832266  0.74416667  0.7625      0.70399329  0.73333916  0.35169751
   0.675       0.67916667  0.66436433  0.67231962]
 [ 0.33570314  0.6675      0.67083333  0.64253804  0.65734488  0.22695652
   0.59404762  0.76388889  0.34057774  0.58629579]
 [ 0.5950092   0.78833333  0.82777778  0.71978555  0.78173965  0.59196271
   0.8025      0.7875      0.77076089  0.7780267 ]
 [ 0.39220789  0.7025      0.69305556  0.67333933  0.68205267  0.56361909
   0.785       0.78194444  0.76141681  0.77145577]
 [ 0.31549147  0.65583333  0.66527778  0.63698199  0.65073232  0.3988996
   0.7025      0.69444444  0.68311356  0.68792208]
 [ 0.73380619  0.85833333  0.89861111  0.79397109  0.85393107  0.72172843
   0.86833333  0.86388889  0.86219362  0.85837607]
 [ 0.16230567  0.57619048  0.69444444  0.40710508  0.56287546  0.24561772
   0.61666667  0.69305556  0.5166335   0.61478854]
 [ 0.09086924  0.545       0.55555556  0.46664898  0.52760545  0.25484962
   0.6275      0.63611111  0.57560656  0.615     ]
 [ 0.25736541  0.63083333  0.63888889  0.59647647  0.61521062  0.33916426
   0.6725      0.6875      0.55697402  0.6352886 ]
 [ 0.72255665  0.8525      0.87638889  0.83369686  0.8569006   0.56667738
   0.785       0.79861111  0.7656783   0.77772866]
 [ 0.44160121  0.72        0.73055556  0.68677512  0.70812798  0.31545512
   0.6575      0.67083333  0.60606343  0.64537962]
 [ 0.2837296   0.63166667  0.68472222  0.58452343  0.63672161  0.46050974
   0.7275      0.73194444  0.70156023  0.71949856]
 [ 0.09770939  0.5452381   0.64583333  0.37092685  0.52514819  0.28548785
   0.64666667  0.69166667  0.58815334  0.63609807]
 [ 0.27037399  0.62666667  0.66944444  0.54476379  0.61569764  0.40874126
   0.69        0.75972222  0.61883033  0.69490842]
 [ 0.29972354  0.65416667  0.65694444  0.64311805  0.64582251  0.43490757
   0.70666667  0.73888889  0.66839584  0.70551365]
 [ 0.07607143  0.55089286  0.77361111  0.19258201  0.51583172  0.47457957
   0.74083333  0.74861111  0.71767112  0.72889083]
 [ 0.3327972   0.65833333  0.72777778  0.59465316  0.66082584  0.31720582
   0.6575      0.65694444  0.63724858  0.64782107]
 [ 0.39851482  0.70333333  0.7375      0.62868215  0.68469281  0.2622786
   0.6325      0.67916667  0.52611178  0.61263903]
 [-0.00421984  0.49761905  0.65972222  0.26140284  0.49413919  0.07571838
   0.54        0.54583333  0.49415505  0.52756494]
 [ 0.19191638  0.5975      0.59444444  0.58814138  0.59059524  0.41839161
   0.705       0.75        0.64510196  0.70202964]
 [ 0.21171664  0.60333333  0.62083333  0.55387436  0.59481796  0.1662306
   0.57380952  0.73888889  0.32809733  0.55674908]
 [ 0.14502003  0.5725      0.58194444  0.50760626  0.55328366  0.53671846
   0.76833333  0.78472222  0.70719396  0.75379593]
 [ 0.31323019  0.65666667  0.67361111  0.58615736  0.63694805  0.6374486
   0.82        0.81944444  0.79505455  0.80797813]
 [ 0.303504    0.6475      0.65555556  0.63390183  0.64629509  0.44995759
   0.7225      0.7375      0.7029396   0.72031385]
 [ 0.45604396  0.73333333  0.78611111  0.61804485  0.70435731  0.59153332
   0.8         0.79166667  0.77993019  0.78588023]
 [ 0.25407885  0.6275      0.66805556  0.53652905  0.61560273  0.43985229
   0.71666667  0.72638889  0.69960104  0.71124459]
 [-0.03545788  0.48333333  0.55833333  0.31220734  0.46151515  0.31627296
   0.6575      0.67222222  0.58788671  0.63911894]
 [ 0.76278316  0.88416667  0.88194444  0.8758598   0.87775919  0.27525641
   0.63916667  0.65694444  0.59183166  0.62585137]]
KNN mean:
[0.32632043 0.66183135 0.70134259 0.58660117 0.65065429 0.39816971
 0.69736905 0.72805556 0.63633615 0.68662717]
---------------------------
---------------------------
DT performance:
[[ 4.39470228e-01  7.42500000e-01  7.47222222e-01  6.80902577e-01
   7.41259019e-01  4.37309075e-01  6.72619048e-01  7.63888889e-01
   5.59734600e-01  6.78423243e-01]
 [ 2.44240644e-01  6.25000000e-01  6.27777778e-01  5.85828763e-01
   6.20873016e-01  3.91541879e-01  7.06666667e-01  7.25000000e-01
   6.73163485e-01  6.98686314e-01]
 [ 3.05429357e-01  6.64166667e-01  6.72222222e-01  6.90846619e-01
   6.57326840e-01  3.36748281e-01  6.52500000e-01  6.59722222e-01
   6.46007883e-01  6.37929570e-01]
 [ 2.80800833e-01  6.17500000e-01  6.15277778e-01  6.01108349e-01
   6.01511544e-01  1.34209404e-01  5.83333333e-01  6.72222222e-01
   4.28040165e-01  5.57942474e-01]
 [ 4.86838649e-01  7.27500000e-01  7.47222222e-01  7.37668930e-01
   7.23141581e-01  5.22738789e-01  7.75000000e-01  7.75000000e-01
   6.93822877e-01  7.72478355e-01]
 [ 4.84386589e-01  7.60000000e-01  7.51388889e-01  7.39801941e-01
   7.41316739e-01  5.62601940e-01  8.10000000e-01  8.05555556e-01
   7.80032143e-01  8.00873016e-01]
 [ 8.07889600e-02  5.73333333e-01  5.72222222e-01  5.38097417e-01
   5.55970418e-01  2.33747764e-01  6.50000000e-01  6.45833333e-01
   6.21711613e-01  6.38376623e-01]
 [ 8.44755245e-01  9.08333333e-01  9.11111111e-01  9.09172818e-01
   9.01431069e-01  3.60034965e-01  7.18333333e-01  7.23611111e-01
   7.11738566e-01  7.06355866e-01]
 [ 3.24677077e-01  6.53571429e-01  6.93055556e-01  6.13848514e-01
   6.42171162e-01  7.23076923e-02  5.50000000e-01  6.12500000e-01
   4.07957657e-01  5.32494727e-01]
 [ 1.95783549e-01  5.30000000e-01  5.31944444e-01  6.00096347e-01
   5.24018759e-01  7.41476455e-02  5.47500000e-01  5.45833333e-01
   5.03966763e-01  5.20854978e-01]
 [ 1.38758031e-01  6.25833333e-01  6.29166667e-01  5.93241070e-01
   6.01315074e-01  4.31894934e-01  7.31666667e-01  7.31944444e-01
   7.28596684e-01  7.23686869e-01]
 [ 6.46004246e-01  8.10000000e-01  8.20833333e-01  8.61609578e-01
   8.04991675e-01  5.16671593e-01  7.43333333e-01  7.54166667e-01
   7.15578155e-01  7.26103619e-01]
 [ 2.63016550e-01  6.42500000e-01  6.48611111e-01  5.30407989e-01
   6.27489177e-01  2.28510700e-01  6.39166667e-01  6.48611111e-01
   5.28767460e-01  6.31212121e-01]
 [ 1.64055944e-01  6.38333333e-01  6.93055556e-01  4.72626911e-01
   6.13892774e-01  2.70874482e-01  5.80000000e-01  5.75000000e-01
   5.80934154e-01  5.56944444e-01]
 [ 3.43570640e-01  6.13095238e-01  6.70833333e-01  5.54436031e-01
   6.01350316e-01  4.00109890e-01  7.23333333e-01  7.47222222e-01
   5.18772606e-01  7.01554834e-01]
 [ 2.12130491e-01  5.82500000e-01  5.91666667e-01  5.78238639e-01
   5.74507715e-01  2.88508159e-01  7.00000000e-01  7.38888889e-01
   6.26006056e-01  6.88453213e-01]
 [ 6.52651101e-01  8.06666667e-01  8.09722222e-01  8.06207918e-01
   7.95676546e-01  5.72182980e-01  7.65000000e-01  7.73611111e-01
   7.67827241e-01  7.55927128e-01]
 [-1.23300098e-02  4.83035714e-01  7.38888889e-01  1.64252113e-01
   4.74890110e-01  2.85097616e-01  6.27500000e-01  6.36111111e-01
   5.69352971e-01  5.94596237e-01]
 [ 2.45228105e-01  6.00000000e-01  6.69444444e-01  5.14786316e-01
   5.87977578e-01  4.91059191e-01  7.32500000e-01  7.38888889e-01
   7.24029429e-01  7.27965368e-01]
 [ 2.68121878e-01  6.45833333e-01  6.80555556e-01  6.34123627e-01
   6.29293207e-01  1.81035631e-01  6.16666667e-01  6.45833333e-01
   5.10343043e-01  6.00496170e-01]
 [-7.45341615e-04  4.88095238e-01  6.44444444e-01  2.84969242e-01
   4.72890859e-01  7.63133208e-02  5.85000000e-01  5.91666667e-01
   5.31078641e-01  5.78059163e-01]
 [ 1.80513298e-01  5.97500000e-01  5.97222222e-01  6.19715524e-01
   5.76310356e-01  3.37009657e-01  6.65000000e-01  7.02777778e-01
   6.03850715e-01  6.62777223e-01]
 [ 1.93145939e-01  5.82500000e-01  5.88888889e-01  5.23152629e-01
   5.75660173e-01  1.41476929e-01  5.65476190e-01  7.02777778e-01
   4.42245041e-01  5.31830669e-01]
 [ 1.50666784e-01  6.37500000e-01  6.37500000e-01  6.20775580e-01
   6.16071429e-01  4.40639282e-01  7.42500000e-01  7.50000000e-01
   7.28523082e-01  7.30081308e-01]
 [ 4.04502814e-01  7.28333333e-01  7.27777778e-01  7.50555252e-01
   7.22132035e-01  5.26345260e-01  7.77500000e-01  7.73611111e-01
   7.35204431e-01  7.66360029e-01]
 [ 4.23038976e-01  6.67500000e-01  6.65277778e-01  7.00966945e-01
   6.58986291e-01  3.20266657e-01  6.43333333e-01  6.37500000e-01
   6.62984151e-01  6.25717893e-01]
 [ 5.33146853e-01  7.66666667e-01  7.86111111e-01  7.33623590e-01
   7.49667000e-01  4.12391136e-01  7.30000000e-01  7.25000000e-01
   7.20618479e-01  7.22756133e-01]
 [ 1.31135531e-01  4.86666667e-01  5.12500000e-01  4.79164256e-01
   4.66861472e-01  4.11519445e-01  7.14166667e-01  7.38888889e-01
   6.83087391e-01  7.10681818e-01]
 [ 3.32385392e-01  6.54166667e-01  6.91666667e-01  4.60179418e-01
   6.15973471e-01  2.28909633e-01  6.60000000e-01  6.62500000e-01
   6.40220507e-01  6.52171717e-01]
 [ 5.91833353e-01  7.85833333e-01  8.05555556e-01  7.90311890e-01
   7.82130370e-01  5.13030303e-01  7.82500000e-01  8.02777778e-01
   7.86180210e-01  7.80084915e-01]]
DT mean:
[0.31826672 0.65481548 0.68263889 0.61235723 0.64190293 0.33997447
 0.67968651 0.70023148 0.62767921 0.66706253]
---------------------------
---------------------------
RF performance:
[[0.58345575 0.8125     0.81666667 0.8113462  0.81053391 0.3433051
  0.68095238 0.78472222 0.51962153 0.69056943]
 [0.31673003 0.6975     0.70555556 0.65976709 0.68489261 0.38848305
  0.74166667 0.76111111 0.7302438  0.73754163]
 [0.44399674 0.69166667 0.70416667 0.65018463 0.68591825 0.40029157
  0.6275     0.63472222 0.62302318 0.62648629]
 [0.4432838  0.66       0.65416667 0.65868887 0.64845238 0.12714286
  0.56428571 0.69583333 0.4290252  0.55342657]
 [0.50810659 0.71583333 0.74861111 0.76716325 0.70659646 0.52966251
  0.79       0.78611111 0.72873827 0.78169553]
 [0.62062718 0.745      0.74305556 0.72827038 0.7341811  0.50039344
  0.78       0.77083333 0.70938066 0.76215007]
 [0.20911212 0.64333333 0.67222222 0.57018595 0.62833888 0.26346366
  0.71       0.70555556 0.63591043 0.70333333]
 [0.82237762 0.9        0.92222222 0.91692847 0.90078422 0.57926407
  0.78333333 0.78333333 0.74812266 0.77010406]
 [0.31302697 0.68571429 0.75138889 0.72445144 0.6770646  0.14071928
  0.525      0.61388889 0.34784979 0.48954296]
 [0.24320826 0.5875     0.59722222 0.5574336  0.56446331 0.26304744
  0.6225     0.62638889 0.63207912 0.61683983]
 [0.23542221 0.6        0.60277778 0.5529907  0.59000361 0.3352275
  0.69666667 0.70972222 0.6785035  0.68850649]
 [0.59288261 0.82       0.83194444 0.81342939 0.81874403 0.53626375
  0.79333333 0.80833333 0.65832166 0.77202575]
 [0.29746096 0.655      0.6625     0.6178903  0.64844156 0.32770677
  0.65916667 0.67083333 0.52582304 0.63762349]
 [0.045      0.545      0.60138889 0.35795766 0.5233461  0.27669905
  0.66       0.6625     0.65171684 0.65498918]
 [0.33091127 0.67857143 0.75       0.55302557 0.66859807 0.11327339
  0.71333333 0.75       0.53609953 0.71610972]
 [0.19564856 0.69416667 0.7375     0.6445178  0.69787296 0.46847153
  0.74666667 0.78333333 0.69959908 0.74072761]
 [0.70364471 0.8375     0.83194444 0.86596036 0.82095432 0.66123663
  0.79083333 0.80694444 0.8187471  0.79011489]
 [0.12236025 0.74553571 0.83055556 0.16425211 0.70648352 0.41287816
  0.6325     0.65694444 0.61814493 0.62304113]
 [0.45370629 0.7        0.77361111 0.62400322 0.69745921 0.49625527
  0.765      0.77361111 0.7655904  0.76333333]
 [0.40191142 0.69083333 0.71666667 0.60362706 0.69034216 0.24357143
  0.59333333 0.65416667 0.45505603 0.57348818]
 [0.13367866 0.55119048 0.71666667 0.25891781 0.54674367 0.13631507
  0.555      0.55833333 0.47294271 0.55085859]
 [0.30113008 0.68       0.68333333 0.66961178 0.67302309 0.21582418
  0.64166667 0.68055556 0.57217407 0.63029138]
 [0.32822814 0.53       0.54722222 0.47580518 0.50995588 0.18166226
  0.58333333 0.71388889 0.27051344 0.55367674]
 [0.12565532 0.5975     0.60416667 0.49629278 0.59180736 0.50002627
  0.77583333 0.77361111 0.73999655 0.76187229]
 [0.39037164 0.68666667 0.69166667 0.76148968 0.67884199 0.53427789
  0.79       0.78472222 0.77652337 0.77856061]
 [0.47290228 0.7725     0.77222222 0.65818254 0.76638528 0.45732255
  0.67333333 0.67083333 0.69699987 0.66260823]
 [0.41734932 0.71666667 0.76111111 0.74001959 0.70697469 0.42525674
  0.7075     0.70277778 0.71760785 0.69677128]
 [0.06209277 0.50583333 0.55694444 0.36962363 0.48578449 0.49134805
  0.69333333 0.69444444 0.6749618  0.67827561]
 [0.10286056 0.63583333 0.69305556 0.55650104 0.62771062 0.20649205
  0.64       0.65       0.59139465 0.62159896]
 [0.61191729 0.84083333 0.85416667 0.76617955 0.83970613 0.55781885
  0.7625     0.78611111 0.69140283 0.75814519]]
RF mean:
[0.36096865 0.68742262 0.71782407 0.61982325 0.67768015 0.37045668
 0.68995238 0.71513889 0.62387046 0.67947694]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.54444444  0.          0.35238095  0.
   0.5         0.75        0.          0.42821429]
 [ 0.00540541  0.5025      0.54444444  0.05        0.37018315  0.
   0.5         0.61388889  0.          0.37978022]
 [ 0.17359667  0.57916667  0.6375      0.30773503  0.50457875  0.38713305
   0.6925      0.69444444  0.68109352  0.68835137]
 [-0.01162162  0.495       0.51111111  0.05477226  0.3568315   0.
   0.5         0.73888889  0.          0.42446429]
 [ 0.37084148  0.66666667  0.75        0.50721508  0.64556444  0.52018516
   0.765       0.75277778  0.74455798  0.74792569]
 [ 0.05405405  0.525       0.54444444  0.1         0.38798535  0.57041579
   0.79        0.78194444  0.76965451  0.77600649]
 [ 0.          0.5         0.56944444  0.          0.36263736  0.06210359
   0.53        0.49305556  0.23392331  0.41480852]
 [ 0.36415584  0.65833333  0.775       0.4649501   0.64252414  0.
   0.5         0.625       0.          0.38406593]
 [ 0.          0.5         0.69444444  0.          0.40946429  0.
   0.5         0.68333333  0.          0.40571429]
 [ 0.13493658  0.5725      0.53472222  0.34666951  0.46581419  0.
   0.5         0.54444444  0.          0.35238095]
 [ 0.          0.5         0.59166667  0.          0.37120879  0.
   0.5         0.58055556  0.          0.36692308]
 [ 0.          0.5         0.61388889  0.          0.37978022  0.
   0.5         0.625       0.          0.38406593]
 [ 0.          0.5         0.55694444  0.          0.35750916  0.
   0.5         0.56944444  0.          0.36263736]
 [ 0.          0.5         0.67083333  0.          0.40131868 -0.025
   0.4875      0.52083333  0.          0.34137529]
 [ 0.          0.5         0.71666667  0.          0.41696429  0.
   0.5         0.67083333  0.          0.40131868]
 [ 0.          0.5         0.60277778  0.          0.37549451  0.
   0.5         0.67083333  0.          0.40131868]
 [ 0.          0.5         0.58055556  0.          0.36692308  0.
   0.5         0.59166667  0.          0.37120879]
 [ 0.          0.5         0.83055556  0.          0.4532598   0.
   0.5         0.59166667  0.          0.37120879]
 [ 0.          0.5         0.68333333  0.          0.40571429  0.
   0.5         0.54444444  0.          0.35238095]
 [ 0.          0.5         0.64722222  0.          0.39263736  0.
   0.5         0.63611111  0.          0.38835165]
 [ 0.          0.5         0.77222222  0.          0.43571429  0.
   0.5         0.53333333  0.          0.3474359 ]
 [ 0.13810358  0.5675      0.56805556  0.31492565  0.47541126  0.
   0.5         0.63611111  0.          0.38835165]
 [ 0.          0.5         0.56944444  0.          0.36263736  0.
   0.5         0.75        0.          0.42821429]
 [ 0.          0.5         0.54444444  0.          0.35238095  0.24514123
   0.6125      0.67222222  0.36009536  0.5493007 ]
 [ 0.          0.5         0.56944444  0.          0.36263736  0.36915904
   0.6825      0.68194444  0.56731471  0.63548618]
 [ 0.          0.5         0.52222222  0.          0.34249084  0.35412909
   0.665       0.70694444  0.54623808  0.63822011]
 [ 0.          0.5         0.68333333  0.          0.40571429  0.61650496
   0.8125      0.80694444  0.80132733  0.80394661]
 [ 0.          0.5         0.63611111  0.          0.38835165  0.
   0.5         0.59166667  0.          0.37120879]
 [ 0.          0.5         0.64722222  0.          0.39263736  0.
   0.5         0.54444444  0.          0.35238095]
 [ 0.11812014  0.55416667  0.64861111  0.17844571  0.4657326   0.
   0.5         0.64722222  0.          0.39263736]]
SVM mean:
[0.04491974 0.52069444 0.62537037 0.07749044 0.41341607 0.10332573
 0.55125    0.64166667 0.15680683 0.45498946]
---------------------------
---------------------------
GBM performance:
[[ 0.60363165  0.8225      0.83055556  0.78823323  0.81639805  0.25594203
   0.60833333  0.79444444  0.33441678  0.58863095]
 [ 0.23031535  0.6175      0.62638889  0.57097779  0.60083417  0.42959788
   0.68833333  0.74722222  0.60576162  0.67413253]
 [ 0.42803652  0.71916667  0.74166667  0.68703628  0.71759574  0.30724145
   0.6525      0.65694444  0.62972303  0.64345072]
 [ 0.35935632  0.68        0.67916667  0.65896097  0.66886364  0.01957839
   0.48452381  0.69444444  0.1352604   0.43100733]
 [ 0.57725695  0.78        0.81527778  0.7344553   0.78466533  0.42168665
   0.7275      0.71944444  0.72470093  0.71537157]
 [ 0.55785714  0.795       0.78611111  0.78575805  0.78388889  0.56512957
   0.7625      0.75972222  0.76009354  0.75438312]
 [ 0.35227244  0.6775      0.68611111  0.65010018  0.66262821  0.31427189
   0.6675      0.66805556  0.67849186  0.66156205]
 [ 0.8951049   0.925       0.94444444  0.93623607  0.92847652  0.48221526
   0.73083333  0.75        0.69507769  0.70796842]
 [ 0.19818182  0.60238095  0.72638889  0.39557702  0.58736264 -0.02
   0.49166667  0.64722222  0.05        0.4052381 ]
 [ 0.13201646  0.5925      0.59166667  0.51315194  0.57179654  0.18273928
   0.59        0.60277778  0.50641809  0.55905761]
 [ 0.11242934  0.565       0.59305556  0.46110915  0.54087302  0.51680162
   0.77        0.77638889  0.77553394  0.76636003]
 [ 0.64788736  0.81166667  0.84305556  0.78669617  0.81775502  0.62398595
   0.81333333  0.84166667  0.77653904  0.80933733]
 [ 0.3532555   0.6775      0.68472222  0.64560649  0.66556832  0.22205803
   0.61166667  0.62638889  0.58861529  0.60520202]
 [ 0.16055944  0.55666667  0.69166667  0.22103434  0.50778388  0.38567981
   0.695       0.6875      0.66421381  0.67565296]
 [ 0.12347826  0.55119048  0.72777778  0.19915638  0.50955586  0.27636364
   0.64166667  0.75        0.34391576  0.61844322]
 [ 0.03998249  0.52        0.60138889  0.21570194  0.45590576  0.37636364
   0.66666667  0.76111111  0.53097479  0.66058608]
 [ 0.69539969  0.8375      0.84305556  0.81819097  0.83193723  0.37505402
   0.695       0.72638889  0.66366299  0.69231796]
 [ 0.00582298  0.47321429  0.78611111  0.          0.43914216  0.40802746
   0.7         0.72916667  0.5935073   0.67592491]
 [ 0.32895105  0.675       0.77222222  0.531053    0.66268232  0.47794968
   0.745       0.75972222  0.65661572  0.72630592]
 [ 0.34889952  0.6425      0.71388889  0.56865996  0.64148352  0.18802294
   0.57166667  0.65833333  0.32567617  0.52746254]
 [-0.0173913   0.49285714  0.76111111  0.          0.43196429  0.17557766
   0.59        0.59166667  0.56282521  0.58027778]
 [ 0.39122889  0.6825      0.68611111  0.66357749  0.68173882  0.25346496
   0.6225      0.69166667  0.53531202  0.61016484]
 [ 0.25197487  0.63333333  0.65833333  0.55691096  0.62478438  0.13745342
   0.55952381  0.75972222  0.20688672  0.52042582]
 [ 0.23914634  0.63        0.6375      0.59095347  0.62502165  0.43790375
   0.72        0.72638889  0.65460752  0.69707431]
 [ 0.31995982  0.67583333  0.68194444  0.6205216   0.65752248  0.63887867
   0.8         0.79722222  0.79304524  0.79090548]
 [ 0.55539851  0.765       0.76944444  0.75858569  0.75953463  0.34481245
   0.6825      0.69305556  0.66040201  0.67466089]
 [ 0.43501166  0.675       0.7625      0.55175141  0.66026474  0.42361909
   0.7025      0.70138889  0.69118907  0.69090021]
 [ 0.23760082  0.61083333  0.69444444  0.31637667  0.57281718  0.37788683
   0.68083333  0.71805556  0.59027819  0.66898019]
 [ 0.21337662  0.59833333  0.69166667  0.37096932  0.55647353  0.33140704
   0.665       0.67222222  0.61910503  0.65249833]
 [ 0.57587842  0.78        0.78194444  0.77553821  0.7663703   0.52396936
   0.7775      0.82916667  0.70015113  0.77835331]]
GBM mean:
[0.34509599 0.66884921 0.72699074 0.54576267 0.65105629 0.34845608
 0.67046825 0.71791667 0.56843336 0.65208788]
---------------------------
---------------------------
BDDAE performance:
[[-0.21359959  0.3925      0.4         0.35422761  0.37858672  0.15438789
   0.57214286  0.71111111  0.42835213  0.56385323]
 [ 0.24044296  0.62125     0.62222222  0.60564602  0.61250644  0.37933452
   0.68311688  0.71111111  0.66233977  0.68430057]
 [ 0.10525884  0.5525      0.57222222  0.49518027  0.53354238  0.41111111
   0.70555556  0.70555556  0.69445235  0.70071951]
 [ 0.15555556  0.57777778  0.57777778  0.54268764  0.56131764  0.08281098
   0.53384615  0.65555556  0.40131058  0.52615495]
 [ 0.42698668  0.71558442  0.72222222  0.70901723  0.71033356  0.15555556
   0.57777778  0.57777778  0.55019023  0.56463149]
 [ 0.43333333  0.71666667  0.71666667  0.68152948  0.70066025 -0.1
   0.45        0.45        0.35830349  0.41439319]
 [-0.04093963  0.48        0.49444444  0.42571454  0.45965811  0.08888889
   0.54444444  0.54444444  0.51149616  0.52832631]
 [ 0.17622693  0.57916667  0.67222222  0.4694213   0.57000368 -0.17274681
   0.41883117  0.46111111  0.30623854  0.40611575]
 [ 0.44526688  0.71666667  0.76111111  0.68960775  0.71647359  0.20081203
   0.5875      0.7         0.41573332  0.56658787]
 [ 0.14444444  0.57222222  0.57222222  0.55960368  0.56630817  0.15421757
   0.57625     0.58888889  0.55247696  0.57005479]
 [-0.02288978  0.49155844  0.52777778  0.39875589  0.46814083  0.08664048
   0.54125     0.56666667  0.44586463  0.51412273]
 [-0.06559632  0.46623377  0.5         0.3940913   0.45290565  0.27165661
   0.63766234  0.65555556  0.61122802  0.62738481]
 [ 0.21038729  0.60625     0.60555556  0.59656556  0.59947448  0.26429539
   0.63125     0.63333333  0.59388021  0.61437477]
 [ 0.30447205  0.65        0.69444444  0.62314454  0.64662492  0.2186316
   0.61125     0.60555556  0.59461567  0.59872204]
 [ 0.04772217  0.52        0.68888889  0.26831528  0.4933378   0.09108682
   0.54166667  0.59444444  0.47670089  0.53301268]
 [ 0.20990821  0.60324675  0.63888889  0.5512286   0.59009533  0.17782337
   0.57916667  0.68333333  0.41614026  0.55706859]
 [ 0.35818129  0.68        0.68888889  0.6228882   0.65604758  0.3043229
   0.64415584  0.68888889  0.57254151  0.63798377]
 [ 0.20732698  0.63        0.80555556  0.39093796  0.58849227 -0.04611498
   0.47987013  0.51666667  0.41283876  0.46123523]
 [ 0.26798429  0.63333333  0.68333333  0.59172534  0.62540332  0.08646964
   0.54375     0.54444444  0.51960561  0.53100878]
 [ 0.07766408  0.53333333  0.59444444  0.48392236  0.52875305  0.03570191
   0.51688312  0.57777778  0.39227852  0.4889598 ]
 [ 0.12736733  0.55892857  0.71666667  0.44676187  0.55598531  0.19197736
   0.595       0.6         0.58233884  0.59032381]
 [ 0.01111111  0.50555556  0.50555556  0.48118061  0.49351665  0.07100462
   0.53441558  0.58333333  0.42662277  0.51832527]
 [ 0.05417703  0.52625     0.52777778  0.52110176  0.52309499  0.42018752
   0.6857967   0.83333333  0.58328083  0.70139572]
 [ 0.01762581  0.5075      0.51666667  0.48949291  0.50177612  0.23156888
   0.615       0.62222222  0.59858428  0.60938194]
 [ 0.11818367  0.55875     0.56666667  0.52451354  0.54598128  0.18888889
   0.59444444  0.59444444  0.58347238  0.5896484 ]
 [ 0.11111111  0.55555556  0.55555556  0.52693401  0.54243238  0.12096048
   0.55875     0.57222222  0.53152701  0.55087528]
 [ 0.5017023   0.75        0.78333333  0.73415591  0.74869816  0.47777778
   0.73888889  0.73888889  0.72311041  0.73236781]
 [ 0.1507686   0.57467532  0.59444444  0.5415581   0.56269839  0.44510126
   0.70974026  0.75        0.68340995  0.71749941]
 [ 0.08798306  0.54583333  0.59444444  0.50944572  0.54093592  0.29637037
   0.64625     0.66111111  0.62260575  0.64123723]
 [ 0.24697962  0.62337662  0.64444444  0.6081532   0.62014439  0.10222622
   0.55833333  0.60555556  0.4523743   0.53415384]]
BDDAE mean:
[0.16317154 0.5814905  0.61814815 0.52791694 0.56979765 0.17969829
 0.58709963 0.62444444 0.5234638  0.57580732]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.54444444 0.         0.35238095 0.
  0.5        0.75       0.         0.42821429]
 [0.         0.5        0.54444444 0.         0.35238095 0.
  0.5        0.61388889 0.         0.37978022]
 [0.         0.5        0.56944444 0.         0.36263736 0.
  0.5        0.52222222 0.         0.34249084]
 [0.         0.5        0.52222222 0.         0.34249084 0.
  0.5        0.73888889 0.         0.42446429]
 [0.         0.5        0.625      0.         0.38406593 0.
  0.5        0.45555556 0.         0.31282051]
 [0.         0.5        0.52222222 0.         0.34249084 0.
  0.5        0.52222222 0.         0.34249084]
 [0.         0.5        0.56944444 0.         0.36263736 0.
  0.5        0.45555556 0.         0.31282051]
 [0.         0.5        0.67083333 0.         0.40131868 0.
  0.5        0.625      0.         0.38406593]
 [0.         0.5        0.69444444 0.         0.40946429 0.
  0.5        0.68333333 0.         0.40571429]
 [0.         0.5        0.45555556 0.         0.31282051 0.
  0.5        0.54444444 0.         0.35238095]
 [0.         0.5        0.59166667 0.         0.37120879 0.
  0.5        0.58055556 0.         0.36692308]
 [0.         0.5        0.61388889 0.         0.37978022 0.
  0.5        0.625      0.         0.38406593]
 [0.         0.5        0.55694444 0.         0.35750916 0.
  0.5        0.56944444 0.         0.36263736]
 [0.         0.5        0.67083333 0.         0.40131868 0.
  0.5        0.53333333 0.         0.3474359 ]
 [0.         0.5        0.71666667 0.         0.41696429 0.
  0.5        0.67083333 0.         0.40131868]
 [0.         0.5        0.60277778 0.         0.37549451 0.
  0.5        0.67083333 0.         0.40131868]
 [0.         0.5        0.58055556 0.         0.36692308 0.
  0.5        0.59166667 0.         0.37120879]
 [0.         0.5        0.83055556 0.         0.4532598  0.
  0.5        0.59166667 0.         0.37120879]
 [0.         0.5        0.68333333 0.         0.40571429 0.
  0.5        0.54444444 0.         0.35238095]
 [0.         0.5        0.64722222 0.         0.39263736 0.
  0.5        0.63611111 0.         0.38835165]
 [0.         0.5        0.77222222 0.         0.43571429 0.
  0.5        0.53333333 0.         0.3474359 ]
 [0.         0.5        0.51111111 0.         0.33754579 0.
  0.5        0.63611111 0.         0.38835165]
 [0.         0.5        0.56944444 0.         0.36263736 0.
  0.5        0.75       0.         0.42821429]
 [0.         0.5        0.54444444 0.         0.35238095 0.
  0.5        0.58055556 0.         0.36692308]
 [0.         0.5        0.56944444 0.         0.36263736 0.
  0.5        0.51111111 0.         0.33754579]
 [0.         0.5        0.52222222 0.         0.34249084 0.
  0.5        0.56944444 0.         0.36263736]
 [0.         0.5        0.68333333 0.         0.40571429 0.
  0.5        0.45555556 0.         0.31282051]
 [0.         0.5        0.63611111 0.         0.38835165 0.
  0.5        0.59166667 0.         0.37120879]
 [0.         0.5        0.64722222 0.         0.39263736 0.
  0.5        0.54444444 0.         0.35238095]
 [0.         0.5        0.60277778 0.         0.37549451 0.
  0.5        0.64722222 0.         0.39263736]]
DUMMY mean:
[0.         0.5        0.60902778 0.         0.37663674 0.
 0.5        0.59148148 0.         0.36974161]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_43_2
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.326 0.662 0.701 0.587 0.651 0.398 0.697 0.728 0.636 0.687]
 [0.318 0.655 0.683 0.612 0.642 0.34  0.68  0.7   0.628 0.667]
 [0.361 0.687 0.718 0.62  0.678 0.37  0.69  0.715 0.624 0.679]
 [0.045 0.521 0.625 0.077 0.413 0.103 0.551 0.642 0.157 0.455]
 [0.345 0.669 0.727 0.546 0.651 0.348 0.67  0.718 0.568 0.652]
 [0.163 0.581 0.618 0.528 0.57  0.18  0.587 0.624 0.523 0.576]
 [0.    0.5   0.609 0.    0.377 0.    0.5   0.591 0.    0.37 ]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.2   0.099 0.088 0.155 0.102 0.148 0.077 0.064 0.124 0.077]
 [0.198 0.1   0.089 0.154 0.102 0.149 0.074 0.067 0.106 0.079]
 [0.195 0.095 0.09  0.169 0.098 0.153 0.078 0.066 0.128 0.082]
 [0.099 0.045 0.083 0.148 0.074 0.194 0.097 0.082 0.278 0.138]
 [0.216 0.107 0.081 0.235 0.122 0.158 0.081 0.061 0.189 0.095]
 [0.162 0.081 0.094 0.107 0.084 0.153 0.074 0.083 0.106 0.081]
 [0.    0.    0.081 0.    0.03  0.    0.    0.079 0.    0.031]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 61.  15.  13.  26.  16.  37.  11.   9.  19.  11.]
 [ 62.  15.  13.  25.  16.  44.  11.  10.  17.  12.]
 [ 54.  14.  13.  27.  14.  41.  11.   9.  21.  12.]
 [220.   9.  13. 191.  18. 188.  18.  13. 177.  30.]
 [ 63.  16.  11.  43.  19.  45.  12.   8.  33.  15.]
 [ 99.  14.  15.  20.  15.  85.  13.  13.  20.  14.]
 [  0.   0.  13.   0.   8.   0.   0.  13.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_43_2
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  18.0
step (sec):  13.504
overlap:  True
perc. of overlap:  24.977777777777778
overlap duration (sec):  4.496
Number of windows / instances:  88
Elapsed time: 722.5076862891515 minutes
Elapsed time: 12.041794771485858 hours
