2024-05-09 20:08:59.064960: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-09 20:09:02.912892: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-09 20:09:12.570798: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  19.0
step (sec):  19.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_44\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0207 - mean_squared_error: 0.0207 - val_loss: 0.0257 - val_mean_squared_error: 0.0257
(19008, 1, 5)
Model: "sequential_1195"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
┌─────────────────────┬───────────────────┬────────────┬───────────────────┐
│ Layer (type)        │ Output Shape      │    Param # │ Connected to      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputGSR            │ (None, 19008, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputPPG            │ (None, 19008, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1194     │ (None, 1188, 6)   │        411 │ inputGSR[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1195     │ (None, 1188, 6)   │        411 │ inputPPG[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_597     │ (None, 1188, 12)  │          0 │ sequential_1194[… │
│ (Concatenate)       │                   │            │ sequential_1195[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ permute_597         │ (None, 12, 1188)  │          0 │ concatenate_597[… │
│ (Permute)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_597         │ (None, 14256)     │          0 │ permute_597[0][0] │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_597         │ (None, 14256)     │          0 │ flatten_597[0][0] │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_597 (Dense)   │ (None, 1)         │     14,257 │ dropout_597[0][0] │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 15,079 (58.90 KB)
 Trainable params: 15,079 (58.90 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m58s[0m 1s/step - binary_accuracy: 0.0000e+00 - loss: 1.2013
[1m14/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.3469 - loss: 0.9788    
[1m31/45[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.4705 - loss: 0.8738
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 9ms/step - binary_accuracy: 0.5154 - loss: 0.8268 - val_binary_accuracy: 0.4000 - val_loss: 1.1394
Epoch 2/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.8349
[1m15/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7370 - loss: 0.6938     
[1m32/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7056 - loss: 0.6471
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - binary_accuracy: 0.6919 - loss: 0.6328 - val_binary_accuracy: 0.4000 - val_loss: 1.2862
Epoch 3/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 0.0000e+00 - loss: 1.0301
[1m15/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.4040 - loss: 1.0259     
[1m32/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5613 - loss: 0.8354
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - binary_accuracy: 0.6192 - loss: 0.7505 - val_binary_accuracy: 0.4000 - val_loss: 1.7427
Epoch 4/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.0903
[1m18/45[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8087 - loss: 0.3993 
[1m36/45[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7922 - loss: 0.4704
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7828 - loss: 0.4909 - val_binary_accuracy: 0.4000 - val_loss: 1.3197
Epoch 5/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 15ms/step - binary_accuracy: 1.0000 - loss: 0.3172
[1m14/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.8447 - loss: 0.4435 
[1m32/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8436 - loss: 0.4327
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - binary_accuracy: 0.8327 - loss: 0.4445 - val_binary_accuracy: 0.4000 - val_loss: 1.5451
Epoch 6/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.0995
[1m15/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.9067 - loss: 0.3544 
[1m32/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8356 - loss: 0.4379
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - binary_accuracy: 0.8222 - loss: 0.4563 - val_binary_accuracy: 0.4000 - val_loss: 1.6606
Epoch 7/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.0556
[1m14/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.9091 - loss: 0.2947 
[1m31/45[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8104 - loss: 0.4435
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - binary_accuracy: 0.7832 - loss: 0.4725 - val_binary_accuracy: 0.2000 - val_loss: 1.7165
Epoch 8/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2818
[1m13/45[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8917 - loss: 0.3210 
[1m27/45[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8412 - loss: 0.3755
[1m40/45[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8359 - loss: 0.3909
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - binary_accuracy: 0.8332 - loss: 0.3956 - val_binary_accuracy: 0.2000 - val_loss: 1.6839
Epoch 9/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 25ms/step - binary_accuracy: 0.0000e+00 - loss: 1.1668
[1m15/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6175 - loss: 0.6884     
[1m29/45[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7025 - loss: 0.5874
[1m43/45[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7368 - loss: 0.5408
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7409 - loss: 0.5343 - val_binary_accuracy: 0.2000 - val_loss: 1.8599
Epoch 10/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 26ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7519
[1m15/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6247 - loss: 0.4416     
[1m29/45[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6789 - loss: 0.4300
[1m43/45[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7043 - loss: 0.4276
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7089 - loss: 0.4254 - val_binary_accuracy: 0.2000 - val_loss: 2.0080

[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 114ms/step
[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 114ms/step
predicted [0.15046251 0.88987565 0.62574416 0.925244   0.6227126  0.8454229
 0.57617104 0.94361854 0.30998632 0.9067259  0.8766166  0.9698868
 0.01615369]
predicted [0 1 1 1 1 1 1 1 0 1 1 1 0]
expected [False  True  True  True False  True  True False  True  True False  True
 False]
accuracy: 0.6923076923076923
confusion matrix: 
[[2 3]
 [1 7]]
              precision    recall  f1-score   support

       False       0.67      0.40      0.50         5
        True       0.70      0.88      0.78         8

    accuracy                           0.69        13
   macro avg       0.68      0.64      0.64        13
weighted avg       0.69      0.69      0.67        13

macro avg f1-score: 0.6388888888888888
macro avg (UAR): 0.6375
Sensitivity:  0.4
Specificity:  0.875
g-mean:  0.5916079783099616
-------- Model Performance ----------: 
accuracy:  [0.46153846 0.61538462 0.46153846 0.46153846 0.38461538 0.46153846
 0.61538462 0.69230769 0.         0.        ]
gmean:  [0.47434165 0.54772256 0.         0.35355339 0.31622777 0.35355339
 0.41833001 0.59160798 0.         0.        ]
f1_score:  [0.46153846 0.5751634  0.31578947 0.40522876 0.35       0.40522876
 0.5112782  0.63888889 0.         0.        ]
UAR:  [0.4875 0.575  0.375  0.4125 0.35   0.4125 0.5375 0.6375 0.     0.    ]
Cohen Kappa score:  [-0.02247191  0.15584416 -0.28169014 -0.18181818 -0.3        -0.18181818
  0.08450704  0.2972973   0.          0.        ]
Split Repetition number:  8
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [33 49 26 10 59 28 11 61 27 40 50 15  0 13 25 20 37  2 46  9 45 56 38 14
 31  8 24 54 44 48 52 42 58  4 21 36  7 19 34  1 32 23 16  5  6 39 18 43
 47 60] TEST: [12 35 29 51 62 17 53 55 30 57  3 22 41]
(DL) TRAIN number of instances:  50
(DL) TEST number of instances:  13
(DL) Total number of instances (TRAIN+TEST):  63
C:\Users\Javier\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\convolutional\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(
----- train_GSR_AE -------
Model: "sequential_1196"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2392              │ (None, 4752, 6)        │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2392           │ (None, 4752, 6)        │           366 │
│ (Conv1DTranspose)               │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2393              │ (None, 19008, 6)       │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2393           │ (None, 19008, 1)       │           121 │
│ (Conv1DTranspose)               │                        │               │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m49s[0m 1s/step - loss: 15.3013 - mean_squared_error: 15.3013
[1m11/45[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 12.6876 - mean_squared_error: 12.6876
[1m21/45[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 11.4289 - mean_squared_error: 11.4289
[1m34/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 5ms/step - loss: 11.0767 - mean_squared_error: 11.0767
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.8968 - mean_squared_error: 10.8968
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 10ms/step - loss: 10.8991 - mean_squared_error: 10.8991 - val_loss: 20.6002 - val_mean_squared_error: 20.6002
Epoch 2/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 26ms/step - loss: 15.3743 - mean_squared_error: 15.3743
[1m14/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 7.4826 - mean_squared_error: 7.4826   
[1m26/45[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 8.5208 - mean_squared_error: 8.5208
[1m39/45[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - loss: 9.4140 - mean_squared_error: 9.4140
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.5126 - mean_squared_error: 9.5126 - val_loss: 20.5633 - val_mean_squared_error: 20.5633
Epoch 3/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 25ms/step - loss: 14.7182 - mean_squared_error: 14.7182
[1m15/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 6.7917 - mean_squared_error: 6.7917   
[1m28/45[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 6.7812 - mean_squared_error: 6.7812
[1m40/45[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - loss: 7.0387 - mean_squared_error: 7.0387
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 7.4035 - mean_squared_error: 7.4035 - val_loss: 20.5558 - val_mean_squared_error: 20.5558
Epoch 4/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 26ms/step - loss: 41.4241 - mean_squared_error: 41.4241
[1m13/45[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 14.1009 - mean_squared_error: 14.1009 
[1m25/45[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 11.4034 - mean_squared_error: 11.4034
[1m38/45[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - loss: 10.8630 - mean_squared_error: 10.8630
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.7070 - mean_squared_error: 10.7070 - val_loss: 20.5509 - val_mean_squared_error: 20.5509
Epoch 5/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 3.0166 - mean_squared_error: 3.0166
[1m15/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 6.6559 - mean_squared_error: 6.6559 
[1m31/45[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - loss: 8.4038 - mean_squared_error: 8.4038
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.6793 - mean_squared_error: 8.6793 - val_loss: 20.5477 - val_mean_squared_error: 20.5477
(50, 1188, 6)
Model: "sequential_1196"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1197"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2394              │ (None, 4752, 6)        │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2394           │ (None, 4752, 6)        │           366 │
│ (Conv1DTranspose)               │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2395              │ (None, 19008, 6)       │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2395           │ (None, 19008, 1)       │           121 │
│ (Conv1DTranspose)               │                        │               │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m49s[0m 1s/step - loss: 0.7628 - mean_squared_error: 0.7628
[1m11/45[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 6ms/step - loss: 0.7077 - mean_squared_error: 0.7077
[1m26/45[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.4761 - mean_squared_error: 0.4761
[1m41/45[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 5ms/step - loss: 0.3692 - mean_squared_error: 0.3692
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 9ms/step - loss: 0.3452 - mean_squared_error: 0.3452 - val_loss: 0.0374 - val_mean_squared_error: 0.0374
Epoch 2/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 0.0122 - mean_squared_error: 0.0122
[1m14/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0149 - mean_squared_error: 0.0149 
[1m30/45[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0216 - mean_squared_error: 0.0216
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 0.0229 - mean_squared_error: 0.0229
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0230 - mean_squared_error: 0.0230 - val_loss: 0.0335 - val_mean_squared_error: 0.0335
Epoch 3/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - loss: 0.0124 - mean_squared_error: 0.0124
[1m17/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0129 - mean_squared_error: 0.0129 
[1m33/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0150 - mean_squared_error: 0.0150
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0168 - mean_squared_error: 0.0168 - val_loss: 0.0300 - val_mean_squared_error: 0.0300
Epoch 4/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 0.0084 - mean_squared_error: 0.0084
[1m16/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0126 - mean_squared_error: 0.0126 
[1m32/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0161 - mean_squared_error: 0.0161
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0179 - mean_squared_error: 0.0179 - val_loss: 0.0282 - val_mean_squared_error: 0.0282
Epoch 5/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 0.0040 - mean_squared_error: 0.0040
[1m17/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0115 - mean_squared_error: 0.0115 
[1m33/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0139 - mean_squared_error: 0.0139
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0150 - mean_squared_error: 0.0150 - val_loss: 0.0278 - val_mean_squared_error: 0.0278
(19008, 1, 5)
Model: "sequential_1197"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
┌─────────────────────┬───────────────────┬────────────┬───────────────────┐
│ Layer (type)        │ Output Shape      │    Param # │ Connected to      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputGSR            │ (None, 19008, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputPPG            │ (None, 19008, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1196     │ (None, 1188, 6)   │        411 │ inputGSR[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1197     │ (None, 1188, 6)   │        411 │ inputPPG[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_598     │ (None, 1188, 12)  │          0 │ sequential_1196[… │
│ (Concatenate)       │                   │            │ sequential_1197[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ permute_598         │ (None, 12, 1188)  │          0 │ concatenate_598[… │
│ (Permute)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_598         │ (None, 14256)     │          0 │ permute_598[0][0] │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_598         │ (None, 14256)     │          0 │ flatten_598[0][0] │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_598 (Dense)   │ (None, 1)         │     14,257 │ dropout_598[0][0] │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 15,079 (58.90 KB)
 Trainable params: 15,079 (58.90 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m57s[0m 1s/step - binary_accuracy: 0.0000e+00 - loss: 0.9084
[1m13/45[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.3356 - loss: 0.7512    
[1m30/45[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.4208 - loss: 0.7169
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 9ms/step - binary_accuracy: 0.4375 - loss: 0.7176 - val_binary_accuracy: 1.0000 - val_loss: 0.4507
Epoch 2/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.3909
[1m14/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.3848 - loss: 0.8064     
[1m32/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5416 - loss: 0.7312
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - binary_accuracy: 0.5834 - loss: 0.7003 - val_binary_accuracy: 1.0000 - val_loss: 0.1702
Epoch 3/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1029
[1m18/45[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6171 - loss: 0.6856 
[1m36/45[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5231 - loss: 0.7974
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5111 - loss: 0.8092 - val_binary_accuracy: 1.0000 - val_loss: 0.1068
Epoch 4/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.0911
[1m18/45[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8512 - loss: 0.4182 
[1m35/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8282 - loss: 0.4430
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7966 - loss: 0.4682 - val_binary_accuracy: 0.0000e+00 - val_loss: 1.3207
Epoch 5/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.0186
[1m19/45[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.3984 - loss: 0.8040     
[1m36/45[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.4326 - loss: 0.7917
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.4551 - loss: 0.7779 - val_binary_accuracy: 1.0000 - val_loss: 0.3358
Epoch 6/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.6083
[1m19/45[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7554 - loss: 0.6739 
[1m36/45[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7752 - loss: 0.6260
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7775 - loss: 0.6081 - val_binary_accuracy: 1.0000 - val_loss: 0.4763
Epoch 7/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.5519
[1m15/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8128 - loss: 0.5599 
[1m32/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7806 - loss: 0.5626
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7666 - loss: 0.5602 - val_binary_accuracy: 0.8000 - val_loss: 0.4841
Epoch 8/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 0.0000e+00 - loss: 1.0803
[1m16/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7395 - loss: 0.5979     
[1m34/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7629 - loss: 0.5494
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7583 - loss: 0.5440 - val_binary_accuracy: 1.0000 - val_loss: 0.3152
Epoch 9/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.7351
[1m17/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6528 - loss: 0.6652     
[1m35/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7351 - loss: 0.5612
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7446 - loss: 0.5436 - val_binary_accuracy: 0.8000 - val_loss: 0.6880
Epoch 10/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.4359
[1m19/45[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.9377 - loss: 0.4743 
[1m37/45[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.9082 - loss: 0.4607
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8940 - loss: 0.4596 - val_binary_accuracy: 0.6000 - val_loss: 0.7789

[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.5879817  0.41402125 0.18866919 0.5309814  0.51288056 0.6201241
 0.5842736  0.54572606 0.05029953 0.6640581  0.46499246 0.37027624
 0.17896362]
predicted [1 0 0 1 1 1 1 1 0 1 0 0 0]
expected [False  True  True  True False  True False  True  True False False  True
  True]
accuracy: 0.3076923076923077
confusion matrix: 
[[1 4]
 [5 3]]
              precision    recall  f1-score   support

       False       0.17      0.20      0.18         5
        True       0.43      0.38      0.40         8

    accuracy                           0.31        13
   macro avg       0.30      0.29      0.29        13
weighted avg       0.33      0.31      0.32        13

macro avg f1-score: 0.2909090909090909
macro avg (UAR): 0.2875
Sensitivity:  0.2
Specificity:  0.375
g-mean:  0.2738612787525831
-------- Model Performance ----------: 
accuracy:  [0.46153846 0.61538462 0.46153846 0.46153846 0.38461538 0.46153846
 0.61538462 0.69230769 0.30769231 0.        ]
gmean:  [0.47434165 0.54772256 0.         0.35355339 0.31622777 0.35355339
 0.41833001 0.59160798 0.27386128 0.        ]
f1_score:  [0.46153846 0.5751634  0.31578947 0.40522876 0.35       0.40522876
 0.5112782  0.63888889 0.29090909 0.        ]
UAR:  [0.4875 0.575  0.375  0.4125 0.35   0.4125 0.5375 0.6375 0.2875 0.    ]
Cohen Kappa score:  [-0.02247191  0.15584416 -0.28169014 -0.18181818 -0.3        -0.18181818
  0.08450704  0.2972973  -0.40963855  0.        ]
Split Repetition number:  9
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [17 45 14 25 38 61  6 55 22 53 59 54 58 32 24 11 43 41 30  8 37 52 56 42
 57 21 44  7  4 51  2  3 33 18 49 16 10 46 15 27 31 13 35 34 29  5 26 50
 60 39] TEST: [ 1  9 40 36 19 47 62 20 12 23 48 28  0]
(DL) TRAIN number of instances:  50
(DL) TEST number of instances:  13
(DL) Total number of instances (TRAIN+TEST):  63
C:\Users\Javier\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\convolutional\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(
----- train_GSR_AE -------
Model: "sequential_1198"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2396              │ (None, 4752, 6)        │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2396           │ (None, 4752, 6)        │           366 │
│ (Conv1DTranspose)               │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2397              │ (None, 19008, 6)       │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2397           │ (None, 19008, 1)       │           121 │
│ (Conv1DTranspose)               │                        │               │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m48s[0m 1s/step - loss: 60.5397 - mean_squared_error: 60.5397
[1m13/45[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 26.2592 - mean_squared_error: 26.2592
[1m27/45[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 20.1826 - mean_squared_error: 20.1826
[1m42/45[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 5ms/step - loss: 17.6161 - mean_squared_error: 17.6161
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 9ms/step - loss: 17.2626 - mean_squared_error: 17.2626 - val_loss: 18.2213 - val_mean_squared_error: 18.2213
Epoch 2/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 9.2253 - mean_squared_error: 9.2253
[1m16/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 11.7063 - mean_squared_error: 11.7063
[1m32/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - loss: 11.3048 - mean_squared_error: 11.3048
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.3058 - mean_squared_error: 11.3058 - val_loss: 18.1770 - val_mean_squared_error: 18.1770
Epoch 3/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 0.2001 - mean_squared_error: 0.2001
[1m18/45[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 10.0562 - mean_squared_error: 10.0562
[1m34/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - loss: 9.8448 - mean_squared_error: 9.8448  
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.2000 - mean_squared_error: 10.2000 - val_loss: 18.1687 - val_mean_squared_error: 18.1687
Epoch 4/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - loss: 8.9044 - mean_squared_error: 8.9044
[1m17/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 7.2289 - mean_squared_error: 7.2289 
[1m33/45[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - loss: 8.5573 - mean_squared_error: 8.5573
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.0405 - mean_squared_error: 9.0405 - val_loss: 18.1643 - val_mean_squared_error: 18.1643
Epoch 5/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 48.1415 - mean_squared_error: 48.1415
[1m15/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 18.2860 - mean_squared_error: 18.2860 
[1m31/45[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - loss: 14.8777 - mean_squared_error: 14.8777
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 13.8140 - mean_squared_error: 13.8140 - val_loss: 18.1616 - val_mean_squared_error: 18.1616
(50, 1188, 6)
Model: "sequential_1198"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2398              │ (None, 4752, 6)        │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2398           │ (None, 4752, 6)        │           366 │
│ (Conv1DTranspose)               │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2399              │ (None, 19008, 6)       │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2399           │ (None, 19008, 1)       │           121 │
│ (Conv1DTranspose)               │                        │               │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m49s[0m 1s/step - loss: 0.1397 - mean_squared_error: 0.1397
[1m13/45[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.1051 - mean_squared_error: 0.1051
[1m26/45[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0848 - mean_squared_error: 0.0848
[1m40/45[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - loss: 0.0718 - mean_squared_error: 0.0718
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 9ms/step - loss: 0.0679 - mean_squared_error: 0.0679 - val_loss: 0.0438 - val_mean_squared_error: 0.0438
Epoch 2/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - loss: 0.0124 - mean_squared_error: 0.0124
[1m13/45[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0199 - mean_squared_error: 0.0199 
[1m28/45[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0225 - mean_squared_error: 0.0225
[1m44/45[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 4ms/step - loss: 0.0220 - mean_squared_error: 0.0220
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0220 - mean_squared_error: 0.0220 - val_loss: 0.0380 - val_mean_squared_error: 0.0380
Epoch 3/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 0.0196 - mean_squared_error: 0.0196
[1m16/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0188 - mean_squared_error: 0.0188 
[1m31/45[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0200 - mean_squared_error: 0.0200
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0197 - mean_squared_error: 0.0197 - val_loss: 0.0357 - val_mean_squared_error: 0.0357
Epoch 4/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 25ms/step - loss: 0.0332 - mean_squared_error: 0.0332
[1m13/45[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0215 - mean_squared_error: 0.0215 
[1m28/45[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0221 - mean_squared_error: 0.0221
[1m44/45[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 4ms/step - loss: 0.0207 - mean_squared_error: 0.0207
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0387 - val_mean_squared_error: 0.0387
Epoch 5/5

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 0.0015 - mean_squared_error: 0.0015
[1m16/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0210 - mean_squared_error: 0.0210 
[1m28/45[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0215 - mean_squared_error: 0.0215
[1m44/45[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 4ms/step - loss: 0.0201 - mean_squared_error: 0.0201
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0338 - val_mean_squared_error: 0.0338
(19008, 1, 5)
Model: "sequential_1199"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
┌─────────────────────┬───────────────────┬────────────┬───────────────────┐
│ Layer (type)        │ Output Shape      │    Param # │ Connected to      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputGSR            │ (None, 19008, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputPPG            │ (None, 19008, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1198     │ (None, 1188, 6)   │        411 │ inputGSR[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1199     │ (None, 1188, 6)   │        411 │ inputPPG[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_599     │ (None, 1188, 12)  │          0 │ sequential_1198[… │
│ (Concatenate)       │                   │            │ sequential_1199[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ permute_599         │ (None, 12, 1188)  │          0 │ concatenate_599[… │
│ (Permute)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_599         │ (None, 14256)     │          0 │ permute_599[0][0] │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_599         │ (None, 14256)     │          0 │ flatten_599[0][0] │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_599 (Dense)   │ (None, 1)         │     14,257 │ dropout_599[0][0] │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 15,079 (58.90 KB)
 Trainable params: 15,079 (58.90 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m57s[0m 1s/step - binary_accuracy: 1.0000 - loss: 0.6156
[1m14/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.4920 - loss: 0.6698
[1m31/45[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5052 - loss: 0.6681
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 9ms/step - binary_accuracy: 0.5399 - loss: 0.6631 - val_binary_accuracy: 0.6000 - val_loss: 0.6671
Epoch 2/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2560
[1m14/45[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8434 - loss: 0.4283 
[1m28/45[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7739 - loss: 0.4991
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7511 - loss: 0.5316
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - binary_accuracy: 0.7498 - loss: 0.5339 - val_binary_accuracy: 0.6000 - val_loss: 0.6580
Epoch 3/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.1805
[1m18/45[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.3111 - loss: 0.8531     
[1m35/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.4347 - loss: 0.7541
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.4846 - loss: 0.7147 - val_binary_accuracy: 0.6000 - val_loss: 0.7749
Epoch 4/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 36ms/step - binary_accuracy: 1.0000 - loss: 0.1584
[1m17/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5883 - loss: 0.8447 
[1m35/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6065 - loss: 0.7658
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6318 - loss: 0.7302 - val_binary_accuracy: 0.6000 - val_loss: 0.7882
Epoch 5/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.0652
[1m18/45[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6530 - loss: 0.6151 
[1m36/45[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6885 - loss: 0.6129
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7038 - loss: 0.6043 - val_binary_accuracy: 0.6000 - val_loss: 0.7823
Epoch 6/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4825
[1m18/45[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8451 - loss: 0.4954 
[1m35/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7975 - loss: 0.5207
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7836 - loss: 0.5222 - val_binary_accuracy: 0.4000 - val_loss: 0.7377
Epoch 7/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1027
[1m17/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.9702 - loss: 0.3134 
[1m34/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.9027 - loss: 0.3771
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8840 - loss: 0.3891 - val_binary_accuracy: 0.2000 - val_loss: 0.7725
Epoch 8/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4631
[1m17/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6836 - loss: 0.5252 
[1m35/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7219 - loss: 0.5003
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7389 - loss: 0.4918 - val_binary_accuracy: 0.4000 - val_loss: 0.8570
Epoch 9/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0834
[1m17/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7898 - loss: 0.4467 
[1m34/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7782 - loss: 0.4762
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7841 - loss: 0.4679 - val_binary_accuracy: 0.4000 - val_loss: 0.9112
Epoch 10/10

[1m 1/45[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.0173
[1m17/45[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7558 - loss: 0.3576 
[1m34/45[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7951 - loss: 0.3539
[1m45/45[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7978 - loss: 0.3655 - val_binary_accuracy: 0.2000 - val_loss: 1.1032

[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.64465314 0.80530226 0.46974015 0.13533668 0.4341558  0.4595267
 0.73598504 0.35265914 0.42116928 0.5166999  0.73099434 0.09140243
 0.95978117]
predicted [1 1 0 0 0 0 1 0 0 1 1 0 1]
expected [False False  True  True  True  True False  True False  True  True  True
 False]
accuracy: 0.23076923076923078
confusion matrix: 
[[1 4]
 [6 2]]
              precision    recall  f1-score   support

       False       0.14      0.20      0.17         5
        True       0.33      0.25      0.29         8

    accuracy                           0.23        13
   macro avg       0.24      0.23      0.23        13
weighted avg       0.26      0.23      0.24        13

macro avg f1-score: 0.22619047619047616
macro avg (UAR): 0.225
Sensitivity:  0.2
Specificity:  0.25
g-mean:  0.22360679774997896
-------- Model Performance ----------: 
accuracy:  [0.46153846 0.61538462 0.46153846 0.46153846 0.38461538 0.46153846
 0.61538462 0.69230769 0.30769231 0.23076923]
gmean:  [0.47434165 0.54772256 0.         0.35355339 0.31622777 0.35355339
 0.41833001 0.59160798 0.27386128 0.2236068 ]
f1_score:  [0.46153846 0.5751634  0.31578947 0.40522876 0.35       0.40522876
 0.5112782  0.63888889 0.29090909 0.22619048]
UAR:  [0.4875 0.575  0.375  0.4125 0.35   0.4125 0.5375 0.6375 0.2875 0.225 ]
Cohen Kappa score:  [-0.02247191  0.15584416 -0.28169014 -0.18181818 -0.3        -0.18181818
  0.08450704  0.2972973  -0.40963855 -0.51162791]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  19.008
step (sec):  19.008
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  63
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.768  0.892  0.89   0.883  0.882  0.245  0.628  0.645  0.591  0.614]
 [ 0.416  0.742  0.779  0.731  0.748  0.318  0.555  0.621  0.395  0.524]
 [ 0.648  0.846  0.871  0.705  0.842  0.489  0.665  0.726  0.586  0.664]
 [ 0.     0.5    0.638  0.     0.389  0.     0.5    0.667  0.     0.4  ]
 [ 0.492  0.767  0.795  0.701  0.752  0.086  0.512  0.633  0.157  0.461]
 [-0.135  0.43   0.469  0.355  0.418 -0.061  0.474  0.569  0.298  0.455]
 [ 0.     0.5    0.638  0.     0.389  0.     0.5    0.667  0.     0.4  ]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.53389855  0.77083333  0.76428571  0.75454513  0.7590873   0.51012422
   0.76        0.83333333  0.64888188  0.73878788]
 [ 0.35684848  0.6875      0.67857143  0.63280123  0.66059524  0.27135799
   0.63333333  0.7047619   0.43968295  0.60194444]
 [ 0.38142292  0.69166667  0.71666667  0.57927717  0.66357143  0.33846154
   0.67083333  0.65714286  0.57683001  0.62714286]
 [ 0.29315385  0.65        0.6452381   0.5812157   0.62130952  0.01809524
   0.4975      0.65952381  0.15        0.45569264]
 [ 0.52430359  0.75416667  0.80952381  0.63463192  0.73702381  0.32853958
   0.6625      0.66190476  0.61505098  0.64634921]
 [ 0.30064103  0.65416667  0.64761905  0.57591841  0.62035714  0.50391919
   0.75833333  0.74761905  0.71941592  0.7322619 ]
 [ 0.4071118   0.69583333  0.71190476  0.67034026  0.68980159  0.37564103
   0.6875      0.68571429  0.67315974  0.67952381]
 [ 0.66275728  0.8175      0.85952381  0.79492819  0.82608947  0.64619525
   0.83333333  0.84047619  0.76698677  0.80543651]
 [ 0.3212465   0.655       0.73095238  0.54569186  0.64606421  0.32460723
   0.66        0.71904762  0.5654706   0.65358586]
 [ 0.19933333  0.6         0.6         0.54780496  0.5825      0.23849184
   0.62083333  0.62142857  0.52214333  0.59095238]
 [ 0.21586204  0.60416667  0.64761905  0.43361359  0.56638889  0.22051948
   0.61666667  0.63571429  0.51240956  0.58797619]
 [ 0.43266046  0.71666667  0.75714286  0.62197528  0.70218254  0.23378788
   0.62916667  0.62857143  0.54554136  0.59559524]
 [ 0.54696029  0.76666667  0.77857143  0.72575502  0.75456349  0.04480331
   0.51666667  0.53809524  0.37424081  0.48162698]
 [-0.19048701  0.415       0.47857143  0.16719585  0.37666667  0.3573913
   0.68333333  0.68095238  0.53112971  0.63071429]
 [ 0.02742236  0.5225      0.66190476  0.21392552  0.48027778  0.23152927
   0.62333333  0.64761905  0.53768609  0.60043651]
 [ 0.08553548  0.53333333  0.60238095  0.32558616  0.50033189  0.35668067
   0.665       0.76190476  0.50732551  0.65462121]
 [ 0.37217316  0.6875      0.69761905  0.60393372  0.65687951  0.25950028
   0.625       0.67857143  0.50815601  0.60746032]
 [ 0.02215686  0.50666667  0.7952381   0.07071068  0.47484848  0.46909524
   0.74166667  0.7452381   0.69461316  0.72607143]
 [ 0.50048097  0.76        0.77857143  0.73795954  0.7443254   0.27482051
   0.64583333  0.63333333  0.4737242   0.57595238]
 [ 0.30271974  0.65416667  0.71428571  0.49846171  0.62396825  0.31823177
   0.65416667  0.7         0.46986706  0.61941919]
 [-0.02285714  0.5         0.73095238  0.08944272  0.45368687  0.12564103
   0.5625      0.56190476  0.53661386  0.54940476]
 [ 0.19389855  0.59583333  0.59761905  0.49983304  0.56599206  0.40473838
   0.69583333  0.72857143  0.67575502  0.69662698]
 [ 0.05748165  0.52916667  0.58095238  0.36378638  0.50146825  0.17310924
   0.575       0.7952381   0.21213203  0.53949495]
 [ 0.07379894  0.5375      0.54285714  0.41697818  0.50496032  0.52406205
   0.75833333  0.76666667  0.71294276  0.74146825]
 [ 0.32791787  0.66666667  0.66666667  0.61938193  0.64690476  0.52423188
   0.7625      0.76190476  0.74702157  0.75551587]
 [ 0.24423113  0.61666667  0.63571429  0.53937254  0.59753968  0.23092885
   0.6125      0.62857143  0.56649971  0.60718254]
 [ 0.43188593  0.7225      0.78095238  0.60235479  0.69237734  0.50533333
   0.75416667  0.75952381  0.67581168  0.72707792]
 [ 0.35549275  0.68333333  0.70238095  0.60557481  0.66186508  0.45902409
   0.72083333  0.76190476  0.63232276  0.70563492]
 [-0.18465086  0.40833333  0.47857143  0.17844571  0.38174603  0.31837474
   0.65416667  0.68333333  0.52768222  0.62257937]
 [ 0.76837474  0.89166667  0.89047619  0.88287864  0.88162698  0.24452859
   0.62833333  0.6452381   0.59090534  0.61412698]]
KNN mean:
[0.28472584 0.64316667 0.68944444 0.51714402 0.61916667 0.3277255
 0.66363889 0.69579365 0.55700009 0.63902213]
---------------------------
---------------------------
DT performance:
[[ 0.43333333  0.68333333  0.66428571  0.69467841  0.64559524  0.24481287
   0.6125      0.70952381  0.47020429  0.6044228 ]
 [ 0.29869565  0.67083333  0.68571429  0.55068718  0.64535714  0.52636364
   0.76666667  0.78333333  0.70793515  0.75265873]
 [ 0.41981774  0.69166667  0.7         0.69716503  0.66013348  0.20777001
   0.64166667  0.63809524  0.58147411  0.61571429]
 [ 0.14933333  0.55833333  0.55714286  0.52506621  0.53404762  0.09426718
   0.59        0.66666667  0.41969791  0.55720058]
 [ 0.24833333  0.60833333  0.61904762  0.62051439  0.57309524  0.32512077
   0.67916667  0.67857143  0.68602593  0.66313492]
 [ 0.23097436  0.6375      0.62857143  0.59384695  0.6175      0.50720624
   0.74166667  0.73095238  0.7292025   0.72289683]
 [ 0.42485093  0.72083333  0.74285714  0.62578628  0.7202381   0.2240524
   0.61666667  0.61428571  0.56901485  0.60095238]
 [ 0.61223968  0.8075      0.84285714  0.85888797  0.80767677  0.44057143
   0.77083333  0.79285714  0.7091282   0.76825397]
 [ 0.16841991  0.6125      0.67380952  0.40643072  0.59545455  0.35198819
   0.635       0.68571429  0.44210132  0.59140332]
 [ 0.13302899  0.57083333  0.57142857  0.52120047  0.56107143  0.01033333
   0.55        0.54761905  0.49117175  0.53178571]
 [ 0.07898551  0.575       0.6         0.42948944  0.56059524  0.46702899
   0.77916667  0.78809524  0.65855708  0.75543651]
 [ 0.50913521  0.74583333  0.76190476  0.72822402  0.73388889  0.29055901
   0.6875      0.67857143  0.64713103  0.65706349]
 [ 0.18953416  0.5875      0.59047619  0.57698289  0.56861111  0.13342857
   0.54583333  0.57380952  0.53946733  0.53011905]
 [-0.20480418  0.385       0.42142857  0.20908244  0.3572619   0.303
   0.6125      0.60238095  0.59577688  0.58214286]
 [ 0.44986729  0.7575      0.75238095  0.59877654  0.70579365  0.02693973
   0.54916667  0.57857143  0.3899686   0.52396825]
 [-0.03695527  0.50416667  0.55238095  0.20606602  0.44493506  0.29120448
   0.6275      0.71190476  0.41198013  0.60827201]
 [ 0.5025974   0.72916667  0.72619048  0.71336457  0.71079365  0.58782051
   0.82083333  0.82857143  0.83811926  0.81301587]
 [ 0.23434174  0.68166667  0.7952381   0.53903884  0.64212121  0.47735743
   0.67083333  0.67857143  0.65764547  0.63952381]
 [ 0.37968944  0.735       0.77857143  0.59877654  0.72027778  0.13397436
   0.625       0.61904762  0.63796528  0.61190476]
 [ 0.32363072  0.575       0.62142857  0.52412527  0.55361111 -0.07099831
   0.5125      0.54761905  0.42287807  0.48813492]
 [-0.02407746  0.535       0.7         0.24421497  0.51171717  0.21802899
   0.5875      0.59047619  0.47280674  0.57440476]
 [ 0.15802899  0.6         0.6         0.58366559  0.58595238  0.60055901
   0.77083333  0.80238095  0.75358931  0.7622619 ]
 [ 0.25458242  0.59166667  0.61666667  0.52120906  0.57956349 -0.04604212
   0.515       0.64761905  0.26392552  0.49987734]
 [ 0.16161616  0.53333333  0.53333333  0.45216132  0.50964286  0.50332701
   0.7375      0.74761905  0.70185091  0.7090873 ]
 [ 0.32095916  0.6375      0.62857143  0.57758135  0.60809524  0.45869565
   0.64583333  0.65        0.60435553  0.63845238]
 [ 0.34766667  0.64583333  0.64047619  0.64451502  0.6247619   0.56656522
   0.7875      0.79285714  0.7307079   0.78349206]
 [ 0.35065583  0.6475      0.71428571  0.51417684  0.63333333  0.5750303
   0.75416667  0.75952381  0.69870524  0.7280303 ]
 [-0.01839248  0.48333333  0.50238095  0.39082483  0.45198413  0.42307039
   0.7625      0.76190476  0.75227271  0.74404762]
 [ 0.18436665  0.625       0.62380952  0.38430826  0.58190476  0.1040303
   0.57916667  0.57619048  0.52720568  0.56309524]
 [ 0.41604762  0.74166667  0.77857143  0.7306825   0.74821429  0.31761905
   0.555       0.62142857  0.3952806   0.5241811 ]]
DT mean:
[0.25655009 0.62927778 0.65412698 0.542051   0.60644096 0.30978949
 0.65766667 0.68015873 0.58353818 0.6381645 ]
---------------------------
---------------------------
RF performance:
[[ 0.472669    0.70833333  0.69761905  0.70562954  0.66694444  0.18786749
   0.7125      0.78095238  0.2026586   0.66015512]
 [ 0.40056522  0.6         0.61190476  0.55395446  0.5618254   0.2026294
   0.74166667  0.78095238  0.59525776  0.72369048]
 [ 0.25512422  0.69583333  0.71190476  0.707837    0.67469697  0.28952991
   0.64583333  0.63809524  0.59410536  0.62607143]
 [ 0.14230769  0.62916667  0.63333333  0.63188104  0.61833333  0.04119586
   0.6125      0.71904762  0.          0.59479798]
 [ 0.43795784  0.74166667  0.76428571  0.70927715  0.74095238  0.41889855
   0.63333333  0.63333333  0.54065787  0.60242063]
 [ 0.35982051  0.7         0.6952381   0.58919426  0.68706349  0.56525253
   0.82916667  0.82857143  0.76368824  0.81642857]
 [ 0.10999435  0.60833333  0.63571429  0.48527459  0.58559524  0.28672596
   0.6625      0.66190476  0.46532384  0.65107143]
 [ 0.78882831  0.83        0.85952381  0.84294955  0.83212121  0.39972527
   0.71666667  0.75        0.65049838  0.71051587]
 [ 0.20712957  0.58        0.64761905  0.42120906  0.56650794  0.14251918
   0.6425      0.69761905  0.47803619  0.62828644]
 [ 0.225       0.54166667  0.53571429  0.4729494   0.51583333  0.16156988
   0.59583333  0.61904762  0.51572491  0.57533189]
 [ 0.26163203  0.60416667  0.61666667  0.47062306  0.59198413  0.34062582
   0.6875      0.70952381  0.65708294  0.6852381 ]
 [ 0.62683983  0.7125      0.7452381   0.53680266  0.68497475  0.38308503
   0.7375      0.70952381  0.62899023  0.70257937]
 [ 0.34267834  0.65833333  0.67380952  0.62518975  0.63623016  0.01934225
   0.49166667  0.51904762  0.60192638  0.47785714]
 [-0.12108978  0.50916667  0.57857143  0.15722864  0.48051587  0.13071906
   0.6375      0.63333333  0.68314564  0.60964286]
 [ 0.21813665  0.645       0.76190476  0.37563001  0.64414502  0.1712987
   0.63166667  0.68571429  0.59261232  0.6066811 ]
 [-0.06008225  0.45833333  0.52142857  0.22908837  0.38588745  0.37428571
   0.6525      0.73095238  0.51766796  0.62888889]
 [ 0.65462451  0.78333333  0.78333333  0.81539265  0.7652381   0.36441472
   0.75        0.72857143  0.70491999  0.71289683]
 [ 0.34453782  0.70833333  0.7952381   0.2601534   0.65979798  0.46930837
   0.72083333  0.73809524  0.75708101  0.69928571]
 [ 0.61139194  0.6375      0.68333333  0.59138709  0.62295455  0.32966667
   0.70833333  0.7         0.53744533  0.69214286]
 [ 0.36143892  0.67083333  0.71428571  0.52907816  0.66912698  0.16333811
   0.525       0.56666667  0.28575892  0.47366522]
 [ 0.04142857  0.54        0.75238095  0.06324555  0.51651515  0.11533333
   0.525       0.52619048  0.46514912  0.50869048]
 [ 0.22736364  0.57083333  0.57619048  0.59290874  0.55900794  0.45241126
   0.625       0.67142857  0.57954062  0.60503968]
 [ 0.24108138  0.55        0.56904762  0.47764823  0.52849206  0.1670844
   0.5575      0.70714286  0.35355339  0.537114  ]
 [ 0.17039394  0.5125      0.50714286  0.46031622  0.48738095  0.52383983
   0.63333333  0.65238095  0.69639964  0.61890332]
 [ 0.31615584  0.65        0.66428571  0.54670649  0.62357143  0.36503497
   0.7625      0.76190476  0.75791248  0.7527381 ]
 [ 0.12533333  0.6375      0.63571429  0.30689141  0.60253968  0.40738573
   0.64166667  0.65        0.63656965  0.63238095]
 [ 0.42837474  0.6675      0.76428571  0.48091352  0.64275613  0.52948718
   0.78333333  0.78095238  0.60157338  0.76369048]
 [ 0.04110823  0.5         0.5547619   0.31164083  0.48519841  0.46080952
   0.79166667  0.79285714  0.61976892  0.77519841]
 [-0.03340768  0.55833333  0.60714286  0.41681205  0.51584776  0.22799236
   0.6125      0.61904762  0.63704619  0.58214286]
 [ 0.64761905  0.84583333  0.87142857  0.705153    0.84190476  0.48850649
   0.665       0.72619048  0.58631657  0.66368326]]
RF mean:
[0.29483186 0.63516667 0.67230159 0.5024322  0.61313143 0.30599612
 0.66441667 0.69063492 0.55688039 0.64390765]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.53809524  0.          0.34909091  0.
   0.5         0.74761905  0.          0.42681818]
 [ 0.          0.5         0.57142857  0.          0.36242424  0.
   0.5         0.63809524  0.          0.38909091]
 [ 0.          0.5         0.5547619   0.          0.35575758  0.13675214
   0.57083333  0.55714286  0.34687618  0.48535714]
 [ 0.          0.5         0.52142857  0.          0.34242424  0.
   0.5         0.73095238  0.          0.42136364]
 [ 0.          0.5         0.62142857  0.          0.38242424  0.32853958
   0.6625      0.66190476  0.60611934  0.64218254]
 [ 0.14615385  0.575       0.56428571  0.17071068  0.43416667  0.40183983
   0.69583333  0.71666667  0.60115157  0.66890332]
 [ 0.          0.5         0.6047619   0.          0.37575758  0.11123188
   0.55416667  0.55        0.15236034  0.41861111]
 [ 0.          0.5         0.68095238  0.          0.405       0.
   0.5         0.63809524  0.          0.38909091]
 [ 0.          0.5         0.68095238  0.          0.405       0.
   0.5         0.68095238  0.          0.405     ]
 [ 0.          0.5         0.52142857  0.          0.34242424  0.
   0.5         0.5547619   0.          0.35575758]
 [ 0.          0.5         0.62142857  0.          0.38242424  0.
   0.5         0.58809524  0.          0.36909091]
 [ 0.          0.5         0.62142857  0.          0.38242424  0.
   0.5         0.6047619   0.          0.37575758]
 [ 0.          0.5         0.58809524  0.          0.36909091  0.
   0.5         0.57142857  0.          0.36242424]
 [ 0.          0.5         0.66666667  0.          0.39969697  0.
   0.5         0.53809524  0.          0.34909091]
 [ 0.          0.5         0.71428571  0.          0.41590909  0.
   0.5         0.65238095  0.          0.39439394]
 [ 0.          0.5         0.62142857  0.          0.38242424  0.
   0.5         0.68095238  0.          0.405     ]
 [ 0.          0.5         0.57142857  0.          0.36242424  0.
   0.5         0.6047619   0.          0.37575758]
 [ 0.          0.5         0.81190476  0.          0.447669    0.
   0.5         0.57142857  0.          0.36242424]
 [ 0.          0.5         0.68095238  0.          0.405       0.
   0.5         0.53809524  0.          0.34909091]
 [ 0.          0.5         0.63809524  0.          0.38909091  0.
   0.5         0.62142857  0.          0.38242424]
 [ 0.          0.5         0.79761905  0.          0.44318182 -0.04769231
   0.475       0.46428571  0.06969234  0.33357143]
 [ 0.03822222  0.52083333  0.50714286  0.10773503  0.37380952  0.
   0.5         0.63809524  0.          0.38909091]
 [ 0.          0.5         0.58809524  0.          0.36909091  0.
   0.5         0.74761905  0.          0.42681818]
 [ 0.          0.5         0.53809524  0.          0.34909091  0.
   0.5         0.5547619   0.          0.35575758]
 [ 0.          0.5         0.5547619   0.          0.35575758  0.11571906
   0.55833333  0.55        0.15236034  0.41861111]
 [ 0.          0.5         0.53809524  0.          0.34909091  0.17575758
   0.58333333  0.61428571  0.28867513  0.49333333]
 [ 0.          0.5         0.69761905  0.          0.41045455  0.03623188
   0.51666667  0.53333333  0.13938469  0.4006241 ]
 [ 0.          0.5         0.63809524  0.          0.38909091  0.
   0.5         0.6047619   0.          0.37575758]
 [ 0.          0.5         0.62142857  0.          0.38242424  0.
   0.5         0.5547619   0.          0.35575758]
 [ 0.          0.5         0.63809524  0.          0.38909091  0.
   0.5         0.66666667  0.          0.39969697]]
SVM mean:
[0.00614587 0.50319444 0.61714286 0.00928152 0.38339019 0.04194599
 0.52055556 0.61253968 0.078554   0.40922162]
---------------------------
---------------------------
GBM performance:
[[ 0.44989855  0.7625      0.75952381  0.71621197  0.74468254  0.12945195
   0.56        0.75238095  0.26791246  0.53207071]
 [ 0.44640504  0.75        0.78095238  0.64381762  0.73162698  0.59004329
   0.7875      0.83095238  0.70122898  0.7734127 ]
 [ 0.37775558  0.6875      0.71666667  0.63656965  0.65783189  0.2034188
   0.60416667  0.59047619  0.5290129   0.57238095]
 [ 0.19230769  0.57916667  0.57857143  0.56911573  0.57166667  0.17945195
   0.585       0.71904762  0.35943917  0.56616162]
 [ 0.34679089  0.66666667  0.7         0.58214804  0.64190476  0.29089855
   0.59583333  0.5952381   0.44097022  0.54944444]
 [ 0.21022222  0.59166667  0.58333333  0.5559618   0.56916667  0.495338
   0.75        0.74761905  0.72117973  0.73392857]
 [ 0.26537474  0.62916667  0.67857143  0.49022747  0.60572872  0.04782051
   0.525       0.52142857  0.46211129  0.49785714]
 [ 0.59316466  0.765       0.82380952  0.69083589  0.76071068  0.37907792
   0.65833333  0.71666667  0.51728504  0.66876984]
 [ 0.11596639  0.575       0.6952381   0.33460652  0.54767677  0.13319328
   0.5925      0.7         0.39584377  0.57457071]
 [ 0.03002899  0.53333333  0.53333333  0.42682728  0.49928571  0.03672596
   0.5         0.51666667  0.41497084  0.4797619 ]
 [ 0.16450216  0.55        0.61904762  0.41462644  0.51075397  0.52536232
   0.75833333  0.77380952  0.73003397  0.75190476]
 [ 0.49067965  0.74583333  0.77857143  0.64162148  0.72329365  0.323786
   0.66666667  0.6952381   0.57556542  0.64337302]
 [ 0.29798137  0.65833333  0.69047619  0.57169612  0.63922078 -0.06966874
   0.4875      0.52857143  0.35260387  0.45674603]
 [-0.12714286  0.44        0.56666667  0.06324555  0.37969697  0.45656522
   0.73333333  0.72619048  0.70561739  0.71456349]
 [ 0.11857143  0.6075      0.71666667  0.25268827  0.55191919  0.06279221
   0.53166667  0.63333333  0.2529285   0.48664141]
 [-0.07727273  0.4625      0.57380952  0.          0.36106061  0.14428571
   0.59        0.71428571  0.27537759  0.5575    ]
 [ 0.51878788  0.76666667  0.76904762  0.70157338  0.74095238  0.36191888
   0.625       0.66428571  0.53854107  0.59714286]
 [ 0.0147619   0.50166667  0.74761905  0.1         0.47469697  0.26218615
   0.625       0.63333333  0.50679327  0.59095238]
 [ 0.09751918  0.545       0.66428571  0.25943917  0.50156566  0.32902899
   0.64166667  0.63333333  0.59670649  0.60095238]
 [ 0.40256446  0.69583333  0.73809524  0.57628549  0.67727633  0.09074459
   0.55        0.61190476  0.33451172  0.51358586]
 [ 0.          0.5         0.79761905  0.          0.44318182  0.16903704
   0.58333333  0.57619048  0.46041903  0.55222222]
 [ 0.291       0.6625      0.66190476  0.59544463  0.63714286  0.35670996
   0.675       0.74761905  0.48783152  0.63626984]
 [ 0.11352814  0.5375      0.58095238  0.40367279  0.49666667  0.03361345
   0.515       0.73333333  0.26213203  0.46786436]
 [ 0.07069697  0.54166667  0.5547619   0.41812865  0.51035714  0.56696337
   0.78333333  0.79761905  0.71029548  0.76140332]
 [ 0.16815385  0.59166667  0.59761905  0.45772288  0.5632684   0.29969565
   0.65        0.65238095  0.62388174  0.63869048]
 [ 0.3025974   0.64583333  0.66904762  0.48886473  0.60718254  0.54933333
   0.74166667  0.75        0.70184038  0.73488095]
 [ 0.26170807  0.63        0.72857143  0.41999153  0.59017677  0.51585082
   0.79166667  0.79285714  0.7555314   0.77607143]
 [-0.13679654  0.45416667  0.54047619  0.14082483  0.40969697  0.62127329
   0.8         0.82857143  0.77048899  0.80083333]
 [ 0.23463203  0.61666667  0.68809524  0.39524724  0.57366522  0.28504141
   0.60833333  0.61904762  0.5735694   0.59170635]
 [ 0.4917619   0.76666667  0.7952381   0.70139317  0.75210317  0.08571429
   0.5125      0.63333333  0.15731322  0.46080808]]
GBM mean:
[0.22420497 0.61533333 0.67761905 0.44162628 0.58247198 0.28185514
 0.63427778 0.68119048 0.50606456 0.6094157 ]
---------------------------
---------------------------
BDDAE performance:
[[-0.31968655  0.34166667  0.34615385  0.29254118  0.33280428  0.08098386
   0.54166667  0.65384615  0.43155834  0.52629191]
 [ 0.20773262  0.6047619   0.60769231  0.59049387  0.59895694  0.32554913
   0.66625     0.67692308  0.63992569  0.65255952]
 [ 0.01692882  0.50833333  0.50769231  0.49480704  0.50142108  0.30751704
   0.6547619   0.65384615  0.64227032  0.6473618 ]
 [ 0.16965637  0.58452381  0.59230769  0.52510019  0.56748429  0.10850142
   0.55        0.64615385  0.42828985  0.54008055]
 [ 0.0403006   0.5175      0.50769231  0.48133326  0.48984644  0.30196395
   0.6547619   0.63846154  0.59907598  0.61884264]
 [ 0.25602023  0.62857143  0.63076923  0.58853076  0.60984519 -0.01880923
   0.49166667  0.48461538  0.45749179  0.47115891]
 [ 0.18164499  0.5925      0.62307692  0.5431652   0.57713143  0.00564605
   0.50357143  0.51538462  0.38010286  0.46480519]
 [ 0.26597709  0.63194444  0.69230769  0.60245917  0.62971977  0.19143894
   0.59375     0.63846154  0.51452392  0.58142342]
 [ 0.52426501  0.75555556  0.81538462  0.71216831  0.75272034  0.3604308
   0.65972222  0.76923077  0.5535448   0.66329783]
 [ 0.10361012  0.55119048  0.56153846  0.52048547  0.54185231  0.18684441
   0.59285714  0.6         0.57398803  0.58705597]
 [ 0.13153498  0.565       0.60769231  0.4301204   0.52991939  0.17760673
   0.59        0.63846154  0.44098257  0.56221692]
 [-0.08444778  0.45875     0.5         0.36117284  0.44317635  0.54184289
   0.77875     0.76923077  0.76797409  0.76335717]
 [ 0.20882987  0.61        0.63076923  0.57081295  0.59559636  0.18952906
   0.5952381   0.59230769  0.58224732  0.58682942]
 [ 0.13730917  0.57361111  0.59230769  0.56711449  0.56068999  0.36630423
   0.68452381  0.68461538  0.65986765  0.67302337]
 [-0.16684708  0.41666667  0.5         0.26788879  0.40601113 -0.01801582
   0.48875     0.5         0.47567645  0.48288191]
 [-0.05507113  0.47125     0.51538462  0.40177914  0.4629078   0.11472772
   0.54861111  0.69230769  0.3001387   0.51487697]
 [ 0.38974013  0.69404762  0.7         0.65975628  0.68015598  0.16254726
   0.58125     0.62307692  0.48953641  0.55876302]
 [ 0.13741335  0.58636364  0.78461538  0.34938678  0.55905091  0.06456533
   0.53214286  0.53846154  0.45967964  0.50681777]
 [ 0.09551237  0.54861111  0.61538462  0.47941102  0.54084833  0.0696848
   0.53571429  0.53846154  0.50342441  0.52078428]
 [ 0.01593491  0.5075      0.56923077  0.34618728  0.4779964  -0.08405269
   0.45875     0.52307692  0.32795547  0.43894897]
 [ 0.222162    0.6         0.76153846  0.45736056  0.59858876  0.02412355
   0.51190476  0.51538462  0.4955354   0.50495515]
 [ 0.09885716  0.55119048  0.53846154  0.48356271  0.51936494  0.1135121
   0.55125     0.62307692  0.39569356  0.52213868]
 [-0.21248327  0.3925      0.42307692  0.30751612  0.37873912  0.6657105
   0.81333333  0.89230769  0.78889358  0.83068011]
 [ 0.06395205  0.53214286  0.53076923  0.50415393  0.51762196  0.19277663
   0.59404762  0.6         0.56340555  0.58262055]
 [ 0.1824067   0.59285714  0.59230769  0.56959984  0.58054279  0.0482318
   0.525       0.52307692  0.47656728  0.50776571]
 [ 0.24837731  0.62142857  0.63076923  0.60239258  0.61745116  0.35167225
   0.67261905  0.68461538  0.64849914  0.66787571]
 [ 0.38955019  0.68194444  0.76153846  0.63428023  0.68898767  0.50181288
   0.74880952  0.75384615  0.72635483  0.74250902]
 [-0.03368458  0.48375     0.50769231  0.41524755  0.47220749  0.27312754
   0.63125     0.66153846  0.6077558   0.63155494]
 [ 0.04949698  0.52375     0.56153846  0.48344211  0.5174662   0.33441556
   0.66666667  0.66923077  0.65270172  0.66078671]
 [-0.13514164  0.43        0.46923077  0.35528048  0.41802155 -0.06083068
   0.47361111  0.56923077  0.29781606  0.45526016]]
BDDAE mean:
[0.10432837 0.55193038 0.58923077 0.48658502 0.53890421 0.1959786
 0.59637434 0.62897436 0.52938257 0.58225081]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.53809524 0.         0.34909091 0.
  0.5        0.74761905 0.         0.42681818]
 [0.         0.5        0.57142857 0.         0.36242424 0.
  0.5        0.63809524 0.         0.38909091]
 [0.         0.5        0.5547619  0.         0.35575758 0.
  0.5        0.47857143 0.         0.32333333]
 [0.         0.5        0.52142857 0.         0.34242424 0.
  0.5        0.73095238 0.         0.42136364]
 [0.         0.5        0.62142857 0.         0.38242424 0.
  0.5        0.52142857 0.         0.34242424]
 [0.         0.5        0.47857143 0.         0.32333333 0.
  0.5        0.53809524 0.         0.34909091]
 [0.         0.5        0.6047619  0.         0.37575758 0.
  0.5        0.47857143 0.         0.32333333]
 [0.         0.5        0.68095238 0.         0.405      0.
  0.5        0.63809524 0.         0.38909091]
 [0.         0.5        0.68095238 0.         0.405      0.
  0.5        0.68095238 0.         0.405     ]
 [0.         0.5        0.52142857 0.         0.34242424 0.
  0.5        0.5547619  0.         0.35575758]
 [0.         0.5        0.62142857 0.         0.38242424 0.
  0.5        0.58809524 0.         0.36909091]
 [0.         0.5        0.62142857 0.         0.38242424 0.
  0.5        0.6047619  0.         0.37575758]
 [0.         0.5        0.58809524 0.         0.36909091 0.
  0.5        0.57142857 0.         0.36242424]
 [0.         0.5        0.66666667 0.         0.39969697 0.
  0.5        0.53809524 0.         0.34909091]
 [0.         0.5        0.71428571 0.         0.41590909 0.
  0.5        0.65238095 0.         0.39439394]
 [0.         0.5        0.62142857 0.         0.38242424 0.
  0.5        0.68095238 0.         0.405     ]
 [0.         0.5        0.57142857 0.         0.36242424 0.
  0.5        0.6047619  0.         0.37575758]
 [0.         0.5        0.81190476 0.         0.447669   0.
  0.5        0.57142857 0.         0.36242424]
 [0.         0.5        0.68095238 0.         0.405      0.
  0.5        0.53809524 0.         0.34909091]
 [0.         0.5        0.63809524 0.         0.38909091 0.
  0.5        0.62142857 0.         0.38242424]
 [0.         0.5        0.79761905 0.         0.44318182 0.
  0.5        0.50714286 0.         0.33606061]
 [0.         0.5        0.50714286 0.         0.33606061 0.
  0.5        0.63809524 0.         0.38909091]
 [0.         0.5        0.58809524 0.         0.36909091 0.
  0.5        0.74761905 0.         0.42681818]
 [0.         0.5        0.53809524 0.         0.34909091 0.
  0.5        0.5547619  0.         0.35575758]
 [0.         0.5        0.5547619  0.         0.35575758 0.
  0.5        0.50714286 0.         0.33606061]
 [0.         0.5        0.53809524 0.         0.34909091 0.
  0.5        0.53809524 0.         0.34909091]
 [0.         0.5        0.69761905 0.         0.41045455 0.
  0.5        0.52142857 0.         0.34242424]
 [0.         0.5        0.63809524 0.         0.38909091 0.
  0.5        0.6047619  0.         0.37575758]
 [0.         0.5        0.62142857 0.         0.38242424 0.
  0.5        0.5547619  0.         0.35575758]
 [0.         0.5        0.63809524 0.         0.38909091 0.
  0.5        0.66666667 0.         0.39969697]]
DUMMY mean:
[0.         0.5        0.61428571 0.         0.37843745 0.
 0.5        0.59396825 0.         0.37057576]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_44
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.285 0.643 0.689 0.517 0.619 0.328 0.664 0.696 0.557 0.639]
 [0.257 0.629 0.654 0.542 0.606 0.31  0.658 0.68  0.584 0.638]
 [0.295 0.635 0.672 0.502 0.613 0.306 0.664 0.691 0.557 0.644]
 [0.006 0.503 0.617 0.009 0.383 0.042 0.521 0.613 0.079 0.409]
 [0.224 0.615 0.678 0.442 0.582 0.282 0.634 0.681 0.506 0.609]
 [0.104 0.552 0.589 0.487 0.539 0.196 0.596 0.629 0.529 0.582]
 [0.    0.5   0.614 0.    0.378 0.    0.5   0.594 0.    0.371]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.226 0.112 0.098 0.205 0.121 0.145 0.074 0.071 0.138 0.079]
 [0.182 0.091 0.094 0.151 0.096 0.196 0.09  0.082 0.138 0.093]
 [0.221 0.094 0.095 0.182 0.102 0.15  0.08  0.075 0.164 0.083]
 [0.027 0.014 0.075 0.036 0.028 0.099 0.049 0.069 0.165 0.075]
 [0.193 0.097 0.085 0.211 0.11  0.191 0.094 0.087 0.171 0.104]
 [0.181 0.089 0.105 0.113 0.092 0.18  0.088 0.091 0.127 0.095]
 [0.    0.    0.078 0.    0.03  0.    0.    0.075 0.    0.029]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 79.  17.  14.  40.  20.  44.  11.  10.  25.  12.]
 [ 71.  14.  14.  28.  16.  63.  14.  12.  24.  15.]
 [ 75.  15.  14.  36.  17.  49.  12.  11.  29.  13.]
 [439.   3.  12. 388.   7. 236.   9.  11. 210.  18.]
 [ 86.  16.  13.  48.  19.  68.  15.  13.  34.  17.]
 [173.  16.  18.  23.  17.  92.  15.  14.  24.  16.]
 [  0.   0.  13.   0.   8.   0.   0.  13.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_44
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  19.008
step (sec):  19.008
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  63
Elapsed time: 519.5486018856367 minutes
Elapsed time: 8.659143364760611 hours
