2024-05-10 04:48:53.942146: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-10 04:48:57.729248: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-10 04:49:07.458872: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  19.0
step (sec):  14.25
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  4.75
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_45\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

[0m 1s/step - loss: 0.8397 - mean_squared_error: 0.8397
[1m12/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 6ms/step - loss: 9.1956 - mean_squared_error: 9.1956 
[1m28/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 10.7725 - mean_squared_error: 10.7725
[1m44/57[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - loss: 10.4874 - mean_squared_error: 10.4874
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 8ms/step - loss: 10.3639 - mean_squared_error: 10.3639 - val_loss: 28.6550 - val_mean_squared_error: 28.6550
Epoch 2/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 0.2296 - mean_squared_error: 0.2296
[1m11/57[0m [32m━━━[0m[37m━━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 6.0784 - mean_squared_error: 6.0784 
[1m25/57[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 7.3850 - mean_squared_error: 7.3850
[1m41/57[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - loss: 7.8723 - mean_squared_error: 7.8723
[1m54/57[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 4ms/step - loss: 8.3919 - mean_squared_error: 8.3919
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 8.4671 - mean_squared_error: 8.4671 - val_loss: 28.6280 - val_mean_squared_error: 28.6281
Epoch 3/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 15.6843 - mean_squared_error: 15.6843
[1m15/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 10.2363 - mean_squared_error: 10.2363 
[1m28/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 10.9299 - mean_squared_error: 10.9299
[1m43/57[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - loss: 10.6918 - mean_squared_error: 10.6918
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.4455 - mean_squared_error: 10.4455
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.4274 - mean_squared_error: 10.4274 - val_loss: 28.6188 - val_mean_squared_error: 28.6188
Epoch 4/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 0.0755 - mean_squared_error: 0.0755
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 7.2818 - mean_squared_error: 7.2818 
[1m27/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 9.9339 - mean_squared_error: 9.9339
[1m39/57[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 5ms/step - loss: 10.2826 - mean_squared_error: 10.2826
[1m54/57[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 4ms/step - loss: 10.1546 - mean_squared_error: 10.1546
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.1043 - mean_squared_error: 10.1043 - val_loss: 28.6137 - val_mean_squared_error: 28.6137
Epoch 5/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 1.7412 - mean_squared_error: 1.7412
[1m15/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 10.6161 - mean_squared_error: 10.6161
[1m29/57[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 11.2682 - mean_squared_error: 11.2682
[1m44/57[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - loss: 10.9538 - mean_squared_error: 10.9538
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.6385 - mean_squared_error: 10.6385 - val_loss: 28.6119 - val_mean_squared_error: 28.6119
(64, 1188, 6)
Model: "sequential_1196"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1197"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2394              │ (None, 4752, 6)        │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2394           │ (None, 4752, 6)        │           366 │
│ (Conv1DTranspose)               │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2395              │ (None, 19008, 6)       │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2395           │ (None, 19008, 1)       │           121 │
│ (Conv1DTranspose)               │                        │               │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1:03[0m 1s/step - loss: 0.9571 - mean_squared_error: 0.9571
[1m11/57[0m [32m━━━[0m[37m━━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.6720 - mean_squared_error: 0.6720 
[1m22/57[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.5084 - mean_squared_error: 0.5084
[1m34/57[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.4049 - mean_squared_error: 0.4049
[1m47/57[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 5ms/step - loss: 0.3366 - mean_squared_error: 0.3366
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 8ms/step - loss: 0.2974 - mean_squared_error: 0.2974 - val_loss: 0.0247 - val_mean_squared_error: 0.0247
Epoch 2/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 25ms/step - loss: 0.0114 - mean_squared_error: 0.0114
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0207 - mean_squared_error: 0.0207 
[1m25/57[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0225 - mean_squared_error: 0.0225
[1m38/57[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0248 - mean_squared_error: 0.0248
[1m51/57[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - loss: 0.0257 - mean_squared_error: 0.0257
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0259 - mean_squared_error: 0.0259 - val_loss: 0.0206 - val_mean_squared_error: 0.0206
Epoch 3/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 26ms/step - loss: 0.0097 - mean_squared_error: 0.0097
[1m12/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0106 - mean_squared_error: 0.0106 
[1m23/57[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0134 - mean_squared_error: 0.0134
[1m35/57[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0166 - mean_squared_error: 0.0166
[1m47/57[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 5ms/step - loss: 0.0181 - mean_squared_error: 0.0181
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0201 - val_mean_squared_error: 0.0201
Epoch 4/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 25ms/step - loss: 0.0123 - mean_squared_error: 0.0123
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0123 - mean_squared_error: 0.0123 
[1m26/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0129 - mean_squared_error: 0.0129
[1m39/57[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0151 - mean_squared_error: 0.0151
[1m54/57[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 4ms/step - loss: 0.0171 - mean_squared_error: 0.0171
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0172 - val_mean_squared_error: 0.0172
Epoch 5/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - loss: 0.0670 - mean_squared_error: 0.0670
[1m16/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0311 - mean_squared_error: 0.0311 
[1m31/57[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0260 - mean_squared_error: 0.0260
[1m43/57[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0236 - mean_squared_error: 0.0236
[1m56/57[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 4ms/step - loss: 0.0224 - mean_squared_error: 0.0224
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0224 - mean_squared_error: 0.0224 - val_loss: 0.0180 - val_mean_squared_error: 0.0180
(19008, 1, 5)
Model: "sequential_1197"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
┌─────────────────────┬───────────────────┬────────────┬───────────────────┐
│ Layer (type)        │ Output Shape      │    Param # │ Connected to      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputGSR            │ (None, 19008, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputPPG            │ (None, 19008, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1196     │ (None, 1188, 6)   │        411 │ inputGSR[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1197     │ (None, 1188, 6)   │        411 │ inputPPG[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_598     │ (None, 1188, 12)  │          0 │ sequential_1196[… │
│ (Concatenate)       │                   │            │ sequential_1197[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ permute_598         │ (None, 12, 1188)  │          0 │ concatenate_598[… │
│ (Permute)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_598         │ (None, 14256)     │          0 │ permute_598[0][0] │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_598         │ (None, 14256)     │          0 │ flatten_598[0][0] │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_598 (Dense)   │ (None, 1)         │     14,257 │ dropout_598[0][0] │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 15,079 (58.90 KB)
 Trainable params: 15,079 (58.90 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1:13[0m 1s/step - binary_accuracy: 0.0000e+00 - loss: 1.1724
[1m14/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.2346 - loss: 0.9227     
[1m28/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.2879 - loss: 0.8590
[1m46/57[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.3548 - loss: 0.8241
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 8ms/step - binary_accuracy: 0.3747 - loss: 0.8159 - val_binary_accuracy: 0.8571 - val_loss: 0.5594
Epoch 2/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3959
[1m17/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6991 - loss: 0.6444 
[1m34/57[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6610 - loss: 0.6540
[1m51/57[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6420 - loss: 0.6718
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6356 - loss: 0.6755 - val_binary_accuracy: 0.8571 - val_loss: 0.4936
Epoch 3/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.5736
[1m14/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.9143 - loss: 0.4643 
[1m31/57[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8099 - loss: 0.5906
[1m48/57[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7533 - loss: 0.6365
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7311 - loss: 0.6487 - val_binary_accuracy: 0.8571 - val_loss: 0.4753
Epoch 4/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.3016
[1m15/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5322 - loss: 0.5633 
[1m32/57[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5598 - loss: 0.5793
[1m49/57[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5846 - loss: 0.5871
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5980 - loss: 0.5886 - val_binary_accuracy: 0.8571 - val_loss: 0.3902
Epoch 5/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 0.0000e+00 - loss: 1.1162
[1m14/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.5248 - loss: 0.8426     
[1m28/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6339 - loss: 0.7598
[1m42/57[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6679 - loss: 0.7214
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6860 - loss: 0.6920 - val_binary_accuracy: 0.8571 - val_loss: 0.3994
Epoch 6/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.3807
[1m15/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7211 - loss: 0.5556 
[1m33/57[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7104 - loss: 0.5869
[1m50/57[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7056 - loss: 0.5880
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7082 - loss: 0.5873 - val_binary_accuracy: 0.8571 - val_loss: 0.4051
Epoch 7/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.3547
[1m15/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8983 - loss: 0.4689 
[1m33/57[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8044 - loss: 0.5374
[1m50/57[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7806 - loss: 0.5615
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7770 - loss: 0.5623 - val_binary_accuracy: 0.8571 - val_loss: 0.4202
Epoch 8/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 23ms/step - binary_accuracy: 0.0000e+00 - loss: 0.9111
[1m16/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5874 - loss: 0.5702     
[1m33/57[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6842 - loss: 0.5024
[1m51/57[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6882 - loss: 0.5214
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6922 - loss: 0.5264 - val_binary_accuracy: 0.5714 - val_loss: 0.6548
Epoch 9/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.9577
[1m17/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5117 - loss: 0.6370     
[1m35/57[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.5922 - loss: 0.5862
[1m52/57[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6250 - loss: 0.5618
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6301 - loss: 0.5583 - val_binary_accuracy: 0.5714 - val_loss: 0.5714
Epoch 10/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 25ms/step - binary_accuracy: 1.0000 - loss: 0.4443
[1m17/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.9512 - loss: 0.3663 
[1m35/57[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8930 - loss: 0.4197
[1m52/57[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8734 - loss: 0.4326
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8678 - loss: 0.4375 - val_binary_accuracy: 0.8571 - val_loss: 0.5110

[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.5506693  0.87429774 0.64042866 0.72456306 0.25693142 0.59290135
 0.8984849  0.50791514 0.8865588  0.4807566  0.67547286 0.67927456
 0.16007128 0.3151037  0.61091405 0.69037914]
predicted [1 1 1 1 0 1 1 1 1 0 1 1 0 0 1 1]
expected [ True  True  True  True False  True  True False False  True  True  True
 False  True False False]
accuracy: 0.625
confusion matrix: 
[[2 4]
 [2 8]]
              precision    recall  f1-score   support

       False       0.50      0.33      0.40         6
        True       0.67      0.80      0.73        10

    accuracy                           0.62        16
   macro avg       0.58      0.57      0.56        16
weighted avg       0.60      0.62      0.60        16

macro avg f1-score: 0.5636363636363637
macro avg (UAR): 0.5666666666666667
Sensitivity:  0.3333333333333333
Specificity:  0.8
g-mean:  0.5163977794943222
-------- Model Performance ----------: 
accuracy:  [0.5625 0.5625 0.6875 0.75   0.5    0.5625 0.5    0.625  0.625  0.    ]
gmean:  [0.54772256 0.54772256 0.54772256 0.57735027 0.5        0.54772256
 0.5        0.59160798 0.51639778 0.        ]
f1_score:  [0.5465587  0.5465587  0.61352657 0.66666667 0.49206349 0.5465587
 0.49206349 0.6        0.56363636 0.        ]
UAR:  [0.55       0.55       0.61666667 0.66666667 0.56666667 0.55
 0.5        0.6        0.56666667 0.        ]
Cohen Kappa score:  [0.09677419 0.09677419 0.25925926 0.38461538 0.11111111 0.09677419
 0.         0.2        0.14285714 0.        ]
Split Repetition number:  9
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [55 27 40  6 41 30 34 13  1 74 23 48 69 26 33 78  7  0 11 35 77 18 70 20
 45 43 58 47 63 56 62 29 38 10 14 53 21  2 57 79 50 75 12 76 64 28 37 46
 25 68  5 15 16 39 52 49  9 61 24 54 17 60 72 19] TEST: [73 59 22  3 51 65 36  4 66 42 67  8 71 31 44 32]
(DL) TRAIN number of instances:  64
(DL) TEST number of instances:  16
(DL) Total number of instances (TRAIN+TEST):  80
C:\Users\Javier\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\LocalCache\local-packages\Python312\site-packages\keras\src\layers\convolutional\base_conv.py:99: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.
  super().__init__(
----- train_GSR_AE -------
Model: "sequential_1198"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2396              │ (None, 4752, 6)        │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2396           │ (None, 4752, 6)        │           366 │
│ (Conv1DTranspose)               │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2397              │ (None, 19008, 6)       │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2397           │ (None, 19008, 1)       │           121 │
│ (Conv1DTranspose)               │                        │               │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1:03[0m 1s/step - loss: 22.0614 - mean_squared_error: 22.0614
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 14.7061 - mean_squared_error: 14.7061 
[1m27/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 15.3276 - mean_squared_error: 15.3276
[1m42/57[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 5ms/step - loss: 14.9681 - mean_squared_error: 14.9681
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 14.3090 - mean_squared_error: 14.3090
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 8ms/step - loss: 14.2826 - mean_squared_error: 14.2826 - val_loss: 14.0454 - val_mean_squared_error: 14.0454
Epoch 2/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - loss: 7.7781 - mean_squared_error: 7.7781
[1m11/57[0m [32m━━━[0m[37m━━━━━━━━━━━━━━━━━[0m [1m0s[0m 6ms/step - loss: 6.2584 - mean_squared_error: 6.2584 
[1m26/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 8.0405 - mean_squared_error: 8.0405
[1m40/57[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 5ms/step - loss: 8.5190 - mean_squared_error: 8.5190
[1m55/57[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 4ms/step - loss: 9.0699 - mean_squared_error: 9.0699
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 9.1944 - mean_squared_error: 9.1944 - val_loss: 14.0254 - val_mean_squared_error: 14.0254
Epoch 3/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - loss: 2.9210 - mean_squared_error: 2.9210
[1m 9/57[0m [32m━━━[0m[37m━━━━━━━━━━━━━━━━━[0m [1m0s[0m 6ms/step - loss: 5.7894 - mean_squared_error: 5.7894 
[1m20/57[0m [32m━━━━━━━[0m[37m━━━━━━━━━━━━━[0m [1m0s[0m 6ms/step - loss: 9.2697 - mean_squared_error: 9.2697
[1m32/57[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 10.2791 - mean_squared_error: 10.2791
[1m44/57[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 5ms/step - loss: 10.6927 - mean_squared_error: 10.6927
[1m56/57[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 5ms/step - loss: 10.8793 - mean_squared_error: 10.8793
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 10.8973 - mean_squared_error: 10.8973 - val_loss: 14.0199 - val_mean_squared_error: 14.0199
Epoch 4/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 27ms/step - loss: 2.6764 - mean_squared_error: 2.6764
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 11.1530 - mean_squared_error: 11.1530
[1m25/57[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 11.7703 - mean_squared_error: 11.7703
[1m38/57[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 4ms/step - loss: 11.8040 - mean_squared_error: 11.8040
[1m51/57[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - loss: 11.8865 - mean_squared_error: 11.8865
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.8495 - mean_squared_error: 11.8495 - val_loss: 14.0167 - val_mean_squared_error: 14.0167
Epoch 5/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 27ms/step - loss: 16.3898 - mean_squared_error: 16.3898
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - loss: 13.2261 - mean_squared_error: 13.2261 
[1m25/57[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 11.6349 - mean_squared_error: 11.6349
[1m37/57[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 11.4942 - mean_squared_error: 11.4942
[1m49/57[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - loss: 11.4048 - mean_squared_error: 11.4048
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 11.4607 - mean_squared_error: 11.4607 - val_loss: 14.0151 - val_mean_squared_error: 14.0151
(64, 1188, 6)
Model: "sequential_1198"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2398              │ (None, 4752, 6)        │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2398           │ (None, 4752, 6)        │           366 │
│ (Conv1DTranspose)               │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ up_sampling1d_2399              │ (None, 19008, 6)       │             0 │
│ (UpSampling1D)                  │                        │               │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ conv1d_transpose_2399           │ (None, 19008, 1)       │           121 │
│ (Conv1DTranspose)               │                        │               │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1:17[0m 1s/step - loss: 0.1353 - mean_squared_error: 0.1353
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0663 - mean_squared_error: 0.0663 
[1m24/57[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0583 - mean_squared_error: 0.0583
[1m38/57[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0523 - mean_squared_error: 0.0523
[1m54/57[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 4ms/step - loss: 0.0483 - mean_squared_error: 0.0483
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 8ms/step - loss: 0.0473 - mean_squared_error: 0.0473 - val_loss: 0.0192 - val_mean_squared_error: 0.0192
Epoch 2/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 0.0073 - mean_squared_error: 0.0073
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0086 - mean_squared_error: 0.0086 
[1m25/57[0m [32m━━━━━━━━[0m[37m━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0110 - mean_squared_error: 0.0110
[1m39/57[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0137 - mean_squared_error: 0.0137
[1m54/57[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 5ms/step - loss: 0.0150 - mean_squared_error: 0.0150
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0153 - mean_squared_error: 0.0153 - val_loss: 0.0172 - val_mean_squared_error: 0.0172
Epoch 3/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - loss: 0.0083 - mean_squared_error: 0.0083
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0114 - mean_squared_error: 0.0114 
[1m26/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0111 - mean_squared_error: 0.0111
[1m39/57[0m [32m━━━━━━━━━━━━━[0m[37m━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0113 - mean_squared_error: 0.0113
[1m55/57[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 4ms/step - loss: 0.0127 - mean_squared_error: 0.0127
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0129 - mean_squared_error: 0.0129 - val_loss: 0.0155 - val_mean_squared_error: 0.0155
Epoch 4/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - loss: 0.0035 - mean_squared_error: 0.0035
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0106 - mean_squared_error: 0.0106 
[1m27/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0142 - mean_squared_error: 0.0142
[1m40/57[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 4ms/step - loss: 0.0153 - mean_squared_error: 0.0153
[1m56/57[0m [32m━━━━━━━━━━━━━━━━━━━[0m[37m━[0m [1m0s[0m 4ms/step - loss: 0.0159 - mean_squared_error: 0.0159
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0150 - val_mean_squared_error: 0.0150
Epoch 5/5

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - loss: 0.0042 - mean_squared_error: 0.0042
[1m12/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 6ms/step - loss: 0.0082 - mean_squared_error: 0.0082 
[1m26/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0113 - mean_squared_error: 0.0113
[1m42/57[0m [32m━━━━━━━━━━━━━━[0m[37m━━━━━━[0m [1m0s[0m 5ms/step - loss: 0.0128 - mean_squared_error: 0.0128
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - loss: 0.0140 - mean_squared_error: 0.0140
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - loss: 0.0140 - mean_squared_error: 0.0140 - val_loss: 0.0173 - val_mean_squared_error: 0.0173
(19008, 1, 5)
Model: "sequential_1199"
┌─────────────────────────────────┬────────────────────────┬───────────────┐
│ Layer (type)                    │ Output Shape           │       Param # │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stConvL (Conv1D)               │ (None, 19008, 5)       │           105 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 1stPoolL (AveragePooling1D)     │ (None, 4752, 5)        │             0 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndConvL (Conv1D)               │ (None, 4752, 6)        │           306 │
├─────────────────────────────────┼────────────────────────┼───────────────┤
│ 2ndPoolL (AveragePooling1D)     │ (None, 1188, 6)        │             0 │
└─────────────────────────────────┴────────────────────────┴───────────────┘
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
┌─────────────────────┬───────────────────┬────────────┬───────────────────┐
│ Layer (type)        │ Output Shape      │    Param # │ Connected to      │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputGSR            │ (None, 19008, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ inputPPG            │ (None, 19008, 1)  │          0 │ -                 │
│ (InputLayer)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1198     │ (None, 1188, 6)   │        411 │ inputGSR[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ sequential_1199     │ (None, 1188, 6)   │        411 │ inputPPG[0][0]    │
│ (Sequential)        │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ concatenate_599     │ (None, 1188, 12)  │          0 │ sequential_1198[… │
│ (Concatenate)       │                   │            │ sequential_1199[… │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ permute_599         │ (None, 12, 1188)  │          0 │ concatenate_599[… │
│ (Permute)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ flatten_599         │ (None, 14256)     │          0 │ permute_599[0][0] │
│ (Flatten)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dropout_599         │ (None, 14256)     │          0 │ flatten_599[0][0] │
│ (Dropout)           │                   │            │                   │
├─────────────────────┼───────────────────┼────────────┼───────────────────┤
│ dense_599 (Dense)   │ (None, 1)         │     14,257 │ dropout_599[0][0] │
└─────────────────────┴───────────────────┴────────────┴───────────────────┘
 Total params: 15,079 (58.90 KB)
 Trainable params: 15,079 (58.90 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1:13[0m 1s/step - binary_accuracy: 1.0000 - loss: 0.6271
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.7919 - loss: 0.6396 
[1m29/57[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7890 - loss: 0.5803
[1m46/57[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7382 - loss: 0.6423
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m2s[0m 8ms/step - binary_accuracy: 0.7084 - loss: 0.6660 - val_binary_accuracy: 0.2857 - val_loss: 0.8158
Epoch 2/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.4082
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.7584 - loss: 0.5413 
[1m30/57[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6447 - loss: 0.5929
[1m44/57[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6279 - loss: 0.6128
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6233 - loss: 0.6257 - val_binary_accuracy: 0.5714 - val_loss: 0.5973
Epoch 3/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.6589
[1m18/57[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.9223 - loss: 0.5345 
[1m35/57[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8656 - loss: 0.5383
[1m52/57[0m [32m━━━━━━━━━━━━━━━━━━[0m[37m━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8218 - loss: 0.5549
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8066 - loss: 0.5611 - val_binary_accuracy: 0.5714 - val_loss: 0.5929
Epoch 4/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2250
[1m16/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8267 - loss: 0.4063 
[1m33/57[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8022 - loss: 0.4580
[1m50/57[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7667 - loss: 0.4995
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7482 - loss: 0.5158 - val_binary_accuracy: 0.5714 - val_loss: 0.6072
Epoch 5/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.6289
[1m14/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 5ms/step - binary_accuracy: 0.6414 - loss: 0.6610 
[1m31/57[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6540 - loss: 0.6343
[1m48/57[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6479 - loss: 0.6279
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.6517 - loss: 0.6201 - val_binary_accuracy: 0.5714 - val_loss: 0.5906
Epoch 6/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 32ms/step - binary_accuracy: 0.0000e+00 - loss: 1.0441
[1m17/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7284 - loss: 0.4916     
[1m35/57[0m [32m━━━━━━━━━━━━[0m[37m━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7515 - loss: 0.4818
[1m51/57[0m [32m━━━━━━━━━━━━━━━━━[0m[37m━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7463 - loss: 0.4991
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7449 - loss: 0.5048 - val_binary_accuracy: 0.5714 - val_loss: 0.6136
Epoch 7/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4327
[1m18/57[0m [32m━━━━━━[0m[37m━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7975 - loss: 0.4134 
[1m33/57[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7622 - loss: 0.4417
[1m48/57[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7461 - loss: 0.4618
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.7420 - loss: 0.4690 - val_binary_accuracy: 0.5714 - val_loss: 0.6530
Epoch 8/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0652
[1m13/57[0m [32m━━━━[0m[37m━━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8705 - loss: 0.4374 
[1m28/57[0m [32m━━━━━━━━━[0m[37m━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8575 - loss: 0.4418
[1m45/57[0m [32m━━━━━━━━━━━━━━━[0m[37m━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8371 - loss: 0.4575
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8248 - loss: 0.4715 - val_binary_accuracy: 0.5714 - val_loss: 0.7128
Epoch 9/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2868
[1m16/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8503 - loss: 0.4299 
[1m33/57[0m [32m━━━━━━━━━━━[0m[37m━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8226 - loss: 0.4347
[1m46/57[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8132 - loss: 0.4401
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 5ms/step - binary_accuracy: 0.8162 - loss: 0.4372 - val_binary_accuracy: 0.5714 - val_loss: 0.7255
Epoch 10/10

[1m 1/57[0m [37m━━━━━━━━━━━━━━━━━━━━[0m [1m1s[0m 25ms/step - binary_accuracy: 1.0000 - loss: 0.1610
[1m15/57[0m [32m━━━━━[0m[37m━━━━━━━━━━━━━━━[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8288 - loss: 0.2969 
[1m31/57[0m [32m━━━━━━━━━━[0m[37m━━━━━━━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.7910 - loss: 0.3894
[1m46/57[0m [32m━━━━━━━━━━━━━━━━[0m[37m━━━━[0m [1m0s[0m 3ms/step - binary_accuracy: 0.8048 - loss: 0.4019
[1m57/57[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 4ms/step - binary_accuracy: 0.8093 - loss: 0.4091 - val_binary_accuracy: 0.4286 - val_loss: 0.6973

[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 125ms/step
[1m1/1[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 125ms/step
predicted [0.5644177  0.781072   0.9307451  0.33873454 0.7382847  0.8372301
 0.5518615  0.29073367 0.6644755  0.3666813  0.5626608  0.9156951
 0.19458418 0.2561879  0.7583044  0.27433464]
predicted [1 1 1 0 1 1 1 0 1 0 1 1 0 0 1 0]
expected [False  True  True False  True  True  True False  True  True False False
 False  True  True  True]
accuracy: 0.625
confusion matrix: 
[[3 3]
 [3 7]]
              precision    recall  f1-score   support

       False       0.50      0.50      0.50         6
        True       0.70      0.70      0.70        10

    accuracy                           0.62        16
   macro avg       0.60      0.60      0.60        16
weighted avg       0.62      0.62      0.62        16

macro avg f1-score: 0.6
macro avg (UAR): 0.6
Sensitivity:  0.5
Specificity:  0.7
g-mean:  0.5916079783099616
-------- Model Performance ----------: 
accuracy:  [0.5625 0.5625 0.6875 0.75   0.5    0.5625 0.5    0.625  0.625  0.625 ]
gmean:  [0.54772256 0.54772256 0.54772256 0.57735027 0.5        0.54772256
 0.5        0.59160798 0.51639778 0.59160798]
f1_score:  [0.5465587  0.5465587  0.61352657 0.66666667 0.49206349 0.5465587
 0.49206349 0.6        0.56363636 0.6       ]
UAR:  [0.55       0.55       0.61666667 0.66666667 0.56666667 0.55
 0.5        0.6        0.56666667 0.6       ]
Cohen Kappa score:  [0.09677419 0.09677419 0.25925926 0.38461538 0.11111111 0.09677419
 0.         0.2        0.14285714 0.2       ]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  19.008
step (sec):  14.256
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  4.752
Number of windows / instances:  80
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.796 0.906 0.9   0.901 0.897 0.463 0.735 0.775 0.677 0.725]
 [0.657 0.815 0.825 0.813 0.808 0.52  0.773 0.788 0.722 0.759]
 [0.711 0.829 0.838 0.861 0.818 0.396 0.783 0.825 0.788 0.775]
 [0.038 0.517 0.612 0.058 0.403 0.    0.5   0.662 0.    0.398]
 [0.58  0.779 0.8   0.761 0.783 0.475 0.718 0.788 0.685 0.737]
 [0.159 0.577 0.6   0.547 0.567 0.065 0.532 0.581 0.477 0.522]
 [0.    0.5   0.6   0.    0.374 0.    0.5   0.662 0.    0.398]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.46389356  0.735       0.7375      0.70691682  0.72050505  0.24095238
   0.60833333  0.7625      0.39896651  0.59519148]
 [ 0.40588235  0.7075      0.7         0.6953172   0.69428571  0.39824176
   0.69416667  0.7375      0.58863347  0.67857698]
 [ 0.40612045  0.705       0.7125      0.68173786  0.69383838  0.26578947
   0.63583333  0.625       0.61162762  0.61690476]
 [ 0.475       0.7375      0.7375      0.72711088  0.73365079 -0.08666667
   0.46333333  0.65        0.10927108  0.42908425]
 [ 0.39703297  0.6975      0.725       0.67256191  0.69399711  0.425
   0.7125      0.7125      0.67558338  0.69510823]
 [ 0.36934389  0.68916667  0.6875      0.65583953  0.67017316  0.55
   0.77583333  0.775       0.75259169  0.76374459]
 [ 0.23891187  0.625       0.6625      0.46674569  0.58397935  0.24666667
   0.62333333  0.625       0.60333399  0.61382395]
 [ 0.84928571  0.92333333  0.9375      0.91473981  0.9231524   0.48404762
   0.75666667  0.75        0.73806667  0.73356421]
 [ 0.33347985  0.65666667  0.75        0.52500413  0.64268398  0.07824176
   0.54166667  0.65        0.34259589  0.51326673]
 [ 0.40833333  0.7075      0.7         0.67559567  0.68888889  0.1785014
   0.59166667  0.6         0.51175168  0.5603158 ]
 [ 0.22347985  0.60916667  0.6375      0.55440858  0.6010101   0.43010235
   0.71333333  0.75        0.61032365  0.69476024]
 [ 0.6057326   0.7925      0.825       0.73859118  0.79457431  0.55032213
   0.77        0.7875      0.75808456  0.77205628]
 [ 0.42460317  0.71333333  0.7125      0.67671361  0.69292929  0.11222222
   0.56        0.55        0.4373038   0.50585859]
 [ 0.16967033  0.58666667  0.6625      0.44436383  0.56842713  0.46428571
   0.72583333  0.7375      0.69933558  0.72050505]
 [-0.0552381   0.48        0.6375      0.12228475  0.42832501  0.32640271
   0.67        0.7         0.61107847  0.65508658]
 [ 0.18705128  0.59083333  0.6375      0.44218502  0.56105894  0.44336996
   0.715       0.7875      0.57876937  0.70937063]
 [ 0.10909825  0.56333333  0.6         0.39182456  0.52487568  0.19357143
   0.5925      0.625       0.56378963  0.59122655]
 [-0.03428571  0.48452381  0.8125      0.          0.44703297  0.32395173
   0.66833333  0.6625      0.64510497  0.64785714]
 [ 0.51545033  0.75666667  0.7875      0.73360508  0.75103064  0.29921569
   0.65        0.65        0.63105265  0.64007937]
 [ 0.30776557  0.65166667  0.7125      0.56298243  0.63361305  0.27194139
   0.63166667  0.7         0.52547982  0.62391608]
 [ 0.17642857  0.58690476  0.7125      0.41000968  0.56798535 -0.07983193
   0.46        0.4625      0.42058282  0.44779221]
 [ 0.35        0.675       0.675       0.6555183   0.66626984  0.3148384
   0.655       0.6875      0.56125592  0.63935731]
 [ 0.09246499  0.54833333  0.5625      0.43377878  0.5219697   0.2247619
   0.60833333  0.75        0.38762074  0.60340326]
 [ 0.23666667  0.62        0.625       0.61377361  0.61634921  0.32967033
   0.65916667  0.6875      0.5966008   0.64677489]
 [ 0.41119048  0.70416667  0.7125      0.69049665  0.70122655  0.5
   0.75083333  0.75        0.73886004  0.74420635]
 [ 0.225       0.61416667  0.6125      0.50574526  0.57522367  0.37496947
   0.68666667  0.6875      0.64991095  0.66691919]
 [ 0.42003663  0.71666667  0.7625      0.57890521  0.69076257  0.6
   0.8         0.8         0.7831769   0.79380952]
 [ 0.15088235  0.58083333  0.6         0.46751248  0.54247863  0.63430403
   0.81166667  0.8375      0.76407103  0.81078644]
 [ 0.17587912  0.59        0.6375      0.48595937  0.57336219  0.28362961
   0.6425      0.65        0.60018929  0.62240981]
 [ 0.79642857  0.90583333  0.9         0.90065778  0.89656566  0.46298535
   0.735       0.775       0.67653863  0.72460872]]
KNN mean:
[0.32785297 0.66515873 0.70583333 0.57102952 0.64667418 0.32804956
 0.66363889 0.6975     0.58571839 0.64867884]
---------------------------
---------------------------
DT performance:
[[ 0.5297619   0.77833333  0.775       0.7603383   0.75896104  0.09095238
   0.59166667  0.7125      0.38599085  0.56217782]
 [ 0.25294118  0.655       0.65        0.63233562  0.64080808  0.25176902
   0.64583333  0.65        0.58233974  0.62664502]
 [ 0.30422969  0.64833333  0.6625      0.62628875  0.64683983  0.24666667
   0.62416667  0.625       0.60198949  0.61936508]
 [ 0.37794118  0.7025      0.7         0.69473182  0.69468254  0.01904762
   0.56333333  0.6875      0.35322677  0.55762904]
 [ 0.34976837  0.67166667  0.6875      0.631214    0.65151515  0.45
   0.725       0.725       0.70957541  0.7116811 ]
 [ 0.30254902  0.70166667  0.7         0.59415583  0.67870851  0.47436975
   0.71916667  0.725       0.68784792  0.7044733 ]
 [ 0.30891187  0.63083333  0.65        0.55298118  0.60878066  0.15
   0.63833333  0.6375      0.60250346  0.62992063]
 [ 0.89142857  0.915       0.9375      0.91251373  0.91718504  0.40471019
   0.75333333  0.7625      0.66509076  0.73090188]
 [ 0.5243609   0.75333333  0.7875      0.6871555   0.75290709  0.19973138
   0.62666667  0.6875      0.5528838   0.61703408]
 [ 0.22166667  0.67333333  0.675       0.54852806  0.64795815  0.01818874
   0.5525      0.5625      0.43796902  0.53316017]
 [ 0.15332579  0.62083333  0.625       0.39957957  0.60588745  0.42347554
   0.6875      0.7125      0.70937457  0.6722422 ]
 [ 0.68092652  0.86166667  0.875       0.83662996  0.86011544  0.50239927
   0.68333333  0.7125      0.73009976  0.67601732]
 [ 0.2351634   0.60666667  0.6         0.52699803  0.56501443  0.00470588
   0.47916667  0.475       0.40521896  0.45041126]
 [-0.00896825  0.53666667  0.575       0.45146137  0.51677434  0.08767722
   0.57916667  0.5875      0.51238208  0.56612554]
 [ 0.07430403  0.56        0.6375      0.35314882  0.5557043  -0.039429
   0.56833333  0.6         0.44840018  0.5465368 ]
 [ 0.28202112  0.60583333  0.625       0.51440683  0.59056277  0.34216117
   0.71666667  0.75        0.56276794  0.69623932]
 [ 0.52681319  0.77083333  0.7875      0.77243225  0.76959596  0.52571429
   0.7075      0.725       0.6833083   0.70026696]
 [ 0.24512821  0.66785714  0.7625      0.39258201  0.56947719  0.49655678
   0.74333333  0.75        0.70218858  0.73562771]
 [ 0.38913165  0.69166667  0.7375      0.62399201  0.68848263  0.42460784
   0.69666667  0.7         0.7160364   0.6866811 ]
 [ 0.31942793  0.61        0.65        0.53144011  0.57534299  0.24683473
   0.51        0.55        0.44201117  0.48411255]
 [-0.00666667  0.56309524  0.675       0.32227023  0.50628039 -0.23166667
   0.41583333  0.425       0.40001775  0.4033189 ]
 [ 0.2         0.55        0.55        0.51462644  0.53136364  0.3956184
   0.67833333  0.675       0.66072276  0.63750916]
 [ 0.22285714  0.60833333  0.6125      0.58492372  0.59225108  0.41047619
   0.64833333  0.725       0.52029097  0.63104895]
 [ 0.30507519  0.64083333  0.6375      0.56335683  0.6101443   0.62166667
   0.80583333  0.8         0.7898121   0.7934127 ]
 [ 0.53928571  0.8         0.8         0.72892417  0.7732684   0.55
   0.73833333  0.7375      0.77170961  0.72771284]
 [ 0.375       0.65333333  0.65        0.59675534  0.63306638  0.28056022
   0.65666667  0.6375      0.64287761  0.63031746]
 [ 0.47190476  0.74833333  0.7875      0.64848377  0.73097347  0.625
   0.7875      0.7875      0.81712084  0.78238095]
 [ 0.19732924  0.63166667  0.6375      0.50488008  0.61181097  0.42633269
   0.71916667  0.725       0.672995    0.69577201]
 [ 0.22609459  0.66333333  0.6875      0.48664902  0.63688145  0.29803922
   0.70333333  0.7         0.62286653  0.69261905]
 [ 0.65679487  0.815       0.825       0.81274304  0.80844877  0.51973605
   0.77333333  0.7875      0.72187726  0.75927128]]
DT mean:
[0.33828359 0.67786508 0.69875    0.59355088 0.65765975 0.30719674
 0.65794444 0.67791667 0.60371652 0.64202041]
---------------------------
---------------------------
RF performance:
[[ 0.55989011  0.82166667  0.825       0.84594824  0.81823232  0.43095238
   0.58333333  0.725       0.42144113  0.57609724]
 [ 0.21056022  0.78333333  0.775       0.69919552  0.75987013  0.47739927
   0.74083333  0.7625      0.62574057  0.7229637 ]
 [ 0.41119048  0.6875      0.7         0.66782674  0.69059163  0.25
   0.67416667  0.675       0.64727351  0.66642857]
 [ 0.29666667  0.76333333  0.7625      0.67232258  0.76031746 -0.03809524
   0.59        0.725       0.18683447  0.57082917]
 [ 0.4752381   0.74666667  0.775       0.69323512  0.7431746   0.475
   0.7375      0.7375      0.72308898  0.72904762]
 [ 0.42754902  0.72666667  0.725       0.66632262  0.71968254  0.47142857
   0.76083333  0.7625      0.73099175  0.75571429]
 [ 0.41377613  0.61416667  0.6375      0.59980803  0.58743701  0.13214286
   0.63833333  0.6375      0.57649256  0.62960317]
 [ 0.86642857  0.90666667  0.9375      0.8954138   0.91294261  0.51166667
   0.71333333  0.7375      0.704831    0.68748751]
 [ 0.36250916  0.75166667  0.8         0.51880515  0.75745921  0.22549451
   0.52333333  0.625       0.36470524  0.50165168]
 [ 0.26833333  0.56333333  0.5625      0.60676491  0.53587302  0.25120879
   0.6425      0.65        0.56636967  0.62534632]
 [ 0.35407994  0.58916667  0.6125      0.62036928  0.57362915  0.57190476
   0.6875      0.7375      0.67765283  0.68002886]
 [ 0.52817927  0.78083333  0.8         0.80030979  0.78487013  0.45259319
   0.73666667  0.7625      0.71623317  0.71984682]
 [ 0.31659664  0.68166667  0.7         0.68224079  0.66320346  0.1804902
   0.57333333  0.575       0.51056815  0.54285714]
 [ 0.15620879  0.54333333  0.6375      0.39226858  0.499667    0.35322344
   0.64        0.65        0.71659884  0.6246176 ]
 [ 0.37333333  0.59833333  0.7         0.50351331  0.562664    0.0814652
   0.60333333  0.65        0.42387505  0.59621212]
 [ 0.19215363  0.61583333  0.6375      0.65917805  0.60637807  0.4209707
   0.74        0.8         0.58194256  0.72965035]
 [ 0.60322344  0.76916667  0.7875      0.73170912  0.76368687  0.42571429
   0.75666667  0.775       0.82088526  0.74642136]
 [-0.05010989  0.6547619   0.825       0.29258201  0.62205128  0.4569888
   0.77916667  0.8         0.74970937  0.77656566]
 [ 0.36203297  0.60333333  0.7         0.61845427  0.58702187  0.46640271
   0.6825      0.6875      0.59821737  0.66531746]
 [ 0.42544567  0.72166667  0.725       0.60341227  0.6932684   0.20761905
   0.655       0.7125      0.44923669  0.65957376]
 [-0.03487179  0.55119048  0.75        0.23938469  0.51380619 -0.10666667
   0.43333333  0.4375      0.42460796  0.4049062 ]
 [ 0.525       0.6625      0.6625      0.63494071  0.64691919  0.34785714
   0.635       0.675       0.55102489  0.62240093]
 [ 0.13862961  0.51166667  0.525       0.55450737  0.49226551  0.28761905
   0.71666667  0.7875      0.53683447  0.69898768]
 [ 0.22777778  0.61083333  0.6125      0.69029601  0.60347042  0.39960784
   0.7575      0.7625      0.76045644  0.74492785]
 [ 0.36346154  0.65916667  0.6625      0.67303729  0.63459596  0.48928571
   0.76083333  0.7625      0.72906916  0.7581746 ]
 [ 0.15588235  0.63583333  0.6375      0.58712736  0.61858586  0.2752381
   0.66666667  0.6625      0.56322014  0.62735209]
 [ 0.4357326   0.735       0.7875      0.72710304  0.72307026  0.5
   0.7625      0.7625      0.74998178  0.7565873 ]
 [-0.00741866  0.64        0.6625      0.45666126  0.63132756  0.48619048
   0.77833333  0.7875      0.70112869  0.77448052]
 [ 0.07700711  0.62333333  0.6625      0.55694651  0.60571429  0.33405696
   0.66583333  0.6625      0.65858513  0.65434343]
 [ 0.71119048  0.82916667  0.8375      0.86110555  0.81839105  0.39642857
   0.78333333  0.825       0.78812646  0.7754329 ]]
RF mean:
[0.33818922 0.67939286 0.71416667 0.62502633 0.6643389  0.34047291
 0.68061111 0.71041667 0.60852411 0.6674618 ]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.5375      0.          0.34871795  0.
   0.5         0.75        0.          0.42857143]
 [ 0.03846154  0.51666667  0.5625      0.05773503  0.38205128  0.
   0.5         0.6125      0.          0.37948718]
 [ 0.06346154  0.52916667  0.5875      0.10773503  0.41020979  0.28846154
   0.6425      0.65        0.49863918  0.5915368 ]
 [ 0.19666667  0.59833333  0.6         0.37846298  0.52363636  0.
   0.5         0.7375      0.          0.42417582]
 [ 0.26923077  0.61666667  0.7         0.40414519  0.57692308  0.425
   0.7125      0.7125      0.66667055  0.6912987 ]
 [ 0.          0.5         0.5375      0.          0.34871795  0.5
   0.75083333  0.75        0.72080796  0.73580808]
 [ 0.          0.5         0.6125      0.          0.37948718  0.07166667
   0.53583333  0.5375      0.17302967  0.41333333]
 [ 0.17538462  0.575       0.7375      0.24391576  0.52783883  0.
   0.5         0.6375      0.          0.38901099]
 [ 0.          0.5         0.675       0.          0.4021978   0.
   0.5         0.6875      0.          0.40659341]
 [ 0.05        0.52583333  0.525       0.11324555  0.38493506  0.
   0.5         0.5625      0.          0.35897436]
 [ 0.          0.5         0.6125      0.          0.37948718  0.
   0.5         0.6125      0.          0.37948718]
 [ 0.          0.5         0.6         0.          0.37435897  0.
   0.5         0.625       0.          0.38461538]
 [ 0.          0.5         0.575       0.          0.36410256  0.
   0.5         0.575       0.          0.36410256]
 [ 0.          0.5         0.6625      0.          0.3978022   0.
   0.5         0.55        0.          0.35384615]
 [ 0.          0.5         0.7125      0.          0.41538462  0.
   0.5         0.65        0.          0.39340659]
 [ 0.          0.5         0.6125      0.          0.37948718  0.
   0.5         0.6625      0.          0.3978022 ]
 [ 0.          0.5         0.6         0.          0.37435897  0.
   0.5         0.5875      0.          0.36923077]
 [ 0.          0.5         0.8375      0.          0.4552381   0.
   0.5         0.6         0.          0.37435897]
 [ 0.          0.5         0.7         0.          0.41098901  0.
   0.5         0.525       0.          0.34358974]
 [ 0.          0.5         0.6625      0.          0.3978022   0.
   0.5         0.65        0.          0.39340659]
 [ 0.          0.5         0.775       0.          0.43619048  0.
   0.5         0.525       0.          0.34358974]
 [ 0.375       0.6875      0.6875      0.64870346  0.66977633  0.
   0.5         0.6375      0.          0.38901099]
 [ 0.          0.5         0.5625      0.          0.35897436  0.
   0.5         0.7375      0.          0.42417582]
 [ 0.          0.5         0.55        0.          0.35384615  0.26181319
   0.62083333  0.6625      0.40485474  0.55967366]
 [ 0.          0.5         0.5625      0.          0.35897436 -0.01911765
   0.49083333  0.4875      0.09494105  0.35727273]
 [ 0.          0.5         0.525       0.          0.34358974  0.
   0.5         0.5625      0.          0.35897436]
 [ 0.          0.5         0.6875      0.          0.40659341  0.525
   0.7625      0.7625      0.75046727  0.75785714]
 [ 0.          0.5         0.6125      0.          0.37948718  0.
   0.5         0.6         0.          0.37435897]
 [ 0.          0.5         0.625       0.          0.38461538  0.
   0.5         0.5625      0.          0.35897436]
 [ 0.03846154  0.51666667  0.6125      0.05773503  0.4025641   0.
   0.5         0.6625      0.          0.3978022 ]]
SVM mean:
[0.04022222 0.51886111 0.62833333 0.06705593 0.41094459 0.06842746
 0.53386111 0.62916667 0.11031368 0.42981087]
---------------------------
---------------------------
GBM performance:
[[ 0.58783129  0.7925      0.8         0.74736265  0.78015152  0.02857143
   0.51666667  0.7         0.18001978  0.46987013]
 [ 0.29941392  0.60833333  0.625       0.58006312  0.59480519  0.44776557
   0.715       0.75        0.6527566   0.71396104]
 [ 0.33131868  0.65916667  0.6875      0.58452909  0.64484682  0.32754902
   0.67666667  0.675       0.66177038  0.6684127 ]
 [ 0.42166667  0.74833333  0.75        0.72086946  0.74674603  0.03692308
   0.515       0.7375      0.07071068  0.45567766]
 [ 0.58681319  0.78        0.825       0.73978395  0.78363636  0.625
   0.8125      0.8125      0.80887638  0.80571429]
 [ 0.44183007  0.70083333  0.6875      0.70332064  0.67912698  0.44642857
   0.73166667  0.7375      0.68483233  0.71426407]
 [ 0.35702543  0.6775      0.725       0.509766    0.6534904   0.275
   0.62583333  0.625       0.6049161   0.6184127 ]
 [ 0.80846154  0.88333333  0.925       0.87471079  0.89230769  0.36538462
   0.70666667  0.7375      0.57700217  0.67227828]
 [ 0.37042448  0.67        0.7625      0.61186893  0.6671695  -0.01377289
   0.49666667  0.625       0.18328944  0.45018981]
 [ 0.34078947  0.6725      0.6625      0.62450907  0.64858586  0.22524326
   0.6025      0.6         0.56162082  0.57320346]
 [ 0.23027473  0.62        0.6625      0.53137554  0.61281385  0.38144689
   0.68083333  0.7375      0.55005737  0.6636186 ]
 [ 0.68787546  0.83166667  0.8625      0.80415938  0.83636364  0.49443223
   0.74        0.775       0.6928654   0.7376912 ]
 [ 0.33347985  0.65583333  0.6875      0.59634717  0.64326118  0.14168498
   0.56666667  0.6         0.3873856   0.52969697]
 [ 0.15802198  0.57166667  0.7         0.30649452  0.53778888  0.24917582
   0.6325      0.65        0.57282048  0.61883117]
 [ 0.08205128  0.55666667  0.7125      0.26986706  0.53443223 -0.00247253
   0.5         0.6         0.1855303   0.44302364]
 [ 0.11386447  0.5625      0.6375      0.33656243  0.52177156  0.22322344
   0.60833333  0.725       0.24619938  0.56336996]
 [ 0.5523551   0.8         0.8125      0.76568445  0.79662338  0.24333333
   0.6375      0.65        0.57350801  0.61411089]
 [ 0.12571429  0.58452381  0.8375      0.19258201  0.53699634  0.40930403
   0.67083333  0.6875      0.62587135  0.65177489]
 [ 0.03142857  0.51166667  0.65        0.16091086  0.46913087  0.45
   0.7175      0.725       0.61923042  0.69113997]
 [ 0.21393773  0.61        0.7         0.44519076  0.59413253  0.24981685
   0.61333333  0.7125      0.37104424  0.58980353]
 [-0.02        0.49166667  0.7625      0.          0.43179487 -0.07225275
   0.44833333  0.4625      0.3870334   0.43733045]
 [ 0.225       0.6125      0.6125      0.56346423  0.59358586  0.16943223
   0.60666667  0.6625      0.38064103  0.56213786]
 [ 0.17210816  0.60416667  0.625       0.4806948   0.57861472  0.33428571
   0.65666667  0.7875      0.4867103   0.64390942]
 [ 0.10717518  0.5475      0.55        0.41184566  0.51259019  0.42822344
   0.71166667  0.725       0.6900298   0.70704185]
 [ 0.34835165  0.67416667  0.7         0.54755381  0.64045288  0.575
   0.79        0.7875      0.76048758  0.77944444]
 [ 0.47142857  0.74583333  0.75        0.65197682  0.72489899  0.42567958
   0.6975      0.6875      0.64924953  0.66626263]
 [ 0.47465201  0.73        0.8         0.6923032   0.72476856  0.575
   0.775       0.775       0.76210092  0.76880952]
 [ 0.1398384   0.55916667  0.6         0.45488819  0.5346953   0.3956044
   0.69333333  0.7375      0.49262737  0.65716672]
 [ 0.13018315  0.56        0.65        0.29787453  0.51174825  0.40462185
   0.68583333  0.6875      0.6771834   0.67662338]
 [ 0.58005818  0.77916667  0.8         0.76082153  0.7826912   0.47501832
   0.71833333  0.7875      0.68503002  0.73724942]]
GBM mean:
[0.32344578 0.66003968 0.71875    0.53224602 0.64033405 0.31048835
 0.65166667 0.69875    0.52604669 0.62936735]
---------------------------
---------------------------
BDDAE performance:
[[-0.04074046  0.47857143  0.48125     0.46237223  0.47102467 -0.08040404
   0.46666667  0.675       0.0935149   0.43184046]
 [ 0.37341573  0.68650794  0.69375     0.67365566  0.68243117  0.33827993
   0.67833333  0.68125     0.65438777  0.65885232]
 [-0.08750592  0.4547619   0.46875     0.43199125  0.45041783 -0.05
   0.475       0.475       0.46160886  0.46877585]
 [-0.1875      0.40625     0.40625     0.34837474  0.38698158  0.19251748
   0.6         0.725       0.45740901  0.57838297]
 [ 0.1603718   0.57833333  0.58125     0.56328588  0.56529142  0.3
   0.65        0.65        0.63221355  0.64189563]
 [ 0.4814476   0.73492063  0.75        0.71847051  0.73606563  0.15
   0.575       0.575       0.54811349  0.56259775]
 [ 0.17969334  0.59166667  0.63125     0.51138979  0.573343    0.2375
   0.61875     0.61875     0.57620965  0.59856109]
 [ 0.42027322  0.69545455  0.76875     0.65665997  0.70494962  0.11129171
   0.555       0.58125     0.52512305  0.54650309]
 [ 0.40440383  0.67818182  0.775       0.6094419   0.68752222  0.40546625
   0.67        0.79375     0.54637571  0.67745136]
 [-0.15        0.425       0.425       0.40257916  0.41503473  0.30370255
   0.65        0.65625     0.64068462  0.64781456]
 [-0.00716055  0.49666667  0.55        0.37700603  0.47565493  0.15272566
   0.57333333  0.625       0.47263041  0.55141827]
 [-0.12206197  0.44166667  0.49375     0.33211803  0.42486933  0.42649927
   0.71666667  0.725       0.70800817  0.70977308]
 [ 0.02814031  0.51428571  0.5125      0.50787939  0.50835773  0.18586374
   0.5952381   0.6         0.57181211  0.58416101]
 [ 0.05346751  0.53363636  0.56875     0.48529075  0.51046855  0.14017607
   0.57301587  0.5625      0.55024743  0.55446453]
 [-0.01849791  0.49090909  0.6         0.28891242  0.46495774  0.23786037
   0.61666667  0.6375      0.60040697  0.61333779]
 [-0.06496334  0.465       0.51875     0.36401478  0.44438452  0.14857721
   0.56363636  0.7         0.35369207  0.54352381]
 [ 0.41862967  0.71666667  0.725       0.69138369  0.70043071  0.14406838
   0.56666667  0.6         0.42428175  0.53024975]
 [ 0.35429364  0.70641026  0.79375     0.63009854  0.67002678  0.1272515
   0.56        0.6         0.52845779  0.55864321]
 [ 0.09537539  0.54181818  0.625       0.46331472  0.54215547  0.35
   0.675       0.675       0.65292298  0.66515484]
 [ 0.15188008  0.57272727  0.6375      0.50126554  0.55681953  0.04220812
   0.52        0.5875      0.38471918  0.48783728]
 [ 0.01051282  0.5         0.6875      0.17479965  0.46555396  0.225
   0.6125      0.6125      0.58344421  0.5984342 ]
 [ 0.1         0.55        0.55        0.51938862  0.53510433  0.22307159
   0.60333333  0.6625      0.53838238  0.59680281]
 [ 0.09328966  0.5468254   0.55625     0.53230232  0.54262456  0.40065268
   0.675       0.8125      0.57097486  0.68499368]
 [-0.09532581  0.45238095  0.45        0.41466562  0.43203548  0.19936128
   0.5984127   0.6125      0.58201182  0.59602976]
 [-0.03804806  0.48253968  0.4875      0.46880397  0.47779206  0.225
   0.6125      0.6125      0.59976898  0.60694374]
 [ 0.15        0.575       0.575       0.53736656  0.55740175  0.22997552
   0.61269841  0.625       0.59879351  0.61104745]
 [ 0.27344989  0.63909091  0.70625     0.54519601  0.62138113  0.4625
   0.73125     0.73125     0.70771763  0.72177206]
 [-0.03437171  0.48        0.525       0.43358927  0.47374497  0.45411927
   0.71        0.7625      0.67344488  0.71904511]
 [ 0.14573583  0.57833333  0.58125     0.56096901  0.56248564  0.45487588
   0.72539683  0.7375      0.70920698  0.72322999]
 [ 0.15881655  0.57666667  0.6         0.54678542  0.56676327  0.06534125
   0.53181818  0.58125     0.47663086  0.52198495]]
BDDAE mean:
[0.1069007  0.55300907 0.59083333 0.49177905 0.54020248 0.22678272
 0.6103961  0.64979167 0.54743985 0.59971741]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.5375     0.         0.34871795 0.
  0.5        0.75       0.         0.42857143]
 [0.         0.5        0.55       0.         0.35384615 0.
  0.5        0.6125     0.         0.37948718]
 [0.         0.5        0.5625     0.         0.35897436 0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.4875     0.         0.32727273 0.
  0.5        0.7375     0.         0.42417582]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.5375     0.         0.34871795 0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.4875     0.         0.32727273]
 [0.         0.5        0.6875     0.         0.40659341 0.
  0.5        0.6375     0.         0.38901099]
 [0.         0.5        0.675      0.         0.4021978  0.
  0.5        0.6875     0.         0.40659341]
 [0.         0.5        0.5125     0.         0.33846154 0.
  0.5        0.5625     0.         0.35897436]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.6125     0.         0.37948718]
 [0.         0.5        0.6        0.         0.37435897 0.
  0.5        0.625      0.         0.38461538]
 [0.         0.5        0.575      0.         0.36410256 0.
  0.5        0.575      0.         0.36410256]
 [0.         0.5        0.6625     0.         0.3978022  0.
  0.5        0.55       0.         0.35384615]
 [0.         0.5        0.7125     0.         0.41538462 0.
  0.5        0.65       0.         0.39340659]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.6625     0.         0.3978022 ]
 [0.         0.5        0.6        0.         0.37435897 0.
  0.5        0.5875     0.         0.36923077]
 [0.         0.5        0.8375     0.         0.4552381  0.
  0.5        0.6        0.         0.37435897]
 [0.         0.5        0.7        0.         0.41098901 0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.6625     0.         0.3978022  0.
  0.5        0.65       0.         0.39340659]
 [0.         0.5        0.775      0.         0.43619048 0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.5        0.         0.33333333 0.
  0.5        0.6375     0.         0.38901099]
 [0.         0.5        0.5625     0.         0.35897436 0.
  0.5        0.7375     0.         0.42417582]
 [0.         0.5        0.55       0.         0.35384615 0.
  0.5        0.5625     0.         0.35897436]
 [0.         0.5        0.5625     0.         0.35897436 0.
  0.5        0.5125     0.         0.33846154]
 [0.         0.5        0.525      0.         0.34358974 0.
  0.5        0.5625     0.         0.35897436]
 [0.         0.5        0.6875     0.         0.40659341 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.6        0.         0.37435897]
 [0.         0.5        0.625      0.         0.38461538 0.
  0.5        0.5625     0.         0.35897436]
 [0.         0.5        0.6        0.         0.37435897 0.
  0.5        0.6625     0.         0.3978022 ]]
DUMMY mean:
[0.         0.5        0.61166667 0.         0.37742435 0.
 0.5        0.5975     0.         0.37220335]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_45
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.328 0.665 0.706 0.571 0.647 0.328 0.664 0.697 0.586 0.649]
 [0.338 0.678 0.699 0.594 0.658 0.307 0.658 0.678 0.604 0.642]
 [0.338 0.679 0.714 0.625 0.664 0.34  0.681 0.71  0.609 0.667]
 [0.04  0.519 0.628 0.067 0.411 0.068 0.534 0.629 0.11  0.43 ]
 [0.323 0.66  0.719 0.532 0.64  0.31  0.652 0.699 0.526 0.629]
 [0.107 0.553 0.591 0.492 0.54  0.227 0.61  0.65  0.547 0.6  ]
 [0.    0.5   0.612 0.    0.377 0.    0.5   0.598 0.    0.372]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.203 0.101 0.084 0.19  0.111 0.175 0.087 0.08  0.146 0.093]
 [0.196 0.091 0.089 0.14  0.099 0.213 0.092 0.088 0.129 0.094]
 [0.209 0.094 0.09  0.146 0.103 0.164 0.083 0.079 0.144 0.088]
 [0.09  0.043 0.076 0.151 0.072 0.155 0.077 0.074 0.232 0.113]
 [0.203 0.099 0.086 0.208 0.114 0.182 0.09  0.074 0.194 0.102]
 [0.185 0.092 0.106 0.122 0.095 0.14  0.067 0.073 0.123 0.074]
 [0.    0.    0.079 0.    0.03  0.    0.    0.072 0.    0.028]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 62.  15.  12.  33.  17.  53.  13.  11.  25.  14.]
 [ 58.  13.  13.  24.  15.  69.  14.  13.  21.  15.]
 [ 62.  14.  13.  23.  16.  48.  12.  11.  24.  13.]
 [224.   8.  12. 225.  18. 227.  14.  12. 210.  26.]
 [ 63.  15.  12.  39.  18.  59.  14.  11.  37.  16.]
 [173.  17.  18.  25.  18.  62.  11.  11.  22.  12.]
 [  0.   0.  13.   0.   8.   0.   0.  12.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_45
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  19.008
step (sec):  14.256
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  4.752
Number of windows / instances:  80
Elapsed time: 643.4156214237213 minutes
Elapsed time: 10.723593690395354 hours
