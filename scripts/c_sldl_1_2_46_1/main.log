2024-05-13 20:53:47.240183: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-13 20:53:50.784955: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-13 20:53:59.498570: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  20.0
step (sec):  20.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_46_1\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:


â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 20000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 5000, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 5000, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1250, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
----- train_PPG_AE -------
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 20000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 5000, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 5000, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1250, 6)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2398              â”‚ (None, 5000, 6)        â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2398           â”‚ (None, 5000, 6)        â”‚           366 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ up_sampling1d_2399              â”‚ (None, 20000, 6)       â”‚             0 â”‚
â”‚ (UpSampling1D)                  â”‚                        â”‚               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ conv1d_transpose_2399           â”‚ (None, 20000, 1)       â”‚           121 â”‚
â”‚ (Conv1DTranspose)               â”‚                        â”‚               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 898 (3.51 KB)
 Trainable params: 898 (3.51 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/5

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m44s[0m 1s/step - loss: 0.3773 - mean_squared_error: 0.3773
[1m 5/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - loss: 0.2373 - mean_squared_error: 0.2373
[1m 9/43[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - loss: 0.1823 - mean_squared_error: 0.1823
[1m13/43[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.1544 - mean_squared_error: 0.1544
[1m18/43[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.1335 - mean_squared_error: 0.1335
[1m23/43[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.1195 - mean_squared_error: 0.1195
[1m28/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.1090 - mean_squared_error: 0.1090
[1m32/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.1024 - mean_squared_error: 0.1024
[1m37/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0954 - mean_squared_error: 0.0954
[1m42/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 13ms/step - loss: 0.0900 - mean_squared_error: 0.0900
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 18ms/step - loss: 0.0881 - mean_squared_error: 0.0881 - val_loss: 0.0275 - val_mean_squared_error: 0.0275
Epoch 2/5

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0095 - mean_squared_error: 0.0095
[1m 6/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0117 - mean_squared_error: 0.0117
[1m10/43[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0115 - mean_squared_error: 0.0115
[1m14/43[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - loss: 0.0129 - mean_squared_error: 0.0129
[1m19/43[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0150 - mean_squared_error: 0.0150
[1m23/43[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - loss: 0.0162 - mean_squared_error: 0.0162
[1m28/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - loss: 0.0176 - mean_squared_error: 0.0176
[1m33/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0186 - mean_squared_error: 0.0186
[1m37/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0193 - mean_squared_error: 0.0193
[1m42/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 14ms/step - loss: 0.0198 - mean_squared_error: 0.0198
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 15ms/step - loss: 0.0200 - mean_squared_error: 0.0200 - val_loss: 0.0245 - val_mean_squared_error: 0.0245
Epoch 3/5

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0089 - mean_squared_error: 0.0089
[1m 6/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0130 - mean_squared_error: 0.0130
[1m10/43[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0147 - mean_squared_error: 0.0147
[1m14/43[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0145 - mean_squared_error: 0.0145
[1m19/43[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0144 - mean_squared_error: 0.0144
[1m24/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0146 - mean_squared_error: 0.0146
[1m27/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0146 - mean_squared_error: 0.0146
[1m32/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0149 - mean_squared_error: 0.0149
[1m37/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0155 - mean_squared_error: 0.0155
[1m42/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 13ms/step - loss: 0.0159 - mean_squared_error: 0.0159
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 14ms/step - loss: 0.0161 - mean_squared_error: 0.0161 - val_loss: 0.0222 - val_mean_squared_error: 0.0222
Epoch 4/5

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0049 - mean_squared_error: 0.0049
[1m 6/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0293 - mean_squared_error: 0.0293
[1m11/43[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0292 - mean_squared_error: 0.0292
[1m16/43[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0278 - mean_squared_error: 0.0278
[1m21/43[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0266 - mean_squared_error: 0.0266
[1m25/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0256 - mean_squared_error: 0.0256
[1m30/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0248 - mean_squared_error: 0.0248
[1m35/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0241 - mean_squared_error: 0.0241
[1m40/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - loss: 0.0235 - mean_squared_error: 0.0235
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 14ms/step - loss: 0.0231 - mean_squared_error: 0.0231 - val_loss: 0.0218 - val_mean_squared_error: 0.0218
Epoch 5/5

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0168 - mean_squared_error: 0.0168
[1m 6/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0222 - mean_squared_error: 0.0222
[1m11/43[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0190 - mean_squared_error: 0.0190
[1m15/43[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0197 - mean_squared_error: 0.0197
[1m20/43[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0198 - mean_squared_error: 0.0198
[1m25/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0193 - mean_squared_error: 0.0193
[1m30/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0190 - mean_squared_error: 0.0190
[1m35/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0188 - mean_squared_error: 0.0188
[1m40/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - loss: 0.0187 - mean_squared_error: 0.0187 - val_loss: 0.0209 - val_mean_squared_error: 0.0209
(20000, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 20000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 5000, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 5000, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1250, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 20000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 20000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 1250, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 1250, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 1250, 12)  â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 1250)  â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 15000)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 15000)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     15,001 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 15,823 (61.81 KB)
 Trainable params: 15,823 (61.81 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m52s[0m 1s/step - binary_accuracy: 1.0000 - loss: 0.6591
[1m 5/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6133 - loss: 0.7094
[1m11/43[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5633 - loss: 0.7888
[1m15/43[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5468 - loss: 0.8081
[1m20/43[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5187 - loss: 0.8199
[1m25/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5052 - loss: 0.8168
[1m30/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5067 - loss: 0.8047
[1m36/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5109 - loss: 0.7977
[1m40/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5182 - loss: 0.7883
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 18ms/step - binary_accuracy: 0.5238 - loss: 0.7835 - val_binary_accuracy: 0.6000 - val_loss: 0.8417
Epoch 2/10

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0935
[1m 5/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7433 - loss: 0.5440
[1m10/43[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7425 - loss: 0.5138
[1m14/43[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7364 - loss: 0.5226
[1m19/43[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7157 - loss: 0.5438
[1m24/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7010 - loss: 0.5550
[1m29/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6963 - loss: 0.5629
[1m34/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6959 - loss: 0.5696
[1m38/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6926 - loss: 0.5770
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6875 - loss: 0.5845
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.6867 - loss: 0.5859 - val_binary_accuracy: 0.6000 - val_loss: 0.6400
Epoch 3/10

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4681
[1m 6/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8083 - loss: 0.5801
[1m10/43[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7102 - loss: 0.6131
[1m14/43[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6382 - loss: 0.6443
[1m19/43[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5878 - loss: 0.6697
[1m24/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5712 - loss: 0.6785
[1m30/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5693 - loss: 0.6833
[1m35/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5755 - loss: 0.6849
[1m40/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5792 - loss: 0.6867
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.5830 - loss: 0.6853 - val_binary_accuracy: 0.6000 - val_loss: 0.7008
Epoch 4/10

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.3503
[1m 5/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7433 - loss: 0.4567
[1m10/43[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.6399 - loss: 0.5784
[1m15/43[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.6302 - loss: 0.6102
[1m18/43[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6298 - loss: 0.6151
[1m23/43[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6336 - loss: 0.6185
[1m27/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6355 - loss: 0.6236
[1m31/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.6355 - loss: 0.6276
[1m36/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6395 - loss: 0.6272
[1m40/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6438 - loss: 0.6253
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 14ms/step - binary_accuracy: 0.6466 - loss: 0.6238 - val_binary_accuracy: 0.6000 - val_loss: 0.6857
Epoch 5/10

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.2410
[1m 6/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9722 - loss: 0.2720
[1m11/43[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9056 - loss: 0.3238
[1m16/43[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8900 - loss: 0.3395
[1m21/43[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8812 - loss: 0.3508
[1m26/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8674 - loss: 0.3667
[1m31/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8421 - loss: 0.3989
[1m36/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8240 - loss: 0.4233
[1m41/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8086 - loss: 0.4427
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.7999 - loss: 0.4531 - val_binary_accuracy: 0.2000 - val_loss: 0.7833
Epoch 6/10

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7696
[1m 7/43[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.1813 - loss: 0.7849    
[1m11/43[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.2287 - loss: 0.7682
[1m17/43[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.3210 - loss: 0.7197
[1m22/43[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.3732 - loss: 0.6999
[1m26/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.4067 - loss: 0.6839
[1m31/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.4445 - loss: 0.6617
[1m36/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.4751 - loss: 0.6440
[1m40/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.4925 - loss: 0.6346
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.5075 - loss: 0.6273 - val_binary_accuracy: 0.6000 - val_loss: 0.7885
Epoch 7/10

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3574
[1m 5/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8033 - loss: 0.5071
[1m10/43[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.7246 - loss: 0.5315
[1m15/43[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7386 - loss: 0.5082
[1m19/43[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7574 - loss: 0.4916
[1m24/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7741 - loss: 0.4746
[1m28/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7754 - loss: 0.4735
[1m34/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7750 - loss: 0.4765
[1m38/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7749 - loss: 0.4779
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.7715 - loss: 0.4813 - val_binary_accuracy: 0.4000 - val_loss: 0.8521
Epoch 8/10

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.1824
[1m 7/43[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.9796 - loss: 0.2741
[1m12/43[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.8564 - loss: 0.4132
[1m17/43[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8195 - loss: 0.4537
[1m22/43[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8083 - loss: 0.4647
[1m26/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8064 - loss: 0.4644
[1m32/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.8098 - loss: 0.4569
[1m37/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8093 - loss: 0.4612
[1m42/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8088 - loss: 0.4657
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.8080 - loss: 0.4673 - val_binary_accuracy: 0.6000 - val_loss: 0.8259
Epoch 9/10

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3730
[1m 5/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.3673
[1m 9/43[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9579 - loss: 0.3899
[1m14/43[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.9427 - loss: 0.4016
[1m18/43[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - binary_accuracy: 0.9325 - loss: 0.4017
[1m23/43[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.9056 - loss: 0.4183
[1m27/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.8920 - loss: 0.4259
[1m33/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.8770 - loss: 0.4340
[1m38/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.8647 - loss: 0.4407
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8561 - loss: 0.4451
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 14ms/step - binary_accuracy: 0.8546 - loss: 0.4457 - val_binary_accuracy: 0.4000 - val_loss: 0.8839
Epoch 10/10

[1m 1/43[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - binary_accuracy: 1.0000 - loss: 0.0671
[1m 6/43[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 1.0000 - loss: 0.2396
[1m10/43[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 1.0000 - loss: 0.2523
[1m16/43[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9787 - loss: 0.2717
[1m20/43[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9504 - loss: 0.3092
[1m25/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9238 - loss: 0.3404
[1m30/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9044 - loss: 0.3600
[1m35/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8895 - loss: 0.3737
[1m40/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8784 - loss: 0.3841
[1m43/43[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.8722 - loss: 0.3888 - val_binary_accuracy: 0.4000 - val_loss: 0.9561

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.21813692 0.6498178  0.23991854 0.08141474 0.04288599 0.40940043
 0.74382246 0.22447786 0.48077404 0.09445795 0.72709364 0.5156128 ]
predicted [0 1 0 0 0 0 1 0 0 0 1 1]
expected [False  True  True False  True False False False  True  True  True  True]
accuracy: 0.5833333333333334
confusion matrix: 
[[4 1]
 [4 3]]
              precision    recall  f1-score   support

       False       0.50      0.80      0.62         5
        True       0.75      0.43      0.55         7

    accuracy                           0.58        12
   macro avg       0.62      0.61      0.58        12
weighted avg       0.65      0.58      0.57        12

macro avg f1-score: 0.5804195804195804
macro avg (UAR): 0.6142857142857143
Sensitivity:  0.8
Specificity:  0.42857142857142855
g-mean:  0.5855400437691199
-------- Model Performance ----------: 
accuracy:  [0.5        0.5        0.83333333 0.5        0.5        0.33333333
 0.5        0.66666667 0.41666667 0.58333333]
gmean:  [0.37796447 0.         0.77459667 0.47809144 0.47809144 0.3380617
 0.         0.65465367 0.3380617  0.58554004]
f1_score:  [0.4375     0.33333333 0.8125     0.48571429 0.48571429 0.33333333
 0.33333333 0.65714286 0.37777778 0.58041958]
UAR:  [0.45714286 0.42857143 0.8        0.48571429 0.48571429 0.34285714
 0.42857143 0.65714286 0.38571429 0.61428571]
Cohen Kappa score:  [-0.09090909 -0.16129032  0.63636364 -0.02857143 -0.02857143 -0.2972973
 -0.16129032  0.31428571 -0.23529412  0.21052632]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  20.0
step (sec):  20.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  60
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.675  0.846  0.833  0.823  0.823  0.496  0.75   0.783  0.683  0.733]
 [ 0.649  0.817  0.817  0.688  0.808  0.249  0.662  0.683  0.66   0.62 ]
 [ 0.666  0.821  0.817  0.799  0.801  0.424  0.704  0.733  0.612  0.66 ]
 [ 0.     0.5    0.6    0.     0.373  0.     0.5    0.65   0.     0.393]
 [ 0.619  0.804  0.817  0.788  0.8    0.263  0.675  0.717  0.5    0.625]
 [ 0.016  0.509  0.533  0.403  0.484 -0.036  0.475  0.517  0.407  0.471]
 [ 0.     0.5    0.6    0.     0.373  0.     0.5    0.65   0.     0.393]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.6         0.8         0.8         0.78433366  0.79392857  0.37428571
   0.6775      0.8         0.4767767   0.65979798]
 [ 0.5         0.75833333  0.75        0.75031152  0.74666667  0.29285714
   0.6375      0.71666667  0.48460652  0.6202381 ]
 [ 0.28761905  0.6375      0.65        0.58972553  0.62626984  0.33333333
   0.66666667  0.66666667  0.59091355  0.64380952]
 [ 0.45833333  0.72916667  0.73333333  0.68426678  0.70964286  0.13238095
   0.5475      0.68333333  0.24142136  0.51388889]
 [ 0.58690476  0.77916667  0.83333333  0.70737927  0.7793254   0.3
   0.65        0.66666667  0.56850705  0.62595238]
 [ 0.36666667  0.68333333  0.68333333  0.63573818  0.66464286  0.66666667
   0.83333333  0.83333333  0.82491496  0.82952381]
 [ 0.26095238  0.625       0.65        0.50276392  0.58757937  0.43333333
   0.71666667  0.71666667  0.68364485  0.70178571]
 [ 0.72571429  0.865       0.88333333  0.84299611  0.85555556  0.65151515
   0.82916667  0.81666667  0.80773503  0.80821429]
 [ 0.31095238  0.6625      0.71666667  0.49191974  0.61674603  0.12428571
   0.555       0.66666667  0.34142136  0.53769841]
 [ 0.11333333  0.55416667  0.55        0.50067059  0.5322619   0.25547619
   0.62916667  0.65        0.49405922  0.58527778]
 [ 0.05785714  0.5375      0.58333333  0.33968295  0.49690476  0.08333333
   0.54583333  0.58333333  0.39121375  0.51452381]
 [ 0.54642857  0.7625      0.8         0.74416009  0.77035714  0.41238095
   0.70833333  0.71666667  0.64698816  0.68698413]
 [ 0.39047619  0.69166667  0.73333333  0.57390248  0.66694444  0.0047619
   0.50833333  0.51666667  0.4625331   0.49333333]
 [ 0.14047619  0.57083333  0.63333333  0.40857782  0.54706349  0.46190476
   0.73333333  0.75        0.66941592  0.71801587]
 [ 0.03452381  0.5075      0.58333333  0.33371173  0.4927381   0.38666667
   0.6875      0.73333333  0.57551127  0.66730159]
 [ 0.09880952  0.54583333  0.6         0.3969395   0.53186508  0.49166667
   0.7375      0.8         0.6306825   0.72980159]
 [ 0.2547619   0.63333333  0.65        0.55360146  0.61702381  0.23166667
   0.625       0.66666667  0.4050756   0.55928571]
 [-0.06        0.47        0.76666667  0.          0.43272727  0.28
   0.65416667  0.65        0.55109491  0.6097619 ]
 [ 0.13642857  0.575       0.63333333  0.40442241  0.54107143  0.3
   0.65        0.63333333  0.58213672  0.61666667]
 [ 0.25119048  0.62916667  0.66666667  0.49701105  0.60178571  0.19309524
   0.6         0.61666667  0.47426348  0.56619048]
 [-0.10904762  0.4225      0.58333333  0.12247449  0.41       -0.3
   0.35        0.35        0.22189514  0.31690476]
 [ 0.2         0.6         0.6         0.51970366  0.57761905  0.4602381
   0.72083333  0.75        0.69475662  0.72123016]
 [-0.17857143  0.40416667  0.43333333  0.13164966  0.34333333  0.16857143
   0.585       0.75        0.30157475  0.55656566]
 [-0.08666667  0.45416667  0.45        0.38890014  0.43166667  0.43333333
   0.72083333  0.71666667  0.60748208  0.67642857]
 [ 0.625       0.8125      0.81666667  0.79893825  0.80595238  0.5
   0.75        0.75        0.67979044  0.72535714]
 [ 0.21666667  0.60833333  0.61666667  0.52329381  0.57630952  0.25833333
   0.625       0.63333333  0.59929125  0.62154762]
 [ 0.2947619   0.67        0.71666667  0.50150891  0.6075      0.34
   0.675       0.66666667  0.60269259  0.64059524]
 [ 0.13166667  0.56666667  0.6         0.41039363  0.53230159  0.34761905
   0.66666667  0.71666667  0.5035448   0.63365079]
 [-0.11547619  0.44583333  0.53333333  0.11123724  0.38107143  0.32380952
   0.65833333  0.68333333  0.59635623  0.64765873]
 [ 0.67532468  0.84583333  0.83333333  0.82270587  0.82349206  0.49642857
   0.75        0.78333333  0.6831769   0.73297619]]
KNN mean:
[0.25716955 0.62825    0.66944444 0.50243068 0.60334488 0.31459812
 0.65647222 0.68944444 0.54644923 0.63203223]
---------------------------
---------------------------
DT performance:
[[ 0.46666667  0.71666667  0.71666667  0.72810676  0.69988095  0.29809524
   0.66        0.75        0.471554    0.61497835]
 [ 0.35833333  0.65        0.66666667  0.62645435  0.60333333  0.30857143
   0.65833333  0.7         0.5480399   0.64301587]
 [ 0.14        0.5875      0.58333333  0.47558616  0.57011905  0.33333333
   0.65        0.65        0.60589655  0.63833333]
 [ 0.43333333  0.6875      0.68333333  0.68260384  0.66785714 -0.11642857
   0.3825      0.5         0.28284271  0.37027778]
 [ 0.44285714  0.72083333  0.75        0.6747218   0.70916667  0.27142857
   0.65416667  0.66666667  0.57256321  0.64059524]
 [ 0.21666667  0.64166667  0.65        0.51699892  0.62488095  0.51333333
   0.75        0.75        0.77659863  0.74666667]
 [ 0.24818182  0.59166667  0.6         0.59018741  0.57349206  0.15151515
   0.57083333  0.56666667  0.54257649  0.53047619]
 [ 0.92380952  0.975       0.96666667  0.95731322  0.96666667  0.42571429
   0.79583333  0.8         0.63365344  0.77960317]
 [ 0.22928571  0.6125      0.7         0.38637033  0.57833333 -0.07077922
   0.545       0.6         0.11324555  0.47642857]
 [ 0.0847619   0.6125      0.61666667  0.55465987  0.59896825 -0.06969697
   0.50416667  0.51666667  0.49664124  0.46392857]
 [ 0.10261905  0.5875      0.61666667  0.38289938  0.54714286  0.35857143
   0.675       0.7         0.63714099  0.66849206]
 [ 0.71547619  0.81666667  0.83333333  0.80424413  0.81710317  0.3641342
   0.64166667  0.63333333  0.64234155  0.61384921]
 [ 0.09833333  0.55833333  0.56666667  0.37134727  0.5297619   0.02333333
   0.45416667  0.46666667  0.34998785  0.41238095]
 [-0.03952381  0.44583333  0.48333333  0.3369182   0.40964286  0.15833333
   0.61666667  0.61666667  0.54488209  0.60178571]
 [ 0.1502381   0.63        0.61666667  0.5872825   0.58686508  0.01642857
   0.4875      0.53333333  0.33194792  0.4552381 ]
 [-0.24945887  0.37916667  0.41666667  0.16528126  0.35349206  0.20952381
   0.6625      0.7         0.54102495  0.64809524]
 [ 0.48690476  0.775       0.78333333  0.64238769  0.75642857  0.41904762
   0.74583333  0.76666667  0.62067779  0.72912698]
 [ 0.00675325  0.48        0.71666667  0.15268827  0.44338384  0.28809524
   0.7         0.71666667  0.51457444  0.68373016]
 [ 0.24857143  0.625       0.66666667  0.58851232  0.60940476  0.46666667
   0.65416667  0.65        0.60426953  0.64154762]
 [ 0.32261905  0.65        0.66666667  0.54079194  0.63373016  0.29166667
   0.63333333  0.61666667  0.55834481  0.57535714]
 [ 0.43008658  0.655       0.7         0.50925568  0.5910101   0.2
   0.51666667  0.51666667  0.59578903  0.49761905]
 [ 0.2         0.6         0.6         0.54292948  0.58821429  0.13047619
   0.60416667  0.63333333  0.55935262  0.57563492]
 [-0.01666667  0.525       0.53333333  0.41652238  0.48869048 -0.01880952
   0.4425      0.58333333  0.23371173  0.4477381 ]
 [ 0.18214286  0.60416667  0.61666667  0.56443764  0.59801587  0.525
   0.75        0.75        0.77291814  0.73095238]
 [ 0.43166667  0.675       0.66666667  0.65339192  0.65261905  0.46666667
   0.71666667  0.71666667  0.60150813  0.68880952]
 [ 0.33047619  0.68333333  0.68333333  0.6156475   0.65265873  0.425
   0.68333333  0.7         0.58400392  0.6672619 ]
 [ 0.4891342   0.7625      0.78333333  0.73710092  0.73952381  0.56666667
   0.79583333  0.78333333  0.81527377  0.77261905]
 [ 0.17365801  0.62916667  0.65        0.47229621  0.60373016  0.44047619
   0.70416667  0.7         0.63744533  0.68087302]
 [ 0.06809524  0.58333333  0.61666667  0.38573374  0.56452381 -0.0030303
   0.55416667  0.55        0.44056915  0.52583333]
 [ 0.64880952  0.81666667  0.81666667  0.68846366  0.80825397  0.2491342
   0.6625      0.68333333  0.65997182  0.61996032]]
DT mean:
[0.27746104 0.64258333 0.66555556 0.54503783 0.61889646 0.25408225
 0.62905556 0.65055556 0.54297824 0.60470695]
---------------------------
---------------------------
RF performance:
[[ 0.66666667  0.78333333  0.78333333  0.80824829  0.77261905  0.3552381
   0.7075      0.78333333  0.54748737  0.68541486]
 [ 0.38484848  0.69583333  0.7         0.65835696  0.67238095  0.17214286
   0.63333333  0.68333333  0.51655444  0.61678571]
 [ 0.24047619  0.62083333  0.63333333  0.58554938  0.60777778  0.36666667
   0.71666667  0.71666667  0.70589655  0.71142857]
 [ 0.33333333  0.57916667  0.58333333  0.60651847  0.55357143  0.22285714
   0.5         0.58333333  0.23054881  0.45638889]
 [ 0.47261905  0.70833333  0.75        0.72859775  0.70246032  0.33333333
   0.7         0.71666667  0.64376095  0.68059524]
 [ 0.34880952  0.61666667  0.61666667  0.56497084  0.58916667  0.7
   0.8         0.8         0.78601733  0.79202381]
 [ 0.20428571  0.6         0.61666667  0.4906344   0.57670635  0.24
   0.6625      0.65        0.48281338  0.63452381]
 [ 0.86380952  0.925       0.93333333  0.91462644  0.92222222  0.57928571
   0.77916667  0.78333333  0.66044836  0.7643254 ]
 [ 0.35547619  0.7375      0.76666667  0.5026586   0.71634921  0.11928571
   0.6075      0.66666667  0.43460652  0.57392857]
 [ 0.23333333  0.6125      0.61666667  0.47424689  0.60083333  0.15714286
   0.54166667  0.55        0.59851037  0.51833333]
 [ 0.23904762  0.65833333  0.7         0.38856181  0.63805556  0.325
   0.625       0.66666667  0.60012272  0.61611111]
 [ 0.70595238  0.78333333  0.81666667  0.8216856   0.75587302  0.5202381
   0.6625      0.68333333  0.63577632  0.63420635]
 [ 0.21547619  0.62916667  0.65        0.52710643  0.60142857  0.25142857
   0.64583333  0.66666667  0.44830418  0.62261905]
 [ 0.16404762  0.47916667  0.55        0.40236285  0.42789683  0.29166667
   0.67916667  0.68333333  0.56611272  0.65777778]
 [ 0.09857143  0.6175      0.68333333  0.44318517  0.6052381   0.15666667
   0.5625      0.63333333  0.2780239   0.52718254]
 [ 0.17880952  0.525       0.53333333  0.48035297  0.4943254   0.46071429
   0.6625      0.73333333  0.64191974  0.65666667]
 [ 0.66547619  0.825       0.83333333  0.78813414  0.81107143  0.48214286
   0.68333333  0.71666667  0.56336457  0.65801587]
 [ 0.12095238  0.5075      0.78333333  0.34959612  0.47558442  0.425
   0.68333333  0.68333333  0.68268125  0.65142857]
 [ 0.22571429  0.5875      0.66666667  0.32336928  0.54912698  0.30666667
   0.62916667  0.61666667  0.60538874  0.6072619 ]
 [ 0.46309524  0.65833333  0.68333333  0.68829754  0.62559524  0.18333333
   0.675       0.66666667  0.56323243  0.65035714]
 [ 0.01125541  0.565       0.66666667  0.35634511  0.49142496 -0.06666667
   0.46666667  0.46666667  0.23248972  0.43178571]
 [ 0.23333333  0.56666667  0.56666667  0.55924316  0.55261905  0.27
   0.7875      0.78333333  0.5754966   0.7656746 ]
 [ 0.04666667  0.525       0.53333333  0.4616643   0.49059524  0.24428571
   0.5725      0.7         0.41906707  0.55708153]
 [ 0.26666667  0.6         0.6         0.49991045  0.57619048  0.19
   0.73333333  0.71666667  0.56162423  0.71011905]
 [ 0.41547619  0.7         0.71666667  0.64788237  0.68928571  0.53333333
   0.71666667  0.71666667  0.70135332  0.70630952]
 [ 0.38        0.6625      0.66666667  0.53727313  0.6452381   0.32857143
   0.68333333  0.7         0.62053513  0.67166667]
 [ 0.38809524  0.6575      0.71666667  0.58277687  0.6427381   0.25833333
   0.70416667  0.7         0.74304281  0.65547619]
 [ 0.16190476  0.52916667  0.58333333  0.66522318  0.50138889  0.57380952
   0.7375      0.78333333  0.60443293  0.73166667]
 [ 0.01666667  0.5125      0.55        0.33485613  0.49265873  0.21030303
   0.56666667  0.56666667  0.53513973  0.5497619 ]
 [ 0.66580087  0.82083333  0.81666667  0.79863643  0.80126984  0.4241342
   0.70416667  0.73333333  0.61193109  0.65960317]]
RF mean:
[0.32555556 0.64297222 0.67722222 0.56636237 0.61938973 0.32049711
 0.66097222 0.685      0.55988944 0.63848401]
---------------------------
---------------------------
SVM performance:
[[ 0.3         0.65        0.65        0.58364485  0.62678571  0.
   0.5         0.76666667  0.          0.43272727]
 [ 0.          0.5         0.55        0.          0.35333333  0.
   0.5         0.65        0.          0.39333333]
 [ 0.          0.5         0.56666667  0.          0.36        0.33333333
   0.66666667  0.66666667  0.653037    0.66095238]
 [ 0.05833333  0.52916667  0.53333333  0.11897227  0.39166667  0.
   0.5         0.7         0.          0.41090909]
 [ 0.          0.5         0.63333333  0.          0.38666667  0.5
   0.75        0.76666667  0.67422615  0.72880952]
 [ 0.          0.5         0.53333333  0.          0.34666667  0.33333333
   0.66666667  0.66666667  0.52925643  0.6147619 ]
 [ 0.          0.5         0.61666667  0.          0.38        0.04
   0.525       0.51666667  0.07071068  0.36666667]
 [ 0.          0.5         0.7         0.          0.41090909  0.
   0.5         0.61666667  0.          0.38      ]
 [ 0.          0.5         0.66666667  0.          0.4         0.
   0.5         0.7         0.          0.41090909]
 [ 0.          0.5         0.53333333  0.          0.34666667  0.
   0.5         0.56666667  0.          0.36      ]
 [ 0.          0.5         0.63333333  0.          0.38666667  0.
   0.5         0.6         0.          0.37333333]
 [ 0.          0.5         0.61666667  0.          0.38        0.
   0.5         0.61666667  0.          0.38      ]
 [ 0.          0.5         0.56666667  0.          0.36        0.
   0.5         0.56666667  0.          0.36      ]
 [ 0.          0.5         0.63333333  0.          0.38666667  0.
   0.5         0.56666667  0.          0.36      ]
 [ 0.          0.5         0.7         0.          0.41090909  0.
   0.5         0.66666667  0.          0.4       ]
 [ 0.          0.5         0.61666667  0.          0.38        0.
   0.5         0.66666667  0.          0.4       ]
 [ 0.          0.5         0.56666667  0.          0.36        0.
   0.5         0.6         0.          0.37333333]
 [ 0.          0.5         0.81666667  0.          0.44909091  0.
   0.5         0.58333333  0.          0.36666667]
 [ 0.          0.5         0.66666667  0.          0.4         0.01818182
   0.5125      0.5         0.05        0.34857143]
 [ 0.          0.5         0.61666667  0.          0.38        0.
   0.5         0.6         0.          0.37333333]
 [ 0.          0.5         0.75        0.          0.42727273 -0.3
   0.35        0.35        0.14142136  0.29690476]
 [ 0.46666667  0.73333333  0.73333333  0.70184038  0.71928571  0.
   0.5         0.61666667  0.          0.38      ]
 [ 0.          0.5         0.58333333  0.          0.36666667  0.
   0.5         0.75        0.          0.42727273]
 [ 0.          0.5         0.53333333  0.          0.34666667  0.03333333
   0.51666667  0.56666667  0.05773503  0.3825    ]
 [ 0.          0.5         0.58333333  0.          0.36666667  0.4
   0.7         0.7         0.6370384   0.67904762]
 [ 0.          0.5         0.53333333  0.          0.34666667  0.
   0.5         0.53333333  0.          0.34666667]
 [ 0.          0.5         0.7         0.          0.41090909 -0.03333333
   0.48333333  0.51666667  0.          0.33833333]
 [ 0.          0.5         0.63333333  0.          0.38666667  0.
   0.5         0.6         0.          0.37333333]
 [ 0.          0.5         0.63333333  0.          0.38666667  0.
   0.5         0.56666667  0.          0.36      ]
 [ 0.          0.5         0.6         0.          0.37333333  0.
   0.5         0.65        0.          0.39333333]]
SVM mean:
[0.0275     0.51375    0.62333333 0.04681525 0.4008943  0.04416162
 0.52236111 0.61444444 0.09378083 0.4157233 ]
---------------------------
---------------------------
GBM performance:
[[ 0.5         0.75        0.75        0.72575502  0.73964286  0.31428571
   0.65        0.81666667  0.39142136  0.62594517]
 [ 0.36666667  0.66666667  0.68333333  0.60485474  0.645       0.43095238
   0.7125      0.78333333  0.55049838  0.6834127 ]
 [ 0.29166667  0.61666667  0.63333333  0.55151865  0.5797619   0.33333333
   0.66666667  0.66666667  0.62542276  0.65547619]
 [ 0.29166667  0.64583333  0.65        0.52486882  0.60404762 -0.07714286
   0.465       0.65        0.          0.39212121]
 [ 0.52142857  0.75        0.8         0.68666859  0.74785714  0.4
   0.7         0.71666667  0.62254247  0.67833333]
 [ 0.21666667  0.625       0.63333333  0.5275935   0.59666667  0.46666667
   0.73333333  0.73333333  0.7045512   0.72666667]
 [ 0.28452381  0.65        0.7         0.50965088  0.63702381  0.34
   0.65833333  0.65        0.60067666  0.62821429]
 [ 0.66309524  0.8875      0.91666667  0.8306825   0.89396825  0.40952381
   0.70833333  0.73333333  0.62419467  0.6875    ]
 [ 0.125       0.5625      0.68333333  0.23194792  0.50694444  0.00142857
   0.4925      0.65        0.12247449  0.43712121]
 [ 0.09166667  0.52916667  0.53333333  0.43109571  0.5002381   0.09428571
   0.525       0.55        0.42879263  0.50277778]
 [ 0.1202381   0.54166667  0.63333333  0.29372696  0.50912698  0.33357143
   0.64166667  0.66666667  0.58958481  0.62265873]
 [ 0.6952381   0.82083333  0.85        0.79384695  0.82313492  0.50428571
   0.75416667  0.76666667  0.72726041  0.74210317]
 [ 0.12380952  0.55833333  0.58333333  0.5216856   0.54634921  0.11190476
   0.50416667  0.53333333  0.39664124  0.48968254]
 [ 0.13214286  0.53333333  0.61666667  0.31359758  0.48492063  0.28095238
   0.64583333  0.65        0.56543716  0.61194444]
 [ 0.10142857  0.575       0.7         0.27247449  0.51902597 -0.14571429
   0.4375      0.55        0.12071068  0.37666667]
 [-0.11761905  0.44583333  0.5         0.13535534  0.39166667  0.30595238
   0.65        0.75        0.35997182  0.6009127 ]
 [ 0.61190476  0.80416667  0.83333333  0.73813414  0.8018254   0.45238095
   0.725       0.76666667  0.61016062  0.7034127 ]
 [-0.04        0.49        0.8         0.          0.44363636  0.48095238
   0.72083333  0.75        0.61358543  0.70337302]
 [ 0.27142857  0.625       0.7         0.50708101  0.61190476  0.53333333
   0.75        0.75        0.67422615  0.73416667]
 [ 0.3197619   0.65833333  0.68333333  0.55508419  0.63285714  0.39166667
   0.70416667  0.71666667  0.58645826  0.66119048]
 [ 0.26190476  0.645       0.76666667  0.3601534   0.59434343 -0.13333333
   0.43333333  0.43333333  0.32440169  0.41416667]
 [ 0.23333333  0.63333333  0.63333333  0.57674988  0.61833333  0.18
   0.5875      0.66666667  0.35101499  0.54634921]
 [ 0.21190476  0.60416667  0.63333333  0.41783039  0.55920635  0.05
   0.5175      0.71666667  0.19318517  0.49126263]
 [ 0.22380952  0.6125      0.61666667  0.53978219  0.58646825  0.39166667
   0.7125      0.71666667  0.69105894  0.69988095]
 [ 0.30714286  0.65416667  0.66666667  0.60786827  0.63277778  0.36666667
   0.68333333  0.68333333  0.6119479   0.6697619 ]
 [ 0.36380952  0.68333333  0.68333333  0.60842028  0.65361111  0.36666667
   0.7         0.71666667  0.58029824  0.68571429]
 [ 0.53095238  0.7625      0.83333333  0.65049838  0.74886724  0.46666667
   0.74166667  0.73333333  0.65711693  0.70964286]
 [ 0.23809524  0.63333333  0.71666667  0.46902344  0.62055556  0.36309524
   0.64166667  0.66666667  0.57538521  0.62210317]
 [-0.00833333  0.48333333  0.56666667  0.26389584  0.42718254  0.24309524
   0.64583333  0.66666667  0.56359151  0.62690476]
 [ 0.61928571  0.80416667  0.81666667  0.7879158   0.79996032  0.26341991
   0.675       0.71666667  0.50049838  0.62531746]]
GBM mean:
[0.2850873  0.64172222 0.69388889 0.50125868 0.61523016 0.28401876
 0.63944444 0.68555556 0.49877034 0.61182612]
---------------------------
---------------------------
BDDAE performance:
[[-0.13333333  0.43333333  0.43333333  0.36834081  0.41046738 -0.02134921
   0.49444444  0.64166667  0.23941166  0.47097155]
 [ 0.13532464  0.56571429  0.58333333  0.54025891  0.55903902  0.46766807
   0.725       0.76666667  0.70845966  0.73093954]
 [ 0.05317675  0.52428571  0.54166667  0.50210254  0.51937032  0.28333333
   0.64166667  0.64166667  0.61756392  0.63070818]
 [ 0.01666667  0.50833333  0.50833333  0.44942589  0.47983541  0.0893746
   0.5375      0.63333333  0.38227288  0.51277845]
 [ 0.18973242  0.6125      0.58333333  0.575266    0.56398348  0.2
   0.6         0.6         0.57576487  0.58858142]
 [ 0.25        0.625       0.625       0.59049232  0.60866078  0.01666667
   0.50833333  0.50833333  0.45094162  0.48886169]
 [ 0.17323052  0.58285714  0.61666667  0.49770588  0.56395345  0.06666667
   0.53333333  0.53333333  0.50592673  0.52048868]
 [ 0.1135989   0.55        0.64166667  0.4083191   0.53223752 -0.07461994
   0.46285714  0.5         0.37166841  0.44812792]
 [ 0.24428571  0.6125      0.70833333  0.44618341  0.59584083  0.31736264
   0.6375      0.75833333  0.4280239   0.61389257]
 [-0.1         0.45        0.45        0.42337121  0.43797758  0.06675041
   0.53285714  0.55833333  0.48678674  0.5205523 ]
 [-0.10947641  0.44375     0.55        0.21401195  0.41834488  0.03887277
   0.51571429  0.55833333  0.41196764  0.49259394]
 [-0.0797216   0.46        0.48333333  0.38429903  0.44023574  0.3621724
   0.68428571  0.69166667  0.65628199  0.6709691 ]
 [-0.08049192  0.46142857  0.45833333  0.43141251  0.44264666  0.28369059
   0.64714286  0.64166667  0.63056317  0.63244311]
 [ 0.05542607  0.525       0.54166667  0.50129639  0.50937966  0.27458039
   0.64142857  0.64166667  0.62678468  0.63067974]
 [ 0.17238742  0.5875      0.65        0.49988467  0.57162244  0.12284056
   0.575       0.575       0.54921987  0.54715184]
 [-0.09150962  0.45714286  0.51666667  0.19552436  0.39625424  0.02461538
   0.5125      0.65833333  0.11614378  0.44237948]
 [ 0.42034147  0.71428571  0.71666667  0.69455573  0.70228823 -0.02695683
   0.48714286  0.54166667  0.24648204  0.43215293]
 [ 0.0625      0.54        0.7         0.37514029  0.52361738 -0.05275115
   0.47142857  0.48333333  0.45514984  0.46649684]
 [ 0.07567116  0.53125     0.575       0.48792712  0.52904469 -0.08333333
   0.45833333  0.45833333  0.41662224  0.44419289]
 [ 0.02701608  0.51285714  0.54166667  0.3891669   0.47915243 -0.18673335
   0.40857143  0.45        0.21443835  0.37308664]
 [-0.17073593  0.41111111  0.58333333  0.09746625  0.39329961  0.08333333
   0.54166667  0.54166667  0.50016849  0.52140911]
 [-0.15        0.425       0.425       0.33628135  0.39794539  0.08769544
   0.54285714  0.56666667  0.51259991  0.53784979]
 [ 0.18198057  0.59142857  0.6         0.56954669  0.581921    0.50712843
   0.75555556  0.83333333  0.67980028  0.742912  ]
 [ 0.03333333  0.51666667  0.51666667  0.45298133  0.49410818  0.29783058
   0.64714286  0.65833333  0.61546379  0.63635795]
 [ 0.1393071   0.57        0.59166667  0.49849026  0.55073302  0.06666667
   0.53333333  0.53333333  0.51497059  0.52432678]
 [ 0.05        0.525       0.525       0.48970548  0.50827669  0.05
   0.525       0.525       0.49126738  0.50921884]
 [ 0.21522463  0.60625     0.66666667  0.53810672  0.59928025  0.38333333
   0.69166667  0.69166667  0.66778627  0.68073302]
 [-0.0323454   0.4875      0.54166667  0.39291748  0.47331036  0.17795116
   0.58142857  0.625       0.50991551  0.56772292]
 [-0.0867308   0.45        0.475       0.4016062   0.43484651  0.23801375
   0.61571429  0.64166667  0.58110392  0.60992672]
 [ 0.01579517  0.50857143  0.53333333  0.40250611  0.48367688 -0.03587277
   0.475       0.51666667  0.40662324  0.47094378]]
BDDAE mean:
[0.05302179 0.52630886 0.56277778 0.43847643 0.50671167 0.13416435
 0.56614683 0.59916667 0.48567245 0.54864832]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.5        0.         0.33333333 0.
  0.5        0.76666667 0.         0.43272727]
 [0.         0.5        0.55       0.         0.35333333 0.
  0.5        0.65       0.         0.39333333]
 [0.         0.5        0.56666667 0.         0.36       0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.48333333 0.         0.325      0.
  0.5        0.7        0.         0.41090909]
 [0.         0.5        0.63333333 0.         0.38666667 0.
  0.5        0.51666667 0.         0.34      ]
 [0.         0.5        0.53333333 0.         0.34666667 0.
  0.5        0.53333333 0.         0.34666667]
 [0.         0.5        0.61666667 0.         0.38       0.
  0.5        0.48333333 0.         0.325     ]
 [0.         0.5        0.7        0.         0.41090909 0.
  0.5        0.61666667 0.         0.38      ]
 [0.         0.5        0.66666667 0.         0.4        0.
  0.5        0.7        0.         0.41090909]
 [0.         0.5        0.53333333 0.         0.34666667 0.
  0.5        0.56666667 0.         0.36      ]
 [0.         0.5        0.63333333 0.         0.38666667 0.
  0.5        0.6        0.         0.37333333]
 [0.         0.5        0.61666667 0.         0.38       0.
  0.5        0.61666667 0.         0.38      ]
 [0.         0.5        0.56666667 0.         0.36       0.
  0.5        0.56666667 0.         0.36      ]
 [0.         0.5        0.63333333 0.         0.38666667 0.
  0.5        0.56666667 0.         0.36      ]
 [0.         0.5        0.7        0.         0.41090909 0.
  0.5        0.66666667 0.         0.4       ]
 [0.         0.5        0.61666667 0.         0.38       0.
  0.5        0.66666667 0.         0.4       ]
 [0.         0.5        0.56666667 0.         0.36       0.
  0.5        0.6        0.         0.37333333]
 [0.         0.5        0.81666667 0.         0.44909091 0.
  0.5        0.58333333 0.         0.36666667]
 [0.         0.5        0.66666667 0.         0.4        0.
  0.5        0.48333333 0.         0.325     ]
 [0.         0.5        0.61666667 0.         0.38       0.
  0.5        0.6        0.         0.37333333]
 [0.         0.5        0.75       0.         0.42727273 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.5        0.         0.33333333 0.
  0.5        0.61666667 0.         0.38      ]
 [0.         0.5        0.58333333 0.         0.36666667 0.
  0.5        0.75       0.         0.42727273]
 [0.         0.5        0.53333333 0.         0.34666667 0.
  0.5        0.55       0.         0.35333333]
 [0.         0.5        0.58333333 0.         0.36666667 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.53333333 0.         0.34666667 0.
  0.5        0.53333333 0.         0.34666667]
 [0.         0.5        0.7        0.         0.41090909 0.
  0.5        0.53333333 0.         0.34666667]
 [0.         0.5        0.63333333 0.         0.38666667 0.
  0.5        0.6        0.         0.37333333]
 [0.         0.5        0.63333333 0.         0.38666667 0.
  0.5        0.56666667 0.         0.36      ]
 [0.         0.5        0.6        0.         0.37333333 0.
  0.5        0.65       0.         0.39333333]]
DUMMY mean:
[0.         0.5        0.60888889 0.         0.37602525 0.
 0.5        0.59277778 0.         0.36972727]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_46_1
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.257 0.628 0.669 0.502 0.603 0.315 0.656 0.689 0.546 0.632]
 [0.277 0.643 0.666 0.545 0.619 0.254 0.629 0.651 0.543 0.605]
 [0.326 0.643 0.677 0.566 0.619 0.32  0.661 0.685 0.56  0.638]
 [0.028 0.514 0.623 0.047 0.401 0.044 0.522 0.614 0.094 0.416]
 [0.285 0.642 0.694 0.501 0.615 0.284 0.639 0.686 0.499 0.612]
 [0.053 0.526 0.563 0.438 0.507 0.134 0.566 0.599 0.486 0.549]
 [0.    0.5   0.609 0.    0.376 0.    0.5   0.593 0.    0.37 ]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.248 0.125 0.11  0.217 0.135 0.189 0.095 0.094 0.146 0.1  ]
 [0.24  0.114 0.106 0.171 0.121 0.194 0.103 0.089 0.153 0.107]
 [0.212 0.105 0.099 0.157 0.113 0.161 0.079 0.074 0.134 0.084]
 [0.098 0.049 0.07  0.161 0.078 0.149 0.075 0.086 0.211 0.105]
 [0.204 0.105 0.097 0.197 0.12  0.187 0.096 0.081 0.187 0.105]
 [0.139 0.07  0.078 0.119 0.074 0.171 0.085 0.091 0.144 0.091]
 [0.    0.    0.075 0.    0.029 0.    0.    0.075 0.    0.029]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 96.  20.  16.  43.  22.  60.  14.  14.  27.  16.]
 [ 86.  18.  16.  31.  20.  76.  16.  14.  28.  18.]
 [ 65.  16.  15.  28.  18.  50.  12.  11.  24.  13.]
 [356.  10.  11. 344.  19. 337.  14.  14. 225.  25.]
 [ 72.  16.  14.  39.  20.  66.  15.  12.  37.  17.]
 [262.  13.  14.  27.  15. 127.  15.  15.  30.  17.]
 [  0.   0.  12.   0.   8.   0.   0.  13.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_46_1
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  20.0
step (sec):  20.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  60
Elapsed time: 604.6149618983269 minutes
Elapsed time: 10.076916031638781 hours
