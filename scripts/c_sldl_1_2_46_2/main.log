2024-05-14 06:58:49.937696: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-14 06:58:53.425595: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-14 06:59:02.600437: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  20.0
step (sec):  15.0
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  5.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_46_2\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value



[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m58s[0m 1s/step - loss: 0.2014 - mean_squared_error: 0.2014
[1m 5/56[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.1439 - mean_squared_error: 0.1439
[1m 9/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.1129 - mean_squared_error: 0.1129
[1m14/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0976 - mean_squared_error: 0.0976
[1m19/56[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0888 - mean_squared_error: 0.0888
[1m24/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0821 - mean_squared_error: 0.0821
[1m28/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0775 - mean_squared_error: 0.0775
[1m33/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0730 - mean_squared_error: 0.0730
[1m37/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0700 - mean_squared_error: 0.0700
[1m42/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0668 - mean_squared_error: 0.0668
[1m47/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0642 - mean_squared_error: 0.0642
[1m52/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - loss: 0.0619 - mean_squared_error: 0.0619
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - loss: 0.0603 - mean_squared_error: 0.0603
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 17ms/step - loss: 0.0599 - mean_squared_error: 0.0599 - val_loss: 0.0079 - val_mean_squared_error: 0.0079
Epoch 2/5

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - loss: 0.0225 - mean_squared_error: 0.0225
[1m 6/56[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0312 - mean_squared_error: 0.0312
[1m10/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0284 - mean_squared_error: 0.0284
[1m15/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0265 - mean_squared_error: 0.0265
[1m19/56[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0265 - mean_squared_error: 0.0265
[1m24/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0263 - mean_squared_error: 0.0263
[1m28/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0262 - mean_squared_error: 0.0262
[1m32/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0260 - mean_squared_error: 0.0260
[1m37/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0257 - mean_squared_error: 0.0257
[1m41/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0255 - mean_squared_error: 0.0255
[1m46/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0253 - mean_squared_error: 0.0253
[1m50/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0251 - mean_squared_error: 0.0251
[1m55/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 14ms/step - loss: 0.0248 - mean_squared_error: 0.0248
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 14ms/step - loss: 0.0247 - mean_squared_error: 0.0247 - val_loss: 0.0064 - val_mean_squared_error: 0.0064
Epoch 3/5

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0089 - mean_squared_error: 0.0089
[1m 5/56[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - loss: 0.0058 - mean_squared_error: 0.0058
[1m10/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0139 - mean_squared_error: 0.0139
[1m15/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0149 - mean_squared_error: 0.0149
[1m19/56[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0158 - mean_squared_error: 0.0158
[1m24/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0164 - mean_squared_error: 0.0164
[1m28/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0172 - mean_squared_error: 0.0172
[1m33/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0178 - mean_squared_error: 0.0178
[1m37/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0181 - mean_squared_error: 0.0181
[1m41/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0184 - mean_squared_error: 0.0184
[1m46/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0187 - mean_squared_error: 0.0187
[1m51/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 14ms/step - loss: 0.0189 - mean_squared_error: 0.0189
[1m55/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 14ms/step - loss: 0.0190 - mean_squared_error: 0.0190
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 15ms/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0059 - val_mean_squared_error: 0.0059
Epoch 4/5

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0207 - mean_squared_error: 0.0207
[1m 6/56[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0265 - mean_squared_error: 0.0265
[1m10/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0247 - mean_squared_error: 0.0247
[1m15/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0218 - mean_squared_error: 0.0218
[1m20/56[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0211 - mean_squared_error: 0.0211
[1m24/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0215 - mean_squared_error: 0.0215
[1m29/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0216 - mean_squared_error: 0.0216
[1m34/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0213 - mean_squared_error: 0.0213
[1m38/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0211 - mean_squared_error: 0.0211
[1m43/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0208 - mean_squared_error: 0.0208
[1m47/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 14ms/step - loss: 0.0207 - mean_squared_error: 0.0207
[1m52/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 14ms/step - loss: 0.0206 - mean_squared_error: 0.0206
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 14ms/step - loss: 0.0206 - mean_squared_error: 0.0206
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 14ms/step - loss: 0.0206 - mean_squared_error: 0.0206 - val_loss: 0.0063 - val_mean_squared_error: 0.0063
Epoch 5/5

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 32ms/step - loss: 0.0057 - mean_squared_error: 0.0057
[1m 6/56[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0195 - mean_squared_error: 0.0195
[1m11/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0185 - mean_squared_error: 0.0185
[1m15/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0191 - mean_squared_error: 0.0191
[1m20/56[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0193 - mean_squared_error: 0.0193
[1m25/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0193 - mean_squared_error: 0.0193
[1m29/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0195 - mean_squared_error: 0.0195
[1m34/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0194 - mean_squared_error: 0.0194
[1m39/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0196 - mean_squared_error: 0.0196
[1m43/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0197 - mean_squared_error: 0.0197
[1m48/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - loss: 0.0197 - mean_squared_error: 0.0197
[1m53/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - loss: 0.0196 - mean_squared_error: 0.0196
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 14ms/step - loss: 0.0194 - mean_squared_error: 0.0194 - val_loss: 0.0080 - val_mean_squared_error: 0.0080
(20000, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 20000, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 5000, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 5000, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1250, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 20000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 20000, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 1250, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 1250, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 1250, 12)  â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 1250)  â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 15000)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 15000)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     15,001 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 15,823 (61.81 KB)
 Trainable params: 15,823 (61.81 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:08[0m 1s/step - binary_accuracy: 0.0000e+00 - loss: 0.7323
[1m 5/56[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.1567 - loss: 0.8123    
[1m10/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.2008 - loss: 0.8415
[1m15/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.2453 - loss: 0.8376
[1m20/56[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.2833 - loss: 0.8274
[1m25/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.3258 - loss: 0.8118
[1m30/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.3466 - loss: 0.8113
[1m34/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.3610 - loss: 0.8086
[1m39/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.3777 - loss: 0.8019
[1m44/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.3948 - loss: 0.7933
[1m49/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.4109 - loss: 0.7844
[1m54/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.4263 - loss: 0.7754
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 21ms/step - binary_accuracy: 0.4357 - loss: 0.7691 - val_binary_accuracy: 0.7143 - val_loss: 0.6541
Epoch 2/10

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 0.9886
[1m 6/56[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.3222 - loss: 0.9966    
[1m11/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.4148 - loss: 0.9570
[1m16/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.4849 - loss: 0.9008
[1m21/56[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5248 - loss: 0.8606
[1m26/56[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5569 - loss: 0.8291
[1m32/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5889 - loss: 0.7955
[1m37/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.6097 - loss: 0.7744
[1m41/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6237 - loss: 0.7586
[1m46/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6370 - loss: 0.7437
[1m51/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6458 - loss: 0.7345
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6523 - loss: 0.7272
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.6534 - loss: 0.7261 - val_binary_accuracy: 0.7143 - val_loss: 0.5956
Epoch 3/10

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 30ms/step - binary_accuracy: 0.0000e+00 - loss: 1.0852
[1m 6/56[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.2472 - loss: 0.8857    
[1m10/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.3231 - loss: 0.8230
[1m16/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.3850 - loss: 0.7652
[1m21/56[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.4409 - loss: 0.7232
[1m26/56[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.4811 - loss: 0.6981
[1m31/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5146 - loss: 0.6772
[1m37/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5472 - loss: 0.6563
[1m42/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5648 - loss: 0.6502
[1m47/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5803 - loss: 0.6446
[1m52/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5907 - loss: 0.6417
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - binary_accuracy: 0.5973 - loss: 0.6398
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.5988 - loss: 0.6395 - val_binary_accuracy: 0.7143 - val_loss: 0.5867
Epoch 4/10

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 35ms/step - binary_accuracy: 1.0000 - loss: 0.5364
[1m 6/56[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 1.0000 - loss: 0.4088
[1m10/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.9900 - loss: 0.4134
[1m14/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9533 - loss: 0.4357
[1m19/56[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.9287 - loss: 0.4441
[1m24/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9043 - loss: 0.4601
[1m28/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8856 - loss: 0.4747
[1m33/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.8638 - loss: 0.4944
[1m38/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.8488 - loss: 0.5072
[1m43/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8369 - loss: 0.5164
[1m48/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8250 - loss: 0.5249
[1m54/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8117 - loss: 0.5324
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 14ms/step - binary_accuracy: 0.8056 - loss: 0.5356 - val_binary_accuracy: 0.7143 - val_loss: 0.6153
Epoch 5/10

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7331
[1m 7/56[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 10ms/step - binary_accuracy: 0.3816 - loss: 0.6838    
[1m12/56[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 11ms/step - binary_accuracy: 0.4692 - loss: 0.6731
[1m17/56[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5351 - loss: 0.6364
[1m22/56[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5729 - loss: 0.6184
[1m26/56[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.5946 - loss: 0.6101
[1m31/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.6178 - loss: 0.5971
[1m35/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.6248 - loss: 0.5940
[1m39/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6311 - loss: 0.5911
[1m44/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6405 - loss: 0.5847
[1m49/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6481 - loss: 0.5793
[1m54/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6547 - loss: 0.5756
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.6568 - loss: 0.5747 - val_binary_accuracy: 0.7143 - val_loss: 0.5763
Epoch 6/10

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2978
[1m 6/56[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7583 - loss: 0.5200
[1m10/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.7735 - loss: 0.4962
[1m15/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7926 - loss: 0.4725
[1m20/56[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7949 - loss: 0.4619
[1m25/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.7839 - loss: 0.4626
[1m31/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7680 - loss: 0.4699
[1m36/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7578 - loss: 0.4749
[1m40/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7515 - loss: 0.4780
[1m46/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7479 - loss: 0.4805
[1m50/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7462 - loss: 0.4813
[1m55/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7453 - loss: 0.4808
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 14ms/step - binary_accuracy: 0.7454 - loss: 0.4801 - val_binary_accuracy: 0.7143 - val_loss: 0.6000
Epoch 7/10

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3289
[1m 5/56[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.8433 - loss: 0.4487
[1m10/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7992 - loss: 0.4417
[1m15/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7883 - loss: 0.4457
[1m20/56[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7968 - loss: 0.4368
[1m25/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7922 - loss: 0.4380
[1m29/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7907 - loss: 0.4374
[1m35/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7905 - loss: 0.4374
[1m40/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7924 - loss: 0.4377
[1m45/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7912 - loss: 0.4398
[1m50/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7887 - loss: 0.4423
[1m55/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 12ms/step - binary_accuracy: 0.7856 - loss: 0.4460
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.7850 - loss: 0.4467 - val_binary_accuracy: 0.7143 - val_loss: 0.5826
Epoch 8/10

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 34ms/step - binary_accuracy: 1.0000 - loss: 0.4656
[1m 5/56[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - binary_accuracy: 0.9600 - loss: 0.3544
[1m 9/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - binary_accuracy: 0.9172 - loss: 0.3513
[1m14/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.9166 - loss: 0.3278
[1m19/56[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.9141 - loss: 0.3171
[1m24/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9113 - loss: 0.3115
[1m29/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9049 - loss: 0.3179
[1m34/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8959 - loss: 0.3296
[1m39/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8919 - loss: 0.3363
[1m44/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8879 - loss: 0.3430
[1m48/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8856 - loss: 0.3481
[1m53/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8805 - loss: 0.3547
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.8775 - loss: 0.3583 - val_binary_accuracy: 0.5714 - val_loss: 0.6107
Epoch 9/10

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3701
[1m 6/56[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.9389 - loss: 0.3081
[1m11/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.8975 - loss: 0.3255
[1m15/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8851 - loss: 0.3402
[1m19/56[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8678 - loss: 0.3580
[1m24/56[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8478 - loss: 0.3731
[1m28/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8386 - loss: 0.3805
[1m33/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8289 - loss: 0.3887
[1m38/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8221 - loss: 0.3958
[1m42/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8202 - loss: 0.3971
[1m46/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8183 - loss: 0.3996
[1m51/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8181 - loss: 0.4000
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8191 - loss: 0.3979
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 14ms/step - binary_accuracy: 0.8191 - loss: 0.3976 - val_binary_accuracy: 0.7143 - val_loss: 0.7818
Epoch 10/10

[1m 1/56[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1658
[1m 6/56[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.6556 - loss: 0.4874
[1m10/56[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.6875 - loss: 0.4479
[1m15/56[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7138 - loss: 0.4162
[1m20/56[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7435 - loss: 0.3840
[1m26/56[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7730 - loss: 0.3538
[1m30/56[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7858 - loss: 0.3404
[1m35/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7990 - loss: 0.3295
[1m41/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8083 - loss: 0.3245
[1m45/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8105 - loss: 0.3243
[1m50/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8116 - loss: 0.3250
[1m55/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.8132 - loss: 0.3251
[1m56/56[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 13ms/step - binary_accuracy: 0.8142 - loss: 0.3254 - val_binary_accuracy: 0.4286 - val_loss: 0.6451

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 110ms/step
predicted [0.40893754 0.2761367  0.7625782  0.3157097  0.74491346 0.39078066
 0.7941184  0.9150656  0.20555964 0.22576469 0.50644875 0.17713027
 0.43740252 0.97777086 0.44042838 0.46099395]
predicted [0 0 1 0 1 0 1 1 0 0 1 0 0 1 0 0]
expected [False  True  True False  True  True  True False  True  True False False
 False  True  True  True]
accuracy: 0.5
confusion matrix: 
[[4 2]
 [6 4]]
              precision    recall  f1-score   support

       False       0.40      0.67      0.50         6
        True       0.67      0.40      0.50        10

    accuracy                           0.50        16
   macro avg       0.53      0.53      0.50        16
weighted avg       0.57      0.50      0.50        16

macro avg f1-score: 0.5
macro avg (UAR): 0.5333333333333333
Sensitivity:  0.6666666666666666
Specificity:  0.4
g-mean:  0.5163977794943222
-------- Model Performance ----------: 
accuracy:  [0.4375 0.25   0.4375 0.6875 0.5    0.625  0.5625 0.375  0.5625 0.5   ]
gmean:  [0.31622777 0.2236068  0.31622777 0.40824829 0.4472136  0.63245553
 0.54772256 0.28867513 0.54772256 0.51639778]
f1_score:  [0.37662338 0.23809524 0.37662338 0.54285714 0.46666667 0.61904762
 0.5465587  0.33333333 0.5465587  0.5       ]
UAR:  [0.38333333 0.23333333 0.38333333 0.58333333 0.46666667 0.63333333
 0.55       0.33333333 0.55       0.53333333]
Cohen Kappa score:  [-0.24137931 -0.5        -0.24137931  0.2        -0.06666667  0.25
  0.09677419 -0.33333333  0.09677419  0.05882353]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  20.0
step (sec):  15.008
overlap:  True
perc. of overlap:  24.96
overlap duration (sec):  4.992
Number of windows / instances:  79
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.791  0.907  0.898  0.902  0.894  0.298  0.655  0.682  0.568  0.634]
 [ 0.516  0.78   0.796  0.731  0.776  0.324  0.687  0.732  0.654  0.678]
 [ 0.716  0.843  0.836  0.874  0.83   0.431  0.703  0.745  0.529  0.698]
 [ 0.     0.5    0.62   0.     0.383  0.     0.5    0.646  0.     0.392]
 [ 0.644  0.823  0.834  0.806  0.817  0.428  0.703  0.771  0.576  0.687]
 [-0.068  0.465  0.494  0.424  0.455 -0.046  0.477  0.538  0.344  0.452]
 [ 0.     0.5    0.62   0.     0.383  0.     0.5    0.646  0.     0.392]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.50469697  0.75333333  0.75892857  0.72767942  0.74183983  0.17809524
   0.59166667  0.78571429  0.29109722  0.55812854]
 [ 0.48369565  0.745       0.74464286  0.72614254  0.73404762  0.4095135
   0.69        0.73571429  0.6269979   0.69323232]
 [ 0.43261905  0.7125      0.7375      0.68394895  0.70978355  0.33178947
   0.66833333  0.65714286  0.64132112  0.64936508]
 [ 0.50512422  0.75083333  0.75714286  0.73236143  0.74616883  0.24101271
   0.61        0.70714286  0.50823759  0.61118881]
 [ 0.60486425  0.79333333  0.825       0.76061085  0.79381674  0.54166667
   0.77083333  0.77142857  0.74228504  0.75870491]
 [ 0.38636364  0.6925      0.69642857  0.66906956  0.68301587  0.57254902
   0.78666667  0.78392857  0.77472875  0.77861111]
 [ 0.22279487  0.6075      0.64464286  0.51121908  0.59283383  0.391
   0.69583333  0.69464286  0.6815546   0.68904762]
 [ 0.79761905  0.905       0.9125      0.89802157  0.89763348  0.66075569
   0.83666667  0.83571429  0.82451933  0.82455988]
 [ 0.22478022  0.61166667  0.68392857  0.48950008  0.58445721  0.10304507
   0.56        0.63571429  0.32958223  0.51119436]
 [ 0.39615385  0.7         0.69642857  0.67145825  0.68551587  0.23552381
   0.62        0.63214286  0.56754184  0.60734488]
 [ 0.13601565  0.56166667  0.59642857  0.50158827  0.55734488  0.29911812
   0.65333333  0.69821429  0.51260618  0.62856421]
 [ 0.52027196  0.75666667  0.78571429  0.71751895  0.74919913  0.48432712
   0.73666667  0.76071429  0.72077899  0.738557  ]
 [ 0.31208059  0.65583333  0.66964286  0.63632363  0.6498557   0.09573022
   0.55166667  0.55535714  0.44162254  0.51017871]
 [ 0.012558    0.49        0.59285714  0.26436388  0.46830836  0.33309524
   0.66416667  0.68214286  0.59490485  0.64532468]
 [ 0.04386986  0.52666667  0.63214286  0.31166943  0.49419913  0.26703297
   0.63666667  0.68392857  0.53303121  0.61638112]
 [ 0.12608575  0.5625      0.58035714  0.43592139  0.52043124  0.39344969
   0.68666667  0.74821429  0.60086424  0.67588578]
 [ 0.22172639  0.61333333  0.63571429  0.5218844   0.59131313  0.20179089
   0.59833333  0.63214286  0.53825528  0.59165945]
 [-0.03428571  0.47857143  0.82321429  0.          0.45032967  0.25102757
   0.62833333  0.63392857  0.57702753  0.61094156]
 [ 0.28691877  0.65333333  0.71071429  0.54394297  0.62601399  0.20233002
   0.6075      0.60892857  0.53688172  0.57935065]
 [ 0.52668498  0.76        0.8         0.68677273  0.74530803  0.39387279
   0.69333333  0.72142857  0.66525814  0.68935786]
 [ 0.15255411  0.58452381  0.70892857  0.38145978  0.5626024   0.066
   0.5325      0.53214286  0.51697354  0.52404762]
 [ 0.26666667  0.63333333  0.63392857  0.56451033  0.60891414  0.30198125
   0.64833333  0.68392857  0.58690638  0.63872294]
 [ 0.25925741  0.625       0.65892857  0.52427858  0.60032468  0.22666667
   0.61666667  0.74642857  0.42274861  0.59478022]
 [ 0.12564103  0.565       0.55535714  0.53501759  0.54406205  0.58573338
   0.7925      0.79642857  0.76983714  0.78450216]
 [ 0.55047619  0.775       0.7875      0.76059456  0.7718759   0.53369565
   0.76666667  0.76964286  0.71712084  0.75314574]
 [ 0.39166667  0.69583333  0.69642857  0.67297287  0.68545094  0.43833333
   0.7225      0.73392857  0.65828036  0.7010409 ]
 [ 0.56980392  0.79666667  0.83214286  0.74241112  0.7806993   0.48636364
   0.74166667  0.74642857  0.71742654  0.7334127 ]
 [ 0.15763736  0.575       0.61785714  0.5136007   0.5702381   0.48453458
   0.74166667  0.775       0.66674724  0.72699856]
 [ 0.0967033   0.54333333  0.61964286  0.27038939  0.48237762  0.15915584
   0.57833333  0.59285714  0.49687518  0.56062771]
 [ 0.79123188  0.90666667  0.89821429  0.90189293  0.89428571  0.29753932
   0.655       0.68214286  0.56838219  0.63405483]]
KNN mean:
[0.33574255 0.66768651 0.7097619  0.57857084 0.65074156 0.33889098
 0.66941667 0.70077381 0.59434648 0.65396373]
---------------------------
---------------------------
DT performance:
[[0.47679089 0.78833333 0.79464286 0.69823132 0.77999278 0.14123249
  0.55119048 0.69464286 0.41435985 0.53894106]
 [0.2014429  0.61416667 0.63214286 0.52633873 0.59911255 0.34251432
  0.65166667 0.65714286 0.632708   0.6340404 ]
 [0.38998535 0.6975     0.72321429 0.70487736 0.69886003 0.36333333
  0.60666667 0.60714286 0.56938329 0.58679654]
 [0.40521569 0.69583333 0.69464286 0.62947783 0.67937951 0.36551821
  0.66666667 0.70714286 0.54537389 0.63324453]
 [0.55534044 0.79       0.8125     0.74333884 0.78354257 0.59166667
  0.80833333 0.80892857 0.76675554 0.80321429]
 [0.29994118 0.6925     0.68571429 0.6913244  0.67330447 0.39166667
  0.68       0.68392857 0.65390521 0.67050866]
 [0.23811355 0.63166667 0.66071429 0.52687788 0.61388167 0.241
  0.56666667 0.56785714 0.57272368 0.55761905]
 [0.71501401 0.905      0.925      0.88485652 0.90506383 0.48052252
  0.735      0.76071429 0.63057482 0.73227273]
 [0.31395604 0.66833333 0.69642857 0.53742892 0.622114   0.27194139
  0.615      0.66071429 0.38265001 0.58329559]
 [0.25       0.6        0.6        0.53926114 0.58795094 0.31754902
  0.6875     0.6875     0.64742738 0.67611111]
 [0.1607272  0.56416667 0.56785714 0.49913773 0.53165224 0.40125536
  0.70666667 0.72321429 0.673327   0.69070707]
 [0.66408903 0.85083333 0.84642857 0.8224941  0.83888528 0.48940994
  0.74333333 0.75892857 0.72809641 0.72632756]
 [0.31742857 0.63       0.64464286 0.57310981 0.62974026 0.2052122
  0.55583333 0.56607143 0.53706389 0.52791486]
 [0.0565528  0.585      0.64642857 0.44546016 0.56784299 0.30412999
  0.64916667 0.66785714 0.56908678 0.6334127 ]
 [0.31131868 0.64333333 0.72142857 0.56378613 0.62234932 0.24289855
  0.68       0.73571429 0.54542839 0.67100011]
 [0.06018315 0.56666667 0.59464286 0.55782073 0.55318903 0.33194617
  0.645      0.67321429 0.47984778 0.60371406]
 [0.59199634 0.83666667 0.84642857 0.81049463 0.83152597 0.59841737
  0.74833333 0.75892857 0.78060002 0.74531385]
 [0.04936436 0.55833333 0.78214286 0.25713173 0.55130869 0.27719937
  0.64166667 0.64285714 0.6324766  0.61928571]
 [0.13882353 0.64333333 0.675      0.5536045  0.60846154 0.45692308
  0.75       0.75714286 0.65548356 0.73047619]
 [0.28890433 0.65666667 0.67142857 0.55315767 0.62093074 0.0272994
  0.52       0.57142857 0.52480324 0.51484127]
 [0.0707563  0.50952381 0.64464286 0.33715151 0.48075924 0.16043956
  0.58166667 0.58392857 0.54660164 0.57227633]
 [0.525      0.7375     0.7375     0.67297287 0.72271284 0.47166667
  0.75833333 0.77321429 0.7198457  0.75229437]
 [0.25436508 0.625      0.64464286 0.58070761 0.58774725 0.10718427
  0.62333333 0.68571429 0.41296941 0.59090021]
 [0.19952991 0.6025     0.59464286 0.55763445 0.58119048 0.30155678
  0.73833333 0.74821429 0.67715062 0.73156566]
 [0.41089356 0.6525     0.67321429 0.61808195 0.62744422 0.49456522
  0.74583333 0.74821429 0.68500774 0.74055556]
 [0.42115385 0.7        0.69821429 0.65795311 0.69015873 0.14917184
  0.57666667 0.58214286 0.55435755 0.55487013]
 [0.42647059 0.75       0.79464286 0.68651504 0.74287712 0.425
  0.7        0.70714286 0.58417491 0.66935065]
 [0.24381201 0.575      0.59285714 0.45955851 0.54990454 0.30577756
  0.6625     0.68392857 0.60522531 0.6458658 ]
 [0.05208932 0.57583333 0.58214286 0.51748267 0.56261905 0.02516807
  0.53416667 0.53214286 0.44216356 0.51766955]
 [0.51626775 0.78       0.79642857 0.73085458 0.77602453 0.32374165
  0.68666667 0.73214286 0.65353119 0.67774892]]
DT mean:
[0.32018421 0.67087302 0.69934524 0.59790408 0.65401755 0.32019692
 0.66053968 0.6822619  0.59410343 0.64440448]
---------------------------
---------------------------
RF performance:
[[ 0.53240005  0.745       0.74464286  0.81135744  0.73611111  0.26809524
   0.66666667  0.78571429  0.37825583  0.64035298]
 [ 0.45238095  0.7225      0.72142857  0.67412489  0.71160534  0.64578367
   0.71083333  0.72142857  0.62809082  0.6990368 ]
 [ 0.41846154  0.705       0.725       0.66835845  0.70247475  0.31
   0.75916667  0.76071429  0.59697017  0.75412698]
 [ 0.50770308  0.7425      0.74464286  0.67387658  0.73444444  0.08907994
   0.57166667  0.70535714  0.38534043  0.56260073]
 [ 0.57739927  0.72333333  0.73214286  0.78315118  0.70740981  0.422
   0.75833333  0.75892857  0.69379965  0.74560967]
 [ 0.46960784  0.67416667  0.66964286  0.60457055  0.65698413  0.54932712
   0.805       0.81071429  0.86764492  0.79785714]
 [ 0.22561995  0.61666667  0.64821429  0.53129035  0.59380064  0.347
   0.7         0.69642857  0.64203584  0.6790368 ]
 [ 0.85333333  0.905       0.925       0.8957955   0.90506383  0.55070243
   0.8         0.825       0.70848743  0.79624098]
 [ 0.25698125  0.66666667  0.70535714  0.45564323  0.64151515  0.10585337
   0.54333333  0.61964286  0.48445962  0.50138195]
 [ 0.22115385  0.59583333  0.59464286  0.58412584  0.57293651  0.15285714
   0.65666667  0.66071429  0.6351175   0.65148629]
 [ 0.20438885  0.575       0.59642857  0.60331593  0.57071789  0.38701832
   0.70416667  0.72321429  0.67389647  0.69976912]
 [ 0.62337474  0.81        0.81071429  0.77279604  0.79898268  0.44252471
   0.70333333  0.73214286  0.64972591  0.68227273]
 [ 0.45207437  0.80333333  0.81071429  0.71305363  0.80231602  0.018338
   0.52833333  0.53928571  0.39551339  0.4756746 ]
 [-0.0138711   0.495       0.58035714  0.30855998  0.4913098   0.50909524
   0.73833333  0.75892857  0.70697627  0.73727273]
 [ 0.19394922  0.55166667  0.65892857  0.40727311  0.54644522  0.13718427
   0.51166667  0.61964286  0.35019949  0.49822844]
 [ 0.0512502   0.64        0.67321429  0.57806562  0.61492063  0.30252349
   0.72333333  0.74642857  0.60869376  0.70011544]
 [ 0.63461538  0.84        0.85892857  0.80681946  0.84219697  0.43676503
   0.69833333  0.72142857  0.75071368  0.69707431]
 [ 0.11882353  0.58333333  0.7875      0.17580252  0.55978022  0.38596681
   0.71666667  0.72142857  0.68153648  0.7016811 ]
 [ 0.37465201  0.62166667  0.72142857  0.6127298   0.59610723  0.49287624
   0.73        0.73214286  0.68754966  0.70928571]
 [ 0.44146998  0.69333333  0.71071429  0.63675845  0.64409479  0.08895604
   0.58083333  0.60892857  0.52570375  0.5727381 ]
 [ 0.14636364  0.55952381  0.68392857  0.26166943  0.49604229 -0.18962963
   0.52416667  0.51964286  0.46326042  0.50124098]
 [ 0.4         0.67083333  0.67321429  0.68987563  0.66056999  0.46982008
   0.69333333  0.72321429  0.46838219  0.67038073]
 [ 0.35023379  0.63666667  0.64642857  0.62771876  0.63348846  0.20793651
   0.66666667  0.77321429  0.47209487  0.66826507]
 [ 0.40933333  0.62        0.60714286  0.60756181  0.59634921  0.33095238
   0.73666667  0.73392857  0.74597154  0.71810606]
 [ 0.45261905  0.71666667  0.73392857  0.63313139  0.67748751  0.50833333
   0.75416667  0.75892857  0.80122898  0.74531746]
 [ 0.34230769  0.68333333  0.68392857  0.58505681  0.67742063  0.29214286
   0.6625      0.67142857  0.65224518  0.64776696]
 [ 0.48623249  0.73666667  0.79642857  0.69656594  0.72912587  0.475
   0.77916667  0.78392857  0.65984591  0.76937951]
 [ 0.15891187  0.5075      0.55535714  0.34570004  0.47786991  0.37820802
   0.755       0.775       0.68176985  0.74087857]
 [ 0.13269554  0.54333333  0.59464286  0.50328129  0.49885115  0.13036232
   0.625       0.63392857  0.52478793  0.60792929]
 [ 0.71560163  0.84333333  0.83571429  0.87432375  0.82992063  0.43083851
   0.70333333  0.74464286  0.52932509  0.69838384]]
RF mean:
[0.37300224 0.6742619  0.70767857 0.60407845 0.65687809 0.32253038
 0.68355556 0.71220238 0.6016541  0.66898304]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.54464286  0.          0.35174825  0.
   0.5         0.77321429  0.          0.43567766]
 [ 0.          0.5         0.54464286  0.          0.35174825  0.
   0.5         0.61964286  0.          0.38251748]
 [ 0.          0.5         0.58214286  0.          0.36713287  0.33833333
   0.66916667  0.67142857  0.63818561  0.65584776]
 [ 0.19625375  0.59166667  0.60892857  0.34042098  0.50984848  0.
   0.5         0.72142857  0.          0.41858974]
 [ 0.25113122  0.60833333  0.71071429  0.35938581  0.56596737  0.46666667
   0.73333333  0.73392857  0.69407996  0.71546537]
 [ 0.05469697  0.52583333  0.53392857  0.20245639  0.41833333  0.51457801
   0.7575      0.75714286  0.73215808  0.74493506]
 [ 0.          0.5         0.60714286  0.          0.37738928  0.00869565
   0.50416667  0.50714286  0.1         0.37030303]
 [ 0.31692308  0.63333333  0.77142857  0.39831277  0.61272894  0.
   0.5         0.63392857  0.          0.38782051]
 [ 0.          0.5         0.68392857  0.          0.40540293  0.
   0.5         0.68392857  0.          0.40540293]
 [-0.00277778  0.5         0.49464286  0.05        0.34727273  0.
   0.5         0.56964286  0.          0.36200466]
 [ 0.          0.5         0.58214286  0.          0.36713287  0.
   0.5         0.61964286  0.          0.38251748]
 [ 0.          0.5         0.61964286  0.          0.38251748  0.
   0.5         0.61964286  0.          0.38251748]
 [ 0.          0.5         0.56964286  0.          0.36200466  0.
   0.5         0.59464286  0.          0.37226107]
 [ 0.          0.5         0.68392857  0.          0.40540293  0.
   0.5         0.55714286  0.          0.35687646]
 [ 0.          0.5         0.70892857  0.          0.41419414  0.
   0.5         0.67142857  0.          0.40100733]
 [ 0.          0.5         0.59464286  0.          0.37226107  0.
   0.5         0.65892857  0.          0.39661172]
 [ 0.          0.5         0.58214286  0.          0.36713287  0.
   0.5         0.59464286  0.          0.37226107]
 [ 0.          0.5         0.86071429  0.          0.46234432  0.
   0.5         0.59464286  0.          0.37226107]
 [ 0.          0.5         0.69642857  0.          0.40979853  0.
   0.5         0.55714286  0.          0.35687646]
 [ 0.          0.5         0.65892857  0.          0.39661172  0.
   0.5         0.61964286  0.          0.38251748]
 [ 0.          0.5         0.77321429  0.          0.43567766  0.
   0.5         0.51964286  0.          0.34149184]
 [ 0.06956522  0.53333333  0.53571429  0.08164966  0.38444444  0.
   0.5         0.63392857  0.          0.38782051]
 [ 0.          0.5         0.58214286  0.          0.36713287  0.
   0.5         0.74642857  0.          0.42738095]
 [ 0.          0.5         0.53214286  0.          0.34662005  0.22922078
   0.60833333  0.63392857  0.32103434  0.52043124]
 [ 0.          0.5         0.59464286  0.          0.37226107  0.216
   0.60833333  0.60714286  0.49674698  0.56123377]
 [ 0.00869565  0.50416667  0.50714286  0.05        0.35333333  0.
   0.5         0.55714286  0.          0.35687646]
 [ 0.          0.5         0.70892857  0.          0.41419414  0.53636364
   0.76666667  0.77142857  0.74860867  0.76174603]
 [ 0.          0.5         0.61964286  0.          0.38251748  0.
   0.5         0.60714286  0.          0.37738928]
 [ 0.          0.5         0.60714286  0.          0.37738928  0.
   0.5         0.54464286  0.          0.35174825]
 [ 0.          0.5         0.61964286  0.          0.38251748  0.
   0.5         0.64642857  0.          0.39221612]]
SVM mean:
[0.02981627 0.51322222 0.6239881  0.04940752 0.40210203 0.07699527
 0.53825    0.63422619 0.12436045 0.43775354]
---------------------------
---------------------------
GBM performance:
[[ 0.57679089  0.77583333  0.78214286  0.75356438  0.76729437  0.14
   0.55        0.77321429  0.21213203  0.52395604]
 [ 0.12996262  0.59333333  0.60714286  0.55419975  0.5727417   0.52509204
   0.75        0.78571429  0.7165943   0.7470202 ]
 [ 0.38045788  0.68416667  0.72142857  0.59515844  0.66635115  0.26666667
   0.63416667  0.63392857  0.57509729  0.61194444]
 [ 0.35833333  0.69583333  0.69285714  0.69281432  0.68634921  0.05714286
   0.52166667  0.70892857  0.14142136  0.4762654 ]
 [ 0.61895604  0.79666667  0.8375      0.75923655  0.80002886  0.622
   0.8         0.79821429  0.77656316  0.79555556]
 [ 0.42494118  0.7175      0.71071429  0.69838843  0.70373016  0.43846154
   0.71583333  0.72142857  0.69113215  0.70283189]
 [ 0.30936375  0.615       0.67321429  0.42291943  0.59577756  0.372
   0.65        0.64464286  0.62333817  0.62350649]
 [ 0.82989011  0.90833333  0.9375      0.89044348  0.91109557  0.35831394
   0.67166667  0.72321429  0.55497224  0.65333888]
 [ 0.26989011  0.625       0.73392857  0.37464509  0.58865967  0.0814652
   0.54666667  0.67142857  0.23782051  0.48923909]
 [ 0.19615385  0.5875      0.58214286  0.53673563  0.55055556  0.15231602
   0.59083333  0.60892857  0.55734889  0.58017316]
 [ 0.0671361   0.51916667  0.54642857  0.45029643  0.49614719  0.3512502
   0.67666667  0.71071429  0.53373862  0.65756688]
 [ 0.60112199  0.78666667  0.82321429  0.77230243  0.7953824   0.44638195
   0.72333333  0.75892857  0.6209461   0.70288434]
 [ 0.32750916  0.65833333  0.68571429  0.6377804   0.65713564 -0.11689739
   0.45083333  0.49107143  0.20020847  0.40199856]
 [ 0.11877828  0.58833333  0.69464286  0.31940446  0.55250749  0.25047453
   0.63833333  0.66785714  0.46794419  0.61490454]
 [-0.00506787  0.49833333  0.69464286  0.05773503  0.43270063 -0.02433527
   0.485       0.61964286  0.12844571  0.43469863]
 [-0.07554945  0.48166667  0.54642857  0.14598058  0.42276723  0.30716117
   0.65333333  0.725       0.48731725  0.62208014]
 [ 0.60469963  0.80916667  0.83214286  0.77838182  0.80929293  0.26438408
   0.63833333  0.65714286  0.52896546  0.60360584]
 [-0.01666667  0.49166667  0.84642857  0.          0.45785714  0.26271973
   0.65583333  0.66964286  0.53539544  0.63104618]
 [-0.07772894  0.48833333  0.62321429  0.17844571  0.44943057  0.39858974
   0.69083333  0.70714286  0.5679057   0.65336219]
 [ 0.28327408  0.625       0.69821429  0.44889533  0.58928405  0.11658342
   0.55        0.63392857  0.34641016  0.52560606]
 [ 0.          0.5         0.77321429  0.          0.43567766  0.08611111
   0.54333333  0.53214286  0.49623746  0.52309524]
 [ 0.475       0.7625      0.7625      0.71386473  0.75231602  0.38873195
   0.65333333  0.73392857  0.47277962  0.62473693]
 [ 0.24639194  0.63666667  0.67142857  0.47089875  0.59338412  0.07428571
   0.53333333  0.72142857  0.19915638  0.50096071]
 [ 0.10166667  0.56833333  0.56785714  0.50884261  0.54097403  0.49036354
   0.72583333  0.73571429  0.70087231  0.71934343]
 [ 0.35049174  0.67        0.71071429  0.49647461  0.62727661  0.51956522
   0.77083333  0.77321429  0.70964244  0.76388889]
 [ 0.447       0.725       0.72321429  0.69870346  0.70709235  0.17417184
   0.60083333  0.61964286  0.51978561  0.57699856]
 [ 0.35408964  0.68666667  0.78214286  0.50527885  0.66953713  0.375
   0.7         0.70714286  0.59087364  0.67308081]
 [-0.04331668  0.4775      0.56785714  0.19752942  0.41657343  0.31813187
   0.65916667  0.69464286  0.5442809   0.63240981]
 [ 0.21081419  0.6         0.65892857  0.3936547   0.55930569  0.27211424
   0.6         0.61071429  0.55514191  0.57992063]
 [ 0.64374459  0.82333333  0.83392857  0.80618513  0.81665945  0.42776557
   0.70333333  0.77142857  0.57636477  0.68740093]]
GBM mean:
[0.29027094 0.64652778 0.71071429 0.495292   0.62079619 0.27986705
 0.63611111 0.68702381 0.49562774 0.61111402]
---------------------------
---------------------------
BDDAE performance:
[[ 0.03445938  0.51984127  0.51875     0.47779095  0.49713719  0.05030303
   0.525       0.7         0.25986913  0.49792963]
 [ 0.24934738  0.62539683  0.625       0.61348736  0.61805781  0.4550449
   0.73        0.7375      0.71610886  0.72086996]
 [ 0.08181863  0.54047619  0.55625     0.51618465  0.53505255  0.05
   0.525       0.525       0.50222449  0.51423478]
 [-0.0333194   0.48333333  0.49375     0.44458091  0.46841875  0.25724942
   0.61666667  0.7375      0.53283666  0.6213414 ]
 [ 0.2837346   0.64666667  0.6625      0.61652337  0.63188816  0.375
   0.6875      0.6875      0.66447565  0.67694345]
 [ 0.3875      0.69375     0.69375     0.65794588  0.67701013  0.01678318
   0.51190476  0.50625     0.42719057  0.47407397]
 [-0.10303903  0.44666667  0.5         0.36454016  0.4353094   0.0375
   0.51875     0.51875     0.45878226  0.49012393]
 [ 0.37447537  0.67727273  0.74375     0.63787565  0.6787382   0.16980435
   0.57833333  0.64375     0.47418716  0.56245837]
 [ 0.2664928   0.61818182  0.7375      0.45730076  0.60364414  0.2733997
   0.61181818  0.74375     0.46792494  0.60812897]
 [ 0.0125      0.50625     0.50625     0.48252324  0.49565472  0.10545063
   0.55238095  0.5625      0.53714412  0.54926527]
 [-0.05225639  0.47460317  0.5         0.39013357  0.45437946  0.15161951
   0.57166667  0.63125     0.49608534  0.55756635]
 [ 0.21118513  0.60833333  0.63125     0.56851399  0.59351015  0.55620406
   0.78333333  0.7875      0.77791618  0.77608663]
 [ 0.17600205  0.59047619  0.5875      0.5864394   0.58477824  0.2396987
   0.62833333  0.61875     0.59018817  0.59722201]
 [ 0.19106114  0.59818182  0.65        0.50173446  0.58015699  0.27341651
   0.63968254  0.6375      0.63126976  0.63233695]
 [-0.05278871  0.47454545  0.5625      0.36660004  0.46994291  0.28107768
   0.65090909  0.6625      0.63786357  0.63073161]
 [ 0.0341683   0.51666667  0.575       0.40463884  0.49277959  0.09821978
   0.53727273  0.65625     0.39247323  0.52040441]
 [ 0.49154526  0.74920635  0.75        0.72800161  0.73827795  0.35023428
   0.67333333  0.7         0.63766604  0.6636527 ]
 [ 0.09322325  0.56071429  0.79375     0.28288056  0.52939006  0.07649425
   0.53833333  0.56875     0.51515772  0.53587302]
 [ 0.08587001  0.54090909  0.59375     0.51471485  0.53863748  0.01461856
   0.50793651  0.5125      0.48902786  0.49963819]
 [ 0.02819741  0.51181818  0.60625     0.38369537  0.49792024  0.0241806
   0.51333333  0.575       0.3605      0.48934795]
 [-0.08363636  0.4625      0.64375     0.15583682  0.44074725  0.1625
   0.58125     0.58125     0.56537357  0.57378525]
 [ 0.0875      0.54375     0.54375     0.49668096  0.5204553  -0.08925887
   0.45666667  0.525       0.31052864  0.43027174]
 [ 0.13398197  0.56746032  0.56875     0.55442051  0.55977421  0.27878788
   0.61666667  0.7875      0.4506071   0.61760195]
 [ 0.24316911  0.62380952  0.625       0.59897485  0.61040474  0.38935023
   0.69365079  0.7         0.68619398  0.69203888]
 [ 0.09555314  0.55166667  0.56875     0.5042195   0.53776438  0.1375
   0.56875     0.56875     0.54988657  0.5596495 ]
 [ 0.1875      0.59375     0.59375     0.56903252  0.58236319  0.20418909
   0.60079365  0.61875     0.5470887   0.59065095]
 [ 0.45087607  0.71727273  0.76875     0.68764278  0.71801947  0.3875
   0.69375     0.69375     0.66602249  0.68158581]
 [-0.07081196  0.46666667  0.5         0.40604087  0.45621822  0.56398467
   0.76833333  0.80625     0.74739147  0.77821193]
 [ 0.37837261  0.69        0.7125      0.67092234  0.68511387  0.21893276
   0.60793651  0.625       0.57782436  0.60072588]
 [-0.06803867  0.465       0.49375     0.42444978  0.45463642 -0.04614917
   0.47666667  0.5375      0.34447777  0.45201272]]
BDDAE mean:
[0.13715477 0.56883886 0.61020833 0.50214422 0.55620604 0.20212119
 0.59886508 0.63854167 0.53380955 0.58649214]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.54464286 0.         0.35174825 0.
  0.5        0.77321429 0.         0.43567766]
 [0.         0.5        0.54464286 0.         0.35174825 0.
  0.5        0.61964286 0.         0.38251748]
 [0.         0.5        0.58214286 0.         0.36713287 0.
  0.5        0.51964286 0.         0.34149184]
 [0.         0.5        0.53214286 0.         0.34662005 0.
  0.5        0.72142857 0.         0.41858974]
 [0.         0.5        0.63392857 0.         0.38782051 0.
  0.5        0.49285714 0.         0.33      ]
 [0.         0.5        0.51964286 0.         0.34149184 0.
  0.5        0.53214286 0.         0.34662005]
 [0.         0.5        0.60714286 0.         0.37738928 0.
  0.5        0.49285714 0.         0.33      ]
 [0.         0.5        0.69642857 0.         0.40979853 0.
  0.5        0.63392857 0.         0.38782051]
 [0.         0.5        0.68392857 0.         0.40540293 0.
  0.5        0.68392857 0.         0.40540293]
 [0.         0.5        0.49285714 0.         0.33       0.
  0.5        0.56964286 0.         0.36200466]
 [0.         0.5        0.58214286 0.         0.36713287 0.
  0.5        0.61964286 0.         0.38251748]
 [0.         0.5        0.61964286 0.         0.38251748 0.
  0.5        0.61964286 0.         0.38251748]
 [0.         0.5        0.56964286 0.         0.36200466 0.
  0.5        0.59464286 0.         0.37226107]
 [0.         0.5        0.68392857 0.         0.40540293 0.
  0.5        0.55714286 0.         0.35687646]
 [0.         0.5        0.70892857 0.         0.41419414 0.
  0.5        0.67142857 0.         0.40100733]
 [0.         0.5        0.59464286 0.         0.37226107 0.
  0.5        0.65892857 0.         0.39661172]
 [0.         0.5        0.58214286 0.         0.36713287 0.
  0.5        0.59464286 0.         0.37226107]
 [0.         0.5        0.86071429 0.         0.46234432 0.
  0.5        0.59464286 0.         0.37226107]
 [0.         0.5        0.69642857 0.         0.40979853 0.
  0.5        0.55714286 0.         0.35687646]
 [0.         0.5        0.65892857 0.         0.39661172 0.
  0.5        0.61964286 0.         0.38251748]
 [0.         0.5        0.77321429 0.         0.43567766 0.
  0.5        0.51964286 0.         0.34149184]
 [0.         0.5        0.50714286 0.         0.33636364 0.
  0.5        0.63392857 0.         0.38782051]
 [0.         0.5        0.58214286 0.         0.36713287 0.
  0.5        0.74642857 0.         0.42738095]
 [0.         0.5        0.53214286 0.         0.34662005 0.
  0.5        0.54464286 0.         0.35174825]
 [0.         0.5        0.59464286 0.         0.37226107 0.
  0.5        0.50714286 0.         0.33636364]
 [0.         0.5        0.50714286 0.         0.33636364 0.
  0.5        0.55714286 0.         0.35687646]
 [0.         0.5        0.70892857 0.         0.41419414 0.
  0.5        0.49285714 0.         0.33      ]
 [0.         0.5        0.61964286 0.         0.38251748 0.
  0.5        0.60714286 0.         0.37738928]
 [0.         0.5        0.60714286 0.         0.37738928 0.
  0.5        0.54464286 0.         0.35174825]
 [0.         0.5        0.61964286 0.         0.38251748 0.
  0.5        0.64642857 0.         0.39221612]]
DUMMY mean:
[0.         0.5        0.61488095 0.         0.37865301 0.
 0.5        0.59755952 0.         0.37229559]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_46_2
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.336 0.668 0.71  0.579 0.651 0.339 0.669 0.701 0.594 0.654]
 [0.32  0.671 0.699 0.598 0.654 0.32  0.661 0.682 0.594 0.644]
 [0.373 0.674 0.708 0.604 0.657 0.323 0.684 0.712 0.602 0.669]
 [0.03  0.513 0.624 0.049 0.402 0.077 0.538 0.634 0.124 0.438]
 [0.29  0.647 0.711 0.495 0.621 0.28  0.636 0.687 0.496 0.611]
 [0.137 0.569 0.61  0.502 0.556 0.202 0.599 0.639 0.534 0.586]
 [0.    0.5   0.615 0.    0.379 0.    0.5   0.598 0.    0.372]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.215 0.11  0.092 0.194 0.118 0.155 0.077 0.073 0.125 0.083]
 [0.183 0.096 0.089 0.134 0.102 0.146 0.075 0.072 0.1   0.078]
 [0.201 0.101 0.087 0.171 0.109 0.186 0.081 0.073 0.13  0.088]
 [0.078 0.034 0.085 0.113 0.061 0.164 0.082 0.072 0.25  0.12 ]
 [0.238 0.116 0.097 0.246 0.133 0.174 0.084 0.072 0.177 0.094]
 [0.166 0.082 0.088 0.126 0.085 0.167 0.083 0.087 0.127 0.089]
 [0.    0.    0.083 0.    0.031 0.    0.    0.073 0.    0.028]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 64.  16.  13.  34.  18.  46.  12.  10.  21.  13.]
 [ 57.  14.  13.  22.  16.  46.  11.  11.  17.  12.]
 [ 54.  15.  12.  28.  17.  58.  12.  10.  22.  13.]
 [262.   7.  14. 229.  15. 213.  15.  11. 201.  27.]
 [ 82.  18.  14.  50.  21.  62.  13.  10.  36.  15.]
 [121.  14.  14.  25.  15.  83.  14.  14.  24.  15.]
 [  0.   0.  13.   0.   8.   0.   0.  12.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_46_2
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  20.0
step (sec):  15.008
overlap:  True
perc. of overlap:  24.96
overlap duration (sec):  4.992
Number of windows / instances:  79
Elapsed time: 774.8147577484449 minutes
Elapsed time: 12.913579295807414 hours
