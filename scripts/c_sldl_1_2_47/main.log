2024-05-04 08:05:44.415122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-04 08:05:44.736739: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-04 08:05:44.736976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-04 08:05:44.737350: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-04 08:05:44.738093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-04 08:05:44.738283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-04 08:05:44.738444: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-04 08:05:45.693124: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-04 08:05:45.693339: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-04 08:05:45.693501: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-04 08:05:45.693645: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
Window size (sec):  21.0
step (sec):  21.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
/home/marcos/Dropbox (Maestral)/c_sldl_1_2_47/functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

mean_squared_error: 0.021319/39 [=============>................] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.019521/39 [===============>..............] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.020523/39 [================>.............] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.021925/39 [==================>...........] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.025927/39 [===================>..........] - ETA: 0s - loss: 0.0266 - mean_squared_error: 0.026629/39 [=====================>........] - ETA: 0s - loss: 0.0264 - mean_squared_error: 0.026431/39 [======================>.......] - ETA: 0s - loss: 0.0254 - mean_squared_error: 0.025433/39 [========================>.....] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.024735/39 [=========================>....] - ETA: 0s - loss: 0.0246 - mean_squared_error: 0.024637/39 [===========================>..] - ETA: 0s - loss: 0.0239 - mean_squared_error: 0.023939/39 [==============================] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.022939/39 [==============================] - 1s 33ms/step - loss: 0.0229 - mean_squared_error: 0.0229 - val_loss: 0.0058 - val_mean_squared_error: 0.0058
(20992, 1, 5)
Model: "sequential_1199"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 20992, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 5248, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 5248, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1312, 6)          0         
                                                                 
=================================================================
Total params: 411
Trainable params: 411
Non-trainable params: 0
_________________________________________________________________
Model: "valence_NN"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 inputGSR (InputLayer)          [(None, 20992, 1)]   0           []                               
                                                                                                  
 inputPPG (InputLayer)          [(None, 20992, 1)]   0           []                               
                                                                                                  
 sequential_1198 (Sequential)   (None, 1312, 6)      411         ['inputGSR[0][0]']               
                                                                                                  
 sequential_1199 (Sequential)   (None, 1312, 6)      411         ['inputPPG[0][0]']               
                                                                                                  
 concatenate_599 (Concatenate)  (None, 1312, 12)     0           ['sequential_1198[0][0]',        
                                                                  'sequential_1199[0][0]']        
                                                                                                  
 permute_599 (Permute)          (None, 12, 1312)     0           ['concatenate_599[0][0]']        
                                                                                                  
 flatten_599 (Flatten)          (None, 15744)        0           ['permute_599[0][0]']            
                                                                                                  
 dropout_599 (Dropout)          (None, 15744)        0           ['flatten_599[0][0]']            
                                                                                                  
 dense_599 (Dense)              (None, 1)            15745       ['dropout_599[0][0]']            
                                                                                                  
==================================================================================================
Total params: 16,567
Trainable params: 16,567
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/10
 1/39 [..............................] - ETA: 10s - loss: 0.9040 - binary_accuracy: 0.0000e+0010/39 [======>.......................] - ETA: 0s - loss: 0.7877 - binary_accuracy: 0.5000     19/39 [=============>................] - ETA: 0s - loss: 0.6238 - binary_accuracy: 0.684228/39 [====================>.........] - ETA: 0s - loss: 0.7288 - binary_accuracy: 0.642936/39 [==========================>...] - ETA: 0s - loss: 0.7608 - binary_accuracy: 0.611139/39 [==============================] - 1s 9ms/step - loss: 0.7359 - binary_accuracy: 0.6410 - val_loss: 0.6916 - val_binary_accuracy: 0.6000
Epoch 2/10
 1/39 [..............................] - ETA: 0s - loss: 0.5334 - binary_accuracy: 1.000012/39 [========>.....................] - ETA: 0s - loss: 0.6164 - binary_accuracy: 0.666722/39 [===============>..............] - ETA: 0s - loss: 0.6834 - binary_accuracy: 0.590931/39 [======================>.......] - ETA: 0s - loss: 0.6892 - binary_accuracy: 0.612939/39 [==============================] - 0s 6ms/step - loss: 0.6461 - binary_accuracy: 0.6410 - val_loss: 0.7253 - val_binary_accuracy: 0.6000
Epoch 3/10
 1/39 [..............................] - ETA: 0s - loss: 1.0075 - binary_accuracy: 0.0000e+0016/39 [===========>..................] - ETA: 0s - loss: 0.6576 - binary_accuracy: 0.6250    28/39 [====================>.........] - ETA: 0s - loss: 0.6473 - binary_accuracy: 0.642939/39 [==============================] - 0s 4ms/step - loss: 0.6324 - binary_accuracy: 0.6923 - val_loss: 0.7118 - val_binary_accuracy: 0.6000
Epoch 4/10
 1/39 [..............................] - ETA: 0s - loss: 0.6795 - binary_accuracy: 1.000017/39 [============>.................] - ETA: 0s - loss: 0.7730 - binary_accuracy: 0.588231/39 [======================>.......] - ETA: 0s - loss: 0.7061 - binary_accuracy: 0.645239/39 [==============================] - 0s 4ms/step - loss: 0.6397 - binary_accuracy: 0.6923 - val_loss: 0.7621 - val_binary_accuracy: 0.6000
Epoch 5/10
 1/39 [..............................] - ETA: 0s - loss: 0.5381 - binary_accuracy: 1.000017/39 [============>.................] - ETA: 0s - loss: 0.6423 - binary_accuracy: 0.647133/39 [========================>.....] - ETA: 0s - loss: 0.5972 - binary_accuracy: 0.697039/39 [==============================] - 0s 4ms/step - loss: 0.5467 - binary_accuracy: 0.7436 - val_loss: 0.7209 - val_binary_accuracy: 0.6000
Epoch 6/10
 1/39 [..............................] - ETA: 0s - loss: 0.3353 - binary_accuracy: 1.000017/39 [============>.................] - ETA: 0s - loss: 0.3638 - binary_accuracy: 0.823532/39 [=======================>......] - ETA: 0s - loss: 0.5493 - binary_accuracy: 0.687539/39 [==============================] - 0s 4ms/step - loss: 0.5361 - binary_accuracy: 0.7179 - val_loss: 0.6897 - val_binary_accuracy: 0.6000
Epoch 7/10
 1/39 [..............................] - ETA: 0s - loss: 0.5743 - binary_accuracy: 1.000017/39 [============>.................] - ETA: 0s - loss: 0.4345 - binary_accuracy: 0.882428/39 [====================>.........] - ETA: 0s - loss: 0.4526 - binary_accuracy: 0.821439/39 [==============================] - 0s 4ms/step - loss: 0.4281 - binary_accuracy: 0.8205 - val_loss: 0.7292 - val_binary_accuracy: 0.6000
Epoch 8/10
 1/39 [..............................] - ETA: 0s - loss: 0.2014 - binary_accuracy: 1.000017/39 [============>.................] - ETA: 0s - loss: 0.3510 - binary_accuracy: 0.882431/39 [======================>.......] - ETA: 0s - loss: 0.4186 - binary_accuracy: 0.806539/39 [==============================] - 0s 4ms/step - loss: 0.4116 - binary_accuracy: 0.8462 - val_loss: 0.6662 - val_binary_accuracy: 0.8000
Epoch 9/10
 1/39 [..............................] - ETA: 0s - loss: 0.2883 - binary_accuracy: 1.000018/39 [============>.................] - ETA: 0s - loss: 0.3952 - binary_accuracy: 0.833332/39 [=======================>......] - ETA: 0s - loss: 0.3358 - binary_accuracy: 0.875039/39 [==============================] - 0s 4ms/step - loss: 0.3839 - binary_accuracy: 0.8462 - val_loss: 0.6768 - val_binary_accuracy: 0.6000
Epoch 10/10
 1/39 [..............................] - ETA: 0s - loss: 0.1112 - binary_accuracy: 1.000017/39 [============>.................] - ETA: 0s - loss: 0.3138 - binary_accuracy: 0.941232/39 [=======================>......] - ETA: 0s - loss: 0.3825 - binary_accuracy: 0.875039/39 [==============================] - 0s 4ms/step - loss: 0.3573 - binary_accuracy: 0.8974 - val_loss: 0.6090 - val_binary_accuracy: 0.6000
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 55ms/step
predicted [0.760042   0.8392479  0.921194   0.7495892  0.8917677  0.57554257
 0.6613434  0.2320839  0.40818983 0.945065   0.88210154]
predicted [1 1 1 1 1 1 1 0 0 1 1]
expected [ True  True False  True False  True  True  True False  True False]
accuracy: 0.6363636363636364
confusion matrix: 
[[1 3]
 [1 6]]
              precision    recall  f1-score   support

       False       0.50      0.25      0.33         4
        True       0.67      0.86      0.75         7

    accuracy                           0.64        11
   macro avg       0.58      0.55      0.54        11
weighted avg       0.61      0.64      0.60        11

macro avg f1-score: 0.5416666666666666
macro avg (UAR): 0.5535714285714286
Sensitivity:  0.25
Specificity:  0.8571428571428571
g-mean:  0.4629100498862757
-------- Model Performance ----------: 
accuracy:  [0.54545455 0.63636364 0.63636364 0.45454545 0.54545455 0.54545455
 0.54545455 0.72727273 0.63636364 0.63636364]
gmean:  [0.53452248 0.46291005 0.5976143  0.46291005 0.42257713 0.53452248
 0.42257713 0.73192505 0.         0.46291005]
f1_score:  [0.52991453 0.54166667 0.60714286 0.45       0.47619048 0.52991453
 0.47619048 0.71794872 0.38888889 0.54166667]
UAR:  [0.53571429 0.55357143 0.60714286 0.51785714 0.48214286 0.53571429
 0.48214286 0.73214286 0.5        0.55357143]
Cohen Kappa score:  [ 0.06779661  0.12        0.21428571  0.02941176 -0.03773585  0.06779661
 -0.03773585  0.44067797  0.          0.12      ]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  20.992
step (sec):  20.992
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  55
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.74   0.875  0.867  0.858  0.861  0.451  0.733  0.77   0.627  0.706]
 [ 0.498  0.783  0.793  0.762  0.761  0.324  0.683  0.71   0.59   0.663]
 [ 0.502  0.833  0.843  0.801  0.807  0.296  0.758  0.783  0.628  0.745]
 [ 0.     0.5    0.6    0.     0.374  0.     0.5    0.673  0.     0.401]
 [ 0.486  0.721  0.74   0.676  0.699  0.403  0.708  0.767  0.604  0.689]
 [ 0.098  0.55   0.591  0.463  0.526 -0.248  0.377  0.445  0.201  0.36 ]
 [ 0.     0.5    0.6    0.     0.374  0.     0.5    0.673  0.     0.401]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.53275058  0.76666667  0.78        0.69779637  0.74690476  0.24155844
   0.6         0.77333333  0.34142136  0.59249639]
 [ 0.28513154  0.64166667  0.63333333  0.59207728  0.61904762  0.5262987
   0.7375      0.8         0.69763335  0.75456349]
 [ 0.33463203  0.66666667  0.68666667  0.55791248  0.64059524  0.34978355
   0.675       0.68666667  0.59374021  0.64833333]
 [ 0.44465534  0.725       0.72        0.66767558  0.70238095  0.04285714
   0.525       0.69333333  0.16123724  0.4775    ]
 [ 0.56253746  0.76666667  0.81333333  0.69437822  0.76412698  0.53461538
   0.775       0.76666667  0.7391884   0.75154762]
 [ 0.14652015  0.575       0.57333333  0.52734997  0.56309524  0.59428904
   0.8         0.8         0.77607868  0.78845238]
 [ 0.25754079  0.625       0.63666667  0.55139317  0.59869048  0.33952714
   0.675       0.67666667  0.57221881  0.63892857]
 [ 0.83668831  0.9125      0.93        0.9026586   0.91646825  0.67236097
   0.84583333  0.84333333  0.78896288  0.82095238]
 [ 0.36147186  0.67083333  0.73        0.54057774  0.66210317  0.0017316
   0.50833333  0.65333333  0.14783978  0.45115079]
 [-0.01804862  0.5         0.49666667  0.36935067  0.46071429  0.21132201
   0.60833333  0.61666667  0.53600519  0.59130952]
 [ 0.19584582  0.6         0.62666667  0.47133261  0.57702381  0.1767316
   0.58333333  0.62666667  0.49371481  0.57714286]
 [ 0.3958042   0.7         0.72666667  0.603037    0.67904762  0.29333999
   0.64583333  0.67333333  0.58349004  0.63440476]
 [ 0.26253746  0.625       0.66        0.45867596  0.5890873   0.23311688
   0.61666667  0.60666667  0.52760209  0.58142857]
 [-0.00242258  0.50416667  0.58666667  0.21359758  0.45392857  0.42191142
   0.70833333  0.71333333  0.65159685  0.6922619 ]
 [ 0.10205628  0.55833333  0.67        0.305153    0.51869048  0.27236097
   0.64166667  0.66333333  0.53256985  0.6075    ]
 [ 0.12132035  0.5625      0.61        0.34741798  0.5165873   0.24935065
   0.625       0.69333333  0.42426407  0.58662698]
 [ 0.47820513  0.74583333  0.75333333  0.68640625  0.72392857  0.19184149
   0.6         0.65666667  0.33400999  0.54420635]
 [        nan         nan         nan  0.28660254         nan  0.31705794
   0.65833333  0.69333333  0.54809464  0.63607143]
 [ 0.28632201  0.65833333  0.74333333  0.42556542  0.60960317  0.26538462
   0.63333333  0.62666667  0.56791052  0.60369048]
 [ 0.40911255  0.69583333  0.73333333  0.62377286  0.68337302 -0.05076923
   0.47083333  0.54333333  0.18771568  0.41027778]
 [ 0.02404762  0.535       0.69        0.23761306  0.49277778  0.28275058
   0.64166667  0.63666667  0.55386846  0.61154762]
 [ 0.01774892  0.51666667  0.50666667  0.30199519  0.44630952  0.27844655
   0.62916667  0.66666667  0.58739919  0.63424603]
 [-0.09015152  0.45        0.49333333  0.27015673  0.41761905  0.03454545
   0.5275      0.71333333  0.14783978  0.46952381]
 [-0.16640027  0.4125      0.41666667  0.27449042  0.38083333  0.46742424
   0.72916667  0.75666667  0.66307763  0.71761905]
 [ 0.21222944  0.60833333  0.61        0.52508888  0.57928571  0.41714952
   0.70833333  0.70666667  0.67482267  0.69130952]
 [ 0.25909091  0.625       0.64        0.5999757   0.62035714  0.17948718
   0.59166667  0.6         0.50461121  0.56583333]
 [ 0.40746753  0.70833333  0.75666667  0.61086661  0.68948413  0.54941725
   0.775       0.78        0.74659976  0.76357143]
 [ 0.10692641  0.55        0.57333333  0.48710092  0.54003968  0.23203463
   0.62083333  0.66666667  0.46378638  0.59666667]
 [-0.04935065  0.47916667  0.60333333  0.14142136  0.43396825  0.18537296
   0.5875      0.65333333  0.32513309  0.52607143]
 [ 0.73952714  0.875       0.86666667  0.85841169  0.86119048  0.45147186
   0.73333333  0.77        0.62718028  0.70626984]]
KNN mean:
[0.25702746 0.62965517 0.66436782 0.49432839 0.60300903 0.29875902
 0.64925    0.69188889 0.51665376 0.62238348]
---------------------------
---------------------------
DT performance:
[[ 0.53787879  0.8         0.80666667  0.78602593  0.79833333  0.17935065
   0.6925      0.8         0.47071068  0.66920635]
 [ 0.43476523  0.7         0.69        0.64493682  0.68119048  0.52279387
   0.75416667  0.78        0.74293808  0.75388889]
 [ 0.16774892  0.6         0.59333333  0.52590982  0.56690476  0.17342657
   0.61666667  0.62333333  0.56185917  0.60642857]
 [ 0.4029304   0.69166667  0.67333333  0.69064928  0.66809524  0.04166667
   0.56666667  0.60666667  0.28926114  0.5118254 ]
 [ 0.55868132  0.7875      0.80333333  0.6909014   0.7840873   0.44825175
   0.76666667  0.76666667  0.66388369  0.75892857]
 [ 0.17682318  0.59166667  0.59333333  0.49663265  0.58095238  0.63275058
   0.83333333  0.83666667  0.79242168  0.82345238]
 [ 0.15139194  0.57083333  0.56666667  0.56295743  0.51595238  0.3455711
   0.675       0.67666667  0.61470657  0.65559524]
 [ 0.88534799  0.89583333  0.92666667  0.95731322  0.88507937  0.42127872
   0.62083333  0.64        0.59234155  0.57896825]
 [ 0.47166667  0.72916667  0.74666667  0.75473119  0.70781746  0.17380952
   0.55833333  0.62666667  0.27483482  0.53313492]
 [ 0.15894106  0.6         0.59666667  0.41841833  0.55059524  0.17703963
   0.525       0.55        0.5793659   0.52404762]
 [ 0.32076923  0.56666667  0.59        0.53509358  0.53063492  0.22725608
   0.64166667  0.66666667  0.52548801  0.62190476]
 [ 0.43106061  0.66666667  0.70333333  0.59875998  0.62952381  0.52439893
   0.77916667  0.76        0.72508888  0.7502381 ]
 [ 0.14318182  0.625       0.64666667  0.50117231  0.58380952  0.22517483
   0.61666667  0.61666667  0.46983773  0.58952381]
 [ 0.2521645   0.58333333  0.63666667  0.46462644  0.57865079  0.17004662
   0.56666667  0.56333333  0.52302953  0.52714286]
 [ 0.22917749  0.60833333  0.63666667  0.52675847  0.55992063  0.24513487
   0.5625      0.56333333  0.60007658  0.53130952]
 [ 0.05288878  0.5125      0.53333333  0.54179975  0.49746032  0.24772727
   0.6         0.6         0.44437822  0.54809524]
 [ 0.59562937  0.7875      0.80333333  0.77976503  0.78940476  0.28580586
   0.69583333  0.73333333  0.50618622  0.67071429]
 [        nan         nan         nan  0.54984809         nan  0.20084915
   0.63333333  0.63666667  0.59124581  0.6197619 ]
 [ 0.39465534  0.7         0.75333333  0.58040963  0.67412698  0.37762238
   0.71666667  0.72        0.54996711  0.68202381]
 [ 0.16948052  0.575       0.62333333  0.36986706  0.56563492 -0.01244089
   0.48333333  0.50333333  0.44604723  0.45777778]
 [-0.11136364  0.3975      0.6         0.15067996  0.39468254  0.04139194
   0.54166667  0.54333333  0.39140062  0.50833333]
 [ 0.12472527  0.49166667  0.48333333  0.51649112  0.4677381   0.43946054
   0.725       0.75333333  0.58968295  0.72222222]
 [-0.1033966   0.5         0.52333333  0.28652514  0.4697619   0.17488159
   0.5675      0.65666667  0.3526586   0.49611111]
 [ 0.08513154  0.525       0.51333333  0.36330183  0.49559524  0.46773227
   0.73333333  0.73        0.72492355  0.72119048]
 [ 0.22870463  0.59166667  0.62        0.53065123  0.56190476  0.37820513
   0.73333333  0.74333333  0.54830418  0.72678571]
 [ 0.09871795  0.575       0.56333333  0.40116372  0.5427381   0.66550117
   0.81666667  0.82        0.77002733  0.81166667]
 [ 0.54502165  0.78333333  0.80666667  0.72586368  0.77206349  0.33989344
   0.7         0.70666667  0.64515962  0.68440476]
 [ 0.11955544  0.43333333  0.46666667  0.26547005  0.38690476  0.22188645
   0.58333333  0.60333333  0.39977831  0.54821429]
 [ 0.23294872  0.49583333  0.53666667  0.56215744  0.49309524  0.05554113
   0.54166667  0.55666667  0.50074191  0.49702381]
 [ 0.49844322  0.78333333  0.79333333  0.76176689  0.76051587  0.32441725
   0.68333333  0.71        0.58988307  0.66321429]]
DT mean:
[0.28460936 0.62649425 0.64931034 0.55135492 0.60321292 0.2905475
 0.65102778 0.66977778 0.54920762 0.62643783]
---------------------------
---------------------------
RF performance:
[[ 0.61095571  0.75833333  0.76333333  0.83854107  0.75285714  0.16214286
   0.59        0.78        0.4280239   0.56712121]
 [ 0.56645022  0.7         0.69333333  0.60804654  0.66619048  0.46467699
   0.65833333  0.68666667  0.62399121  0.65996032]
 [ 0.24010989  0.60833333  0.62333333  0.50365813  0.58214286  0.23100233
   0.60833333  0.64333333  0.57365895  0.57047619]
 [ 0.55677656  0.75        0.74333333  0.6896962   0.7347619  -0.05508658
   0.475       0.6         0.26213203  0.47142857]
 [ 0.47322677  0.725       0.76333333  0.68247171  0.69801587  0.52762238
   0.76666667  0.76666667  0.6993045   0.75404762]
 [ 0.25374625  0.69166667  0.69        0.53551812  0.66261905  0.46608392
   0.725       0.73        0.62708569  0.69702381]
 [ 0.31500999  0.5625      0.60333333  0.54895544  0.52797619  0.13663004
   0.7         0.69333333  0.59544463  0.6577381 ]
 [ 0.93333333  0.89583333  0.91        0.9682522   0.87904762  0.65095571
   0.77916667  0.80333333  0.63811926  0.76087302]
 [ 0.36601732  0.75416667  0.77        0.71770685  0.74265873  0.35487013
   0.625       0.7         0.47277275  0.59845238]
 [ 0.2040293   0.59166667  0.58333333  0.34830418  0.56047619  0.1539627
   0.63333333  0.64        0.50924316  0.61392857]
 [ 0.34267399  0.60833333  0.64333333  0.63144871  0.60178571  0.34199134
   0.7375      0.75333333  0.58133317  0.72678571]
 [ 0.60454545  0.80416667  0.81666667  0.66740858  0.79797619  0.3756993
   0.60416667  0.64        0.49014734  0.59289683]
 [ 0.41539128  0.7         0.71333333  0.62418001  0.66107143 -0.06886447
   0.5         0.50333333  0.49998785  0.4952381 ]
 [ 0.32857809  0.60833333  0.65333333  0.45865522  0.58865079  0.18311688
   0.6         0.58666667  0.50555214  0.5647619 ]
 [ 0.22824675  0.6         0.70666667  0.38460652  0.55876984  0.04955877
   0.54166667  0.58        0.26855046  0.51809524]
 [-0.00265568  0.575       0.61        0.32456782  0.54813492  0.26881452
   0.65833333  0.71333333  0.39915638  0.61492063]
 [ 0.77062937  0.8125      0.81333333  0.78610333  0.80214286  0.44428904
   0.70416667  0.73        0.61421951  0.67285714]
 [        nan         nan         nan  0.48944272         nan  0.55277223
   0.78333333  0.82        0.73603452  0.79174603]
 [ 0.30262238  0.675       0.78333333  0.55092019  0.65904762  0.46305361
   0.73333333  0.73666667  0.61042762  0.72166667]
 [ 0.24577922  0.6         0.63666667  0.6319991   0.59888889  0.05320513
   0.5875      0.64        0.29174502  0.56753968]
 [-0.01824675  0.5325      0.75333333  0.18660254  0.50888889 -0.03939394
   0.46666667  0.48        0.48080604  0.44714286]
 [ 0.35619381  0.55        0.54        0.53437544  0.50702381  0.36668831
   0.67083333  0.70666667  0.60437878  0.66428571]
 [ 0.26547619  0.5625      0.58333333  0.57121765  0.55230159  0.08319328
   0.53        0.65666667  0.56246622  0.4584127 ]
 [ 0.19861805  0.5375      0.55333333  0.35437627  0.51321429  0.29504662
   0.63333333  0.65        0.64406115  0.60444444]
 [ 0.33686314  0.6         0.62666667  0.51305223  0.58646825  0.30454545
   0.66666667  0.68        0.58886473  0.66011905]
 [ 0.13531469  0.53333333  0.53        0.58482072  0.49916667  0.27762238
   0.73333333  0.75        0.66933852  0.73035714]
 [ 0.44021812  0.725       0.80333333  0.68946126  0.7247619   0.39465534
   0.69166667  0.69666667  0.66706691  0.66666667]
 [-0.01425408  0.45416667  0.46666667  0.31783039  0.42047619  0.27238428
   0.56666667  0.59333333  0.51877132  0.5347619 ]
 [ 0.00941558  0.51666667  0.57        0.26330984  0.47059524  0.34318182
   0.55        0.59333333  0.44983497  0.5175    ]
 [ 0.50165501  0.83333333  0.84333333  0.80141858  0.8072619   0.29562937
   0.75833333  0.78333333  0.62822402  0.7452381 ]]
RF mean:
[0.34368    0.65054598 0.68241379 0.56023158 0.62804735 0.27833499
 0.64261111 0.67788889 0.54135809 0.62154954]
---------------------------
---------------------------
SVM performance:
[[0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.78333333 0.         0.43858586]
 [0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.55       0.         0.35416667 0.17985348
  0.59166667 0.59666667 0.45390166 0.54      ]
 [0.26190476 0.63333333 0.60666667 0.31547005 0.51130952 0.
  0.5        0.71333333 0.         0.41527778]
 [0.         0.5        0.61666667 0.         0.38083333 0.58461538
  0.8        0.79       0.7708588  0.77928571]
 [0.06190476 0.53333333 0.51       0.15773503 0.39880952 0.34978355
  0.675      0.69333333 0.44391576 0.60119048]
 [0.         0.5        0.61666667 0.         0.38083333 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.69333333 0.         0.40833333 0.
  0.5        0.61666667 0.         0.38083333]
 [0.         0.5        0.65333333 0.         0.39444444 0.
  0.5        0.69333333 0.         0.40833333]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.56666667 0.         0.36083333]
 [0.         0.5        0.61666667 0.         0.38083333 0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.58333333 0.         0.3675     0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.6        0.         0.37416667 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.63333333 0.         0.3875     0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.71333333 0.         0.41527778 0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.61666667 0.         0.38083333 0.
  0.5        0.65333333 0.         0.39444444]
 [0.         0.5        0.56666667 0.         0.36083333 0.
  0.5        0.6        0.         0.37416667]
 [       nan        nan        nan 0.1               nan 0.
  0.5        0.58333333 0.         0.3675    ]
 [0.         0.5        0.69333333 0.         0.40833333 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.63333333 0.         0.3875     0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.78333333 0.         0.43858586 0.
  0.5        0.53       0.         0.3452381 ]
 [0.08571429 0.54166667 0.51       0.15773503 0.39880952 0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.58333333 0.         0.3675     0.
  0.5        0.75       0.         0.42767677]
 [0.         0.5        0.56666667 0.         0.36083333 0.
  0.5        0.56666667 0.         0.36083333]
 [0.         0.5        0.58333333 0.         0.3675     0.17121212
  0.58333333 0.57       0.22844571 0.45833333]
 [0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.69333333 0.         0.40833333 0.16132201
  0.58333333 0.57       0.26783039 0.46809524]
 [0.         0.5        0.61666667 0.         0.38083333 0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.65333333 0.         0.39444444 0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.6        0.         0.37416667 0.
  0.5        0.67333333 0.         0.40138889]]
SVM mean:
[0.01412151 0.50718391 0.60942529 0.02436467 0.38587078 0.04822622
 0.52444444 0.61977778 0.07216508 0.41072198]
---------------------------
---------------------------
GBM performance:
[[ 0.50606061  0.75        0.76666667  0.70017745  0.7447619   0.25714286
   0.6125      0.81666667  0.17071068  0.5795202 ]
 [ 0.2963037   0.675       0.66        0.61235483  0.64416667  0.66186314
   0.82083333  0.85        0.79644029  0.82503968]
 [ 0.14522145  0.55833333  0.57        0.48054177  0.53892857  0.30850816
   0.675       0.68333333  0.602684    0.66547619]
 [ 0.41868132  0.7         0.67333333  0.64089281  0.65666667 -0.11071429
   0.45        0.64333333  0.          0.38833333]
 [ 0.44458874  0.70833333  0.78333333  0.53520305  0.68448413  0.36363636
   0.75833333  0.76333333  0.65031152  0.74369048]
 [ 0.21015651  0.59166667  0.59333333  0.56041903  0.57595238  0.67762238
   0.85833333  0.85666667  0.821711    0.8472619 ]
 [ 0.23484848  0.61666667  0.65666667  0.45604779  0.58547619  0.31223776
   0.65833333  0.65666667  0.58458578  0.63309524]
 [ 0.54718615  0.7875      0.87333333  0.76944525  0.78126984  0.39480519
   0.6125      0.69333333  0.43854107  0.56654762]
 [ 0.37532468  0.68333333  0.76333333  0.44142136  0.64003968  0.1202381
   0.53333333  0.67333333  0.23194792  0.47051587]
 [-0.01611722  0.475       0.46        0.39140062  0.44297619  0.195671
   0.57916667  0.61666667  0.45234819  0.56178571]
 [ 0.11773227  0.55833333  0.61        0.39325042  0.5293254   0.21058941
   0.60833333  0.64        0.43332118  0.57095238]
 [ 0.28939394  0.66666667  0.70333333  0.52864583  0.62952381  0.50784216
   0.74166667  0.75666667  0.67682728  0.74230159]
 [ 0.28894439  0.62083333  0.67333333  0.47735856  0.60123016  0.1980353
   0.58333333  0.59666667  0.5060392   0.5497619 ]
 [ 0.08537296  0.525       0.60333333  0.30121846  0.48904762  0.30677656
   0.64166667  0.64        0.49257649  0.59      ]
 [ 0.20746753  0.63333333  0.72333333  0.38926114  0.5981746   0.08506494
   0.5625      0.64333333  0.33284271  0.52825397]
 [-0.15075758  0.425       0.50666667  0.06123724  0.35083333  0.27554113
   0.64583333  0.71        0.44657436  0.59837302]
 [ 0.6289627   0.8125      0.82333333  0.81427424  0.80916667  0.27405095
   0.65        0.68333333  0.4768969   0.61071429]
 [        nan         nan         nan  0.1                nan  0.40524476
   0.7125      0.73        0.68637033  0.71265873]
 [ 0.13766234  0.575       0.71666667  0.2526586   0.54789683  0.33686314
   0.66666667  0.68        0.56801999  0.62083333]
 [ 0.12077922  0.55833333  0.62666667  0.37593308  0.52980159 -0.02337662
   0.5         0.59        0.2026586   0.45210317]
 [-0.02        0.49        0.76666667  0.          0.43313131  0.1952381
   0.6         0.6         0.55367887  0.58047619]
 [ 0.24688645  0.64166667  0.63666667  0.60184038  0.6247619   0.22012987
   0.59583333  0.67333333  0.42426407  0.58630952]
 [-0.11493506  0.4625      0.50666667  0.16261051  0.39095238  0.12390756
   0.54        0.67333333  0.21123724  0.46507937]
 [-0.11398601  0.45        0.46        0.43401859  0.445       0.45582751
   0.7875      0.78333333  0.72817201  0.77452381]
 [ 0.18106061  0.57083333  0.61666667  0.38479532  0.52988095  0.33275058
   0.66666667  0.67333333  0.63112971  0.65940476]
 [ 0.16433566  0.65        0.65666667  0.48696686  0.62880952  0.56550117
   0.76666667  0.76666667  0.68905433  0.73678571]
 [ 0.54837662  0.7625      0.82333333  0.69407996  0.76345238  0.48381618
   0.725       0.72        0.70040024  0.69809524]
 [-0.00340659  0.42916667  0.50333333  0.12742737  0.3477381   0.28855311
   0.65416667  0.65666667  0.51023023  0.61785714]
 [ 0.11883117  0.525       0.65666667  0.14142136  0.4559127   0.19370629
   0.59583333  0.63333333  0.43286811  0.56130952]
 [ 0.48632201  0.72083333  0.74        0.67620809  0.69920635  0.40313853
   0.70833333  0.76666667  0.60430939  0.68940476]]
GBM mean:
[0.22004473 0.60770115 0.66045977 0.433037   0.57581268 0.30067371
 0.65036111 0.69566667 0.50189172 0.62088215]
---------------------------
---------------------------
BDDAE performance:
[[-0.30775387  0.345       0.34545455  0.27735772  0.32729382  0.22043102
   0.63333333  0.78181818  0.43829152  0.59613421]
 [ 0.01096871  0.505       0.50909091  0.49522341  0.50158453  0.46796572
   0.75        0.73636364  0.72844208  0.7228355 ]
 [-0.1364027   0.43333333  0.45454545  0.28090366  0.39261849  0.07471655
   0.53666667  0.54545455  0.50852666  0.52644383]
 [ 0.04838106  0.52333333  0.53636364  0.4986804   0.51745726 -0.01818309
   0.49791667  0.61818182  0.28409757  0.47592087]
 [-0.02178768  0.49285714  0.46363636  0.42214422  0.44400849  0.20614721
   0.60666667  0.59090909  0.56750856  0.57914863]
 [ 0.3837892   0.69        0.7         0.66604686  0.68322178  0.09453142
   0.54666667  0.54545455  0.51673052  0.53039821]
 [ 0.12764998  0.56428571  0.60909091  0.44387252  0.53339869 -0.22117358
   0.39166667  0.40909091  0.26158873  0.36065962]
 [ 0.26790499  0.63958333  0.71818182  0.53636599  0.62442936  0.29979193
   0.65178571  0.67272727  0.60371213  0.63974467]
 [ 0.27551465  0.62678571  0.68181818  0.54102185  0.61537034  0.33378213
   0.64375     0.8         0.41815406  0.63471448]
 [ 0.31357398  0.66        0.66363636  0.63668138  0.64957792  0.30044648
   0.65        0.65454545  0.63256168  0.64334027]
 [ 0.0209871   0.5125      0.56363636  0.36339152  0.48317954  0.19494603
   0.59285714  0.64545455  0.49662805  0.57254274]
 [ 0.29116448  0.64666667  0.64545455  0.63909505  0.64174825  0.31081387
   0.66607143  0.66363636  0.65813428  0.65039072]
 [-0.16055059  0.42142857  0.42727273  0.35695499  0.40769397  0.44147756
   0.72333333  0.71818182  0.71621988  0.7166589 ]
 [-0.09349473  0.45        0.46363636  0.42369226  0.44284105  0.36662362
   0.685       0.68181818  0.66838851  0.67456821]
 [-0.15611846  0.425       0.52727273  0.22654749  0.41433473  0.29706331
   0.65892857  0.65454545  0.64070933  0.63827867]
 [ 0.13068516  0.56785714  0.6         0.53123807  0.55740676  0.10596626
   0.54642857  0.62727273  0.36764537  0.51759256]
 [ 0.55559166  0.775       0.78181818  0.74519254  0.76558747 -0.01302283
   0.49464286  0.58181818  0.28054968  0.45846648]
 [-0.02041177  0.49722222  0.71818182  0.14524628  0.47080372  0.40218715
   0.70333333  0.7         0.68811315  0.69421107]
 [ 0.12976975  0.56875     0.64545455  0.47956496  0.55902428 -0.05220728
   0.47166667  0.47272727  0.38616084  0.44335467]
 [ 0.10374529  0.55178571  0.6         0.45792399  0.53940117 -0.19192021
   0.40892857  0.47272727  0.19153439  0.37602134]
 [-0.03873612  0.46388889  0.66363636  0.18676265  0.45105951  0.24267329
   0.62166667  0.62727273  0.60067323  0.61376873]
 [ 0.024036    0.51333333  0.50909091  0.43620036  0.48050769 -0.03110188
   0.48392857  0.52727273  0.39048243  0.46964286]
 [ 0.00917074  0.50333333  0.50909091  0.47361332  0.49146825  0.22772848
   0.60208333  0.75454545  0.38150832  0.58737229]
 [ 0.12782175  0.56333333  0.57272727  0.54270946  0.55685315  0.17294743
   0.58666667  0.59090909  0.5664677   0.57799145]
 [-0.00604341  0.49666667  0.50909091  0.44322901  0.48414141  0.05149299
   0.52666667  0.52727273  0.50430158  0.51532884]
 [ 0.30861908  0.65333333  0.66363636  0.5881752   0.63496184  0.18050095
   0.58833333  0.6         0.56016637  0.57958514]
 [ 0.41859751  0.7125      0.76363636  0.68517757  0.70267489  0.32336354
   0.66        0.67272727  0.62495526  0.64902264]
 [ 0.0016954   0.50535714  0.52727273  0.42066318  0.48355142  0.22499237
   0.6         0.68181818  0.46768244  0.5920063 ]
 [-0.07113672  0.46428571  0.50909091  0.39781925  0.45472749  0.2998428
   0.64464286  0.69090909  0.6024419   0.64122517]
 [ 0.0984497   0.55        0.59090909  0.46324687  0.52595238 -0.24759304
   0.37678571  0.44545455  0.20058329  0.36038848]]
BDDAE mean:
[0.087856   0.54408069 0.58242424 0.46015807 0.52789599 0.16884101
 0.58501389 0.6230303  0.49843198 0.56792525]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.78333333 0.         0.43858586]
 [0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.45       0.         0.30952381]
 [0.         0.5        0.45       0.         0.30952381 0.
  0.5        0.71333333 0.         0.41527778]
 [0.         0.5        0.61666667 0.         0.38083333 0.
  0.5        0.51       0.         0.33630952]
 [0.         0.5        0.45       0.         0.30952381 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.61666667 0.         0.38083333 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.69333333 0.         0.40833333 0.
  0.5        0.61666667 0.         0.38083333]
 [0.         0.5        0.65333333 0.         0.39444444 0.
  0.5        0.69333333 0.         0.40833333]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.56666667 0.         0.36083333]
 [0.         0.5        0.61666667 0.         0.38083333 0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.58333333 0.         0.3675     0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.6        0.         0.37416667 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.63333333 0.         0.3875     0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.71333333 0.         0.41527778 0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.61666667 0.         0.38083333 0.
  0.5        0.65333333 0.         0.39444444]
 [0.         0.5        0.56666667 0.         0.36083333 0.
  0.5        0.6        0.         0.37416667]
 [       nan        nan        nan 0.1               nan 0.
  0.5        0.58333333 0.         0.3675    ]
 [0.         0.5        0.69333333 0.         0.40833333 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.63333333 0.         0.3875     0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.78333333 0.         0.43858586 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.51       0.         0.33630952 0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.58333333 0.         0.3675     0.
  0.5        0.75       0.         0.42767677]
 [0.         0.5        0.56666667 0.         0.36083333 0.
  0.5        0.56666667 0.         0.36083333]
 [0.         0.5        0.58333333 0.         0.3675     0.
  0.5        0.45       0.         0.30952381]
 [0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.69333333 0.         0.40833333 0.
  0.5        0.51       0.         0.33630952]
 [0.         0.5        0.61666667 0.         0.38083333 0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.65333333 0.         0.39444444 0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.6        0.         0.37416667 0.
  0.5        0.67333333 0.         0.40138889]]
DUMMY mean:
[0.         0.5        0.60195402 0.00333333 0.37367866 0.
 0.5        0.59411111 0.         0.37038865]
---------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_1_2_47
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.257 0.63  0.664 0.501 0.603 0.298 0.649 0.692 0.516 0.622]
 [0.285 0.626 0.649 0.551 0.603 0.294 0.652 0.671 0.548 0.627]
 [0.344 0.651 0.682 0.563 0.628 0.269 0.638 0.673 0.535 0.616]
 [0.014 0.507 0.609 0.022 0.386 0.05  0.525 0.621 0.075 0.412]
 [0.22  0.608 0.66  0.445 0.576 0.297 0.648 0.694 0.496 0.618]
 [0.088 0.544 0.582 0.46  0.528 0.169 0.585 0.623 0.498 0.568]
 [0.    0.5   0.602 0.    0.374 0.    0.5   0.594 0.    0.37 ]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.238 0.117 0.112 0.186 0.128 0.177 0.088 0.069 0.185 0.101]
 [0.22  0.119 0.113 0.174 0.125 0.171 0.093 0.09  0.133 0.103]
 [0.223 0.108 0.107 0.179 0.114 0.177 0.087 0.079 0.116 0.091]
 [0.051 0.026 0.063 0.068 0.032 0.129 0.065 0.072 0.182 0.09 ]
 [0.208 0.107 0.108 0.207 0.122 0.178 0.094 0.072 0.193 0.108]
 [0.191 0.096 0.104 0.142 0.099 0.188 0.094 0.097 0.152 0.099]
 [0.    0.    0.073 0.    0.028 0.    0.    0.079 0.    0.031]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 93.  19.  17.  37.  21.  59.  14.  10.  36.  16.]
 [ 77.  19.  17.  32.  21.  58.  14.  13.  24.  16.]
 [ 65.  17.  16.  32.  18.  66.  14.  12.  22.  15.]
 [361.   5.  10. 313.   8. 259.  12.  12. 244.  22.]
 [ 95.  18.  16.  47.  21.  60.  15.  10.  39.  17.]
 [217.  18.  18.  31.  19. 111.  16.  16.  30.  17.]
 [  0.   0.  12.   0.   7.   0.   0.  13.   0.   8.]]
-------------------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_1_2_47
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  20.992
step (sec):  20.992
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  55
Elapsed time: 1041.0840042789778 minutes
Elapsed time: 17.351400071316295 hours
