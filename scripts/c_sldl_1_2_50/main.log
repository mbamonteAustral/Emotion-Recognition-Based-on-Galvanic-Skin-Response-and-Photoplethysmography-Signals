2024-05-06 21:44:47.179884: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-06 21:44:47.519467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-06 21:44:47.519693: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-06 21:44:47.520058: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-06 21:44:47.520746: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-06 21:44:47.520925: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-06 21:44:47.521093: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-06 21:44:48.549172: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-06 21:44:48.549412: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-06 21:44:48.549614: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-06 21:44:48.549771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
Window size (sec):  23.0
step (sec):  23.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
/home/marcos/Dropbox (Maestral)/c_sldl_1_2_50/functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

0426 - mean_squared_error: 0.042631/36 [========================>.....] - ETA: 0s - loss: 0.0403 - mean_squared_error: 0.040333/36 [==========================>...] - ETA: 0s - loss: 0.0388 - mean_squared_error: 0.038835/36 [============================>.] - ETA: 0s - loss: 0.0369 - mean_squared_error: 0.036936/36 [==============================] - 34s 73ms/step - loss: 0.0383 - mean_squared_error: 0.0383 - val_loss: 0.0146 - val_mean_squared_error: 0.0146
Epoch 2/5
 1/36 [..............................] - ETA: 1s - loss: 0.0039 - mean_squared_error: 0.0039 3/36 [=>............................] - ETA: 1s - loss: 0.0062 - mean_squared_error: 0.0062 5/36 [===>..........................] - ETA: 1s - loss: 0.0071 - mean_squared_error: 0.0071 7/36 [====>.........................] - ETA: 1s - loss: 0.0075 - mean_squared_error: 0.0075 9/36 [======>.......................] - ETA: 1s - loss: 0.0085 - mean_squared_error: 0.008511/36 [========>.....................] - ETA: 1s - loss: 0.0078 - mean_squared_error: 0.007813/36 [=========>....................] - ETA: 0s - loss: 0.0177 - mean_squared_error: 0.017715/36 [===========>..................] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.016217/36 [=============>................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.015219/36 [==============>...............] - ETA: 0s - loss: 0.0141 - mean_squared_error: 0.014121/36 [================>.............] - ETA: 0s - loss: 0.0134 - mean_squared_error: 0.013423/36 [==================>...........] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.013225/36 [===================>..........] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.015927/36 [=====================>........] - ETA: 0s - loss: 0.0174 - mean_squared_error: 0.017429/36 [=======================>......] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.016731/36 [========================>.....] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.019233/36 [==========================>...] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.019035/36 [============================>.] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.018836/36 [==============================] - 2s 44ms/step - loss: 0.0191 - mean_squared_error: 0.0191 - val_loss: 0.0147 - val_mean_squared_error: 0.0147
Epoch 3/5
 1/36 [..............................] - ETA: 1s - loss: 0.0100 - mean_squared_error: 0.0100 3/36 [=>............................] - ETA: 1s - loss: 0.0086 - mean_squared_error: 0.0086 5/36 [===>..........................] - ETA: 1s - loss: 0.0104 - mean_squared_error: 0.0104 7/36 [====>.........................] - ETA: 1s - loss: 0.0094 - mean_squared_error: 0.0094 9/36 [======>.......................] - ETA: 1s - loss: 0.0132 - mean_squared_error: 0.013211/36 [========>.....................] - ETA: 1s - loss: 0.0184 - mean_squared_error: 0.018413/36 [=========>....................] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.017815/36 [===========>..................] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.016717/36 [=============>................] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.016819/36 [==============>...............] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.019021/36 [================>.............] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.023223/36 [==================>...........] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.021925/36 [===================>..........] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.021127/36 [=====================>........] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.020029/36 [=======================>......] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.020531/36 [========================>.....] - ETA: 0s - loss: 0.0195 - mean_squared_error: 0.019533/36 [==========================>...] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.018935/36 [============================>.] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.018236/36 [==============================] - 1s 42ms/step - loss: 0.0180 - mean_squared_error: 0.0180 - val_loss: 0.0121 - val_mean_squared_error: 0.0121
Epoch 4/5
 1/36 [..............................] - ETA: 0s - loss: 0.0035 - mean_squared_error: 0.0035 3/36 [=>............................] - ETA: 1s - loss: 0.0028 - mean_squared_error: 0.0028 5/36 [===>..........................] - ETA: 1s - loss: 0.0077 - mean_squared_error: 0.0077 7/36 [====>.........................] - ETA: 0s - loss: 0.0068 - mean_squared_error: 0.0068 9/36 [======>.......................] - ETA: 0s - loss: 0.0067 - mean_squared_error: 0.006711/36 [========>.....................] - ETA: 0s - loss: 0.0063 - mean_squared_error: 0.006313/36 [=========>....................] - ETA: 0s - loss: 0.0151 - mean_squared_error: 0.015115/36 [===========>..................] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.013717/36 [=============>................] - ETA: 0s - loss: 0.0126 - mean_squared_error: 0.012619/36 [==============>...............] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.016421/36 [================>.............] - ETA: 0s - loss: 0.0154 - mean_squared_error: 0.015423/36 [==================>...........] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.014425/36 [===================>..........] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.016127/36 [=====================>........] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.017129/36 [=======================>......] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.016731/36 [========================>.....] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.016333/36 [==========================>...] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.015735/36 [============================>.] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.016236/36 [==============================] - 1s 36ms/step - loss: 0.0160 - mean_squared_error: 0.0160 - val_loss: 0.0110 - val_mean_squared_error: 0.0110
Epoch 5/5
 1/36 [..............................] - ETA: 1s - loss: 0.0054 - mean_squared_error: 0.0054 3/36 [=>............................] - ETA: 1s - loss: 0.0256 - mean_squared_error: 0.0256 5/36 [===>..........................] - ETA: 1s - loss: 0.0184 - mean_squared_error: 0.0184 7/36 [====>.........................] - ETA: 1s - loss: 0.0191 - mean_squared_error: 0.0191 9/36 [======>.......................] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.020711/36 [========>.....................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.017913/36 [=========>....................] - ETA: 0s - loss: 0.0157 - mean_squared_error: 0.015715/36 [===========>..................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.014417/36 [=============>................] - ETA: 0s - loss: 0.0132 - mean_squared_error: 0.013219/36 [==============>...............] - ETA: 0s - loss: 0.0161 - mean_squared_error: 0.016121/36 [================>.............] - ETA: 0s - loss: 0.0160 - mean_squared_error: 0.016023/36 [==================>...........] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.019925/36 [===================>..........] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.019427/36 [=====================>........] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.019429/36 [=======================>......] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.018731/36 [========================>.....] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.017833/36 [==========================>...] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.017235/36 [============================>.] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.016636/36 [==============================] - 1s 36ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0141 - val_mean_squared_error: 0.0141
(23008, 1, 5)
Model: "sequential_1195"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 23008, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 5752, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 5752, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1438, 6)          0         
                                                                 
=================================================================
Total params: 411
Trainable params: 411
Non-trainable params: 0
_________________________________________________________________
Model: "valence_NN"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 inputGSR (InputLayer)          [(None, 23008, 1)]   0           []                               
                                                                                                  
 inputPPG (InputLayer)          [(None, 23008, 1)]   0           []                               
                                                                                                  
 sequential_1194 (Sequential)   (None, 1438, 6)      411         ['inputGSR[0][0]']               
                                                                                                  
 sequential_1195 (Sequential)   (None, 1438, 6)      411         ['inputPPG[0][0]']               
                                                                                                  
 concatenate_597 (Concatenate)  (None, 1438, 12)     0           ['sequential_1194[0][0]',        
                                                                  'sequential_1195[0][0]']        
                                                                                                  
 permute_597 (Permute)          (None, 12, 1438)     0           ['concatenate_597[0][0]']        
                                                                                                  
 flatten_597 (Flatten)          (None, 17256)        0           ['permute_597[0][0]']            
                                                                                                  
 dropout_597 (Dropout)          (None, 17256)        0           ['flatten_597[0][0]']            
                                                                                                  
 dense_597 (Dense)              (None, 1)            17257       ['dropout_597[0][0]']            
                                                                                                  
==================================================================================================
Total params: 18,079
Trainable params: 18,079
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/10
 1/36 [..............................] - ETA: 9s - loss: 0.4094 - binary_accuracy: 1.000011/36 [========>.....................] - ETA: 0s - loss: 0.7446 - binary_accuracy: 0.636419/36 [==============>...............] - ETA: 0s - loss: 0.7811 - binary_accuracy: 0.631627/36 [=====================>........] - ETA: 0s - loss: 0.7515 - binary_accuracy: 0.629635/36 [============================>.] - ETA: 0s - loss: 0.7861 - binary_accuracy: 0.571436/36 [==============================] - 1s 9ms/step - loss: 0.7887 - binary_accuracy: 0.5556 - val_loss: 0.5418 - val_binary_accuracy: 0.8000
Epoch 2/10
 1/36 [..............................] - ETA: 0s - loss: 0.3732 - binary_accuracy: 1.000012/36 [=========>....................] - ETA: 0s - loss: 0.5443 - binary_accuracy: 0.833320/36 [===============>..............] - ETA: 0s - loss: 0.6496 - binary_accuracy: 0.750028/36 [======================>.......] - ETA: 0s - loss: 0.6832 - binary_accuracy: 0.678636/36 [==============================] - ETA: 0s - loss: 0.6701 - binary_accuracy: 0.666736/36 [==============================] - 0s 7ms/step - loss: 0.6701 - binary_accuracy: 0.6667 - val_loss: 0.6006 - val_binary_accuracy: 0.8000
Epoch 3/10
 1/36 [..............................] - ETA: 0s - loss: 0.2790 - binary_accuracy: 1.000013/36 [=========>....................] - ETA: 0s - loss: 0.5468 - binary_accuracy: 0.846224/36 [===================>..........] - ETA: 0s - loss: 0.5725 - binary_accuracy: 0.750034/36 [===========================>..] - ETA: 0s - loss: 0.5907 - binary_accuracy: 0.676536/36 [==============================] - 0s 5ms/step - loss: 0.5722 - binary_accuracy: 0.6944 - val_loss: 0.5148 - val_binary_accuracy: 0.8000
Epoch 4/10
 1/36 [..............................] - ETA: 0s - loss: 0.1045 - binary_accuracy: 1.000016/36 [============>.................] - ETA: 0s - loss: 0.6233 - binary_accuracy: 0.625030/36 [========================>.....] - ETA: 0s - loss: 0.5648 - binary_accuracy: 0.733336/36 [==============================] - 0s 4ms/step - loss: 0.5664 - binary_accuracy: 0.7500 - val_loss: 0.5389 - val_binary_accuracy: 0.8000
Epoch 5/10
 1/36 [..............................] - ETA: 0s - loss: 1.4395 - binary_accuracy: 0.0000e+0016/36 [============>.................] - ETA: 0s - loss: 0.6393 - binary_accuracy: 0.5625    31/36 [========================>.....] - ETA: 0s - loss: 0.6553 - binary_accuracy: 0.645236/36 [==============================] - 0s 4ms/step - loss: 0.6471 - binary_accuracy: 0.6667 - val_loss: 0.5667 - val_binary_accuracy: 0.8000
Epoch 6/10
 1/36 [..............................] - ETA: 0s - loss: 0.3730 - binary_accuracy: 1.000014/36 [==========>...................] - ETA: 0s - loss: 0.4191 - binary_accuracy: 0.928628/36 [======================>.......] - ETA: 0s - loss: 0.4921 - binary_accuracy: 0.821436/36 [==============================] - 0s 4ms/step - loss: 0.4646 - binary_accuracy: 0.8333 - val_loss: 0.5859 - val_binary_accuracy: 0.8000
Epoch 7/10
 1/36 [..............................] - ETA: 0s - loss: 0.2427 - binary_accuracy: 1.000016/36 [============>.................] - ETA: 0s - loss: 0.2744 - binary_accuracy: 0.937531/36 [========================>.....] - ETA: 0s - loss: 0.4498 - binary_accuracy: 0.838736/36 [==============================] - 0s 4ms/step - loss: 0.4768 - binary_accuracy: 0.8056 - val_loss: 0.7232 - val_binary_accuracy: 0.4000
Epoch 8/10
 1/36 [..............................] - ETA: 0s - loss: 0.8690 - binary_accuracy: 0.0000e+0015/36 [===========>..................] - ETA: 0s - loss: 0.4138 - binary_accuracy: 0.8667    27/36 [=====================>........] - ETA: 0s - loss: 0.3876 - binary_accuracy: 0.888936/36 [==============================] - 0s 5ms/step - loss: 0.3174 - binary_accuracy: 0.9167 - val_loss: 0.7441 - val_binary_accuracy: 0.8000
Epoch 9/10
 1/36 [..............................] - ETA: 0s - loss: 0.0454 - binary_accuracy: 1.000015/36 [===========>..................] - ETA: 0s - loss: 0.3885 - binary_accuracy: 0.866728/36 [======================>.......] - ETA: 0s - loss: 0.3714 - binary_accuracy: 0.857136/36 [==============================] - 0s 4ms/step - loss: 0.3639 - binary_accuracy: 0.8611 - val_loss: 0.7621 - val_binary_accuracy: 0.6000
Epoch 10/10
 1/36 [..............................] - ETA: 0s - loss: 0.1149 - binary_accuracy: 1.000016/36 [============>.................] - ETA: 0s - loss: 0.3682 - binary_accuracy: 0.875029/36 [=======================>......] - ETA: 0s - loss: 0.3582 - binary_accuracy: 0.896636/36 [==============================] - 0s 4ms/step - loss: 0.3043 - binary_accuracy: 0.9167 - val_loss: 0.8045 - val_binary_accuracy: 0.8000
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 56ms/step
predicted [0.9578514  0.92928225 0.8381475  0.74437475 0.78219926 0.17296302
 0.98036146 0.83056504 0.843042   0.952937   0.9319434 ]
predicted [1 1 1 1 1 0 1 1 1 1 1]
expected [ True  True  True False  True False False  True  True False  True]
accuracy: 0.7272727272727273
confusion matrix: 
[[1 3]
 [0 7]]
              precision    recall  f1-score   support

       False       1.00      0.25      0.40         4
        True       0.70      1.00      0.82         7

    accuracy                           0.73        11
   macro avg       0.85      0.62      0.61        11
weighted avg       0.81      0.73      0.67        11

macro avg f1-score: 0.611764705882353
macro avg (UAR): 0.625
Sensitivity:  0.25
Specificity:  1.0
g-mean:  0.5
-------- Model Performance ----------: 
accuracy:  [0.72727273 0.90909091 0.63636364 0.27272727 0.45454545 0.54545455
 0.45454545 0.72727273 0.         0.        ]
gmean:  [0.73192505 0.9258201  0.46291005 0.26726124 0.37796447 0.53452248
 0.         0.5        0.         0.        ]
f1_score:  [0.71794872 0.90598291 0.54166667 0.26666667 0.41071429 0.52991453
 0.3125     0.61176471 0.         0.        ]
UAR:  [0.73214286 0.92857143 0.55357143 0.26785714 0.41071429 0.53571429
 0.35714286 0.625      0.         0.        ]
Cohen Kappa score:  [ 0.44067797  0.81355932  0.12       -0.41935484 -0.17857143  0.06779661
 -0.32        0.29787234  0.          0.        ]
Split Repetition number:  8
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [32 39  9 20 46 51 21 40  8 36 10 38 13 47 11 44 19 41 37 25 45 16 31  6
  0 49 24 43  2 42  5 28 34 15  4 18 33  3 27 29 12] TEST: [ 1 35 48 26 30 22  7 50 23 14 17]
(DL) TRAIN number of instances:  41
(DL) TEST number of instances:  11
(DL) Total number of instances (TRAIN+TEST):  52
----- train_GSR_AE -------
Model: "sequential_1196"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 23008, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 5752, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 5752, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1438, 6)          0         
                                                                 
 up_sampling1d_2392 (UpSampl  (None, 5752, 6)          0         
 ing1D)                                                          
                                                                 
 conv1d_transpose_2392 (Conv  (None, 5752, 6)          366       
 1DTranspose)                                                    
                                                                 
 up_sampling1d_2393 (UpSampl  (None, 23008, 6)         0         
 ing1D)                                                          
                                                                 
 conv1d_transpose_2393 (Conv  (None, 23008, 1)         121       
 1DTranspose)                                                    
                                                                 
=================================================================
Total params: 898
Trainable params: 898
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
 1/36 [..............................] - ETA: 21:16 - loss: 23.9487 - mean_squared_error: 23.9487 3/36 [=>............................] - ETA: 1s - loss: 11.9124 - mean_squared_error: 11.9124    5/36 [===>..........................] - ETA: 1s - loss: 21.3482 - mean_squared_error: 21.3482 7/36 [====>.........................] - ETA: 1s - loss: 18.7263 - mean_squared_error: 18.7263 9/36 [======>.......................] - ETA: 1s - loss: 18.4550 - mean_squared_error: 18.455011/36 [========>.....................] - ETA: 1s - loss: 16.2620 - mean_squared_error: 16.262013/36 [=========>....................] - ETA: 0s - loss: 17.2449 - mean_squared_error: 17.244915/36 [===========>..................] - ETA: 0s - loss: 15.7778 - mean_squared_error: 15.777817/36 [=============>................] - ETA: 0s - loss: 15.5742 - mean_squared_error: 15.574219/36 [==============>...............] - ETA: 0s - loss: 14.3167 - mean_squared_error: 14.316721/36 [================>.............] - ETA: 0s - loss: 14.2249 - mean_squared_error: 14.224923/36 [==================>...........] - ETA: 0s - loss: 13.7364 - mean_squared_error: 13.736425/36 [===================>..........] - ETA: 0s - loss: 16.0392 - mean_squared_error: 16.039227/36 [=====================>........] - ETA: 0s - loss: 15.2880 - mean_squared_error: 15.288029/36 [=======================>......] - ETA: 0s - loss: 14.7703 - mean_squared_error: 14.770331/36 [========================>.....] - ETA: 0s - loss: 14.8587 - mean_squared_error: 14.858733/36 [==========================>...] - ETA: 0s - loss: 15.0895 - mean_squared_error: 15.089535/36 [============================>.] - ETA: 0s - loss: 14.6480 - mean_squared_error: 14.648036/36 [==============================] - 39s 77ms/step - loss: 14.2828 - mean_squared_error: 14.2828 - val_loss: 4.1808 - val_mean_squared_error: 4.1808
Epoch 2/5
 1/36 [..............................] - ETA: 1s - loss: 0.0379 - mean_squared_error: 0.0379 3/36 [=>............................] - ETA: 1s - loss: 8.0436 - mean_squared_error: 8.0436 5/36 [===>..........................] - ETA: 1s - loss: 8.8828 - mean_squared_error: 8.8828 7/36 [====>.........................] - ETA: 1s - loss: 12.3327 - mean_squared_error: 12.3327 9/36 [======>.......................] - ETA: 1s - loss: 12.1855 - mean_squared_error: 12.185511/36 [========>.....................] - ETA: 1s - loss: 11.5557 - mean_squared_error: 11.555713/36 [=========>....................] - ETA: 0s - loss: 11.2132 - mean_squared_error: 11.213215/36 [===========>..................] - ETA: 0s - loss: 10.3413 - mean_squared_error: 10.341317/36 [=============>................] - ETA: 0s - loss: 10.0994 - mean_squared_error: 10.099419/36 [==============>...............] - ETA: 0s - loss: 9.7087 - mean_squared_error: 9.7087  21/36 [================>.............] - ETA: 0s - loss: 9.6773 - mean_squared_error: 9.677323/36 [==================>...........] - ETA: 0s - loss: 9.0866 - mean_squared_error: 9.086625/36 [===================>..........] - ETA: 0s - loss: 10.6895 - mean_squared_error: 10.689527/36 [=====================>........] - ETA: 0s - loss: 10.2498 - mean_squared_error: 10.249829/36 [=======================>......] - ETA: 0s - loss: 11.2104 - mean_squared_error: 11.210431/36 [========================>.....] - ETA: 0s - loss: 10.9248 - mean_squared_error: 10.924833/36 [==========================>...] - ETA: 0s - loss: 12.7766 - mean_squared_error: 12.776635/36 [============================>.] - ETA: 0s - loss: 12.5771 - mean_squared_error: 12.577136/36 [==============================] - 2s 45ms/step - loss: 12.3565 - mean_squared_error: 12.3565 - val_loss: 4.1485 - val_mean_squared_error: 4.1485
Epoch 3/5
 1/36 [..............................] - ETA: 1s - loss: 8.7032 - mean_squared_error: 8.7032 3/36 [=>............................] - ETA: 1s - loss: 17.9505 - mean_squared_error: 17.9505 5/36 [===>..........................] - ETA: 1s - loss: 15.5780 - mean_squared_error: 15.5780 7/36 [====>.........................] - ETA: 1s - loss: 15.6960 - mean_squared_error: 15.6960 9/36 [======>.......................] - ETA: 1s - loss: 15.0974 - mean_squared_error: 15.097411/36 [========>.....................] - ETA: 1s - loss: 12.4901 - mean_squared_error: 12.490113/36 [=========>....................] - ETA: 0s - loss: 11.7431 - mean_squared_error: 11.743115/36 [===========>..................] - ETA: 0s - loss: 11.1351 - mean_squared_error: 11.135117/36 [=============>................] - ETA: 0s - loss: 10.8249 - mean_squared_error: 10.824919/36 [==============>...............] - ETA: 0s - loss: 11.4367 - mean_squared_error: 11.436721/36 [================>.............] - ETA: 0s - loss: 11.1445 - mean_squared_error: 11.144523/36 [==================>...........] - ETA: 0s - loss: 12.2510 - mean_squared_error: 12.251025/36 [===================>..........] - ETA: 0s - loss: 11.4577 - mean_squared_error: 11.457727/36 [=====================>........] - ETA: 0s - loss: 11.5092 - mean_squared_error: 11.509229/36 [=======================>......] - ETA: 0s - loss: 11.1134 - mean_squared_error: 11.113431/36 [========================>.....] - ETA: 0s - loss: 11.0430 - mean_squared_error: 11.043033/36 [==========================>...] - ETA: 0s - loss: 10.4462 - mean_squared_error: 10.446235/36 [============================>.] - ETA: 0s - loss: 10.7537 - mean_squared_error: 10.753736/36 [==============================] - 2s 42ms/step - loss: 12.3402 - mean_squared_error: 12.3402 - val_loss: 4.1444 - val_mean_squared_error: 4.1444
Epoch 4/5
 1/36 [..............................] - ETA: 1s - loss: 0.3039 - mean_squared_error: 0.3039 3/36 [=>............................] - ETA: 1s - loss: 10.8670 - mean_squared_error: 10.8670 5/36 [===>..........................] - ETA: 1s - loss: 11.5048 - mean_squared_error: 11.5048 7/36 [====>.........................] - ETA: 1s - loss: 15.0362 - mean_squared_error: 15.0362 9/36 [======>.......................] - ETA: 0s - loss: 14.8498 - mean_squared_error: 14.849811/36 [========>.....................] - ETA: 0s - loss: 14.6123 - mean_squared_error: 14.612313/36 [=========>....................] - ETA: 0s - loss: 18.0075 - mean_squared_error: 18.007515/36 [===========>..................] - ETA: 0s - loss: 15.7073 - mean_squared_error: 15.707317/36 [=============>................] - ETA: 0s - loss: 14.1501 - mean_squared_error: 14.150119/36 [==============>...............] - ETA: 0s - loss: 13.7192 - mean_squared_error: 13.719221/36 [================>.............] - ETA: 0s - loss: 13.3246 - mean_squared_error: 13.324623/36 [==================>...........] - ETA: 0s - loss: 12.5975 - mean_squared_error: 12.597525/36 [===================>..........] - ETA: 0s - loss: 12.1350 - mean_squared_error: 12.135027/36 [=====================>........] - ETA: 0s - loss: 12.9927 - mean_squared_error: 12.992729/36 [=======================>......] - ETA: 0s - loss: 12.6560 - mean_squared_error: 12.656031/36 [========================>.....] - ETA: 0s - loss: 11.9352 - mean_squared_error: 11.935233/36 [==========================>...] - ETA: 0s - loss: 11.8507 - mean_squared_error: 11.850735/36 [============================>.] - ETA: 0s - loss: 12.4396 - mean_squared_error: 12.439636/36 [==============================] - 1s 37ms/step - loss: 12.3356 - mean_squared_error: 12.3356 - val_loss: 4.1422 - val_mean_squared_error: 4.1422
Epoch 5/5
 1/36 [..............................] - ETA: 1s - loss: 15.8414 - mean_squared_error: 15.8414 3/36 [=>............................] - ETA: 1s - loss: 24.9249 - mean_squared_error: 24.9249 5/36 [===>..........................] - ETA: 1s - loss: 19.0476 - mean_squared_error: 19.0476 7/36 [====>.........................] - ETA: 1s - loss: 15.7927 - mean_squared_error: 15.7927 9/36 [======>.......................] - ETA: 0s - loss: 14.4178 - mean_squared_error: 14.417811/36 [========>.....................] - ETA: 0s - loss: 12.8345 - mean_squared_error: 12.834513/36 [=========>....................] - ETA: 0s - loss: 11.8686 - mean_squared_error: 11.868615/36 [===========>..................] - ETA: 0s - loss: 11.7021 - mean_squared_error: 11.702117/36 [=============>................] - ETA: 0s - loss: 13.1996 - mean_squared_error: 13.199619/36 [==============>...............] - ETA: 0s - loss: 14.1807 - mean_squared_error: 14.180721/36 [================>.............] - ETA: 0s - loss: 12.9439 - mean_squared_error: 12.943923/36 [==================>...........] - ETA: 0s - loss: 15.4395 - mean_squared_error: 15.439525/36 [===================>..........] - ETA: 0s - loss: 14.3909 - mean_squared_error: 14.390927/36 [=====================>........] - ETA: 0s - loss: 14.0728 - mean_squared_error: 14.072829/36 [=======================>......] - ETA: 0s - loss: 13.6352 - mean_squared_error: 13.635231/36 [========================>.....] - ETA: 0s - loss: 13.1088 - mean_squared_error: 13.108833/36 [==========================>...] - ETA: 0s - loss: 12.8331 - mean_squared_error: 12.833135/36 [============================>.] - ETA: 0s - loss: 12.5710 - mean_squared_error: 12.571036/36 [==============================] - 1s 38ms/step - loss: 12.3328 - mean_squared_error: 12.3328 - val_loss: 4.1407 - val_mean_squared_error: 4.1407
(41, 1438, 6)
Model: "sequential_1196"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 23008, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 5752, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 5752, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1438, 6)          0         
                                                                 
=================================================================
Total params: 411
Trainable params: 411
Non-trainable params: 0
_________________________________________________________________
----- train_PPG_AE -------
Model: "sequential_1197"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 23008, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 5752, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 5752, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1438, 6)          0         
                                                                 
 up_sampling1d_2394 (UpSampl  (None, 5752, 6)          0         
 ing1D)                                                          
                                                                 
 conv1d_transpose_2394 (Conv  (None, 5752, 6)          366       
 1DTranspose)                                                    
                                                                 
 up_sampling1d_2395 (UpSampl  (None, 23008, 6)         0         
 ing1D)                                                          
                                                                 
 conv1d_transpose_2395 (Conv  (None, 23008, 1)         121       
 1DTranspose)                                                    
                                                                 
=================================================================
Total params: 898
Trainable params: 898
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
 1/36 [..............................] - ETA: 18:29 - loss: 0.5730 - mean_squared_error: 0.5730 3/36 [=>............................] - ETA: 1s - loss: 0.6324 - mean_squared_error: 0.6324    5/36 [===>..........................] - ETA: 1s - loss: 0.4985 - mean_squared_error: 0.4985 7/36 [====>.........................] - ETA: 1s - loss: 0.4257 - mean_squared_error: 0.4257 9/36 [======>.......................] - ETA: 1s - loss: 0.3722 - mean_squared_error: 0.372211/36 [========>.....................] - ETA: 1s - loss: 0.3303 - mean_squared_error: 0.330313/36 [=========>....................] - ETA: 0s - loss: 0.2910 - mean_squared_error: 0.291015/36 [===========>..................] - ETA: 0s - loss: 0.2711 - mean_squared_error: 0.271117/36 [=============>................] - ETA: 0s - loss: 0.2531 - mean_squared_error: 0.253119/36 [==============>...............] - ETA: 0s - loss: 0.2287 - mean_squared_error: 0.228721/36 [================>.............] - ETA: 0s - loss: 0.2131 - mean_squared_error: 0.213123/36 [==================>...........] - ETA: 0s - loss: 0.1984 - mean_squared_error: 0.198425/36 [===================>..........] - ETA: 0s - loss: 0.1861 - mean_squared_error: 0.186127/36 [=====================>........] - ETA: 0s - loss: 0.1758 - mean_squared_error: 0.175829/36 [=======================>......] - ETA: 0s - loss: 0.1666 - mean_squared_error: 0.166631/36 [========================>.....] - ETA: 0s - loss: 0.1581 - mean_squared_error: 0.158133/36 [==========================>...] - ETA: 0s - loss: 0.1511 - mean_squared_error: 0.151135/36 [============================>.] - ETA: 0s - loss: 0.1444 - mean_squared_error: 0.144436/36 [==============================] - 34s 74ms/step - loss: 0.1411 - mean_squared_error: 0.1411 - val_loss: 0.0503 - val_mean_squared_error: 0.0503
Epoch 2/5
 1/36 [..............................] - ETA: 0s - loss: 0.0951 - mean_squared_error: 0.0951 3/36 [=>............................] - ETA: 1s - loss: 0.0393 - mean_squared_error: 0.0393 5/36 [===>..........................] - ETA: 1s - loss: 0.0281 - mean_squared_error: 0.0281 7/36 [====>.........................] - ETA: 1s - loss: 0.0380 - mean_squared_error: 0.0380 9/36 [======>.......................] - ETA: 1s - loss: 0.0306 - mean_squared_error: 0.030611/36 [========>.....................] - ETA: 1s - loss: 0.0263 - mean_squared_error: 0.026313/36 [=========>....................] - ETA: 0s - loss: 0.0244 - mean_squared_error: 0.024415/36 [===========>..................] - ETA: 0s - loss: 0.0335 - mean_squared_error: 0.033517/36 [=============>................] - ETA: 0s - loss: 0.0309 - mean_squared_error: 0.030919/36 [==============>...............] - ETA: 0s - loss: 0.0288 - mean_squared_error: 0.028821/36 [================>.............] - ETA: 0s - loss: 0.0276 - mean_squared_error: 0.027623/36 [==================>...........] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.026225/36 [===================>..........] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.025927/36 [=====================>........] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.025829/36 [=======================>......] - ETA: 0s - loss: 0.0271 - mean_squared_error: 0.027131/36 [========================>.....] - ETA: 0s - loss: 0.0262 - mean_squared_error: 0.026233/36 [==========================>...] - ETA: 0s - loss: 0.0258 - mean_squared_error: 0.025835/36 [============================>.] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.025636/36 [==============================] - 2s 44ms/step - loss: 0.0252 - mean_squared_error: 0.0252 - val_loss: 0.0432 - val_mean_squared_error: 0.0432
Epoch 3/5
 1/36 [..............................] - ETA: 1s - loss: 0.0101 - mean_squared_error: 0.0101 3/36 [=>............................] - ETA: 1s - loss: 0.0339 - mean_squared_error: 0.0339 5/36 [===>..........................] - ETA: 1s - loss: 0.0260 - mean_squared_error: 0.0260 7/36 [====>.........................] - ETA: 1s - loss: 0.0209 - mean_squared_error: 0.0209 9/36 [======>.......................] - ETA: 1s - loss: 0.0228 - mean_squared_error: 0.022811/36 [========>.....................] - ETA: 1s - loss: 0.0215 - mean_squared_error: 0.021513/36 [=========>....................] - ETA: 0s - loss: 0.0218 - mean_squared_error: 0.021815/36 [===========>..................] - ETA: 0s - loss: 0.0248 - mean_squared_error: 0.024817/36 [=============>................] - ETA: 0s - loss: 0.0228 - mean_squared_error: 0.022819/36 [==============>...............] - ETA: 0s - loss: 0.0211 - mean_squared_error: 0.021121/36 [================>.............] - ETA: 0s - loss: 0.0241 - mean_squared_error: 0.024123/36 [==================>...........] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.025625/36 [===================>..........] - ETA: 0s - loss: 0.0243 - mean_squared_error: 0.024327/36 [=====================>........] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.023229/36 [=======================>......] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.022131/36 [========================>.....] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.021233/36 [==========================>...] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.020335/36 [============================>.] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.020236/36 [==============================] - 1s 42ms/step - loss: 0.0198 - mean_squared_error: 0.0198 - val_loss: 0.0430 - val_mean_squared_error: 0.0430
Epoch 4/5
 1/36 [..............................] - ETA: 1s - loss: 0.0022 - mean_squared_error: 0.0022 3/36 [=>............................] - ETA: 1s - loss: 0.0114 - mean_squared_error: 0.0114 5/36 [===>..........................] - ETA: 1s - loss: 0.0089 - mean_squared_error: 0.0089 7/36 [====>.........................] - ETA: 1s - loss: 0.0096 - mean_squared_error: 0.0096 9/36 [======>.......................] - ETA: 0s - loss: 0.0108 - mean_squared_error: 0.010811/36 [========>.....................] - ETA: 0s - loss: 0.0165 - mean_squared_error: 0.016513/36 [=========>....................] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.014915/36 [===========>..................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.016317/36 [=============>................] - ETA: 0s - loss: 0.0153 - mean_squared_error: 0.015319/36 [==============>...............] - ETA: 0s - loss: 0.0159 - mean_squared_error: 0.015921/36 [================>.............] - ETA: 0s - loss: 0.0150 - mean_squared_error: 0.015023/36 [==================>...........] - ETA: 0s - loss: 0.0146 - mean_squared_error: 0.014625/36 [===================>..........] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.019127/36 [=====================>........] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.018329/36 [=======================>......] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.017531/36 [========================>.....] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.017333/36 [==========================>...] - ETA: 0s - loss: 0.0167 - mean_squared_error: 0.016735/36 [============================>.] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.017836/36 [==============================] - 1s 36ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0354 - val_mean_squared_error: 0.0354
Epoch 5/5
 1/36 [..............................] - ETA: 1s - loss: 0.0038 - mean_squared_error: 0.0038 3/36 [=>............................] - ETA: 1s - loss: 0.0112 - mean_squared_error: 0.0112 5/36 [===>..........................] - ETA: 1s - loss: 0.0121 - mean_squared_error: 0.0121 7/36 [====>.........................] - ETA: 1s - loss: 0.0197 - mean_squared_error: 0.0197 9/36 [======>.......................] - ETA: 0s - loss: 0.0175 - mean_squared_error: 0.017511/36 [========>.....................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.015813/36 [=========>....................] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.018715/36 [===========>..................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.021017/36 [=============>................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.019419/36 [==============>...............] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.018021/36 [================>.............] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.016623/36 [==================>...........] - ETA: 0s - loss: 0.0187 - mean_squared_error: 0.018725/36 [===================>..........] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.018327/36 [=====================>........] - ETA: 0s - loss: 0.0176 - mean_squared_error: 0.017629/36 [=======================>......] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.017931/36 [========================>.....] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.017933/36 [==========================>...] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.017235/36 [============================>.] - ETA: 0s - loss: 0.0166 - mean_squared_error: 0.016636/36 [==============================] - 1s 37ms/step - loss: 0.0165 - mean_squared_error: 0.0165 - val_loss: 0.0337 - val_mean_squared_error: 0.0337
(23008, 1, 5)
Model: "sequential_1197"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 23008, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 5752, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 5752, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1438, 6)          0         
                                                                 
=================================================================
Total params: 411
Trainable params: 411
Non-trainable params: 0
_________________________________________________________________
Model: "valence_NN"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 inputGSR (InputLayer)          [(None, 23008, 1)]   0           []                               
                                                                                                  
 inputPPG (InputLayer)          [(None, 23008, 1)]   0           []                               
                                                                                                  
 sequential_1196 (Sequential)   (None, 1438, 6)      411         ['inputGSR[0][0]']               
                                                                                                  
 sequential_1197 (Sequential)   (None, 1438, 6)      411         ['inputPPG[0][0]']               
                                                                                                  
 concatenate_598 (Concatenate)  (None, 1438, 12)     0           ['sequential_1196[0][0]',        
                                                                  'sequential_1197[0][0]']        
                                                                                                  
 permute_598 (Permute)          (None, 12, 1438)     0           ['concatenate_598[0][0]']        
                                                                                                  
 flatten_598 (Flatten)          (None, 17256)        0           ['permute_598[0][0]']            
                                                                                                  
 dropout_598 (Dropout)          (None, 17256)        0           ['flatten_598[0][0]']            
                                                                                                  
 dense_598 (Dense)              (None, 1)            17257       ['dropout_598[0][0]']            
                                                                                                  
==================================================================================================
Total params: 18,079
Trainable params: 18,079
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/10
 1/36 [..............................] - ETA: 9s - loss: 0.8148 - binary_accuracy: 0.0000e+0013/36 [=========>....................] - ETA: 0s - loss: 0.8396 - binary_accuracy: 0.4615    23/36 [==================>...........] - ETA: 0s - loss: 0.8532 - binary_accuracy: 0.521732/36 [=========================>....] - ETA: 0s - loss: 0.7613 - binary_accuracy: 0.593836/36 [==============================] - 1s 8ms/step - loss: 0.7489 - binary_accuracy: 0.5833 - val_loss: 0.6372 - val_binary_accuracy: 0.8000
Epoch 2/10
 1/36 [..............................] - ETA: 0s - loss: 0.5236 - binary_accuracy: 1.000012/36 [=========>....................] - ETA: 0s - loss: 0.5411 - binary_accuracy: 0.916721/36 [================>.............] - ETA: 0s - loss: 0.6396 - binary_accuracy: 0.761929/36 [=======================>......] - ETA: 0s - loss: 0.6489 - binary_accuracy: 0.689736/36 [==============================] - 0s 6ms/step - loss: 0.6524 - binary_accuracy: 0.6944 - val_loss: 0.7099 - val_binary_accuracy: 0.4000
Epoch 3/10
 1/36 [..............................] - ETA: 0s - loss: 0.4256 - binary_accuracy: 1.000015/36 [===========>..................] - ETA: 0s - loss: 0.4383 - binary_accuracy: 0.800024/36 [===================>..........] - ETA: 0s - loss: 0.4818 - binary_accuracy: 0.791733/36 [==========================>...] - ETA: 0s - loss: 0.5422 - binary_accuracy: 0.727336/36 [==============================] - 0s 6ms/step - loss: 0.5618 - binary_accuracy: 0.6944 - val_loss: 0.7560 - val_binary_accuracy: 0.4000
Epoch 4/10
 1/36 [..............................] - ETA: 0s - loss: 0.8863 - binary_accuracy: 0.0000e+0016/36 [============>.................] - ETA: 0s - loss: 0.4081 - binary_accuracy: 0.8750    28/36 [======================>.......] - ETA: 0s - loss: 0.5829 - binary_accuracy: 0.750036/36 [==============================] - 0s 5ms/step - loss: 0.5972 - binary_accuracy: 0.6944 - val_loss: 0.7962 - val_binary_accuracy: 0.2000
Epoch 5/10
 1/36 [..............................] - ETA: 0s - loss: 0.3204 - binary_accuracy: 1.000015/36 [===========>..................] - ETA: 0s - loss: 0.4124 - binary_accuracy: 0.866729/36 [=======================>......] - ETA: 0s - loss: 0.4235 - binary_accuracy: 0.827636/36 [==============================] - 0s 4ms/step - loss: 0.4128 - binary_accuracy: 0.8333 - val_loss: 0.5476 - val_binary_accuracy: 0.8000
Epoch 6/10
 1/36 [..............................] - ETA: 0s - loss: 0.0911 - binary_accuracy: 1.000015/36 [===========>..................] - ETA: 0s - loss: 0.3582 - binary_accuracy: 0.800028/36 [======================>.......] - ETA: 0s - loss: 0.4046 - binary_accuracy: 0.821436/36 [==============================] - 0s 4ms/step - loss: 0.4426 - binary_accuracy: 0.8056 - val_loss: 0.6756 - val_binary_accuracy: 0.4000
Epoch 7/10
 1/36 [..............................] - ETA: 0s - loss: 0.4219 - binary_accuracy: 1.000015/36 [===========>..................] - ETA: 0s - loss: 0.3779 - binary_accuracy: 0.800027/36 [=====================>........] - ETA: 0s - loss: 0.3307 - binary_accuracy: 0.851936/36 [==============================] - 0s 5ms/step - loss: 0.3158 - binary_accuracy: 0.8889 - val_loss: 0.5857 - val_binary_accuracy: 0.6000
Epoch 8/10
 1/36 [..............................] - ETA: 0s - loss: 0.0943 - binary_accuracy: 1.000015/36 [===========>..................] - ETA: 0s - loss: 0.2812 - binary_accuracy: 0.933327/36 [=====================>........] - ETA: 0s - loss: 0.2433 - binary_accuracy: 0.963036/36 [==============================] - 0s 5ms/step - loss: 0.3507 - binary_accuracy: 0.8611 - val_loss: 0.5917 - val_binary_accuracy: 0.6000
Epoch 9/10
 1/36 [..............................] - ETA: 0s - loss: 0.1357 - binary_accuracy: 1.000014/36 [==========>...................] - ETA: 0s - loss: 0.2576 - binary_accuracy: 0.857127/36 [=====================>........] - ETA: 0s - loss: 0.3301 - binary_accuracy: 0.888936/36 [==============================] - 0s 4ms/step - loss: 0.3149 - binary_accuracy: 0.8889 - val_loss: 0.5752 - val_binary_accuracy: 0.8000
Epoch 10/10
 1/36 [..............................] - ETA: 0s - loss: 0.3914 - binary_accuracy: 1.000016/36 [============>.................] - ETA: 0s - loss: 0.1854 - binary_accuracy: 1.000029/36 [=======================>......] - ETA: 0s - loss: 0.1623 - binary_accuracy: 1.000036/36 [==============================] - 0s 4ms/step - loss: 0.1845 - binary_accuracy: 0.9722 - val_loss: 0.6074 - val_binary_accuracy: 0.6000
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 57ms/step
predicted [0.9913622  0.9574942  0.18449529 0.7967059  0.8466847  0.8460126
 0.9733124  0.22161412 0.4462349  0.506351   0.8575243 ]
predicted [1 1 0 1 1 1 1 0 0 1 1]
expected [False  True False  True  True  True False False  True  True  True]
accuracy: 0.7272727272727273
confusion matrix: 
[[2 2]
 [1 6]]
              precision    recall  f1-score   support

       False       0.67      0.50      0.57         4
        True       0.75      0.86      0.80         7

    accuracy                           0.73        11
   macro avg       0.71      0.68      0.69        11
weighted avg       0.72      0.73      0.72        11

macro avg f1-score: 0.6857142857142857
macro avg (UAR): 0.6785714285714286
Sensitivity:  0.5
Specificity:  0.8571428571428571
g-mean:  0.6546536707079771
-------- Model Performance ----------: 
accuracy:  [0.72727273 0.90909091 0.63636364 0.27272727 0.45454545 0.54545455
 0.45454545 0.72727273 0.72727273 0.        ]
gmean:  [0.73192505 0.9258201  0.46291005 0.26726124 0.37796447 0.53452248
 0.         0.5        0.65465367 0.        ]
f1_score:  [0.71794872 0.90598291 0.54166667 0.26666667 0.41071429 0.52991453
 0.3125     0.61176471 0.68571429 0.        ]
UAR:  [0.73214286 0.92857143 0.55357143 0.26785714 0.41071429 0.53571429
 0.35714286 0.625      0.67857143 0.        ]
Cohen Kappa score:  [ 0.44067797  0.81355932  0.12       -0.41935484 -0.17857143  0.06779661
 -0.32        0.29787234  0.37735849  0.        ]
Split Repetition number:  9
StratifiedShuffleSplit(n_splits=1, random_state=None, test_size=0.2,
            train_size=None)
TRAIN: [16 42 51 20 41 31 33 29 34  5 48 21 15 32 26 45 46 17 23  1 14 28 30 50
 49 11 10 24  0 27 43  9  7 25  4 44 37 12 40  6 22] TEST: [36  3 38  8 19 47 13  2 35 39 18]
(DL) TRAIN number of instances:  41
(DL) TEST number of instances:  11
(DL) Total number of instances (TRAIN+TEST):  52
----- train_GSR_AE -------
Model: "sequential_1198"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 23008, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 5752, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 5752, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1438, 6)          0         
                                                                 
 up_sampling1d_2396 (UpSampl  (None, 5752, 6)          0         
 ing1D)                                                          
                                                                 
 conv1d_transpose_2396 (Conv  (None, 5752, 6)          366       
 1DTranspose)                                                    
                                                                 
 up_sampling1d_2397 (UpSampl  (None, 23008, 6)         0         
 ing1D)                                                          
                                                                 
 conv1d_transpose_2397 (Conv  (None, 23008, 1)         121       
 1DTranspose)                                                    
                                                                 
=================================================================
Total params: 898
Trainable params: 898
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
 1/36 [..............................] - ETA: 19:45 - loss: 76.3986 - mean_squared_error: 76.3986 3/36 [=>............................] - ETA: 1s - loss: 28.4483 - mean_squared_error: 28.4483    5/36 [===>..........................] - ETA: 1s - loss: 24.1755 - mean_squared_error: 24.1756 7/36 [====>.........................] - ETA: 1s - loss: 19.0587 - mean_squared_error: 19.0587 9/36 [======>.......................] - ETA: 1s - loss: 18.7521 - mean_squared_error: 18.752111/36 [========>.....................] - ETA: 1s - loss: 16.1006 - mean_squared_error: 16.100613/36 [=========>....................] - ETA: 0s - loss: 15.6217 - mean_squared_error: 15.621715/36 [===========>..................] - ETA: 0s - loss: 15.0169 - mean_squared_error: 15.016917/36 [=============>................] - ETA: 0s - loss: 14.4758 - mean_squared_error: 14.475819/36 [==============>...............] - ETA: 0s - loss: 13.6919 - mean_squared_error: 13.691921/36 [================>.............] - ETA: 0s - loss: 14.6089 - mean_squared_error: 14.608923/36 [==================>...........] - ETA: 0s - loss: 13.4417 - mean_squared_error: 13.441725/36 [===================>..........] - ETA: 0s - loss: 13.0670 - mean_squared_error: 13.067027/36 [=====================>........] - ETA: 0s - loss: 12.5699 - mean_squared_error: 12.569929/36 [=======================>......] - ETA: 0s - loss: 12.3985 - mean_squared_error: 12.398531/36 [========================>.....] - ETA: 0s - loss: 11.9522 - mean_squared_error: 11.952233/36 [==========================>...] - ETA: 0s - loss: 12.9585 - mean_squared_error: 12.958535/36 [============================>.] - ETA: 0s - loss: 12.6978 - mean_squared_error: 12.697836/36 [==============================] - 36s 74ms/step - loss: 12.3461 - mean_squared_error: 12.3461 - val_loss: 8.2616 - val_mean_squared_error: 8.2616
Epoch 2/5
 1/36 [..............................] - ETA: 1s - loss: 29.4388 - mean_squared_error: 29.4388 3/36 [=>............................] - ETA: 1s - loss: 11.3070 - mean_squared_error: 11.3070 5/36 [===>..........................] - ETA: 1s - loss: 9.1550 - mean_squared_error: 9.1550   7/36 [====>.........................] - ETA: 1s - loss: 15.0558 - mean_squared_error: 15.0558 9/36 [======>.......................] - ETA: 1s - loss: 12.5482 - mean_squared_error: 12.548211/36 [========>.....................] - ETA: 1s - loss: 10.9198 - mean_squared_error: 10.919913/36 [=========>....................] - ETA: 0s - loss: 9.9738 - mean_squared_error: 9.9738  15/36 [===========>..................] - ETA: 0s - loss: 9.0270 - mean_squared_error: 9.027017/36 [=============>................] - ETA: 0s - loss: 12.4211 - mean_squared_error: 12.421119/36 [==============>...............] - ETA: 0s - loss: 11.5100 - mean_squared_error: 11.510021/36 [================>.............] - ETA: 0s - loss: 10.9224 - mean_squared_error: 10.922423/36 [==================>...........] - ETA: 0s - loss: 11.3841 - mean_squared_error: 11.384125/36 [===================>..........] - ETA: 0s - loss: 10.8452 - mean_squared_error: 10.845227/36 [=====================>........] - ETA: 0s - loss: 11.1200 - mean_squared_error: 11.120029/36 [=======================>......] - ETA: 0s - loss: 10.7061 - mean_squared_error: 10.706131/36 [========================>.....] - ETA: 0s - loss: 11.0579 - mean_squared_error: 11.057933/36 [==========================>...] - ETA: 0s - loss: 10.8969 - mean_squared_error: 10.896935/36 [============================>.] - ETA: 0s - loss: 11.7920 - mean_squared_error: 11.792036/36 [==============================] - 2s 45ms/step - loss: 11.7781 - mean_squared_error: 11.7781 - val_loss: 8.2508 - val_mean_squared_error: 8.2508
Epoch 3/5
 1/36 [..............................] - ETA: 1s - loss: 0.0388 - mean_squared_error: 0.0388 3/36 [=>............................] - ETA: 1s - loss: 4.8044 - mean_squared_error: 4.8044 5/36 [===>..........................] - ETA: 1s - loss: 6.3678 - mean_squared_error: 6.3678 7/36 [====>.........................] - ETA: 1s - loss: 7.6908 - mean_squared_error: 7.6908 9/36 [======>.......................] - ETA: 1s - loss: 7.7758 - mean_squared_error: 7.775811/36 [========>.....................] - ETA: 1s - loss: 6.5533 - mean_squared_error: 6.553313/36 [=========>....................] - ETA: 0s - loss: 6.8601 - mean_squared_error: 6.860115/36 [===========>..................] - ETA: 0s - loss: 10.2825 - mean_squared_error: 10.282517/36 [=============>................] - ETA: 0s - loss: 13.0798 - mean_squared_error: 13.079819/36 [==============>...............] - ETA: 0s - loss: 12.0422 - mean_squared_error: 12.042221/36 [================>.............] - ETA: 0s - loss: 11.4451 - mean_squared_error: 11.445123/36 [==================>...........] - ETA: 0s - loss: 10.7240 - mean_squared_error: 10.724025/36 [===================>..........] - ETA: 0s - loss: 10.5854 - mean_squared_error: 10.585427/36 [=====================>........] - ETA: 0s - loss: 12.4562 - mean_squared_error: 12.456229/36 [=======================>......] - ETA: 0s - loss: 12.9280 - mean_squared_error: 12.928031/36 [========================>.....] - ETA: 0s - loss: 12.3145 - mean_squared_error: 12.314533/36 [==========================>...] - ETA: 0s - loss: 12.0324 - mean_squared_error: 12.032435/36 [============================>.] - ETA: 0s - loss: 11.8700 - mean_squared_error: 11.870036/36 [==============================] - 2s 42ms/step - loss: 11.7691 - mean_squared_error: 11.7691 - val_loss: 8.2466 - val_mean_squared_error: 8.2466
Epoch 4/5
 1/36 [..............................] - ETA: 0s - loss: 42.4123 - mean_squared_error: 42.4123 3/36 [=>............................] - ETA: 1s - loss: 18.5018 - mean_squared_error: 18.5018 5/36 [===>..........................] - ETA: 1s - loss: 17.3060 - mean_squared_error: 17.3060 7/36 [====>.........................] - ETA: 1s - loss: 13.6874 - mean_squared_error: 13.6874 9/36 [======>.......................] - ETA: 0s - loss: 14.0609 - mean_squared_error: 14.060911/36 [========>.....................] - ETA: 0s - loss: 12.7174 - mean_squared_error: 12.717413/36 [=========>....................] - ETA: 0s - loss: 11.8468 - mean_squared_error: 11.846815/36 [===========>..................] - ETA: 0s - loss: 14.2637 - mean_squared_error: 14.263717/36 [=============>................] - ETA: 0s - loss: 14.0548 - mean_squared_error: 14.054819/36 [==============>...............] - ETA: 0s - loss: 13.3264 - mean_squared_error: 13.326421/36 [================>.............] - ETA: 0s - loss: 12.5785 - mean_squared_error: 12.578523/36 [==================>...........] - ETA: 0s - loss: 11.6997 - mean_squared_error: 11.699725/36 [===================>..........] - ETA: 0s - loss: 11.2043 - mean_squared_error: 11.204327/36 [=====================>........] - ETA: 0s - loss: 11.0904 - mean_squared_error: 11.090429/36 [=======================>......] - ETA: 0s - loss: 10.4142 - mean_squared_error: 10.414231/36 [========================>.....] - ETA: 0s - loss: 10.0159 - mean_squared_error: 10.015933/36 [==========================>...] - ETA: 0s - loss: 10.3898 - mean_squared_error: 10.389935/36 [============================>.] - ETA: 0s - loss: 11.7423 - mean_squared_error: 11.742336/36 [==============================] - 1s 37ms/step - loss: 11.7647 - mean_squared_error: 11.7647 - val_loss: 8.2444 - val_mean_squared_error: 8.2444
Epoch 5/5
 1/36 [..............................] - ETA: 1s - loss: 2.0612 - mean_squared_error: 2.0612 3/36 [=>............................] - ETA: 1s - loss: 15.9403 - mean_squared_error: 15.9403 5/36 [===>..........................] - ETA: 1s - loss: 17.4354 - mean_squared_error: 17.4354 7/36 [====>.........................] - ETA: 1s - loss: 13.3272 - mean_squared_error: 13.3272 9/36 [======>.......................] - ETA: 0s - loss: 11.3412 - mean_squared_error: 11.341211/36 [========>.....................] - ETA: 0s - loss: 9.5841 - mean_squared_error: 9.5841  13/36 [=========>....................] - ETA: 0s - loss: 12.5900 - mean_squared_error: 12.590015/36 [===========>..................] - ETA: 0s - loss: 11.6409 - mean_squared_error: 11.640917/36 [=============>................] - ETA: 0s - loss: 10.7579 - mean_squared_error: 10.757919/36 [==============>...............] - ETA: 0s - loss: 9.7050 - mean_squared_error: 9.7050  21/36 [================>.............] - ETA: 0s - loss: 9.4228 - mean_squared_error: 9.422923/36 [==================>...........] - ETA: 0s - loss: 9.4600 - mean_squared_error: 9.460025/36 [===================>..........] - ETA: 0s - loss: 8.8814 - mean_squared_error: 8.881427/36 [=====================>........] - ETA: 0s - loss: 10.8653 - mean_squared_error: 10.865329/36 [=======================>......] - ETA: 0s - loss: 10.6358 - mean_squared_error: 10.635831/36 [========================>.....] - ETA: 0s - loss: 10.4627 - mean_squared_error: 10.462733/36 [==========================>...] - ETA: 0s - loss: 12.1631 - mean_squared_error: 12.163135/36 [============================>.] - ETA: 0s - loss: 11.7394 - mean_squared_error: 11.739436/36 [==============================] - 1s 37ms/step - loss: 11.7618 - mean_squared_error: 11.7618 - val_loss: 8.2413 - val_mean_squared_error: 8.2413
(41, 1438, 6)
Model: "sequential_1198"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 23008, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 5752, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 5752, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1438, 6)          0         
                                                                 
=================================================================
Total params: 411
Trainable params: 411
Non-trainable params: 0
_________________________________________________________________
----- train_PPG_AE -------
Model: "sequential_1199"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 23008, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 5752, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 5752, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1438, 6)          0         
                                                                 
 up_sampling1d_2398 (UpSampl  (None, 5752, 6)          0         
 ing1D)                                                          
                                                                 
 conv1d_transpose_2398 (Conv  (None, 5752, 6)          366       
 1DTranspose)                                                    
                                                                 
 up_sampling1d_2399 (UpSampl  (None, 23008, 6)         0         
 ing1D)                                                          
                                                                 
 conv1d_transpose_2399 (Conv  (None, 23008, 1)         121       
 1DTranspose)                                                    
                                                                 
=================================================================
Total params: 898
Trainable params: 898
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
 1/36 [..............................] - ETA: 18:20 - loss: 0.3598 - mean_squared_error: 0.3598 3/36 [=>............................] - ETA: 1s - loss: 0.1634 - mean_squared_error: 0.1634    5/36 [===>..........................] - ETA: 1s - loss: 0.1045 - mean_squared_error: 0.1045 7/36 [====>.........................] - ETA: 1s - loss: 0.0937 - mean_squared_error: 0.0937 9/36 [======>.......................] - ETA: 1s - loss: 0.0788 - mean_squared_error: 0.078811/36 [========>.....................] - ETA: 1s - loss: 0.0709 - mean_squared_error: 0.070913/36 [=========>....................] - ETA: 0s - loss: 0.0657 - mean_squared_error: 0.065715/36 [===========>..................] - ETA: 0s - loss: 0.0611 - mean_squared_error: 0.061117/36 [=============>................] - ETA: 0s - loss: 0.0577 - mean_squared_error: 0.057719/36 [==============>...............] - ETA: 0s - loss: 0.0538 - mean_squared_error: 0.053821/36 [================>.............] - ETA: 0s - loss: 0.0500 - mean_squared_error: 0.050023/36 [==================>...........] - ETA: 0s - loss: 0.0485 - mean_squared_error: 0.048525/36 [===================>..........] - ETA: 0s - loss: 0.0466 - mean_squared_error: 0.046627/36 [=====================>........] - ETA: 0s - loss: 0.0436 - mean_squared_error: 0.043629/36 [=======================>......] - ETA: 0s - loss: 0.0485 - mean_squared_error: 0.048531/36 [========================>.....] - ETA: 0s - loss: 0.0462 - mean_squared_error: 0.046233/36 [==========================>...] - ETA: 0s - loss: 0.0496 - mean_squared_error: 0.049635/36 [============================>.] - ETA: 0s - loss: 0.0496 - mean_squared_error: 0.049636/36 [==============================] - 34s 74ms/step - loss: 0.0486 - mean_squared_error: 0.0486 - val_loss: 0.0268 - val_mean_squared_error: 0.0268
Epoch 2/5
 1/36 [..............................] - ETA: 1s - loss: 0.0819 - mean_squared_error: 0.0819 3/36 [=>............................] - ETA: 1s - loss: 0.0806 - mean_squared_error: 0.0806 5/36 [===>..........................] - ETA: 1s - loss: 0.0583 - mean_squared_error: 0.0583 7/36 [====>.........................] - ETA: 1s - loss: 0.0561 - mean_squared_error: 0.0561 9/36 [======>.......................] - ETA: 1s - loss: 0.0482 - mean_squared_error: 0.048211/36 [========>.....................] - ETA: 1s - loss: 0.0430 - mean_squared_error: 0.043013/36 [=========>....................] - ETA: 0s - loss: 0.0441 - mean_squared_error: 0.044115/36 [===========>..................] - ETA: 0s - loss: 0.0396 - mean_squared_error: 0.039617/36 [=============>................] - ETA: 0s - loss: 0.0364 - mean_squared_error: 0.036419/36 [==============>...............] - ETA: 0s - loss: 0.0333 - mean_squared_error: 0.033321/36 [================>.............] - ETA: 0s - loss: 0.0316 - mean_squared_error: 0.031623/36 [==================>...........] - ETA: 0s - loss: 0.0376 - mean_squared_error: 0.037625/36 [===================>..........] - ETA: 0s - loss: 0.0352 - mean_squared_error: 0.035227/36 [=====================>........] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.033229/36 [=======================>......] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.032031/36 [========================>.....] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.031033/36 [==========================>...] - ETA: 0s - loss: 0.0297 - mean_squared_error: 0.029735/36 [============================>.] - ETA: 0s - loss: 0.0293 - mean_squared_error: 0.029336/36 [==============================] - 2s 44ms/step - loss: 0.0287 - mean_squared_error: 0.0287 - val_loss: 0.0186 - val_mean_squared_error: 0.0186
Epoch 3/5
 1/36 [..............................] - ETA: 1s - loss: 0.0149 - mean_squared_error: 0.0149 3/36 [=>............................] - ETA: 1s - loss: 0.0537 - mean_squared_error: 0.0537 5/36 [===>..........................] - ETA: 1s - loss: 0.0365 - mean_squared_error: 0.0365 7/36 [====>.........................] - ETA: 1s - loss: 0.0315 - mean_squared_error: 0.0315 9/36 [======>.......................] - ETA: 1s - loss: 0.0278 - mean_squared_error: 0.027811/36 [========>.....................] - ETA: 1s - loss: 0.0243 - mean_squared_error: 0.024313/36 [=========>....................] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.021915/36 [===========>..................] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.019917/36 [=============>................] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.019419/36 [==============>...............] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.021621/36 [================>.............] - ETA: 0s - loss: 0.0233 - mean_squared_error: 0.023323/36 [==================>...........] - ETA: 0s - loss: 0.0216 - mean_squared_error: 0.021625/36 [===================>..........] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.021227/36 [=====================>........] - ETA: 0s - loss: 0.0247 - mean_squared_error: 0.024729/36 [=======================>......] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.023631/36 [========================>.....] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.022733/36 [==========================>...] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.022135/36 [============================>.] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.023836/36 [==============================] - 1s 42ms/step - loss: 0.0232 - mean_squared_error: 0.0232 - val_loss: 0.0161 - val_mean_squared_error: 0.0161
Epoch 4/5
 1/36 [..............................] - ETA: 0s - loss: 0.0094 - mean_squared_error: 0.0094 3/36 [=>............................] - ETA: 1s - loss: 0.0075 - mean_squared_error: 0.0075 5/36 [===>..........................] - ETA: 1s - loss: 0.0319 - mean_squared_error: 0.0319 7/36 [====>.........................] - ETA: 1s - loss: 0.0336 - mean_squared_error: 0.0336 9/36 [======>.......................] - ETA: 0s - loss: 0.0292 - mean_squared_error: 0.029211/36 [========>.....................] - ETA: 0s - loss: 0.0256 - mean_squared_error: 0.025613/36 [=========>....................] - ETA: 0s - loss: 0.0227 - mean_squared_error: 0.022715/36 [===========>..................] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.021417/36 [=============>................] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.020219/36 [==============>...............] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.019621/36 [================>.............] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.021223/36 [==================>...........] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.022925/36 [===================>..........] - ETA: 0s - loss: 0.0222 - mean_squared_error: 0.022227/36 [=====================>........] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.021929/36 [=======================>......] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.022931/36 [========================>.....] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.023833/36 [==========================>...] - ETA: 0s - loss: 0.0229 - mean_squared_error: 0.022935/36 [============================>.] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.022136/36 [==============================] - 1s 37ms/step - loss: 0.0216 - mean_squared_error: 0.0216 - val_loss: 0.0156 - val_mean_squared_error: 0.0156
Epoch 5/5
 1/36 [..............................] - ETA: 1s - loss: 0.0153 - mean_squared_error: 0.0153 3/36 [=>............................] - ETA: 1s - loss: 0.0061 - mean_squared_error: 0.0061 5/36 [===>..........................] - ETA: 1s - loss: 0.0090 - mean_squared_error: 0.0090 7/36 [====>.........................] - ETA: 1s - loss: 0.0176 - mean_squared_error: 0.0176 9/36 [======>.......................] - ETA: 0s - loss: 0.0171 - mean_squared_error: 0.017111/36 [========>.....................] - ETA: 0s - loss: 0.0163 - mean_squared_error: 0.016313/36 [=========>....................] - ETA: 0s - loss: 0.0152 - mean_squared_error: 0.015215/36 [===========>..................] - ETA: 0s - loss: 0.0144 - mean_squared_error: 0.014417/36 [=============>................] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.013719/36 [==============>...............] - ETA: 0s - loss: 0.0137 - mean_squared_error: 0.013721/36 [================>.............] - ETA: 0s - loss: 0.0136 - mean_squared_error: 0.013623/36 [==================>...........] - ETA: 0s - loss: 0.0130 - mean_squared_error: 0.013025/36 [===================>..........] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.019427/36 [=====================>........] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.020629/36 [=======================>......] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.020231/36 [========================>.....] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.021333/36 [==========================>...] - ETA: 0s - loss: 0.0221 - mean_squared_error: 0.022135/36 [============================>.] - ETA: 0s - loss: 0.0213 - mean_squared_error: 0.021336/36 [==============================] - 1s 36ms/step - loss: 0.0208 - mean_squared_error: 0.0208 - val_loss: 0.0144 - val_mean_squared_error: 0.0144
(23008, 1, 5)
Model: "sequential_1199"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 23008, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 5752, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 5752, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1438, 6)          0         
                                                                 
=================================================================
Total params: 411
Trainable params: 411
Non-trainable params: 0
_________________________________________________________________
Model: "valence_NN"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 inputGSR (InputLayer)          [(None, 23008, 1)]   0           []                               
                                                                                                  
 inputPPG (InputLayer)          [(None, 23008, 1)]   0           []                               
                                                                                                  
 sequential_1198 (Sequential)   (None, 1438, 6)      411         ['inputGSR[0][0]']               
                                                                                                  
 sequential_1199 (Sequential)   (None, 1438, 6)      411         ['inputPPG[0][0]']               
                                                                                                  
 concatenate_599 (Concatenate)  (None, 1438, 12)     0           ['sequential_1198[0][0]',        
                                                                  'sequential_1199[0][0]']        
                                                                                                  
 permute_599 (Permute)          (None, 12, 1438)     0           ['concatenate_599[0][0]']        
                                                                                                  
 flatten_599 (Flatten)          (None, 17256)        0           ['permute_599[0][0]']            
                                                                                                  
 dropout_599 (Dropout)          (None, 17256)        0           ['flatten_599[0][0]']            
                                                                                                  
 dense_599 (Dense)              (None, 1)            17257       ['dropout_599[0][0]']            
                                                                                                  
==================================================================================================
Total params: 18,079
Trainable params: 18,079
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/10
 1/36 [..............................] - ETA: 9s - loss: 0.4158 - binary_accuracy: 1.000012/36 [=========>....................] - ETA: 0s - loss: 0.5684 - binary_accuracy: 0.750023/36 [==================>...........] - ETA: 0s - loss: 0.6517 - binary_accuracy: 0.695733/36 [==========================>...] - ETA: 0s - loss: 0.7079 - binary_accuracy: 0.606136/36 [==============================] - 1s 8ms/step - loss: 0.6918 - binary_accuracy: 0.6111 - val_loss: 0.6880 - val_binary_accuracy: 0.6000
Epoch 2/10
 1/36 [..............................] - ETA: 0s - loss: 0.5198 - binary_accuracy: 1.000012/36 [=========>....................] - ETA: 0s - loss: 0.5756 - binary_accuracy: 0.750022/36 [=================>............] - ETA: 0s - loss: 0.4755 - binary_accuracy: 0.818232/36 [=========================>....] - ETA: 0s - loss: 0.6960 - binary_accuracy: 0.718836/36 [==============================] - 0s 6ms/step - loss: 0.6826 - binary_accuracy: 0.6944 - val_loss: 0.7683 - val_binary_accuracy: 0.2000
Epoch 3/10
 1/36 [..............................] - ETA: 0s - loss: 0.8378 - binary_accuracy: 0.0000e+0013/36 [=========>....................] - ETA: 0s - loss: 0.5934 - binary_accuracy: 0.6923    22/36 [=================>............] - ETA: 0s - loss: 0.5783 - binary_accuracy: 0.681832/36 [=========================>....] - ETA: 0s - loss: 0.5717 - binary_accuracy: 0.687536/36 [==============================] - 0s 6ms/step - loss: 0.5487 - binary_accuracy: 0.6944 - val_loss: 0.7087 - val_binary_accuracy: 0.6000
Epoch 4/10
 1/36 [..............................] - ETA: 0s - loss: 0.2547 - binary_accuracy: 1.000016/36 [============>.................] - ETA: 0s - loss: 0.5549 - binary_accuracy: 0.750030/36 [========================>.....] - ETA: 0s - loss: 0.5689 - binary_accuracy: 0.733336/36 [==============================] - 0s 4ms/step - loss: 0.5885 - binary_accuracy: 0.6944 - val_loss: 0.7102 - val_binary_accuracy: 0.6000
Epoch 5/10
 1/36 [..............................] - ETA: 0s - loss: 0.7341 - binary_accuracy: 0.0000e+0015/36 [===========>..................] - ETA: 0s - loss: 0.6768 - binary_accuracy: 0.6667    28/36 [======================>.......] - ETA: 0s - loss: 0.5568 - binary_accuracy: 0.750036/36 [==============================] - 0s 4ms/step - loss: 0.5642 - binary_accuracy: 0.7500 - val_loss: 0.9022 - val_binary_accuracy: 0.6000
Epoch 6/10
 1/36 [..............................] - ETA: 0s - loss: 0.1882 - binary_accuracy: 1.000016/36 [============>.................] - ETA: 0s - loss: 0.5562 - binary_accuracy: 0.687530/36 [========================>.....] - ETA: 0s - loss: 0.5283 - binary_accuracy: 0.733336/36 [==============================] - 0s 4ms/step - loss: 0.5388 - binary_accuracy: 0.7222 - val_loss: 0.7276 - val_binary_accuracy: 0.6000
Epoch 7/10
 1/36 [..............................] - ETA: 0s - loss: 0.8625 - binary_accuracy: 0.0000e+0016/36 [============>.................] - ETA: 0s - loss: 0.3441 - binary_accuracy: 0.8750    30/36 [========================>.....] - ETA: 0s - loss: 0.4585 - binary_accuracy: 0.766736/36 [==============================] - 0s 4ms/step - loss: 0.4679 - binary_accuracy: 0.7500 - val_loss: 0.7816 - val_binary_accuracy: 0.4000
Epoch 8/10
 1/36 [..............................] - ETA: 0s - loss: 0.7633 - binary_accuracy: 0.0000e+0015/36 [===========>..................] - ETA: 0s - loss: 0.4482 - binary_accuracy: 0.8000    29/36 [=======================>......] - ETA: 0s - loss: 0.4034 - binary_accuracy: 0.827636/36 [==============================] - 0s 4ms/step - loss: 0.3742 - binary_accuracy: 0.8611 - val_loss: 0.9661 - val_binary_accuracy: 0.6000
Epoch 9/10
 1/36 [..............................] - ETA: 0s - loss: 1.6497 - binary_accuracy: 0.0000e+0016/36 [============>.................] - ETA: 0s - loss: 0.4192 - binary_accuracy: 0.6875    29/36 [=======================>......] - ETA: 0s - loss: 0.4756 - binary_accuracy: 0.689736/36 [==============================] - 0s 4ms/step - loss: 0.4708 - binary_accuracy: 0.7222 - val_loss: 0.8040 - val_binary_accuracy: 0.6000
Epoch 10/10
 1/36 [..............................] - ETA: 0s - loss: 0.6974 - binary_accuracy: 0.0000e+0016/36 [============>.................] - ETA: 0s - loss: 0.3526 - binary_accuracy: 0.8125    29/36 [=======================>......] - ETA: 0s - loss: 0.3236 - binary_accuracy: 0.862136/36 [==============================] - 0s 4ms/step - loss: 0.3116 - binary_accuracy: 0.8611 - val_loss: 0.8300 - val_binary_accuracy: 0.6000
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 55ms/step
predicted [0.65044755 0.8940788  0.8812971  0.7858467  0.45478654 0.06054911
 0.6873789  0.53782654 0.92181116 0.64682204 0.5833191 ]
predicted [1 1 1 1 0 0 1 1 1 1 1]
expected [ True False  True False  True False  True False  True  True  True]
accuracy: 0.6363636363636364
confusion matrix: 
[[1 3]
 [1 6]]
              precision    recall  f1-score   support

       False       0.50      0.25      0.33         4
        True       0.67      0.86      0.75         7

    accuracy                           0.64        11
   macro avg       0.58      0.55      0.54        11
weighted avg       0.61      0.64      0.60        11

macro avg f1-score: 0.5416666666666666
macro avg (UAR): 0.5535714285714286
Sensitivity:  0.25
Specificity:  0.8571428571428571
g-mean:  0.4629100498862757
-------- Model Performance ----------: 
accuracy:  [0.72727273 0.90909091 0.63636364 0.27272727 0.45454545 0.54545455
 0.45454545 0.72727273 0.72727273 0.63636364]
gmean:  [0.73192505 0.9258201  0.46291005 0.26726124 0.37796447 0.53452248
 0.         0.5        0.65465367 0.46291005]
f1_score:  [0.71794872 0.90598291 0.54166667 0.26666667 0.41071429 0.52991453
 0.3125     0.61176471 0.68571429 0.54166667]
UAR:  [0.73214286 0.92857143 0.55357143 0.26785714 0.41071429 0.53571429
 0.35714286 0.625      0.67857143 0.55357143]
Cohen Kappa score:  [ 0.44067797  0.81355932  0.12       -0.41935484 -0.17857143  0.06779661
 -0.32        0.29787234  0.37735849  0.12      ]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  23.008
step (sec):  23.008
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  52
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.923  0.967  0.96   0.963  0.96   0.306  0.662  0.71   0.528  0.634]
 [ 0.751  0.875  0.883  0.881  0.872  0.276  0.642  0.69   0.315  0.57 ]
 [ 0.629  0.825  0.847  0.803  0.812  0.338  0.679  0.673  0.675  0.642]
 [ 0.     0.5    0.613  0.     0.38   0.     0.5    0.673  0.     0.401]
 [ 0.79   0.892  0.903  0.881  0.892  0.168  0.558  0.65   0.315  0.48 ]
 [ 0.132  0.564  0.609  0.492  0.552 -0.008  0.498  0.573  0.301  0.463]
 [ 0.     0.5    0.613  0.     0.38   0.     0.5    0.673  0.     0.401]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.36390276  0.675       0.68666667  0.59234155  0.65630952  0.43246753
   0.7125      0.82666667  0.5280239   0.69365079]
 [ 0.38894439  0.7         0.69666667  0.66445089  0.67845238  0.31699134
   0.64583333  0.75        0.47252566  0.6297619 ]
 [ 0.35734266  0.66666667  0.71        0.54269454  0.64547619  0.35048285
   0.675       0.67666667  0.61875058  0.6552381 ]
 [ 0.35597736  0.675       0.67666667  0.62506621  0.65678571 -0.03562271
   0.48333333  0.65666667  0.14082483  0.44527778]
 [ 0.51640859  0.74583333  0.81        0.66715097  0.7456746   0.41809857
   0.71666667  0.69333333  0.63600519  0.67119048]
 [ 0.2032967   0.60833333  0.60666667  0.51671391  0.57321429  0.51282051
   0.76666667  0.76666667  0.705368    0.73988095]
 [ 0.27554113  0.63333333  0.68        0.48533712  0.60607143  0.10331335
   0.55        0.54333333  0.38568152  0.49642857]
 [ 0.59220779  0.8125      0.82666667  0.69873457  0.76440476  0.40850816
   0.70833333  0.75333333  0.59585429  0.68587302]
 [ 0.23387446  0.62083333  0.68666667  0.3767073   0.56468254  0.27175325
   0.6625      0.69666667  0.49657436  0.59361111]
 [ 0.24498834  0.61666667  0.63        0.58685739  0.6152381   0.14443889
   0.575       0.57666667  0.50115157  0.55178571]
 [ 0.17668998  0.59166667  0.61333333  0.51209055  0.57809524  0.35909091
   0.675       0.74666667  0.51462644  0.6493254 ]
 [ 0.6535298   0.81666667  0.84333333  0.79293808  0.82087302  0.39370629
   0.70416667  0.71333333  0.63844292  0.67797619]
 [ 0.42798868  0.72083333  0.72666667  0.65155264  0.6927381   0.17111222
   0.58333333  0.59333333  0.5304409   0.56952381]
 [-0.13636364  0.43333333  0.54666667  0.17320508  0.41051587  0.07229437
   0.54166667  0.56        0.39593169  0.50130952]
 [ 0.075       0.5375      0.71333333  0.1         0.46333333  0.08095238
   0.53333333  0.57666667  0.38533712  0.51587302]
 [-0.07878788  0.46666667  0.5         0.31261051  0.4447619   0.22874625
   0.60833333  0.73333333  0.36449237  0.58357143]
 [ 0.24254079  0.62083333  0.65        0.5305183   0.60261905  0.28857809
   0.64166667  0.66        0.56101556  0.62369048]
 [        nan         nan         nan  0.2                nan  0.42798868
   0.71666667  0.72666667  0.64659976  0.6927381 ]
 [ 0.22608225  0.625       0.70666667  0.40122898  0.57472222  0.12649018
   0.55        0.55333333  0.4624557   0.52357143]
 [ 0.26212121  0.62083333  0.69333333  0.4564696   0.6002381   0.33325008
   0.66666667  0.71        0.53316638  0.64563492]
 [-0.11136364  0.4375      0.65333333  0.06123724  0.41440476  0.21864802
   0.60833333  0.6         0.54376095  0.57857143]
 [ 0.23851149  0.625       0.61666667  0.52412527  0.58595238  0.28558941
   0.64166667  0.69666667  0.54597783  0.63146825]
 [ 0.41320346  0.7         0.72333333  0.58080604  0.67630952 -0.11493506
   0.4375      0.65666667  0.          0.39218254]
 [-0.27820513  0.35833333  0.35        0.25436768  0.32785714  0.53426573
   0.76666667  0.78333333  0.72019073  0.76047619]
 [ 0.36054779  0.67916667  0.69666667  0.63000857  0.67035714  0.42600733
   0.71666667  0.70666667  0.69980371  0.70071429]
 [ 0.2992008   0.65        0.67333333  0.52960943  0.61571429  0.41620047
   0.70833333  0.71333333  0.6681748   0.69833333]
 [ 0.42575758  0.72083333  0.77666667  0.56504825  0.68123016  0.39443889
   0.7         0.69666667  0.6588866   0.67678571]
 [ 0.07532468  0.54583333  0.65        0.15731322  0.45448413  0.48939394
   0.74583333  0.78666667  0.62718028  0.71678571]
 [ 0.18381618  0.59166667  0.59666667  0.55090804  0.57928571  0.03814519
   0.51666667  0.53333333  0.33256985  0.47011905]
 [ 0.92307692  0.96666667  0.96        0.96329932  0.96        0.30606061
   0.6625      0.71        0.52760209  0.63373016]]
KNN mean:
[0.27279847 0.63663793 0.67931034 0.49011304 0.60895868 0.27997586
 0.64069444 0.67988889 0.51458052 0.61350265]
---------------------------
---------------------------
DT performance:
[[ 0.51645022  0.79166667  0.80666667  0.68330044  0.7722619   0.23636364
   0.6375      0.73333333  0.51855046  0.60952381]
 [ 0.42132867  0.7         0.69666667  0.59518036  0.67797619  0.21125541
   0.59166667  0.61666667  0.42718028  0.53107143]
 [ 0.06143856  0.58333333  0.57666667  0.47532189  0.54619048  0.154662
   0.58333333  0.58333333  0.52363821  0.56142857]
 [ 0.03150183  0.56666667  0.55333333  0.50680815  0.53666667  0.04078422
   0.45        0.54        0.27742737  0.40718254]
 [ 0.41171329  0.71666667  0.76        0.68666859  0.70797619  0.42798868
   0.725       0.71        0.64543604  0.685     ]
 [ 0.4495671   0.70833333  0.72666667  0.67708214  0.70380952  0.33986014
   0.65        0.65666667  0.62968956  0.62904762]
 [ 0.30612721  0.65        0.67333333  0.59893469  0.64309524  0.13451548
   0.46666667  0.47333333  0.37270587  0.42547619]
 [ 0.74318182  0.875       0.88666667  0.85997182  0.8672619   0.19594406
   0.54166667  0.55666667  0.48431878  0.50440476]
 [ 0.09539461  0.625       0.69        0.33618073  0.57638889  0.12987013
   0.67916667  0.67        0.40980762  0.58940476]
 [ 0.10979021  0.61666667  0.62666667  0.46041903  0.59880952  0.29117549
   0.675       0.66        0.67683001  0.64785714]
 [ 0.22487512  0.61666667  0.62666667  0.62650635  0.60904762  0.12564935
   0.52916667  0.57        0.46800511  0.50646825]
 [ 0.51344988  0.7375      0.73        0.72370346  0.71904762  0.30671662
   0.62916667  0.65        0.57298339  0.60480159]
 [ 0.33892774  0.59583333  0.59333333  0.60328053  0.54785714  0.08836164
   0.55833333  0.57        0.50124386  0.53833333]
 [ 0.12697303  0.56666667  0.6         0.44204328  0.5375      0.0525641
   0.525       0.54        0.53945267  0.48678571]
 [-0.1023976   0.55416667  0.65333333  0.12844571  0.50865079  0.07294372
   0.65416667  0.67666667  0.60185091  0.6377381 ]
 [ 0.18822844  0.61666667  0.62333333  0.50970948  0.59904762  0.23881119
   0.67083333  0.75        0.40236034  0.65277778]
 [ 0.65656011  0.87916667  0.88333333  0.76740858  0.87357143  0.37682318
   0.70833333  0.71        0.4962761   0.68940476]
 [        nan         nan         nan  0.47745967         nan  0.53231768
   0.7         0.71333333  0.69983304  0.67630952]
 [ 0.39025974  0.6625      0.71666667  0.5026586   0.62896825 -0.03181818
   0.525       0.52666667  0.42363821  0.50809524]
 [-0.03640526  0.5375      0.61333333  0.3466979   0.50583333  0.1548951
   0.54166667  0.57666667  0.48316638  0.50928571]
 [ 0.0019697   0.4875      0.63        0.30606602  0.45178571  0.16736597
   0.65833333  0.68333333  0.53449706  0.62880952]
 [ 0.18791209  0.55        0.53        0.55140725  0.51214286  0.13650516
   0.5375      0.58        0.42556542  0.5175    ]
 [ 0.13648019  0.57083333  0.58333333  0.43688672  0.53964286  0.00887446
   0.52083333  0.68        0.15        0.49496032]
 [ 0.46363636  0.70833333  0.71666667  0.59318517  0.66511905  0.20897436
   0.59166667  0.59        0.63112971  0.58119048]
 [ 0.22026307  0.58333333  0.62        0.38425353  0.56547619  0.41110556
   0.7         0.71        0.6222274   0.69357143]
 [ 0.33916084  0.65        0.67666667  0.53147411  0.59809524  0.17161172
   0.6         0.59333333  0.57827016  0.58309524]
 [ 0.49859307  0.76666667  0.75666667  0.73402052  0.73849206  0.23158508
   0.64166667  0.64333333  0.57732923  0.5925    ]
 [ 0.0256327   0.475       0.55333333  0.23794454  0.43690476  0.2412987
   0.6625      0.67666667  0.49831277  0.63107143]
 [ 0.25857476  0.60833333  0.59666667  0.59408462  0.58309524  0.47777223
   0.68333333  0.68666667  0.70184038  0.66547619]
 [ 0.7514319   0.875       0.88333333  0.88080604  0.87230159  0.27554113
   0.64166667  0.69        0.31462644  0.56968254]]
DT mean:
[0.28726274 0.65086207 0.67528736 0.54193033 0.62493158 0.21367727
 0.60930556 0.63388889 0.50627309 0.57860847]
---------------------------
---------------------------
RF performance:
[[ 0.52286047  0.70833333  0.70333333  0.70721508  0.67714286  0.19155844
   0.6125      0.72666667  0.35731322  0.58253968]
 [ 0.28799534  0.63333333  0.63666667  0.63161677  0.6172619   0.31177989
   0.59583333  0.69        0.537821    0.55781746]
 [ 0.3481352   0.59166667  0.6         0.55697154  0.5472619   0.09721945
   0.65        0.65666667  0.63793239  0.63428571]
 [ 0.34230769  0.59166667  0.58666667  0.62515849  0.56785714  0.11448551
   0.51666667  0.63666667  0.2851624   0.45666667]
 [ 0.5242008   0.79583333  0.85333333  0.73080604  0.7793254   0.57307692
   0.75        0.73        0.67388174  0.705     ]
 [ 0.39407259  0.675       0.67        0.69030488  0.63952381  0.41318681
   0.725       0.72666667  0.80336066  0.69488095]
 [ 0.55792541  0.6625      0.68666667  0.67852228  0.65492063  0.12660673
   0.6         0.6         0.27079081  0.53452381]
 [ 0.64935065  0.8875      0.88333333  0.84015593  0.86468254  0.17364302
   0.65416667  0.71        0.51842692  0.6315873 ]
 [ 0.18350649  0.60416667  0.66666667  0.34348343  0.55388889  0.21266234
   0.67083333  0.67333333  0.46288803  0.60130952]
 [ 0.10198135  0.625       0.61666667  0.51708765  0.59261905  0.39941725
   0.56666667  0.57666667  0.45175166  0.53642857]
 [ 0.4992008   0.66666667  0.69        0.6135307   0.66238095  0.2215368
   0.70416667  0.74666667  0.64349395  0.6925    ]
 [ 0.58393939  0.75        0.77        0.75387706  0.72321429  0.26812354
   0.6625      0.68        0.5078116   0.65321429]
 [ 0.27857143  0.65416667  0.65333333  0.42019073  0.60488095  0.26223776
   0.58333333  0.59666667  0.52556542  0.54559524]
 [ 0.11336164  0.5375      0.65666667  0.19711971  0.50281746  0.03265068
   0.49166667  0.50333333  0.56767558  0.4702381 ]
 [ 0.10818182  0.55833333  0.6         0.24783978  0.47170635  0.08576424
   0.5875      0.63        0.52719516  0.57503968]
 [ 0.18547786  0.40833333  0.43333333  0.47852584  0.36130952  0.28558941
   0.65416667  0.72666667  0.32307101  0.61626984]
 [ 0.76095571  0.85416667  0.86333333  0.75502946  0.8497619   0.37041292
   0.66666667  0.67        0.59710756  0.62928571]
 [        nan         nan         nan  0.2                nan  0.2988345
   0.70833333  0.72666667  0.59138847  0.69369048]
 [ 0.28766234  0.7125      0.77        0.39391576  0.69126984  0.36132201
   0.55        0.55666667  0.5624557   0.53166667]
 [ 0.19469697  0.57083333  0.65666667  0.38038423  0.54892857  0.1502664
   0.68333333  0.69333333  0.50184038  0.67753968]
 [ 0.01266234  0.375       0.56333333  0.2         0.36468254  0.21147186
   0.64166667  0.63        0.4743688   0.5975    ]
 [ 0.16836497  0.61666667  0.61333333  0.55745861  0.58571429  0.22624043
   0.67083333  0.69        0.54991238  0.64595238]
 [ 0.20885781  0.50833333  0.56333333  0.48653373  0.48369048  0.04025974
   0.42083333  0.63666667  0.25731322  0.38325397]
 [ 0.31351981  0.51666667  0.54333333  0.53115904  0.45904762  0.56586747
   0.775       0.76666667  0.72371834  0.75321429]
 [ 0.25984848  0.62916667  0.65        0.47676203  0.58809524  0.3992008
   0.66666667  0.68666667  0.6480399   0.6527381 ]
 [ 0.24125874  0.71666667  0.73333333  0.58364485  0.69785714  0.26888112
   0.625       0.63666667  0.52650635  0.60535714]
 [ 0.50017483  0.74166667  0.76333333  0.69699617  0.7297619   0.35526141
   0.64166667  0.65333333  0.51815406  0.61547619]
 [-0.05544456  0.4625      0.53666667  0.30865522  0.43190476  0.36759907
   0.72083333  0.73        0.69729443  0.69238095]
 [ 0.05842491  0.65        0.67666667  0.38568152  0.62297619  0.32169497
   0.70833333  0.70333333  0.62431487  0.67702381]
 [ 0.62905428  0.825       0.84666667  0.80328053  0.81238095  0.33827006
   0.67916667  0.67333333  0.67485587  0.64190476]]
RF mean:
[0.31934847 0.63893678 0.67195402 0.5263969  0.6098919  0.26817072
 0.63944444 0.66877778 0.53471373 0.60949603]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.56        0.          0.3577381   0.
   0.5         0.77333333  0.          0.43555556]
 [ 0.          0.5         0.54        0.          0.34880952  0.
   0.5         0.65333333  0.          0.39388889]
 [ 0.          0.5         0.58        0.          0.36666667 -0.03333333
   0.48333333  0.40333333  0.          0.28690476]
 [ 0.03333333  0.51666667  0.43666667  0.10487548  0.33964286  0.
   0.5         0.73333333  0.          0.42166667]
 [ 0.          0.5         0.63333333  0.          0.38694444  0.45374625
   0.725       0.73        0.5909014   0.67833333]
 [-0.06666667  0.46666667  0.48        0.          0.31904762  0.12575758
   0.55833333  0.58        0.19915638  0.44702381]
 [ 0.          0.5         0.61333333  0.          0.38        0.
   0.5         0.52        0.          0.33988095]
 [ 0.          0.5         0.67333333  0.          0.40083333  0.
   0.5         0.63333333  0.          0.38694444]
 [ 0.          0.5         0.65333333  0.          0.39388889  0.
   0.5         0.69333333  0.          0.40777778]
 [ 0.          0.5         0.52        0.          0.33988095  0.
   0.5         0.56        0.          0.3577381 ]
 [ 0.          0.5         0.58        0.          0.36666667  0.
   0.5         0.63333333  0.          0.38694444]
 [ 0.          0.5         0.61333333  0.          0.38        0.
   0.5         0.61333333  0.          0.38      ]
 [ 0.          0.5         0.59666667  0.          0.37333333  0.
   0.5         0.56        0.          0.3577381 ]
 [ 0.          0.5         0.67333333  0.          0.40083333  0.
   0.5         0.56        0.          0.3577381 ]
 [ 0.          0.5         0.71333333  0.          0.41472222  0.
   0.5         0.63333333  0.          0.38694444]
 [ 0.          0.5         0.59666667  0.          0.37333333  0.
   0.5         0.67333333  0.          0.40083333]
 [ 0.          0.5         0.63333333  0.          0.38694444  0.
   0.5         0.58        0.          0.36666667]
 [        nan         nan         nan  0.2                nan  0.
   0.5         0.58        0.          0.36666667]
 [ 0.          0.5         0.71333333  0.          0.41472222  0.
   0.5         0.52        0.          0.33988095]
 [ 0.          0.5         0.67333333  0.          0.40083333  0.
   0.5         0.61333333  0.          0.38      ]
 [ 0.          0.5         0.77333333  0.          0.43555556  0.
   0.5         0.52        0.          0.33988095]
 [ 0.          0.5         0.52        0.          0.33988095  0.
   0.5         0.63333333  0.          0.38694444]
 [ 0.          0.5         0.59666667  0.          0.37333333  0.
   0.5         0.75333333  0.          0.42861111]
 [ 0.          0.5         0.56        0.          0.3577381   0.
   0.5         0.56        0.          0.3577381 ]
 [ 0.          0.5         0.59666667  0.          0.37333333  0.
   0.5         0.52        0.          0.33988095]
 [ 0.          0.5         0.56        0.          0.3577381   0.
   0.5         0.56        0.          0.3577381 ]
 [ 0.          0.5         0.71333333  0.          0.41472222  0.13333333
   0.56666667  0.48666667  0.15773503  0.39107143]
 [ 0.          0.5         0.65333333  0.          0.39388889  0.
   0.5         0.63333333  0.          0.38694444]
 [ 0.          0.5         0.58        0.          0.36666667  0.
   0.5         0.56        0.          0.3577381 ]
 [ 0.          0.5         0.61333333  0.          0.38        0.
   0.5         0.67333333  0.          0.40083333]]
SVM mean:
[-0.00114943  0.49942529  0.60862069  0.01016252  0.37716201  0.02265013
  0.51111111  0.60488889  0.03159309  0.38755026]
---------------------------
---------------------------
GBM performance:
[[ 0.5512987   0.775       0.78666667  0.6962761   0.76011905  0.12142857
   0.5625      0.75666667  0.2         0.52611111]
 [ 0.30561106  0.65        0.65666667  0.61806177  0.63678571  0.16946387
   0.6125      0.67666667  0.32658375  0.58781746]
 [ 0.11689977  0.58333333  0.59333333  0.49079194  0.54809524  0.27682318
   0.63333333  0.62666667  0.55148739  0.60190476]
 [ 0.26388611  0.625       0.60666667  0.60074465  0.59785714 -0.1262987
   0.43333333  0.63666667  0.          0.38325397]
 [ 0.59458874  0.75833333  0.83        0.71128842  0.74912698  0.48461538
   0.75        0.73333333  0.68920914  0.71666667]
 [ 0.43158508  0.74166667  0.74666667  0.65957541  0.7352381   0.40769231
   0.725       0.73333333  0.67195454  0.68869048]
 [ 0.35151515  0.69166667  0.71333333  0.61902344  0.67634921  0.0032967
   0.48333333  0.48        0.36866186  0.43607143]
 [ 0.64318182  0.825       0.84666667  0.75997182  0.8047619   0.33318182
   0.7         0.72666667  0.5819991   0.68392857]
 [ 0.3395671   0.65416667  0.69        0.4564696   0.59623016 -0.0262987
   0.54583333  0.65333333  0.2         0.48714286]
 [ 0.17342657  0.59166667  0.60666667  0.48433366  0.57619048  0.33649684
   0.64166667  0.64333333  0.59828344  0.61654762]
 [ 0.28986014  0.61666667  0.63        0.57792575  0.61107143  0.22281885
   0.60416667  0.67        0.46842692  0.59289683]
 [ 0.42517483  0.7125      0.73333333  0.62996242  0.69416667  0.28292541
   0.6625      0.69666667  0.51595792  0.62916667]
 [ 0.28569764  0.61666667  0.65333333  0.46842692  0.58623016  0.08403263
   0.55833333  0.57333333  0.52067779  0.55202381]
 [ 0.25790043  0.63333333  0.77        0.38284271  0.5993254   0.16388611
   0.61666667  0.62        0.5761412   0.60285714]
 [-0.03333333  0.48333333  0.67333333  0.05773503  0.42166667  0.02922078
   0.5         0.58        0.32760209  0.46785714]
 [ 0.11491841  0.60833333  0.62333333  0.43351078  0.57488095  0.10017483
   0.55833333  0.67666667  0.20236034  0.5134127 ]
 [ 0.7539627   0.87916667  0.88333333  0.86740858  0.87357143  0.21260406
   0.65833333  0.67333333  0.53305691  0.61702381]
 [        nan         nan         nan  0.4                nan  0.47777223
   0.73333333  0.74666667  0.64410536  0.71059524]
 [ 0.08214286  0.5875      0.67666667  0.23194792  0.53472222  0.03579754
   0.51666667  0.52        0.38433366  0.47357143]
 [-0.03484848  0.4625      0.57333333  0.18618073  0.43059524  0.24292374
   0.59166667  0.63333333  0.50604779  0.58706349]
 [-0.07857143  0.5125      0.74333333  0.1         0.46944444  0.15502831
   0.58333333  0.58333333  0.47127787  0.54857143]
 [ 0.11085581  0.6         0.58666667  0.46935067  0.55785714  0.13961039
   0.57083333  0.65666667  0.34391576  0.53503968]
 [ 0.2427972   0.67916667  0.71333333  0.59883189  0.66607143 -0.0262987
   0.54583333  0.71666667  0.18660254  0.49337302]
 [ 0.43253413  0.75        0.77        0.65723582  0.71428571  0.36223776
   0.7         0.70666667  0.6222782   0.68714286]
 [ 0.19393939  0.59166667  0.66        0.34546537  0.54297619  0.4495671
   0.7         0.70666667  0.62019073  0.6897619 ]
 [ 0.19184149  0.63333333  0.66        0.52960943  0.61095238  0.32813853
   0.675       0.69        0.60403186  0.66285714]
 [ 0.67002165  0.80416667  0.83        0.74741798  0.79444444  0.26903097
   0.675       0.67666667  0.57255106  0.63940476]
 [ 0.11186314  0.55416667  0.65666667  0.27307101  0.49753968  0.11969697
   0.53333333  0.57333333  0.29915638  0.4697619 ]
 [ 0.31305361  0.65        0.68        0.52710288  0.62083333  0.51015651
   0.74166667  0.74666667  0.64907312  0.71178571]
 [ 0.78989344  0.89166667  0.90333333  0.88080604  0.89230159  0.16796537
   0.55833333  0.65        0.31462644  0.48039683]]
GBM mean:
[0.3065953  0.66077586 0.70678161 0.51537896 0.63357553 0.21025635
 0.61236111 0.65877778 0.45168644 0.57975661]
---------------------------
---------------------------
BDDAE performance:
[[-0.10342097  0.45        0.44545455  0.35592323  0.41725455  0.03132131
   0.52291667  0.65454545  0.29834374  0.50252218]
 [ 0.17590781  0.58666667  0.59090909  0.56496246  0.57783383  0.24428096
   0.62678571  0.62727273  0.6035174   0.60656469]
 [-0.10112942  0.45        0.47272727  0.33148858  0.41889042  0.17619747
   0.59166667  0.59090909  0.53903097  0.57302198]
 [ 0.15804178  0.57833333  0.58181818  0.56341891  0.57195693 -0.07330171
   0.46041667  0.60909091  0.20773503  0.44519909]
 [-0.0015702   0.50357143  0.51818182  0.44883374  0.48413156  0.18287566
   0.59333333  0.58181818  0.5404781   0.56064741]
 [ 0.08660666  0.54333333  0.55454545  0.42919401  0.50846595  0.21861371
   0.61        0.60909091  0.58862186  0.59833   ]
 [ 0.28817348  0.63928571  0.66363636  0.61450192  0.63239511 -0.15489911
   0.42166667  0.42727273  0.29737473  0.38628757]
 [ 0.17702585  0.57857143  0.65454545  0.46996461  0.566597   -0.09828389
   0.45714286  0.5         0.34059593  0.43958899]
 [ 0.28159941  0.62678571  0.70909091  0.47405815  0.60514598  0.19435153
   0.58333333  0.72727273  0.39468966  0.56688885]
 [-0.19514461  0.4         0.4         0.3603535   0.38651432  0.04433062
   0.52166667  0.53636364  0.47992598  0.50897158]
 [-0.01859154  0.49        0.5         0.44070244  0.47806388 -0.03660903
   0.48214286  0.54545455  0.32182495  0.45091603]
 [-0.21568182  0.39285714  0.41818182  0.30004834  0.37874847  0.23243473
   0.61785714  0.63636364  0.58304939  0.60189741]
 [ 0.09749276  0.55178571  0.54545455  0.53131155  0.52962177  0.44451657
   0.72833333  0.71818182  0.69678147  0.70821984]
 [ 0.03625903  0.52321429  0.56363636  0.43366872  0.50808178  0.12909983
   0.565       0.56363636  0.54601165  0.55379232]
 [-0.21060639  0.39791667  0.51818182  0.1352604   0.37525048  0.33334003
   0.67321429  0.67272727  0.66758325  0.66123626]
 [-0.06807442  0.47142857  0.49090909  0.37291342  0.45052295  0.0500237
   0.51964286  0.6         0.35184141  0.49764356]
 [ 0.2400623   0.63214286  0.65454545  0.52039315  0.59407145  0.0953889
   0.54666667  0.56363636  0.44570449  0.52229243]
 [ 0.23867053  0.63611111  0.75454545  0.52588564  0.60595361  0.21226824
   0.60333333  0.61818182  0.55823434  0.59532357]
 [ 0.08530649  0.54166667  0.63636364  0.41203209  0.534169   -0.02331114
   0.49166667  0.49090909  0.40633698  0.46366606]
 [-0.10379663  0.45        0.51818182  0.27868861  0.42555556  0.16064951
   0.57678571  0.64545455  0.43850028  0.55105159]
 [ 0.0295202   0.52291667  0.65454545  0.29273049  0.49567319  0.06131577
   0.53        0.53636364  0.51533765  0.52561661]
 [ 0.1181898   0.56        0.56363636  0.49657071  0.53784091 -0.06981187
   0.4625      0.55454545  0.23080643  0.42786944]
 [ 0.17962442  0.59464286  0.57272727  0.54170172  0.55576923  0.39608323
   0.67083333  0.80909091  0.5261664   0.67607327]
 [ 0.04086005  0.52166667  0.52727273  0.46770305  0.5054046   0.3883063
   0.69333333  0.7         0.67931405  0.68976163]
 [-0.2575035   0.36785714  0.4         0.26898495  0.36041514  0.01122893
   0.505       0.50909091  0.48336154  0.49601066]
 [ 0.19849817  0.59833333  0.60909091  0.56858061  0.58908758  0.14985737
   0.57333333  0.58181818  0.54504143  0.56295732]
 [ 0.32964     0.6625      0.73636364  0.63028165  0.66047386  0.38403787
   0.69666667  0.69090909  0.65402735  0.67312271]
 [-0.18153342  0.40357143  0.44545455  0.28876591  0.39139042  0.40390575
   0.69642857  0.73636364  0.66355764  0.69403361]
 [ 0.24470925  0.62166667  0.62727273  0.60987805  0.61822344  0.06987535
   0.53333333  0.54545455  0.50103588  0.52494505]
 [ 0.13193385  0.56428571  0.60909091  0.49179671  0.55245394 -0.00848151
   0.49821429  0.57272727  0.30099092  0.46290859]]
BDDAE mean:
[0.05603563 0.5287037  0.56454545 0.44068658 0.5105319  0.13832017
 0.56844048 0.60515152 0.48019403 0.55091201]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.56       0.         0.3577381  0.
  0.5        0.77333333 0.         0.43555556]
 [0.         0.5        0.54       0.         0.34880952 0.
  0.5        0.65333333 0.         0.39388889]
 [0.         0.5        0.58       0.         0.36666667 0.
  0.5        0.42       0.         0.2952381 ]
 [0.         0.5        0.42       0.         0.2952381  0.
  0.5        0.73333333 0.         0.42166667]
 [0.         0.5        0.63333333 0.         0.38694444 0.
  0.5        0.52       0.         0.33988095]
 [0.         0.5        0.52       0.         0.33988095 0.
  0.5        0.54       0.         0.34880952]
 [0.         0.5        0.61333333 0.         0.38       0.
  0.5        0.52       0.         0.33988095]
 [0.         0.5        0.67333333 0.         0.40083333 0.
  0.5        0.63333333 0.         0.38694444]
 [0.         0.5        0.65333333 0.         0.39388889 0.
  0.5        0.69333333 0.         0.40777778]
 [0.         0.5        0.52       0.         0.33988095 0.
  0.5        0.56       0.         0.3577381 ]
 [0.         0.5        0.58       0.         0.36666667 0.
  0.5        0.63333333 0.         0.38694444]
 [0.         0.5        0.61333333 0.         0.38       0.
  0.5        0.61333333 0.         0.38      ]
 [0.         0.5        0.59666667 0.         0.37333333 0.
  0.5        0.56       0.         0.3577381 ]
 [0.         0.5        0.67333333 0.         0.40083333 0.
  0.5        0.56       0.         0.3577381 ]
 [0.         0.5        0.71333333 0.         0.41472222 0.
  0.5        0.63333333 0.         0.38694444]
 [0.         0.5        0.59666667 0.         0.37333333 0.
  0.5        0.67333333 0.         0.40083333]
 [0.         0.5        0.63333333 0.         0.38694444 0.
  0.5        0.58       0.         0.36666667]
 [       nan        nan        nan 0.2               nan 0.
  0.5        0.58       0.         0.36666667]
 [0.         0.5        0.71333333 0.         0.41472222 0.
  0.5        0.52       0.         0.33988095]
 [0.         0.5        0.67333333 0.         0.40083333 0.
  0.5        0.61333333 0.         0.38      ]
 [0.         0.5        0.77333333 0.         0.43555556 0.
  0.5        0.52       0.         0.33988095]
 [0.         0.5        0.52       0.         0.33988095 0.
  0.5        0.63333333 0.         0.38694444]
 [0.         0.5        0.59666667 0.         0.37333333 0.
  0.5        0.75333333 0.         0.42861111]
 [0.         0.5        0.56       0.         0.3577381  0.
  0.5        0.56       0.         0.3577381 ]
 [0.         0.5        0.59666667 0.         0.37333333 0.
  0.5        0.52       0.         0.33988095]
 [0.         0.5        0.56       0.         0.3577381  0.
  0.5        0.56       0.         0.3577381 ]
 [0.         0.5        0.71333333 0.         0.41472222 0.
  0.5        0.42       0.         0.2952381 ]
 [0.         0.5        0.65333333 0.         0.39388889 0.
  0.5        0.63333333 0.         0.38694444]
 [0.         0.5        0.58       0.         0.36666667 0.
  0.5        0.56       0.         0.3577381 ]
 [0.         0.5        0.61333333 0.         0.38       0.
  0.5        0.67333333 0.         0.40083333]]
DUMMY mean:
[0.         0.5        0.60942529 0.00666667 0.37634921 0.
 0.5        0.59488889 0.         0.37007804]
---------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_1_2_50
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.273  0.637  0.679  0.5    0.609  0.275  0.638  0.678  0.51   0.611]
 [ 0.287  0.651  0.675  0.544  0.625  0.203  0.606  0.631  0.5    0.575]
 [ 0.319  0.639  0.672  0.538  0.61   0.267  0.637  0.667  0.533  0.607]
 [-0.001  0.499  0.609  0.004  0.377  0.023  0.511  0.606  0.033  0.388]
 [ 0.307  0.661  0.707  0.519  0.634  0.201  0.608  0.656  0.445  0.575]
 [ 0.056  0.529  0.565  0.441  0.511  0.138  0.568  0.605  0.48   0.551]
 [ 0.     0.5    0.609  0.     0.376  0.     0.5    0.595  0.     0.37 ]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.241 0.121 0.109 0.202 0.128 0.165 0.085 0.079 0.159 0.091]
 [0.226 0.108 0.098 0.174 0.117 0.124 0.073 0.069 0.122 0.075]
 [0.205 0.121 0.105 0.177 0.129 0.136 0.076 0.06  0.135 0.08 ]
 [0.014 0.007 0.073 0.019 0.026 0.088 0.044 0.084 0.115 0.064]
 [0.228 0.106 0.089 0.206 0.119 0.159 0.08  0.066 0.174 0.089]
 [0.167 0.083 0.094 0.119 0.085 0.163 0.081 0.082 0.137 0.086]
 [0.    0.    0.072 0.    0.028 0.    0.    0.085 0.    0.034]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 8.800e+01  1.900e+01  1.600e+01  4.000e+01  2.100e+01  6.000e+01  1.300e+01  1.200e+01  3.100e+01  1.500e+01]
 [ 7.900e+01  1.700e+01  1.500e+01  3.200e+01  1.900e+01  6.100e+01  1.200e+01  1.100e+01  2.400e+01  1.300e+01]
 [ 6.400e+01  1.900e+01  1.600e+01  3.300e+01  2.100e+01  5.100e+01  1.200e+01  9.000e+00  2.500e+01  1.300e+01]
 [-1.218e+03  1.000e+00  1.200e+01  5.250e+02  7.000e+00  3.760e+02  9.000e+00  1.400e+01  3.520e+02  1.600e+01]
 [ 7.400e+01  1.600e+01  1.300e+01  4.000e+01  1.900e+01  7.900e+01  1.300e+01  1.000e+01  3.900e+01  1.500e+01]
 [ 2.980e+02  1.600e+01  1.700e+01  2.700e+01  1.700e+01  1.180e+02  1.400e+01  1.400e+01  2.900e+01  1.600e+01]
 [ 0.000e+00  0.000e+00  1.200e+01  0.000e+00  7.000e+00  0.000e+00  0.000e+00  1.400e+01  0.000e+00  9.000e+00]]
-------------------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_1_2_50
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  23.008
step (sec):  23.008
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  52
Elapsed time: 1227.7913033088048 minutes
Elapsed time: 20.46318838848008 hours
