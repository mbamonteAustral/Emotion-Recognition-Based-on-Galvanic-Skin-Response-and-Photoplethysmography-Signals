2024-05-10 00:15:14.211051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-10 00:15:14.543387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-10 00:15:14.543632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-10 00:15:14.544003: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-10 00:15:14.544660: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-10 00:15:14.544852: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-10 00:15:14.545017: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-10 00:15:15.507249: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-10 00:15:15.507467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-10 00:15:15.507628: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-10 00:15:15.507766: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
Window size (sec):  25.0
step (sec):  25.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
/home/marcos/Dropbox (Maestral)/c_sldl_1_2_53/functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:


_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 24992, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 6248, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 6248, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1562, 6)          0         
                                                                 
=================================================================
Total params: 411
Trainable params: 411
Non-trainable params: 0
_________________________________________________________________
----- train_PPG_AE -------
Model: "sequential_1199"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 24992, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 6248, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 6248, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1562, 6)          0         
                                                                 
 up_sampling1d_2398 (UpSampl  (None, 6248, 6)          0         
 ing1D)                                                          
                                                                 
 conv1d_transpose_2398 (Conv  (None, 6248, 6)          366       
 1DTranspose)                                                    
                                                                 
 up_sampling1d_2399 (UpSampl  (None, 24992, 6)         0         
 ing1D)                                                          
                                                                 
 conv1d_transpose_2399 (Conv  (None, 24992, 1)         121       
 1DTranspose)                                                    
                                                                 
=================================================================
Total params: 898
Trainable params: 898
Non-trainable params: 0
_________________________________________________________________
Epoch 1/5
 1/32 [..............................] - ETA: 17:59 - loss: 0.2128 - mean_squared_error: 0.2128 3/32 [=>............................] - ETA: 1s - loss: 0.1719 - mean_squared_error: 0.1719    5/32 [===>..........................] - ETA: 1s - loss: 0.1241 - mean_squared_error: 0.1241 7/32 [=====>........................] - ETA: 1s - loss: 0.0967 - mean_squared_error: 0.0967 9/32 [=======>......................] - ETA: 1s - loss: 0.0860 - mean_squared_error: 0.086011/32 [=========>....................] - ETA: 0s - loss: 0.0793 - mean_squared_error: 0.079313/32 [===========>..................] - ETA: 0s - loss: 0.0739 - mean_squared_error: 0.073915/32 [=============>................] - ETA: 0s - loss: 0.0695 - mean_squared_error: 0.069517/32 [==============>...............] - ETA: 0s - loss: 0.0657 - mean_squared_error: 0.065719/32 [================>.............] - ETA: 0s - loss: 0.0619 - mean_squared_error: 0.061920/32 [=================>............] - ETA: 0s - loss: 0.0599 - mean_squared_error: 0.059922/32 [===================>..........] - ETA: 0s - loss: 0.0562 - mean_squared_error: 0.056224/32 [=====================>........] - ETA: 0s - loss: 0.0579 - mean_squared_error: 0.057926/32 [=======================>......] - ETA: 0s - loss: 0.0543 - mean_squared_error: 0.054328/32 [=========================>....] - ETA: 0s - loss: 0.0512 - mean_squared_error: 0.051230/32 [===========================>..] - ETA: 0s - loss: 0.0538 - mean_squared_error: 0.053832/32 [==============================] - ETA: 0s - loss: 0.0511 - mean_squared_error: 0.051132/32 [==============================] - 39s 132ms/step - loss: 0.0511 - mean_squared_error: 0.0511 - val_loss: 0.0102 - val_mean_squared_error: 0.0102
Epoch 2/5
 1/32 [..............................] - ETA: 1s - loss: 0.0192 - mean_squared_error: 0.0192 3/32 [=>............................] - ETA: 1s - loss: 0.0407 - mean_squared_error: 0.0407 5/32 [===>..........................] - ETA: 1s - loss: 0.0323 - mean_squared_error: 0.0323 7/32 [=====>........................] - ETA: 1s - loss: 0.0432 - mean_squared_error: 0.0432 9/32 [=======>......................] - ETA: 1s - loss: 0.0400 - mean_squared_error: 0.040011/32 [=========>....................] - ETA: 0s - loss: 0.0356 - mean_squared_error: 0.035613/32 [===========>..................] - ETA: 0s - loss: 0.0331 - mean_squared_error: 0.033115/32 [=============>................] - ETA: 0s - loss: 0.0306 - mean_squared_error: 0.030617/32 [==============>...............] - ETA: 0s - loss: 0.0329 - mean_squared_error: 0.032919/32 [================>.............] - ETA: 0s - loss: 0.0332 - mean_squared_error: 0.033221/32 [==================>...........] - ETA: 0s - loss: 0.0320 - mean_squared_error: 0.032023/32 [====================>.........] - ETA: 0s - loss: 0.0299 - mean_squared_error: 0.029925/32 [======================>.......] - ETA: 0s - loss: 0.0310 - mean_squared_error: 0.031026/32 [=======================>......] - ETA: 0s - loss: 0.0301 - mean_squared_error: 0.030128/32 [=========================>....] - ETA: 0s - loss: 0.0283 - mean_squared_error: 0.028330/32 [===========================>..] - ETA: 0s - loss: 0.0268 - mean_squared_error: 0.026832/32 [==============================] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.025532/32 [==============================] - 2s 48ms/step - loss: 0.0255 - mean_squared_error: 0.0255 - val_loss: 0.0083 - val_mean_squared_error: 0.0083
Epoch 3/5
 1/32 [..............................] - ETA: 1s - loss: 0.0182 - mean_squared_error: 0.0182 3/32 [=>............................] - ETA: 1s - loss: 0.0129 - mean_squared_error: 0.0129 5/32 [===>..........................] - ETA: 1s - loss: 0.0230 - mean_squared_error: 0.0230 7/32 [=====>........................] - ETA: 1s - loss: 0.0267 - mean_squared_error: 0.0267 9/32 [=======>......................] - ETA: 1s - loss: 0.0228 - mean_squared_error: 0.022811/32 [=========>....................] - ETA: 0s - loss: 0.0212 - mean_squared_error: 0.021213/32 [===========>..................] - ETA: 0s - loss: 0.0191 - mean_squared_error: 0.019115/32 [=============>................] - ETA: 0s - loss: 0.0179 - mean_squared_error: 0.017917/32 [==============>...............] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.016819/32 [================>.............] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.021421/32 [==================>...........] - ETA: 0s - loss: 0.0198 - mean_squared_error: 0.019823/32 [====================>.........] - ETA: 0s - loss: 0.0183 - mean_squared_error: 0.018325/32 [======================>.......] - ETA: 0s - loss: 0.0207 - mean_squared_error: 0.020727/32 [========================>.....] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.023829/32 [==========================>...] - ETA: 0s - loss: 0.0236 - mean_squared_error: 0.023631/32 [============================>.] - ETA: 0s - loss: 0.0232 - mean_squared_error: 0.023232/32 [==============================] - 1s 46ms/step - loss: 0.0227 - mean_squared_error: 0.0227 - val_loss: 0.0123 - val_mean_squared_error: 0.0123
Epoch 4/5
 1/32 [..............................] - ETA: 0s - loss: 0.0206 - mean_squared_error: 0.0206 3/32 [=>............................] - ETA: 1s - loss: 0.0348 - mean_squared_error: 0.0348 5/32 [===>..........................] - ETA: 1s - loss: 0.0245 - mean_squared_error: 0.0245 7/32 [=====>........................] - ETA: 0s - loss: 0.0281 - mean_squared_error: 0.0281 9/32 [=======>......................] - ETA: 0s - loss: 0.0242 - mean_squared_error: 0.024211/32 [=========>....................] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.021013/32 [===========>..................] - ETA: 0s - loss: 0.0263 - mean_squared_error: 0.026315/32 [=============>................] - ETA: 0s - loss: 0.0259 - mean_squared_error: 0.025917/32 [==============>...............] - ETA: 0s - loss: 0.0234 - mean_squared_error: 0.023419/32 [================>.............] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.022621/32 [==================>...........] - ETA: 0s - loss: 0.0255 - mean_squared_error: 0.025523/32 [====================>.........] - ETA: 0s - loss: 0.0238 - mean_squared_error: 0.023825/32 [======================>.......] - ETA: 0s - loss: 0.0226 - mean_squared_error: 0.022627/32 [========================>.....] - ETA: 0s - loss: 0.0217 - mean_squared_error: 0.021729/32 [==========================>...] - ETA: 0s - loss: 0.0208 - mean_squared_error: 0.020831/32 [============================>.] - ETA: 0s - loss: 0.0197 - mean_squared_error: 0.019732/32 [==============================] - 1s 39ms/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0055 - val_mean_squared_error: 0.0055
Epoch 5/5
 1/32 [..............................] - ETA: 1s - loss: 0.0014 - mean_squared_error: 0.0014 3/32 [=>............................] - ETA: 1s - loss: 0.0036 - mean_squared_error: 0.0036 5/32 [===>..........................] - ETA: 1s - loss: 0.0132 - mean_squared_error: 0.0132 7/32 [=====>........................] - ETA: 0s - loss: 0.0110 - mean_squared_error: 0.0110 9/32 [=======>......................] - ETA: 0s - loss: 0.0091 - mean_squared_error: 0.009111/32 [=========>....................] - ETA: 0s - loss: 0.0135 - mean_squared_error: 0.013513/32 [===========>..................] - ETA: 0s - loss: 0.0158 - mean_squared_error: 0.015815/32 [=============>................] - ETA: 0s - loss: 0.0156 - mean_squared_error: 0.015617/32 [==============>...............] - ETA: 0s - loss: 0.0149 - mean_squared_error: 0.014919/32 [================>.............] - ETA: 0s - loss: 0.0154 - mean_squared_error: 0.015421/32 [==================>...........] - ETA: 0s - loss: 0.0193 - mean_squared_error: 0.019323/32 [====================>.........] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.018525/32 [======================>.......] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.019927/32 [========================>.....] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.018829/32 [==========================>...] - ETA: 0s - loss: 0.0178 - mean_squared_error: 0.017831/32 [============================>.] - ETA: 0s - loss: 0.0170 - mean_squared_error: 0.017032/32 [==============================] - 1s 39ms/step - loss: 0.0164 - mean_squared_error: 0.0164 - val_loss: 0.0043 - val_mean_squared_error: 0.0043
(24992, 1, 5)
Model: "sequential_1199"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 24992, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 6248, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 6248, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1562, 6)          0         
                                                                 
=================================================================
Total params: 411
Trainable params: 411
Non-trainable params: 0
_________________________________________________________________
Model: "valence_NN"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 inputGSR (InputLayer)          [(None, 24992, 1)]   0           []                               
                                                                                                  
 inputPPG (InputLayer)          [(None, 24992, 1)]   0           []                               
                                                                                                  
 sequential_1198 (Sequential)   (None, 1562, 6)      411         ['inputGSR[0][0]']               
                                                                                                  
 sequential_1199 (Sequential)   (None, 1562, 6)      411         ['inputPPG[0][0]']               
                                                                                                  
 concatenate_599 (Concatenate)  (None, 1562, 12)     0           ['sequential_1198[0][0]',        
                                                                  'sequential_1199[0][0]']        
                                                                                                  
 permute_599 (Permute)          (None, 12, 1562)     0           ['concatenate_599[0][0]']        
                                                                                                  
 flatten_599 (Flatten)          (None, 18744)        0           ['permute_599[0][0]']            
                                                                                                  
 dropout_599 (Dropout)          (None, 18744)        0           ['flatten_599[0][0]']            
                                                                                                  
 dense_599 (Dense)              (None, 1)            18745       ['dropout_599[0][0]']            
                                                                                                  
==================================================================================================
Total params: 19,567
Trainable params: 19,567
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/10
 1/32 [..............................] - ETA: 8s - loss: 0.7796 - binary_accuracy: 0.0000e+00 9/32 [=======>......................] - ETA: 0s - loss: 0.9228 - binary_accuracy: 0.4444    18/32 [===============>..............] - ETA: 0s - loss: 0.7815 - binary_accuracy: 0.444427/32 [========================>.....] - ETA: 0s - loss: 0.8998 - binary_accuracy: 0.481532/32 [==============================] - 1s 9ms/step - loss: 0.8794 - binary_accuracy: 0.5312 - val_loss: 0.1338 - val_binary_accuracy: 1.0000
Epoch 2/10
 1/32 [..............................] - ETA: 0s - loss: 0.0968 - binary_accuracy: 1.000015/32 [=============>................] - ETA: 0s - loss: 0.7497 - binary_accuracy: 0.400028/32 [=========================>....] - ETA: 0s - loss: 0.6969 - binary_accuracy: 0.464332/32 [==============================] - 0s 4ms/step - loss: 0.6400 - binary_accuracy: 0.5312 - val_loss: 0.2929 - val_binary_accuracy: 1.0000
Epoch 3/10
 1/32 [..............................] - ETA: 0s - loss: 0.4265 - binary_accuracy: 1.000014/32 [============>.................] - ETA: 0s - loss: 0.5441 - binary_accuracy: 0.785727/32 [========================>.....] - ETA: 0s - loss: 0.5972 - binary_accuracy: 0.703732/32 [==============================] - 0s 4ms/step - loss: 0.5886 - binary_accuracy: 0.7188 - val_loss: 0.5883 - val_binary_accuracy: 0.7500
Epoch 4/10
 1/32 [..............................] - ETA: 0s - loss: 0.3558 - binary_accuracy: 1.000015/32 [=============>................] - ETA: 0s - loss: 0.3728 - binary_accuracy: 0.866729/32 [==========================>...] - ETA: 0s - loss: 0.4654 - binary_accuracy: 0.758632/32 [==============================] - 0s 4ms/step - loss: 0.4646 - binary_accuracy: 0.7500 - val_loss: 0.4532 - val_binary_accuracy: 1.0000
Epoch 5/10
 1/32 [..............................] - ETA: 0s - loss: 0.5552 - binary_accuracy: 1.000014/32 [============>.................] - ETA: 0s - loss: 0.4653 - binary_accuracy: 0.785728/32 [=========================>....] - ETA: 0s - loss: 0.3914 - binary_accuracy: 0.821432/32 [==============================] - 0s 4ms/step - loss: 0.4246 - binary_accuracy: 0.7812 - val_loss: 0.4651 - val_binary_accuracy: 0.7500
Epoch 6/10
 1/32 [..............................] - ETA: 0s - loss: 0.4335 - binary_accuracy: 1.000015/32 [=============>................] - ETA: 0s - loss: 0.3477 - binary_accuracy: 0.866728/32 [=========================>....] - ETA: 0s - loss: 0.3221 - binary_accuracy: 0.857132/32 [==============================] - 0s 4ms/step - loss: 0.3279 - binary_accuracy: 0.8750 - val_loss: 0.3140 - val_binary_accuracy: 1.0000
Epoch 7/10
 1/32 [..............................] - ETA: 0s - loss: 0.5319 - binary_accuracy: 1.000015/32 [=============>................] - ETA: 0s - loss: 0.3379 - binary_accuracy: 0.933328/32 [=========================>....] - ETA: 0s - loss: 0.2351 - binary_accuracy: 0.964332/32 [==============================] - 0s 4ms/step - loss: 0.2864 - binary_accuracy: 0.9062 - val_loss: 0.3171 - val_binary_accuracy: 1.0000
Epoch 8/10
 1/32 [..............................] - ETA: 0s - loss: 0.0321 - binary_accuracy: 1.000015/32 [=============>................] - ETA: 0s - loss: 0.0973 - binary_accuracy: 1.000028/32 [=========================>....] - ETA: 0s - loss: 0.2154 - binary_accuracy: 0.892932/32 [==============================] - 0s 4ms/step - loss: 0.2629 - binary_accuracy: 0.8438 - val_loss: 0.5904 - val_binary_accuracy: 0.5000
Epoch 9/10
 1/32 [..............................] - ETA: 0s - loss: 0.1565 - binary_accuracy: 1.000015/32 [=============>................] - ETA: 0s - loss: 0.3015 - binary_accuracy: 0.866728/32 [=========================>....] - ETA: 0s - loss: 0.2454 - binary_accuracy: 0.892932/32 [==============================] - 0s 4ms/step - loss: 0.2634 - binary_accuracy: 0.8750 - val_loss: 0.2501 - val_binary_accuracy: 1.0000
Epoch 10/10
 1/32 [..............................] - ETA: 0s - loss: 0.0458 - binary_accuracy: 1.000015/32 [=============>................] - ETA: 0s - loss: 0.2173 - binary_accuracy: 1.000028/32 [=========================>....] - ETA: 0s - loss: 0.1764 - binary_accuracy: 1.000032/32 [==============================] - 0s 4ms/step - loss: 0.1834 - binary_accuracy: 1.0000 - val_loss: 0.4666 - val_binary_accuracy: 0.7500
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 56ms/step
predicted [0.44512478 0.5775711  0.251602   0.5865489  0.14523764 0.42285746
 0.95445293 0.42594442 0.9206137 ]
predicted [0 1 0 1 0 0 1 0 1]
expected [ True  True False  True  True False  True  True False]
accuracy: 0.5555555555555556
confusion matrix: 
[[2 1]
 [3 3]]
              precision    recall  f1-score   support

       False       0.40      0.67      0.50         3
        True       0.75      0.50      0.60         6

    accuracy                           0.56         9
   macro avg       0.57      0.58      0.55         9
weighted avg       0.63      0.56      0.57         9

macro avg f1-score: 0.55
macro avg (UAR): 0.5833333333333333
Sensitivity:  0.6666666666666666
Specificity:  0.5
g-mean:  0.5773502691896257
-------- Model Performance ----------: 
accuracy:  [0.77777778 1.         0.55555556 0.88888889 0.66666667 0.66666667
 0.88888889 0.66666667 0.66666667 0.55555556]
gmean:  [0.74535599 1.         0.57735027 0.81649658 0.66666667 0.
 0.91287093 0.66666667 0.66666667 0.57735027]
f1_score:  [0.75       1.         0.55       0.86153846 0.64935065 0.4
 0.88311688 0.64935065 0.64935065 0.55      ]
UAR:  [0.75       1.         0.58333333 0.83333333 0.66666667 0.5
 0.91666667 0.66666667 0.66666667 0.58333333]
Cohen Kappa score:  [0.5        1.         0.14285714 0.72727273 0.30769231 0.
 0.76923077 0.30769231 0.30769231 0.14285714]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  24.992
step (sec):  24.992
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  45
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.85   0.933  0.935  0.882  0.911  0.245  0.65   0.65   0.481  0.572]
 [ 0.962  0.933  0.955  0.882  0.923  0.546  0.708  0.71   0.696  0.665]
 [ 0.762  0.842  0.885  0.939  0.832  0.623  0.708  0.74   0.622  0.659]
 [ 0.     0.5    0.625  0.     0.383  0.     0.5    0.625  0.     0.383]
 [ 0.862  0.933  0.955  0.882  0.923  0.535  0.75   0.765  0.652  0.721]
 [ 0.421  0.717  0.733  0.663  0.694 -0.057  0.467  0.533  0.302  0.448]
 [ 0.     0.5    0.625  0.     0.383  0.     0.5    0.625  0.     0.383]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.43534799  0.725       0.71        0.68769745  0.69833333  0.26363636
   0.625       0.805       0.3         0.59047619]
 [ 0.48939394  0.75        0.76        0.70151672  0.73595238  0.35
   0.68333333  0.715       0.52556542  0.63535714]
 [ 0.52062937  0.75833333  0.765       0.66449237  0.73238095  0.25677656
   0.63333333  0.615       0.44711971  0.56190476]
 [ 0.29044289  0.64166667  0.645       0.58080604  0.62119048 -0.02045455
   0.4875      0.665       0.12844571  0.44920635]
 [ 0.60909091  0.8         0.835       0.69142136  0.77857143  0.34615385
   0.675       0.68        0.54318517  0.63416667]
 [ 0.47575758  0.73333333  0.755       0.66128842  0.71821429  0.58275058
   0.79166667  0.805       0.7222274   0.77202381]
 [ 0.31666667  0.65833333  0.68        0.55485474  0.635       0.22927073
   0.61666667  0.61        0.54234155  0.59119048]
 [ 0.80909091  0.9         0.94        0.84142136  0.8968254   0.40671329
   0.71666667  0.725       0.61481604  0.6747619 ]
 [ 0.37575758  0.69166667  0.73        0.52307101  0.65404762  0.125
   0.5625      0.69        0.25773503  0.52321429]
 [ 0.06491841  0.53333333  0.53        0.36927053  0.48214286  0.20582751
   0.60833333  0.605       0.54410536  0.58119048]
 [ 0.23881119  0.61666667  0.66        0.39378169  0.56285714  0.35909091
   0.68333333  0.755       0.42307101  0.6197619 ]
 [ 0.45454545  0.73333333  0.775       0.52307101  0.67404762  0.21565102
   0.60833333  0.625       0.51009536  0.5902381 ]
 [ 0.35426573  0.68333333  0.69        0.5722274   0.64511905  0.32948718
   0.66666667  0.665       0.62247449  0.65666667]
 [-0.02748918  0.48333333  0.56        0.22103434  0.43333333  0.38686314
   0.7         0.695       0.57996242  0.6547619 ]
 [-0.06969697  0.46666667  0.65        0.          0.39027778  0.05909091
   0.53333333  0.64        0.22307101  0.48107143]
 [-0.11053613  0.43333333  0.455       0.22700556  0.38392857  0.22575758
   0.60833333  0.76        0.29915638  0.5640873 ]
 [ 0.41220779  0.70833333  0.71        0.59831277  0.6677381  -0.01515152
   0.48333333  0.6         0.14142136  0.42964286]
 [        nan         nan         nan  0.5                nan  0.34941725
   0.66666667  0.69        0.53854107  0.63702381]
 [ 0.10606061  0.575       0.685       0.27320508  0.51369048  0.03534799
   0.51666667  0.5         0.36009536  0.4602381 ]
 [ 0.16515152  0.6         0.67        0.39174502  0.55738095  0.18484848
   0.59166667  0.625       0.41009536  0.55142857]
 [-0.06515152  0.47083333  0.67        0.08660254  0.42896825  0.07206127
   0.54166667  0.535       0.45545127  0.5102381 ]
 [ 0.11060606  0.55833333  0.585       0.31547005  0.49797619  0.3469697
   0.675       0.735       0.5222274   0.6472619 ]
 [ 0.09848485  0.55        0.63        0.24142136  0.49119048  0.02954545
   0.5125      0.755       0.07071068  0.46130952]
 [ 0.15734266  0.575       0.58        0.42366754  0.52952381  0.32727273
   0.65833333  0.73        0.48284271  0.63619048]
 [ 0.25677656  0.64166667  0.635       0.54948005  0.60083333  0.52435897
   0.76666667  0.76        0.73256985  0.75166667]
 [ 0.04009324  0.525       0.535       0.37163087  0.48547619  0.3010656
   0.65        0.65        0.6175833   0.63452381]
 [ 0.40454545  0.70833333  0.81        0.45236034  0.65626984  0.32435897
   0.66666667  0.66        0.55328053  0.62166667]
 [-0.1030303   0.43333333  0.53        0.12844571  0.38892857  0.35909091
   0.68333333  0.73        0.49378169  0.63690476]
 [ 0.05151515  0.525       0.6         0.19915638  0.45047619  0.05850816
   0.525       0.55        0.26783039  0.45607143]
 [ 0.85        0.93333333  0.935       0.88164966  0.91083333  0.2452381
   0.65        0.65        0.48112971  0.5725    ]]
KNN mean:
[0.26591719 0.63491379 0.67982759 0.45420358 0.5938451  0.24881824
 0.62625    0.67416667 0.44703109 0.58622487]
---------------------------
---------------------------
DT performance:
[[ 0.30524476  0.625       0.62        0.45092019  0.57761905  0.25909091
   0.7375      0.815       0.43400999  0.67412698]
 [ 0.19121212  0.56666667  0.58        0.55387706  0.51392857  0.49978355
   0.76666667  0.76        0.57996242  0.71178571]
 [ 0.20164835  0.66666667  0.66        0.64410536  0.6125      0.37777223
   0.69166667  0.68        0.50925175  0.63619048]
 [ 0.30164835  0.55833333  0.56        0.39378169  0.53619048  0.01013986
   0.59166667  0.66        0.35236034  0.55119048]
 [ 0.5152381   0.725       0.725       0.63854107  0.66571429  0.23403263
   0.61666667  0.62        0.55007658  0.57285714]
 [ 0.48381618  0.78333333  0.795       0.77366754  0.78071429  0.11620047
   0.55833333  0.565       0.55545127  0.54190476]
 [ 0.49227772  0.76666667  0.78        0.73172623  0.7352381   0.16868132
   0.53333333  0.535       0.30472067  0.48857143]
 [ 0.77062937  0.92083333  0.94        0.81504825  0.90063492  0.3852381
   0.725       0.715       0.63618073  0.665     ]
 [ 0.24025974  0.65        0.64        0.57339468  0.60452381  0.38381618
   0.61666667  0.645       0.58493019  0.58238095]
 [-0.02793873  0.475       0.465       0.35485474  0.42190476  0.32985348
   0.7         0.69        0.45092019  0.63857143]
 [ 0.16645022  0.55833333  0.56        0.42320508  0.49083333  0.39121212
   0.74166667  0.74        0.60925175  0.69190476]
 [ 0.53030303  0.825       0.84        0.58854107  0.81238095  0.3539627
   0.64166667  0.665       0.59831277  0.61035714]
 [-0.00952381  0.55        0.55        0.33020951  0.5175     -0.04848485
   0.53333333  0.55        0.35689141  0.48928571]
 [-0.06484848  0.48333333  0.555       0.13938469  0.43392857  0.42322677
   0.7         0.695       0.47543135  0.66452381]
 [ 0.08773227  0.55        0.63        0.3195942   0.5018254  -0.12848485
   0.46666667  0.535       0.22103434  0.38619048]
 [-0.10071262  0.43333333  0.465       0.30865522  0.40357143  0.02954545
   0.6         0.675       0.42844571  0.58928571]
 [ 0.81608392  0.89166667  0.885       0.87543135  0.87619048  0.31972028
   0.69166667  0.715       0.41009536  0.61154762]
 [        nan         nan         nan  0.2                nan  0.42062937
   0.64166667  0.68        0.51783039  0.61821429]
 [ 0.22272727  0.57916667  0.695       0.43896288  0.50694444  0.20164835
   0.49166667  0.495       0.3624557   0.4102381 ]
 [ 0.21095571  0.59166667  0.665       0.4624557   0.57785714  0.11363636
   0.6         0.615       0.61421951  0.57357143]
 [ 0.18841991  0.575       0.67        0.40485474  0.5031746  -0.04798535
   0.51666667  0.51        0.36783039  0.46857143]
 [ 0.12495837  0.56666667  0.565       0.30865522  0.5075      0.07682318
   0.53333333  0.6         0.32176494  0.49321429]
 [ 0.15426573  0.5         0.535       0.28080604  0.47142857  0.23008658
   0.70416667  0.755       0.59132321  0.62968254]
 [-0.09194139  0.575       0.57        0.48330044  0.55666667  0.20829171
   0.55833333  0.58        0.4624557   0.51928571]
 [-0.03636364  0.475       0.505       0.23020951  0.43107143  0.42835498
   0.71666667  0.71        0.64174502  0.68833333]
 [ 0.20201465  0.55833333  0.54        0.40925175  0.50690476  0.1
   0.525       0.54        0.47616194  0.5102381 ]
 [ 0.45075758  0.775       0.78        0.63896288  0.72559524  0.01391941
   0.54166667  0.545       0.25092019  0.46059524]
 [-0.09415584  0.475       0.555       0.15773503  0.43642857  0.06868132
   0.55833333  0.575       0.44174502  0.5225    ]
 [-0.06386946  0.625       0.665       0.43794454  0.57714286  0.26190476
   0.65        0.66        0.59174502  0.65      ]
 [ 0.96153846  0.93333333  0.955       0.88164966  0.92285714  0.54615385
   0.70833333  0.71        0.6955455   0.66511905]]
DT mean:
[0.24582165 0.6295977  0.65344828 0.47499088 0.58995758 0.22424836
 0.62194444 0.64116667 0.47976898 0.5771746 ]
---------------------------
---------------------------
RF performance:
[[ 0.38275058  0.725       0.725       0.60151672  0.7         0.175
   0.625       0.805       0.3         0.57559524]
 [ 0.38275058  0.575       0.595       0.40328053  0.53119048  0.18613054
   0.64166667  0.65        0.45401112  0.59833333]
 [ 0.3017982   0.59166667  0.605       0.54614203  0.55952381  0.17927073
   0.56666667  0.57        0.45925175  0.52904762]
 [ 0.17890443  0.54166667  0.555       0.62556542  0.50119048  0.13387446
   0.54166667  0.625       0.21153551  0.51095238]
 [ 0.52062937  0.775       0.795       0.61481604  0.75404762  0.36037296
   0.675       0.68        0.50211325  0.65238095]
 [ 0.38275058  0.675       0.685       0.69915638  0.65571429  0.37587413
   0.8         0.805       0.66698677  0.78809524]
 [ 0.05461538  0.53333333  0.57        0.55007658  0.51166667  0.1978022
   0.625       0.62        0.48140257  0.58916667]
 [ 0.76153846  0.87916667  0.9         0.82061253  0.85896825  0.51465534
   0.675       0.69        0.6124557   0.63285714]
 [ 0.28461538  0.70833333  0.685       0.41565965  0.6597619   0.16184149
   0.6         0.735       0.54948005  0.56944444]
 [ 0.06773227  0.43333333  0.42        0.45165079  0.37857143  0.38476523
   0.60833333  0.6         0.52760209  0.54333333]
 [ 0.06923077  0.65        0.665       0.41069189  0.63666667  0.39242424
   0.75        0.73        0.54614203  0.68821429]
 [ 0.57575758  0.8         0.82        0.7222274   0.78857143  0.37041292
   0.59166667  0.62        0.52996242  0.55535714]
 [ 0.18916084  0.59166667  0.59        0.64614203  0.53869048  0.15769231
   0.44166667  0.44        0.35236034  0.40333333]
 [-0.17853832  0.4625      0.585       0.31009536  0.44988095  0.34941725
   0.68333333  0.69        0.44614203  0.62702381]
 [ 0.08030303  0.53333333  0.61        0.29174502  0.46321429  0.13456876
   0.575       0.64        0.36783039  0.52321429]
 [-0.03636364  0.46666667  0.48        0.12247449  0.3977381   0.39972028
   0.66666667  0.735       0.31547005  0.60765873]
 [ 0.73311688  0.84166667  0.835       0.84614203  0.82619048  0.27575758
   0.65833333  0.72        0.45151672  0.61738095]
 [        nan         nan         nan  0.38660254         nan  0.49370629
   0.74166667  0.75        0.53938469  0.71095238]
 [ 0.30530303  0.70416667  0.765       0.35773503  0.64059524 -0.01212121
   0.575       0.59        0.54174502  0.54595238]
 [ 0.38939394  0.59166667  0.685       0.4624557   0.565       0.08636364
   0.53333333  0.515       0.36783039  0.48309524]
 [ 0.05797683  0.575       0.775       0.17071068  0.5        -0.21548452
   0.55        0.55        0.4124557   0.50916667]
 [ 0.0548951   0.61666667  0.6         0.43400999  0.57690476  0.15151515
   0.75        0.775       0.40865522  0.73761905]
 [ 0.16165501  0.54166667  0.555       0.42163087  0.48285714  0.23030303
   0.60416667  0.735       0.48660254  0.54706349]
 [-0.0504662   0.69166667  0.695       0.50721508  0.66583333  0.39978355
   0.725       0.755       0.63316638  0.7197619 ]
 [ 0.11491841  0.48333333  0.515       0.56009536  0.44702381  0.20805861
   0.71666667  0.715       0.72163087  0.70666667]
 [ 0.16153846  0.50833333  0.515       0.51698677  0.47583333  0.15198135
   0.6         0.61        0.46842692  0.5897619 ]
 [ 0.28181818  0.68333333  0.735       0.51009536  0.63039683  0.41483516
   0.60833333  0.61        0.47602788  0.56452381]
 [-0.03939394  0.53333333  0.625       0.24711971  0.4727381   0.09184149
   0.60833333  0.67        0.41783039  0.56738095]
 [-0.00909091  0.44166667  0.525       0.22247449  0.39904762  0.15850816
   0.55        0.565       0.37483482  0.5275    ]
 [ 0.76153846  0.84166667  0.885       0.93938469  0.83238095  0.62272727
   0.70833333  0.74        0.62163087  0.65940476]]
RF mean:
[0.23933927 0.62054598 0.655      0.49381704 0.58276546 0.25105328
 0.63319444 0.6645     0.47481615 0.59600794]
---------------------------
---------------------------
SVM performance:
[[0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.775      0.         0.43650794]
 [0.         0.5        0.575      0.         0.36369048 0.
  0.5        0.6        0.         0.37321429]
 [0.         0.5        0.575      0.         0.36369048 0.10454545
  0.55       0.525      0.17071068 0.4152381 ]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.715      0.         0.4156746 ]
 [0.         0.5        0.625      0.         0.3827381  0.2492008
  0.625      0.615      0.38080604 0.53928571]
 [0.15454545 0.575      0.57       0.17071068 0.4375     0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.65       0.         0.3922619  0.18311688
  0.59166667 0.57       0.22844571 0.45833333]
 [0.         0.5        0.715      0.         0.4156746  0.
  0.5        0.65       0.         0.3922619 ]
 [0.         0.5        0.65       0.         0.3922619  0.
  0.5        0.695      0.         0.40873016]
 [0.07472527 0.54166667 0.51       0.18020951 0.40214286 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.65       0.         0.3922619 ]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.575      0.         0.36369048]
 [0.         0.5        0.575      0.         0.36369048 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.715      0.         0.4156746  0.12857143
  0.56666667 0.53       0.15773503 0.41071429]
 [0.         0.5        0.695      0.         0.40873016 0.
  0.5        0.675      0.         0.40178571]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.695      0.         0.40873016]
 [0.         0.5        0.575      0.         0.36369048 0.
  0.5        0.65       0.         0.3922619 ]
 [       nan        nan        nan 0.4               nan 0.
  0.5        0.575      0.         0.36369048]
 [0.         0.5        0.735      0.         0.42261905 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.675      0.         0.40178571 0.
  0.5        0.65       0.         0.3922619 ]
 [0.         0.5        0.775      0.         0.43650794 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.625      0.         0.3827381 ]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.755      0.         0.42956349]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.6        0.         0.37321429]
 [0.         0.5        0.575      0.         0.36369048 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.715      0.         0.4156746  0.13766234
  0.56666667 0.55       0.19915638 0.43452381]
 [0.         0.5        0.65       0.         0.3922619  0.
  0.5        0.625      0.         0.3827381 ]
 [0.         0.5        0.65       0.         0.3922619  0.
  0.5        0.575      0.         0.36369048]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.625      0.         0.3827381 ]]
SVM mean:
[0.00790589 0.50402299 0.62       0.02503067 0.38504926 0.0267699
 0.51333333 0.609      0.03789513 0.39191005]
---------------------------
---------------------------
GBM performance:
[[ 0.40524476  0.65        0.655       0.63460652  0.64571429  0.3
   0.6         0.825       0.1         0.55079365]
 [ 0.42575758  0.71666667  0.745       0.61128842  0.68785714  0.51363636
   0.76666667  0.775       0.64614203  0.71940476]
 [ 0.47062937  0.75833333  0.765       0.64378169  0.72238095  0.21223776
   0.60833333  0.595       0.40092019  0.52809524]
 [-0.03681319  0.48333333  0.48        0.37700556  0.45666667  0.17651515
   0.52916667  0.675       0.32844571  0.5       ]
 [ 0.65909091  0.825       0.86        0.71213203  0.80190476  0.31666667
   0.71666667  0.73        0.58533712  0.70904762]
 [ 0.46586747  0.73333333  0.735       0.71902344  0.71904762  0.25
   0.625       0.635       0.59307214  0.61666667]
 [ 0.43766234  0.71666667  0.78        0.39915638  0.6672619   0.25656011
   0.625       0.625       0.57307101  0.61285714]
 [ 0.72575758  0.87083333  0.9         0.78575892  0.85301587  0.53608392
   0.75833333  0.76        0.63618073  0.70952381]
 [ 0.24941725  0.625       0.67        0.51565965  0.55821429 -0.10606061
   0.45        0.63        0.07071068  0.38134921]
 [-0.01255411  0.48333333  0.465       0.25629488  0.42047619  0.34487179
   0.65833333  0.645       0.38020951  0.60690476]
 [ 0.19638695  0.625       0.665       0.45412415  0.59059524  0.34575758
   0.68333333  0.705       0.48080604  0.61738095]
 [ 0.46818182  0.75833333  0.8         0.60591373  0.72892857  0.28787879
   0.65        0.68        0.49831277  0.62154762]
 [ 0.2027972   0.58333333  0.59        0.54318517  0.54940476  0.10606061
   0.51666667  0.54        0.40689141  0.49880952]
 [-0.10681818  0.47083333  0.63        0.07071068  0.42896825  0.38476523
   0.69166667  0.695       0.47543135  0.64369048]
 [ 0.22121212  0.63333333  0.76        0.27071068  0.57599206 -0.02848485
   0.49166667  0.6         0.12844571  0.41964286]
 [ 0.01666667  0.54166667  0.625       0.15773503  0.45119048  0.07121212
   0.53333333  0.675       0.22844571  0.49503968]
 [ 0.76608392  0.94166667  0.93        0.90472067  0.92666667  0.13333333
   0.575       0.65        0.23938469  0.49119048]
 [        nan         nan         nan  0.2                nan  0.28729604
   0.66666667  0.7         0.58854107  0.65690476]
 [ 0.08311688  0.54583333  0.695       0.15731322  0.4593254   0.23686314
   0.625       0.62        0.43854107  0.56142857]
 [ 0.32062937  0.65833333  0.75        0.48080604  0.64309524  0.23484848
   0.6         0.64        0.43938469  0.56380952]
 [ 0.15454545  0.5875      0.8         0.18660254  0.52539683  0.29125874
   0.65        0.665       0.40472067  0.5922619 ]
 [ 0.04898435  0.51666667  0.505       0.35485474  0.44714286  0.21282051
   0.60833333  0.69        0.35092019  0.55107143]
 [ 0.07272727  0.53333333  0.57        0.28080604  0.49095238  0.31818182
   0.6375      0.78        0.3280239   0.5968254 ]
 [-0.05989011  0.5         0.495       0.31258977  0.45666667  0.14010989
   0.55        0.595       0.31009536  0.48369048]
 [ 0.17121212  0.60833333  0.635       0.37844571  0.57285714  0.4532967
   0.74166667  0.735       0.59318517  0.70785714]
 [ 0.13686314  0.54166667  0.54        0.35151672  0.48166667  0.00128205
   0.45        0.47        0.31927053  0.42440476]
 [ 0.37575758  0.68333333  0.805       0.42307101  0.65496032 -0.03003663
   0.50833333  0.505       0.37163087  0.44107143]
 [-0.03333333  0.48333333  0.6         0.12844571  0.42452381 -0.00606061
   0.5         0.6         0.22844571  0.46607143]
 [ 0.08787879  0.55        0.645       0.21009536  0.4727381   0.26608392
   0.625       0.645       0.45472067  0.59035714]
 [ 0.86153846  0.93333333  0.955       0.88164966  0.92285714  0.53484848
   0.75        0.765       0.65151672  0.72059524]]
GBM mean:
[0.26808967 0.63994253 0.69137931 0.43360014 0.59780925 0.23472755
 0.61305556 0.66166667 0.40836011 0.56927646]
---------------------------
---------------------------
BDDAE performance:
[[-0.21247588  0.3925      0.38888889  0.28492019  0.35354423  0.21662902
   0.61071429  0.75555556  0.42686821  0.60203297]
 [ 0.002807    0.5         0.5         0.47465685  0.49119408  0.25459894
   0.6325      0.62222222  0.59636418  0.60833167]
 [-0.00902404  0.495       0.51111111  0.38801111  0.47053197  0.15294092
   0.575       0.56666667  0.51280925  0.54715368]
 [ 0.30315148  0.65        0.65555556  0.62296589  0.64066017  0.0509965
   0.525       0.62222222  0.23938469  0.4743032 ]
 [ 0.06914997  0.53333333  0.5         0.45057579  0.47717172  0.44027587
   0.725       0.71111111  0.69352411  0.70185786]
 [ 0.28930167  0.65        0.64444444  0.58836458  0.62369408  0.21258884
   0.61        0.6         0.5231715   0.56980603]
 [ 0.01454545  0.50833333  0.6         0.31028485  0.49450549  0.02329916
   0.515       0.51111111  0.38248165  0.47027223]
 [ 0.22692308  0.60833333  0.7         0.44443703  0.59028971  0.24834166
   0.625       0.66666667  0.59479002  0.61802697]
 [ 0.11454545  0.55        0.65555556  0.37845942  0.5360989   0.20545455
   0.59166667  0.72222222  0.27876937  0.54373626]
 [ 0.03415056  0.5175      0.52222222  0.47903132  0.50433455  0.29671585
   0.65        0.65555556  0.62754111  0.64080808]
 [ 0.19141858  0.6         0.64444444  0.52707316  0.58209624 -0.11944056
   0.43333333  0.5         0.30456564  0.42712676]
 [ 0.07437269  0.53333333  0.53333333  0.48060547  0.50191947  0.41677256
   0.7075      0.71111111  0.69506876  0.7034596 ]
 [-0.33865871  0.3275      0.32222222  0.18837042  0.28905844  0.26143206
   0.635       0.62222222  0.60674191  0.61332973]
 [-0.15846154  0.41666667  0.48888889  0.23628161  0.39168831  0.34742506
   0.675       0.67777778  0.65825513  0.66713203]
 [-0.2153047   0.4         0.48888889  0.13435429  0.37216533  0.21603397
   0.60833333  0.63333333  0.58852348  0.59786103]
 [-0.11667685  0.45        0.5         0.23189319  0.40686314  0.11454545
   0.55        0.66666667  0.29696479  0.51681319]
 [ 0.21606406  0.61        0.61111111  0.59963486  0.60427128  0.10286713
   0.54166667  0.62222222  0.40252908  0.53245754]
 [ 0.37243243  0.76875     0.74444444  0.70452404  0.65483516 -0.20524517
   0.3975      0.41111111  0.30128935  0.37205628]
 [ 0.0308657   0.52142857  0.64444444  0.35104891  0.4993544   0.12722504
   0.565       0.56666667  0.52931358  0.54766955]
 [ 0.22524476  0.60833333  0.67777778  0.4934877   0.59307692  0.11223776
   0.55833333  0.62222222  0.4337294   0.54363137]
 [ 0.21122024  0.61071429  0.75555556  0.41314104  0.58722527 -0.061295
   0.47        0.5         0.30928894  0.4290035 ]
 [ 0.02285972  0.51        0.53333333  0.41385335  0.49091658  0.24428571
   0.625       0.68888889  0.4862173   0.605     ]
 [ 0.14058442  0.575       0.58888889  0.51227993  0.5482906   0.44085263
   0.69642857  0.83333333  0.57878019  0.71006868]
 [-0.08702537  0.4575      0.46666667  0.40405353  0.44185065  0.28251658
   0.6375      0.65555556  0.59370376  0.62461427]
 [-0.08299188  0.4575      0.45555556  0.35085888  0.419503    0.01445789
   0.51        0.51111111  0.43735765  0.48189949]
 [-0.04034117  0.48        0.48888889  0.43807054  0.4707684   0.21480031
   0.605       0.62222222  0.56811933  0.59501998]
 [ 0.2879021   0.64166667  0.7         0.56722487  0.62965035  0.3354759
   0.675       0.65555556  0.62856802  0.64181457]
 [ 0.17576424  0.59166667  0.61111111  0.55258285  0.57905678  0.30223776
   0.65        0.7         0.61522403  0.64648851]
 [ 0.37350649  0.68333333  0.74444444  0.57358416  0.67013736  0.27239739
   0.6375      0.64444444  0.61057829  0.62645216]
 [ 0.42052947  0.71666667  0.73333333  0.6629424   0.69427073 -0.05671329
   0.46666667  0.53333333  0.30162091  0.44844544]]
BDDAE mean:
[0.08454598 0.54550198 0.58037037 0.44191907 0.52030078 0.18215702
 0.59015476 0.62703704 0.49407145 0.57022242]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.775      0.         0.43650794]
 [0.         0.5        0.575      0.         0.36369048 0.
  0.5        0.6        0.         0.37321429]
 [0.         0.5        0.575      0.         0.36369048 0.
  0.5        0.45       0.         0.30952381]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.715      0.         0.4156746 ]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.51       0.         0.33630952]
 [0.         0.5        0.51       0.         0.33630952 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.65       0.         0.3922619  0.
  0.5        0.51       0.         0.33630952]
 [0.         0.5        0.715      0.         0.4156746  0.
  0.5        0.65       0.         0.3922619 ]
 [0.         0.5        0.65       0.         0.3922619  0.
  0.5        0.695      0.         0.40873016]
 [0.         0.5        0.51       0.         0.33630952 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.65       0.         0.3922619 ]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.575      0.         0.36369048]
 [0.         0.5        0.575      0.         0.36369048 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.715      0.         0.4156746  0.
  0.5        0.45       0.         0.30952381]
 [0.         0.5        0.695      0.         0.40873016 0.
  0.5        0.675      0.         0.40178571]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.695      0.         0.40873016]
 [0.         0.5        0.575      0.         0.36369048 0.
  0.5        0.65       0.         0.3922619 ]
 [       nan        nan        nan 0.4               nan 0.
  0.5        0.575      0.         0.36369048]
 [0.         0.5        0.735      0.         0.42261905 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.675      0.         0.40178571 0.
  0.5        0.65       0.         0.3922619 ]
 [0.         0.5        0.775      0.         0.43650794 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.625      0.         0.3827381 ]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.755      0.         0.42956349]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.6        0.         0.37321429]
 [0.         0.5        0.575      0.         0.36369048 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.715      0.         0.4156746  0.
  0.5        0.45       0.         0.30952381]
 [0.         0.5        0.65       0.         0.3922619  0.
  0.5        0.625      0.         0.3827381 ]
 [0.         0.5        0.65       0.         0.3922619  0.
  0.5        0.575      0.         0.36369048]
 [0.         0.5        0.625      0.         0.3827381  0.
  0.5        0.625      0.         0.3827381 ]]
DUMMY mean:
[0.         0.5        0.61793103 0.01333333 0.37928982 0.
 0.5        0.595      0.         0.37001323]
---------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_1_2_53
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.266 0.635 0.68  0.453 0.594 0.245 0.625 0.674 0.444 0.584]
 [0.246 0.63  0.653 0.484 0.59  0.217 0.621 0.64  0.478 0.576]
 [0.239 0.621 0.655 0.498 0.583 0.243 0.629 0.662 0.473 0.592]
 [0.008 0.504 0.62  0.012 0.385 0.028 0.514 0.61  0.039 0.393]
 [0.268 0.64  0.691 0.442 0.598 0.233 0.611 0.66  0.402 0.566]
 [0.085 0.546 0.58  0.442 0.52  0.182 0.59  0.627 0.494 0.57 ]
 [0.    0.5   0.618 0.    0.379 0.    0.5   0.596 0.    0.37 ]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.254 0.128 0.113 0.22  0.141 0.152 0.078 0.076 0.169 0.086]
 [0.279 0.138 0.133 0.199 0.148 0.175 0.085 0.084 0.124 0.087]
 [0.247 0.124 0.121 0.195 0.135 0.166 0.077 0.087 0.113 0.082]
 [0.031 0.015 0.07  0.044 0.027 0.064 0.032 0.07  0.092 0.041]
 [0.259 0.132 0.132 0.226 0.148 0.168 0.089 0.08  0.159 0.094]
 [0.189 0.1   0.11  0.138 0.1   0.159 0.08  0.086 0.137 0.086]
 [0.    0.    0.072 0.    0.028 0.    0.    0.086 0.    0.034]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 96.  20.  17.  49.  24.  62.  12.  11.  38.  15.]
 [113.  22.  20.  41.  25.  80.  14.  13.  26.  15.]
 [103.  20.  18.  39.  23.  68.  12.  13.  24.  14.]
 [392.   3.  11. 364.   7. 231.   6.  11. 235.  10.]
 [ 97.  21.  19.  51.  25.  72.  15.  12.  40.  17.]
 [224.  18.  19.  31.  19.  87.  14.  14.  28.  15.]
 [  0.   0.  12.   0.   7.   0.   0.  14.   0.   9.]]
-------------------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_1_2_53
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  24.992
step (sec):  24.992
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  45
Elapsed time: 1510.8528017719586 minutes
Elapsed time: 25.180880029532645 hours
