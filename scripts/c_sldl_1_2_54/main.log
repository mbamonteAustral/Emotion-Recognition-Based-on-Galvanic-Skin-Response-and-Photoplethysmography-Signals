2024-05-11 01:26:18.631066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-11 01:26:18.960034: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-11 01:26:18.960273: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-11 01:26:18.960641: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-11 01:26:18.961379: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-11 01:26:18.961550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-11 01:26:18.961713: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-11 01:26:19.956309: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-11 01:26:19.956523: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-11 01:26:19.956701: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-11 01:26:19.956836: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
Window size (sec):  25.0
step (sec):  18.75
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  6.25
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
/home/marcos/Dropbox (Maestral)/c_sldl_1_2_54/functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:


_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 24992, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 6248, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 6248, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1562, 6)          0         
                                                                 
=================================================================
Total params: 411
Trainable params: 411
Non-trainable params: 0
_________________________________________________________________
Model: "valence_NN"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 inputGSR (InputLayer)          [(None, 24992, 1)]   0           []                               
                                                                                                  
 inputPPG (InputLayer)          [(None, 24992, 1)]   0           []                               
                                                                                                  
 sequential_1198 (Sequential)   (None, 1562, 6)      411         ['inputGSR[0][0]']               
                                                                                                  
 sequential_1199 (Sequential)   (None, 1562, 6)      411         ['inputPPG[0][0]']               
                                                                                                  
 concatenate_599 (Concatenate)  (None, 1562, 12)     0           ['sequential_1198[0][0]',        
                                                                  'sequential_1199[0][0]']        
                                                                                                  
 permute_599 (Permute)          (None, 12, 1562)     0           ['concatenate_599[0][0]']        
                                                                                                  
 flatten_599 (Flatten)          (None, 18744)        0           ['permute_599[0][0]']            
                                                                                                  
 dropout_599 (Dropout)          (None, 18744)        0           ['flatten_599[0][0]']            
                                                                                                  
 dense_599 (Dense)              (None, 1)            18745       ['dropout_599[0][0]']            
                                                                                                  
==================================================================================================
Total params: 19,567
Trainable params: 19,567
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/10
 1/43 [..............................] - ETA: 11s - loss: 0.6260 - binary_accuracy: 1.0000 9/43 [=====>........................] - ETA: 0s - loss: 0.6541 - binary_accuracy: 0.6667 18/43 [===========>..................] - ETA: 0s - loss: 0.8729 - binary_accuracy: 0.611130/43 [===================>..........] - ETA: 0s - loss: 0.8103 - binary_accuracy: 0.633342/43 [============================>.] - ETA: 0s - loss: 0.7798 - binary_accuracy: 0.619043/43 [==============================] - 1s 7ms/step - loss: 0.7911 - binary_accuracy: 0.6047 - val_loss: 0.7518 - val_binary_accuracy: 0.6000
Epoch 2/10
 1/43 [..............................] - ETA: 0s - loss: 0.2356 - binary_accuracy: 1.000010/43 [=====>........................] - ETA: 0s - loss: 0.8102 - binary_accuracy: 0.700019/43 [============>.................] - ETA: 0s - loss: 0.8208 - binary_accuracy: 0.684231/43 [====================>.........] - ETA: 0s - loss: 0.7891 - binary_accuracy: 0.645242/43 [============================>.] - ETA: 0s - loss: 0.7242 - binary_accuracy: 0.642943/43 [==============================] - 0s 6ms/step - loss: 0.7298 - binary_accuracy: 0.6279 - val_loss: 0.7020 - val_binary_accuracy: 0.6000
Epoch 3/10
 1/43 [..............................] - ETA: 0s - loss: 0.9021 - binary_accuracy: 0.0000e+0015/43 [=========>....................] - ETA: 0s - loss: 0.7673 - binary_accuracy: 0.3333    27/43 [=================>............] - ETA: 0s - loss: 0.6942 - binary_accuracy: 0.481539/43 [==========================>...] - ETA: 0s - loss: 0.6032 - binary_accuracy: 0.589743/43 [==============================] - 0s 4ms/step - loss: 0.6030 - binary_accuracy: 0.6047 - val_loss: 0.8176 - val_binary_accuracy: 0.6000
Epoch 4/10
 1/43 [..............................] - ETA: 0s - loss: 0.1326 - binary_accuracy: 1.000014/43 [========>.....................] - ETA: 0s - loss: 0.8180 - binary_accuracy: 0.428627/43 [=================>............] - ETA: 0s - loss: 0.7949 - binary_accuracy: 0.444440/43 [==========================>...] - ETA: 0s - loss: 0.6497 - binary_accuracy: 0.575043/43 [==============================] - 0s 4ms/step - loss: 0.6581 - binary_accuracy: 0.5814 - val_loss: 0.8294 - val_binary_accuracy: 0.6000
Epoch 5/10
 1/43 [..............................] - ETA: 0s - loss: 1.0031 - binary_accuracy: 0.0000e+0014/43 [========>.....................] - ETA: 0s - loss: 0.7179 - binary_accuracy: 0.5714    27/43 [=================>............] - ETA: 0s - loss: 0.5789 - binary_accuracy: 0.703740/43 [==========================>...] - ETA: 0s - loss: 0.5667 - binary_accuracy: 0.700043/43 [==============================] - 0s 4ms/step - loss: 0.5643 - binary_accuracy: 0.6977 - val_loss: 0.6272 - val_binary_accuracy: 0.4000
Epoch 6/10
 1/43 [..............................] - ETA: 0s - loss: 0.9489 - binary_accuracy: 0.0000e+0014/43 [========>.....................] - ETA: 0s - loss: 0.6340 - binary_accuracy: 0.7143    27/43 [=================>............] - ETA: 0s - loss: 0.5783 - binary_accuracy: 0.703739/43 [==========================>...] - ETA: 0s - loss: 0.6135 - binary_accuracy: 0.641043/43 [==============================] - 0s 5ms/step - loss: 0.5778 - binary_accuracy: 0.6744 - val_loss: 0.6245 - val_binary_accuracy: 0.6000
Epoch 7/10
 1/43 [..............................] - ETA: 0s - loss: 1.2607 - binary_accuracy: 0.0000e+0014/43 [========>.....................] - ETA: 0s - loss: 0.4142 - binary_accuracy: 0.9286    26/43 [=================>............] - ETA: 0s - loss: 0.4116 - binary_accuracy: 0.884638/43 [=========================>....] - ETA: 0s - loss: 0.4569 - binary_accuracy: 0.815843/43 [==============================] - 0s 5ms/step - loss: 0.4444 - binary_accuracy: 0.8140 - val_loss: 0.7415 - val_binary_accuracy: 0.6000
Epoch 8/10
 1/43 [..............................] - ETA: 0s - loss: 0.0155 - binary_accuracy: 1.000013/43 [========>.....................] - ETA: 0s - loss: 0.5140 - binary_accuracy: 0.692326/43 [=================>............] - ETA: 0s - loss: 0.4511 - binary_accuracy: 0.730838/43 [=========================>....] - ETA: 0s - loss: 0.3873 - binary_accuracy: 0.789543/43 [==============================] - 0s 5ms/step - loss: 0.3857 - binary_accuracy: 0.8140 - val_loss: 0.6388 - val_binary_accuracy: 0.6000
Epoch 9/10
 1/43 [..............................] - ETA: 0s - loss: 0.1407 - binary_accuracy: 1.000013/43 [========>.....................] - ETA: 0s - loss: 0.3874 - binary_accuracy: 0.846225/43 [================>.............] - ETA: 0s - loss: 0.3702 - binary_accuracy: 0.840038/43 [=========================>....] - ETA: 0s - loss: 0.3565 - binary_accuracy: 0.868443/43 [==============================] - 0s 5ms/step - loss: 0.3575 - binary_accuracy: 0.8605 - val_loss: 0.7645 - val_binary_accuracy: 0.6000
Epoch 10/10
 1/43 [..............................] - ETA: 0s - loss: 0.2695 - binary_accuracy: 1.000015/43 [=========>....................] - ETA: 0s - loss: 0.2503 - binary_accuracy: 0.933328/43 [==================>...........] - ETA: 0s - loss: 0.2860 - binary_accuracy: 0.928641/43 [===========================>..] - ETA: 0s - loss: 0.2783 - binary_accuracy: 0.951243/43 [==============================] - 0s 4ms/step - loss: 0.2857 - binary_accuracy: 0.9302 - val_loss: 0.8325 - val_binary_accuracy: 0.6000
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 59ms/step
predicted [0.32699826 0.85547334 0.01849787 0.7993528  0.605514   0.6411272
 0.28073552 0.02545027 0.7283329  0.4163061  0.83302855 0.7073225
 0.8030308 ]
predicted [0 1 0 1 1 1 0 0 1 0 1 1 1]
expected [False  True  True  True  True False False  True  True False False  True
  True]
accuracy: 0.6923076923076923
confusion matrix: 
[[3 2]
 [2 6]]
              precision    recall  f1-score   support

       False       0.60      0.60      0.60         5
        True       0.75      0.75      0.75         8

    accuracy                           0.69        13
   macro avg       0.68      0.68      0.68        13
weighted avg       0.69      0.69      0.69        13

macro avg f1-score: 0.675
macro avg (UAR): 0.675
Sensitivity:  0.6
Specificity:  0.75
g-mean:  0.6708203932499369
-------- Model Performance ----------: 
accuracy:  [0.53846154 0.53846154 0.69230769 0.69230769 0.61538462 0.61538462
 0.30769231 0.69230769 0.69230769 0.69230769]
gmean:  [0.54772256 0.54772256 0.67082039 0.59160798 0.41833001 0.54772256
 0.31622777 0.59160798 0.70710678 0.67082039]
f1_score:  [0.53571429 0.53571429 0.675      0.63888889 0.5112782  0.5751634
 0.30769231 0.63888889 0.69047619 0.675     ]
UAR:  [0.5875 0.55   0.675  0.6375 0.5375 0.575  0.325  0.6375 0.75   0.675 ]
Cohen Kappa score:  [ 0.15217391  0.09302326  0.35        0.2972973   0.08450704  0.15584416
 -0.31460674  0.2972973   0.43478261  0.35      ]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  24.992
step (sec):  18.752
overlap:  True
perc. of overlap:  24.96798975672215
overlap duration (sec):  6.24
Number of windows / instances:  61
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.907  0.958  0.95   0.952  0.95   0.381  0.696  0.752  0.548  0.666]
 [ 0.694  0.867  0.888  0.813  0.872  0.456  0.796  0.807  0.696  0.781]
 [ 0.779  0.875  0.883  0.873  0.868  0.354  0.746  0.79   0.597  0.724]
 [ 0.     0.5    0.624  0.     0.383  0.     0.5    0.657  0.     0.396]
 [ 0.694  0.842  0.871  0.794  0.843  0.44   0.704  0.76   0.553  0.697]
 [ 0.19   0.595  0.608  0.561  0.578 -0.078  0.457  0.508  0.4    0.449]
 [ 0.     0.5    0.624  0.     0.383  0.     0.5    0.657  0.     0.396]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.66969697  0.83333333  0.83809524  0.8121027   0.82654762  0.51345756
   0.775       0.82380952  0.663096    0.73462121]
 [ 0.3080303   0.6625      0.6547619   0.61812865  0.63761905  0.33095238
   0.65833333  0.70714286  0.52021147  0.63691919]
 [ 0.37862138  0.6875      0.7047619   0.58365344  0.65198413  0.42222222
   0.7125      0.70714286  0.65252919  0.6897619 ]
 [ 0.53866667  0.775       0.76904762  0.74743123  0.75738095 -0.02714286
   0.49        0.63809524  0.17071068  0.46323232]
 [ 0.64112554  0.80833333  0.8547619   0.72760209  0.80083333  0.41666667
   0.70833333  0.71666667  0.65806728  0.69666667]
 [ 0.54166667  0.77083333  0.77142857  0.75386846  0.76380952  0.70956522
   0.85833333  0.85238095  0.84006135  0.84599206]
 [ 0.41313665  0.69583333  0.73571429  0.64644029  0.69829365  0.46956522
   0.73333333  0.73571429  0.65705168  0.70753968]
 [ 0.66857143  0.84        0.86666667  0.77228543  0.81777778  0.57936508
   0.79583333  0.79047619  0.77270587  0.7768254 ]
 [ 0.27380952  0.63333333  0.7047619   0.47336928  0.62055556  0.15642857
   0.5675      0.67619048  0.37336928  0.54843074]
 [ 0.18933333  0.6         0.59047619  0.55756807  0.57869048  0.08424908
   0.55        0.5547619   0.46420463  0.52059524]
 [ 0.25171429  0.62083333  0.69047619  0.47163087  0.59960317  0.34202899
   0.675       0.72380952  0.52834756  0.64884921]
 [ 0.43866667  0.70833333  0.76904762  0.59823536  0.7015873   0.37971861
   0.7         0.70952381  0.58757746  0.65789683]
 [ 0.36136364  0.68333333  0.68809524  0.5799765   0.64642857  0.28266667
   0.64166667  0.65714286  0.5213666   0.60535714]
 [ 0.0302381   0.5175      0.58809524  0.38179602  0.50861111  0.416
   0.70833333  0.70714286  0.60755948  0.67511905]
 [ 0.00623249  0.495       0.60714286  0.23194792  0.47092713  0.23266046
   0.60833333  0.65238095  0.53876193  0.61027778]
 [ 0.0224026   0.51666667  0.58809524  0.23563894  0.45440476  0.17272727
   0.575       0.65952381  0.4053172   0.56444444]
 [ 0.19547619  0.59166667  0.6047619   0.50530253  0.5665873   0.07142857
   0.5375      0.57619048  0.38922988  0.50190476]
 [        nan         nan         nan  0.28164966         nan  0.41428571
   0.70416667  0.72142857  0.68437351  0.7009127 ]
 [ 0.1404489   0.5875      0.67380952  0.35545222  0.53551587  0.29230769
   0.64583333  0.64285714  0.57449042  0.6175    ]
 [ 0.30422078  0.64166667  0.7047619   0.51770685  0.63130952  0.2017619
   0.60416667  0.66904762  0.36450645  0.55011905]
 [-0.07538961  0.475       0.6547619   0.1244828   0.42743506  0.07645688
   0.53333333  0.52142857  0.46177904  0.50595238]
 [ 0.14166667  0.57083333  0.57142857  0.54579762  0.55904762  0.49580952
   0.75        0.76904762  0.70577493  0.74170635]
 [ 0.17214286  0.5875      0.60714286  0.46812865  0.55616522  0.23882353
   0.6325      0.76904762  0.39101747  0.59414141]
 [ 0.00564103  0.5         0.49285714  0.41354536  0.46095238  0.51969697
   0.75833333  0.77142857  0.73340132  0.75059524]
 [ 0.31433333  0.6625      0.65714286  0.62133261  0.6427381   0.60289855
   0.8         0.80238095  0.77979044  0.79361111]
 [ 0.24166667  0.625       0.63809524  0.52666389  0.59642857  0.33033333
   0.66666667  0.66904762  0.62326254  0.6522619 ]
 [ 0.47310924  0.7125      0.81904762  0.59497475  0.72424242  0.52564103
   0.7625      0.75952381  0.74907312  0.75595238]
 [ 0.16904762  0.58333333  0.63809524  0.48244631  0.58007937  0.46961905
   0.72916667  0.76904762  0.6476181   0.71412698]
 [ 0.32333333  0.67083333  0.67142857  0.61661111  0.64392857  0.31449275
   0.65833333  0.67857143  0.46378638  0.6034127 ]
 [ 0.90666667  0.95833333  0.95        0.95236034  0.94952381  0.38146998
   0.69583333  0.75238095  0.5480399   0.66626984]]
KNN mean:
[0.31191862 0.65568966 0.69326765 0.53993766 0.63479338 0.34720522
 0.67452778 0.70611111 0.56923604 0.65103319]
---------------------------
---------------------------
DT performance:
[[ 0.44666667  0.75416667  0.75        0.70757415  0.74154762  0.38659341
   0.76        0.82142857  0.53017421  0.71932179]
 [-0.00952381  0.47916667  0.50952381  0.42484341  0.4565873   0.50714286
   0.7625      0.77380952  0.67201979  0.73723665]
 [ 0.22555556  0.6         0.59285714  0.54090748  0.55535714  0.32222222
   0.6625      0.65714286  0.6124557   0.64535714]
 [ 0.356       0.68333333  0.67380952  0.61052849  0.66952381  0.33369565
   0.635       0.74047619  0.44687357  0.6235101 ]
 [ 0.3257619   0.68333333  0.67380952  0.66524837  0.64916667  0.52380952
   0.74166667  0.75238095  0.72968683  0.73805556]
 [ 0.175       0.57083333  0.57380952  0.47189514  0.53833333  0.38484848
   0.7125      0.7         0.7181748   0.69428571]
 [ 0.19502165  0.58333333  0.61904762  0.53321252  0.57412698  0.06969697
   0.4875      0.49047619  0.42641762  0.4577381 ]
 [ 0.88095238  0.8875      0.91666667  0.89873457  0.8784127   0.52380952
   0.81666667  0.82142857  0.75421124  0.80503968]
 [ 0.25809524  0.63333333  0.6547619   0.55872462  0.61075397  0.19726708
   0.6225      0.67380952  0.42326186  0.58944444]
 [ 0.16266667  0.64583333  0.63809524  0.52913703  0.61333333  0.0970696
   0.50833333  0.54285714  0.37449042  0.4825    ]
 [ 0.3463147   0.70416667  0.72380952  0.42586368  0.6843254   0.28880952
   0.6         0.62142857  0.43395526  0.55765873]
 [ 0.68628571  0.8375      0.86904762  0.7812931   0.82984127  0.62214286
   0.81666667  0.82142857  0.76698677  0.80599206]
 [ 0.16424242  0.55833333  0.54285714  0.50723582  0.50297619  0.10833333
   0.58333333  0.59285714  0.41737732  0.54944444]
 [ 0.1213147   0.6075      0.67380952  0.37581712  0.57265873  0.275
   0.65        0.65238095  0.53971694  0.61444444]
 [ 0.20020525  0.6125      0.67380952  0.49002225  0.59615079  0.32543956
   0.65        0.7047619   0.48729638  0.60698413]
 [ 0.14281385  0.5625      0.59285714  0.38926114  0.52361111  0.07996894
   0.575       0.61190476  0.52978771  0.57349206]
 [ 0.76366667  0.88333333  0.88571429  0.87948944  0.87964286  0.60036232
   0.83333333  0.82380952  0.79488209  0.81392857]
 [        nan         nan         nan  0.6                nan  0.50898268
   0.7375      0.7547619   0.6580004   0.72329365]
 [ 0.36739496  0.72        0.78571429  0.5497451   0.69656566  0.23333333
   0.675       0.67619048  0.65556073  0.66833333]
 [ 0.5152381   0.8125      0.81666667  0.69532716  0.78535714  0.00595238
   0.5625      0.58571429  0.43926114  0.54809524]
 [ 0.17465241  0.6225      0.67380952  0.27339895  0.55673882  0.47948718
   0.68333333  0.67619048  0.72457914  0.65642857]
 [ 0.18333333  0.675       0.67619048  0.54838431  0.66285714  0.33961039
   0.67916667  0.7         0.57574369  0.645     ]
 [ 0.15432712  0.6375      0.63571429  0.54021008  0.60563492  0.38595238
   0.6375      0.75238095  0.55474912  0.60426046]
 [ 0.20833333  0.60833333  0.60714286  0.56042762  0.58845238  0.4030303
   0.68333333  0.70714286  0.63760287  0.65624459]
 [ 0.39580952  0.6125      0.6047619   0.69711971  0.58047619  0.40533333
   0.70416667  0.70238095  0.67463581  0.6897619 ]
 [ 0.06702899  0.575       0.59047619  0.5115695   0.56242063  0.26702899
   0.65416667  0.65952381  0.63712439  0.64571429]
 [ 0.35123188  0.67        0.70238095  0.62012522  0.64972222  0.37536232
   0.6875      0.68809524  0.68468666  0.67904762]
 [ 0.10616318  0.52916667  0.54047619  0.43717032  0.51198413  0.305
   0.67083333  0.68809524  0.59616743  0.64821429]
 [ 0.13571429  0.57083333  0.58809524  0.42543135  0.50861111  0.30512422
   0.62083333  0.64047619  0.50381373  0.60369048]
 [ 0.69404762  0.86666667  0.88809524  0.81275396  0.87174603  0.45647619
   0.79583333  0.80714286  0.69597783  0.78119048]]
DT mean:
[0.30325222 0.6616092  0.67840722 0.56871505 0.63644536 0.33722952
 0.67363889 0.69468254 0.58985572 0.65212362]
---------------------------
---------------------------
RF performance:
[[ 0.66666667  0.82916667  0.83809524  0.7793913   0.82861111  0.18964803
   0.76        0.84285714  0.48614272  0.74489899]
 [ 0.21822323  0.6125      0.62142857  0.60248225  0.59444444  0.51837474
   0.775       0.79285714  0.59895544  0.76420635]
 [ 0.21365079  0.69166667  0.70238095  0.6639865   0.66404762  0.37948718
   0.67916667  0.67619048  0.67902422  0.655     ]
 [ 0.41758242  0.71666667  0.7047619   0.71152609  0.69297619  0.06718487
   0.57        0.68809524  0.19619361  0.53041486]
 [ 0.51731602  0.74166667  0.77142857  0.69308702  0.72746032  0.55714286
   0.69583333  0.70238095  0.68273598  0.68468254]
 [ 0.27948718  0.70416667  0.70238095  0.62423474  0.66809524  0.54289855
   0.79583333  0.78571429  0.821711    0.77837302]
 [ 0.4005176   0.67916667  0.71904762  0.6482041   0.67980159  0.04202899
   0.55833333  0.55952381  0.4775935   0.55404762]
 [ 0.665       0.9375      0.93333333  0.8280239   0.92730159  0.66480952
   0.74166667  0.7547619   0.7993565   0.71035714]
 [ 0.24440994  0.64583333  0.72142857  0.55049838  0.62702381  0.16742297
   0.56        0.62619048  0.53545475  0.53912698]
 [ 0.175       0.6         0.59047619  0.35925782  0.56809524  0.1770696
   0.525       0.5452381   0.54726924  0.48611111]
 [ 0.31821946  0.62083333  0.67380952  0.49873457  0.59710317  0.32612554
   0.60833333  0.63809524  0.51964893  0.60059524]
 [ 0.78628571  0.8375      0.83571429  0.75049838  0.82730159  0.52861284
   0.7375      0.7547619   0.56305223  0.71615079]
 [ 0.29454545  0.625       0.6047619   0.65547667  0.58107143  0.08012422
   0.59166667  0.60238095  0.44603256  0.57488095]
 [ 0.17789916  0.5825      0.65714286  0.54303326  0.55384921  0.43623188
   0.7         0.7047619   0.57827016  0.68714286]
 [-0.03987578  0.64        0.73571429  0.45333856  0.61712121  0.14842857
   0.61666667  0.65238095  0.52894409  0.59456349]
 [ 0.15646998  0.54166667  0.57142857  0.3609415   0.5118254   0.44428571
   0.7         0.74285714  0.60049838  0.69246032]
 [ 0.722       0.89166667  0.90238095  0.86185917  0.89170635  0.20547619
   0.6125      0.60714286  0.66413937  0.58333333]
 [        nan         nan         nan  0.39128709         nan  0.51428571
   0.75833333  0.77142857  0.70277     0.74551587]
 [ 0.37787115  0.7925      0.85        0.57228543  0.78444444  0.47333333
   0.65        0.63809524  0.69333055  0.61559524]
 [ 0.43842857  0.75        0.76666667  0.7539058   0.74210317  0.08650794
   0.57083333  0.57619048  0.35557481  0.54797619]
 [-0.03357143  0.555       0.78571429  0.23086408  0.53494949  0.14202899
   0.65416667  0.65714286  0.37794649  0.63202381]
 [ 0.32222222  0.62916667  0.62380952  0.6652171   0.59619048  0.56247619
   0.78333333  0.78571429  0.66831825  0.76718254]
 [ 0.24190476  0.67083333  0.68571429  0.55276392  0.63674603  0.35787115
   0.69        0.83571429  0.66259717  0.67828644]
 [ 0.20714286  0.57083333  0.57380952  0.53602593  0.56214286  0.38517316
   0.65833333  0.67380952  0.55989442  0.64119048]
 [ 0.63333333  0.68333333  0.68809524  0.64747271  0.67206349  0.44166667
   0.6875      0.68809524  0.67372694  0.68416667]
 [ 0.16956522  0.52916667  0.54285714  0.5386212   0.49440476  0.50533333
   0.65833333  0.67142857  0.60012272  0.64964286]
 [ 0.50238095  0.695       0.73571429  0.69191974  0.69134921  0.29230769
   0.70416667  0.7047619   0.56919586  0.69809524]
 [ 0.36464803  0.6         0.64285714  0.52602788  0.59519841  0.44295238
   0.79583333  0.8047619   0.7460873   0.7897619 ]
 [ 0.15289855  0.57916667  0.59285714  0.41931919  0.55265873  0.3
   0.67916667  0.69285714  0.54349395  0.66380952]
 [ 0.77928571  0.875       0.88333333  0.87307101  0.86849206  0.35377847
   0.74583333  0.79047619  0.59699617  0.72444444]]
RF mean:
[0.35756923 0.6837069  0.71231527 0.59944518 0.6651234  0.34443558
 0.67544444 0.69888889 0.58250258 0.65780123]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.54047619  0.          0.34969697  0.
   0.5         0.77142857  0.          0.43439394]
 [ 0.          0.5         0.57380952  0.          0.3630303   0.
   0.5         0.62380952  0.          0.3830303 ]
 [ 0.          0.5         0.57380952  0.          0.3630303   0.27948718
   0.64166667  0.63809524  0.56328717  0.61011905]
 [ 0.03636364  0.51666667  0.53809524  0.05773503  0.37166667  0.
   0.5         0.72142857  0.          0.4180303 ]
 [ 0.          0.5         0.62380952  0.          0.3830303   0.46666667
   0.73333333  0.75        0.65924316  0.71261905]
 [-0.01733333  0.49166667  0.49047619  0.05773503  0.34880952  0.52108037
   0.7625      0.75238095  0.72540202  0.73992063]
 [ 0.          0.5         0.64047619  0.          0.38969697  0.04166667
   0.52083333  0.52142857  0.07071068  0.37083333]
 [ 0.          0.5         0.68809524  0.          0.40712121  0.
   0.5         0.60714286  0.          0.37636364]
 [ 0.          0.5         0.65714286  0.          0.39636364  0.
   0.5         0.68809524  0.          0.40712121]
 [ 0.          0.5         0.52380952  0.          0.3430303   0.
   0.5         0.55714286  0.          0.35636364]
 [ 0.          0.5         0.65714286  0.          0.39636364  0.
   0.5         0.59047619  0.          0.36969697]
 [ 0.          0.5         0.62380952  0.          0.3830303   0.
   0.5         0.62380952  0.          0.3830303 ]
 [ 0.          0.5         0.57380952  0.          0.3630303   0.
   0.5         0.52380952  0.          0.3430303 ]
 [ 0.          0.5         0.68809524  0.          0.40712121 -0.06133333
   0.47083333  0.46904762  0.08660254  0.35      ]
 [ 0.          0.5         0.68809524  0.          0.40712121  0.
   0.5         0.64047619  0.          0.38969697]
 [ 0.          0.5         0.62380952  0.          0.3830303   0.
   0.5         0.65714286  0.          0.39636364]
 [ 0.          0.5         0.57380952  0.          0.3630303   0.
   0.5         0.59047619  0.          0.36969697]
 [        nan         nan         nan  0.3                nan  0.
   0.5         0.59047619  0.          0.36969697]
 [ 0.          0.5         0.73809524  0.          0.42348485  0.
   0.5         0.52380952  0.          0.3430303 ]
 [ 0.          0.5         0.65714286  0.          0.39636364  0.
   0.5         0.62380952  0.          0.3830303 ]
 [ 0.          0.5         0.77142857  0.          0.43439394  0.
   0.5         0.52380952  0.          0.3430303 ]
 [ 0.016       0.50833333  0.50714286  0.05773503  0.35714286  0.
   0.5         0.62380952  0.          0.3830303 ]
 [ 0.          0.5         0.60714286  0.          0.37636364  0.
   0.5         0.7547619   0.          0.42893939]
 [ 0.          0.5         0.52380952  0.          0.3430303   0.06969697
   0.53333333  0.57142857  0.11547005  0.4075    ]
 [ 0.          0.5         0.57380952  0.          0.3630303   0.04166667
   0.52083333  0.52142857  0.07071068  0.37083333]
 [ 0.          0.5         0.54047619  0.          0.34969697  0.03333333
   0.51666667  0.55714286  0.05773503  0.37886364]
 [ 0.          0.5         0.68809524  0.          0.40712121  0.02564103
   0.5125      0.50952381  0.19284076  0.40130952]
 [ 0.          0.5         0.65714286  0.          0.39636364  0.
   0.5         0.60714286  0.          0.37636364]
 [ 0.          0.5         0.60714286  0.          0.37636364  0.
   0.5         0.55714286  0.          0.35636364]
 [ 0.          0.5         0.62380952  0.          0.3830303   0.
   0.5         0.65714286  0.          0.39636364]]
SVM mean:
[0.00120794 0.50057471 0.61288998 0.0157735  0.38015823 0.04726352
 0.52375    0.6115873  0.0847334  0.41162217]
---------------------------
---------------------------
GBM performance:
[[ 0.59166667  0.8125      0.81666667  0.78832294  0.8027381   0.18142857
   0.595       0.78809524  0.29410963  0.56015152]
 [ 0.1482684   0.55416667  0.58809524  0.39030488  0.51349206  0.46309524
   0.775       0.80714286  0.70828814  0.76457792]
 [ 0.44295238  0.70833333  0.72142857  0.6591536   0.69162698  0.23703704
   0.58333333  0.57857143  0.55352406  0.55579365]
 [ 0.4         0.70416667  0.6952381   0.68156011  0.69047619  0.06583851
   0.54        0.73809524  0.12548293  0.47914141]
 [ 0.54707792  0.7875      0.82142857  0.71524837  0.7802381   0.43333333
   0.7         0.71666667  0.63366559  0.6825    ]
 [ 0.27948718  0.625       0.62142857  0.54209802  0.59892857  0.5544137
   0.77916667  0.76904762  0.75707242  0.76349206]
 [ 0.22261905  0.61666667  0.69047619  0.45357879  0.58441919 -0.06363636
   0.4875      0.49047619  0.36748598  0.47071429]
 [ 0.72380952  0.8625      0.9         0.75731322  0.84063492  0.62380952
   0.82916667  0.83809524  0.81029548  0.82218254]
 [ 0.26880952  0.64583333  0.68809524  0.51213203  0.61742063 -0.09710084
   0.4775      0.62142857  0.13194792  0.43361111]
 [-0.02036364  0.5125      0.50714286  0.32473395  0.46488095  0.11611722
   0.52083333  0.55952381  0.30865522  0.4715873 ]
 [ 0.35940994  0.65416667  0.72380952  0.42336928  0.62916667  0.36350649
   0.6625      0.68809524  0.56447771  0.64015873]
 [ 0.71428571  0.85        0.88333333  0.78856181  0.84412698  0.36547619
   0.70416667  0.73809524  0.59021259  0.68376984]
 [ 0.2280303   0.62083333  0.62142857  0.54705283  0.58619048  0.275
   0.62083333  0.62619048  0.61079743  0.60821429]
 [ 0.21263305  0.59        0.71904762  0.35049838  0.5419228   0.375
   0.70416667  0.7047619   0.62371834  0.6897619 ]
 [ 0.2302521   0.6         0.75238095  0.28284271  0.55757576  0.40595238
   0.70416667  0.76666667  0.53359871  0.66626984]
 [ 0.06214286  0.5125      0.59047619  0.21444371  0.44521284  0.2121118
   0.65        0.71190476  0.48460652  0.64690476]
 [ 0.76366667  0.86666667  0.86904762  0.85557481  0.85928571  0.45595238
   0.70833333  0.72619048  0.64060314  0.68376984]
 [        nan         nan         nan  0.4                nan  0.50779221
   0.74583333  0.77142857  0.6808713   0.73535714]
 [ 0.20857143  0.6175      0.73809524  0.28944272  0.55277778  0.22564103
   0.63333333  0.62619048  0.59105894  0.61464286]
 [ 0.17231602  0.59166667  0.67142857  0.38235151  0.55257937  0.03390476
   0.5         0.58571429  0.2280239   0.46055556]
 [ 0.03714286  0.515       0.77142857  0.07071068  0.46671717  0.44615385
   0.74583333  0.73809524  0.72757276  0.72952381]
 [ 0.3037037   0.62916667  0.62857143  0.55057579  0.6002381   0.32822323
   0.6625      0.70238095  0.53813414  0.63007937]
 [ 0.13289855  0.5625      0.60238095  0.36276004  0.53579365  0.34477591
   0.68        0.81904762  0.47870386  0.65606421]
 [ 0.01309524  0.475       0.48809524  0.41700751  0.45714286  0.46493506
   0.72916667  0.7547619   0.6555314   0.71634921]
 [ 0.31547619  0.67916667  0.69047619  0.47906763  0.6200938   0.472
   0.7375      0.73571429  0.74081541  0.72952381]
 [ 0.11969697  0.58333333  0.6047619   0.47104293  0.55242063  0.29350649
   0.64583333  0.65714286  0.63888547  0.64253968]
 [ 0.26956522  0.6325      0.70238095  0.50510237  0.61888889  0.475
   0.72083333  0.72142857  0.67693089  0.71142857]
 [ 0.07636364  0.55416667  0.67142857  0.25092019  0.48277778  0.4447619
   0.74166667  0.77142857  0.63582498  0.72376984]
 [ 0.1965368   0.59583333  0.64285714  0.32071068  0.54325397  0.38012422
   0.65416667  0.67380952  0.60835696  0.62785714]
 [ 0.69404762  0.84166667  0.87142857  0.79378169  0.84257937  0.44047619
   0.70416667  0.75952381  0.5526586   0.69690476]]
GBM mean:
[0.30048834 0.6483046  0.69975369 0.48600877 0.61633104 0.32748767
 0.66475    0.70619048 0.54973035 0.6432399 ]
---------------------------
---------------------------
BDDAE performance:
[[-0.09588527  0.45119048  0.45384615  0.42478329  0.43950639  0.09492598
   0.555       0.69230769  0.37555664  0.53911559]
 [ 0.15251861  0.575       0.58461538  0.55201913  0.56834943  0.34895085
   0.68        0.68461538  0.67340478  0.67180626]
 [ 0.09879998  0.54880952  0.56153846  0.49852118  0.53012796  0.22165708
   0.61071429  0.61538462  0.58474215  0.5997183 ]
 [ 0.16322346  0.58333333  0.58461538  0.56395576  0.57441398 -0.04028874
   0.48055556  0.60769231  0.23782047  0.45292168]
 [ 0.16987676  0.5875      0.58461538  0.58250036  0.57685995  0.45248697
   0.72738095  0.72307692  0.71612344  0.71945221]
 [ 0.34884642  0.67142857  0.67692308  0.65016738  0.66519703  0.24108475
   0.62261905  0.61538462  0.60194661  0.60811855]
 [ 0.03788153  0.51875     0.56923077  0.35618269  0.4823239   0.07573552
   0.53690476  0.53846154  0.44358357  0.49957059]
 [ 0.25156264  0.625       0.69230769  0.55022727  0.61410354  0.12705983
   0.56875     0.56153846  0.5549562   0.55127005]
 [ 0.10347882  0.5375      0.63846154  0.43668525  0.53556125  0.25858132
   0.61666667  0.73846154  0.42984694  0.60020582]
 [ 0.16761006  0.58214286  0.59230769  0.54921567  0.570323    0.13650602
   0.56666667  0.57692308  0.54486371  0.56083896]
 [-0.02310076  0.49583333  0.6         0.26905396  0.46287935  0.22618274
   0.61125     0.64615385  0.57017899  0.60221062]
 [ 0.10029221  0.55        0.56153846  0.54120911  0.54553932  0.20641294
   0.60375     0.62307692  0.58833888  0.59960668]
 [ 0.27063205  0.63452381  0.63846154  0.61627378  0.62805085  0.14421248
   0.57261905  0.56923077  0.53390668  0.55153114]
 [ 0.00926063  0.51111111  0.59230769  0.35924785  0.48527104  0.07194696
   0.53571429  0.53846154  0.50795906  0.52304088]
 [ 0.1484573   0.57916667  0.63846154  0.51194778  0.56733303  0.23586574
   0.6225      0.62307692  0.59264939  0.60157718]
 [-0.08941559  0.44875     0.46923077  0.39262007  0.43217583  0.04962486
   0.51111111  0.63076923  0.31927053  0.50056595]
 [ 0.52240805  0.76190476  0.76153846  0.7507267   0.75627887  0.16566723
   0.57625     0.63076923  0.46681373  0.56189284]
 [ 0.31700828  0.70416667  0.87692308  0.4783169   0.6482892  -0.21405023
   0.39        0.41538462  0.2899678   0.37343101]
 [ 0.02654776  0.505       0.61538462  0.44787935  0.50648681  0.1240636
   0.56309524  0.55384615  0.53587447  0.54451222]
 [ 0.24717787  0.62083333  0.67692308  0.58708357  0.61676552 -0.07215236
   0.47        0.52307692  0.3089156   0.43763221]
 [ 0.05924199  0.53666667  0.64615385  0.42343134  0.51653936  0.0095473
   0.50357143  0.50769231  0.48571912  0.49720915]
 [ 0.06522035  0.53214286  0.53846154  0.51290644  0.52569941  0.16679355
   0.57875     0.61538462  0.49816266  0.56161901]
 [ 0.08231405  0.545       0.54615385  0.52889422  0.53134414  0.40002004
   0.68        0.83076923  0.53969002  0.68524062]
 [-0.08423218  0.45833333  0.46153846  0.43093021  0.44630876  0.30176747
   0.65        0.66153846  0.60595089  0.63435776]
 [-0.06436021  0.46904762  0.46923077  0.44939844  0.45977939  0.08554355
   0.54285714  0.54615385  0.48161226  0.52144189]
 [ 0.26593547  0.63214286  0.63846154  0.61726646  0.62773279 -0.03417557
   0.48214286  0.49230769  0.42594947  0.45936311]
 [ 0.47207466  0.73472222  0.77692308  0.71245034  0.73179954  0.42589285
   0.71547619  0.70769231  0.6845868   0.69617556]
 [ 0.01052584  0.50694444  0.53846154  0.45204508  0.4882123   0.51216877
   0.7475      0.77692308  0.73043028  0.75279337]
 [-0.02190913  0.48875     0.5         0.46089653  0.47491727  0.38448889
   0.68928571  0.7         0.66760632  0.68471883]
 [ 0.19003188  0.595       0.60769231  0.5609689   0.57838164 -0.07837108
   0.45694444  0.50769231  0.39996893  0.44858474]]
BDDAE mean:
[0.13006745 0.56635648 0.60307692 0.50892683 0.55288503 0.16760498
 0.58226918 0.61512821 0.51321321 0.56801743]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.54047619 0.         0.34969697 0.
  0.5        0.77142857 0.         0.43439394]
 [0.         0.5        0.57380952 0.         0.3630303  0.
  0.5        0.62380952 0.         0.3830303 ]
 [0.         0.5        0.57380952 0.         0.3630303  0.
  0.5        0.49285714 0.         0.33      ]
 [0.         0.5        0.52380952 0.         0.3430303  0.
  0.5        0.72142857 0.         0.4180303 ]
 [0.         0.5        0.62380952 0.         0.3830303  0.
  0.5        0.52380952 0.         0.3430303 ]
 [0.         0.5        0.49285714 0.         0.33       0.
  0.5        0.52380952 0.         0.3430303 ]
 [0.         0.5        0.64047619 0.         0.38969697 0.
  0.5        0.49285714 0.         0.33      ]
 [0.         0.5        0.68809524 0.         0.40712121 0.
  0.5        0.60714286 0.         0.37636364]
 [0.         0.5        0.65714286 0.         0.39636364 0.
  0.5        0.68809524 0.         0.40712121]
 [0.         0.5        0.52380952 0.         0.3430303  0.
  0.5        0.55714286 0.         0.35636364]
 [0.         0.5        0.65714286 0.         0.39636364 0.
  0.5        0.59047619 0.         0.36969697]
 [0.         0.5        0.62380952 0.         0.3830303  0.
  0.5        0.62380952 0.         0.3830303 ]
 [0.         0.5        0.57380952 0.         0.3630303  0.
  0.5        0.52380952 0.         0.3430303 ]
 [0.         0.5        0.68809524 0.         0.40712121 0.
  0.5        0.49285714 0.         0.33      ]
 [0.         0.5        0.68809524 0.         0.40712121 0.
  0.5        0.64047619 0.         0.38969697]
 [0.         0.5        0.62380952 0.         0.3830303  0.
  0.5        0.65714286 0.         0.39636364]
 [0.         0.5        0.57380952 0.         0.3630303  0.
  0.5        0.59047619 0.         0.36969697]
 [       nan        nan        nan 0.3               nan 0.
  0.5        0.59047619 0.         0.36969697]
 [0.         0.5        0.73809524 0.         0.42348485 0.
  0.5        0.52380952 0.         0.3430303 ]
 [0.         0.5        0.65714286 0.         0.39636364 0.
  0.5        0.62380952 0.         0.3830303 ]
 [0.         0.5        0.77142857 0.         0.43439394 0.
  0.5        0.52380952 0.         0.3430303 ]
 [0.         0.5        0.49285714 0.         0.33       0.
  0.5        0.62380952 0.         0.3830303 ]
 [0.         0.5        0.60714286 0.         0.37636364 0.
  0.5        0.7547619  0.         0.42893939]
 [0.         0.5        0.52380952 0.         0.3430303  0.
  0.5        0.54047619 0.         0.34969697]
 [0.         0.5        0.57380952 0.         0.3630303  0.
  0.5        0.50714286 0.         0.33636364]
 [0.         0.5        0.54047619 0.         0.34969697 0.
  0.5        0.54047619 0.         0.34969697]
 [0.         0.5        0.68809524 0.         0.40712121 0.
  0.5        0.49285714 0.         0.33      ]
 [0.         0.5        0.65714286 0.         0.39636364 0.
  0.5        0.60714286 0.         0.37636364]
 [0.         0.5        0.60714286 0.         0.37636364 0.
  0.5        0.55714286 0.         0.35636364]
 [0.         0.5        0.62380952 0.         0.3830303  0.
  0.5        0.65714286 0.         0.39636364]]
DUMMY mean:
[0.         0.5        0.61198686 0.01       0.37758621 0.
 0.5        0.58880952 0.         0.36828283]
---------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_1_2_54
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.312 0.656 0.693 0.549 0.635 0.345 0.674 0.706 0.565 0.649]
 [0.303 0.662 0.678 0.568 0.636 0.331 0.671 0.693 0.588 0.65 ]
 [0.358 0.684 0.712 0.607 0.665 0.339 0.673 0.696 0.578 0.655]
 [0.001 0.501 0.613 0.006 0.38  0.049 0.525 0.612 0.088 0.413]
 [0.3   0.648 0.7   0.489 0.616 0.321 0.662 0.704 0.545 0.64 ]
 [0.13  0.566 0.603 0.509 0.553 0.168 0.582 0.615 0.513 0.568]
 [0.    0.5   0.612 0.    0.378 0.    0.5   0.589 0.    0.368]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.229 0.113 0.102 0.178 0.122 0.175 0.089 0.076 0.15  0.094]
 [0.216 0.107 0.109 0.148 0.116 0.156 0.085 0.083 0.121 0.089]
 [0.221 0.108 0.104 0.15  0.115 0.173 0.076 0.08  0.13  0.08 ]
 [0.008 0.004 0.069 0.018 0.024 0.133 0.067 0.081 0.197 0.098]
 [0.22  0.111 0.105 0.196 0.124 0.175 0.089 0.081 0.176 0.099]
 [0.155 0.08  0.094 0.104 0.081 0.17  0.084 0.09  0.125 0.089]
 [0.    0.    0.07  0.    0.027 0.    0.    0.078 0.    0.03 ]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 73.  17.  15.  32.  19.  51.  13.  11.  27.  14.]
 [ 71.  16.  16.  26.  18.  47.  13.  12.  21.  14.]
 [ 62.  16.  15.  25.  17.  51.  11.  11.  22.  12.]
 [662.   1.  11. 301.   6. 272.  13.  13. 225.  24.]
 [ 73.  17.  15.  40.  20.  54.  13.  12.  32.  15.]
 [119.  14.  16.  20.  15. 101.  14.  15.  24.  16.]
 [  0.   0.  11.   0.   7.   0.   0.  13.   0.   8.]]
-------------------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_1_2_54
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  24.992
step (sec):  18.752
overlap:  True
perc. of overlap:  24.96798975672215
overlap duration (sec):  6.24
Number of windows / instances:  61
Elapsed time: 1844.291979857286 minutes
Elapsed time: 30.738199664288096 hours
