2024-05-12 08:10:49.045041: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-12 08:10:49.351618: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-12 08:10:49.351844: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-12 08:10:49.352228: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-12 08:10:49.352985: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-12 08:10:49.353161: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-12 08:10:49.353338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-12 08:10:50.314915: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-12 08:10:50.315130: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-12 08:10:50.315292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-12 08:10:50.315445: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
Window size (sec):  25.0
step (sec):  12.5
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  12.5
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
/home/marcos/Dropbox (Maestral)/c_sldl_1_2_55/functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

15/63 [======>.......................] - ETA: 1s - loss: 0.0175 - mean_squared_error: 0.017517/63 [=======>......................] - ETA: 1s - loss: 0.0198 - mean_squared_error: 0.019819/63 [========>.....................] - ETA: 1s - loss: 0.0183 - mean_squared_error: 0.018321/63 [=========>....................] - ETA: 1s - loss: 0.0173 - mean_squared_error: 0.017323/63 [=========>....................] - ETA: 1s - loss: 0.0182 - mean_squared_error: 0.018225/63 [==========>...................] - ETA: 1s - loss: 0.0195 - mean_squared_error: 0.019527/63 [===========>..................] - ETA: 1s - loss: 0.0189 - mean_squared_error: 0.018929/63 [============>.................] - ETA: 1s - loss: 0.0210 - mean_squared_error: 0.021031/63 [=============>................] - ETA: 1s - loss: 0.0201 - mean_squared_error: 0.020133/63 [==============>...............] - ETA: 1s - loss: 0.0222 - mean_squared_error: 0.022235/63 [===============>..............] - ETA: 1s - loss: 0.0213 - mean_squared_error: 0.021337/63 [================>.............] - ETA: 0s - loss: 0.0219 - mean_squared_error: 0.021939/63 [=================>............] - ETA: 0s - loss: 0.0210 - mean_squared_error: 0.021041/63 [==================>...........] - ETA: 0s - loss: 0.0202 - mean_squared_error: 0.020243/63 [===================>..........] - ETA: 0s - loss: 0.0196 - mean_squared_error: 0.019645/63 [====================>.........] - ETA: 0s - loss: 0.0203 - mean_squared_error: 0.020347/63 [=====================>........] - ETA: 0s - loss: 0.0214 - mean_squared_error: 0.021449/63 [======================>.......] - ETA: 0s - loss: 0.0209 - mean_squared_error: 0.020951/63 [=======================>......] - ETA: 0s - loss: 0.0205 - mean_squared_error: 0.020553/63 [========================>.....] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.020055/63 [=========================>....] - ETA: 0s - loss: 0.0204 - mean_squared_error: 0.020457/63 [==========================>...] - ETA: 0s - loss: 0.0199 - mean_squared_error: 0.019959/63 [===========================>..] - ETA: 0s - loss: 0.0200 - mean_squared_error: 0.020061/63 [============================>.] - ETA: 0s - loss: 0.0194 - mean_squared_error: 0.019463/63 [==============================] - ETA: 0s - loss: 0.0190 - mean_squared_error: 0.019063/63 [==============================] - 2s 38ms/step - loss: 0.0190 - mean_squared_error: 0.0190 - val_loss: 0.0218 - val_mean_squared_error: 0.0218
Epoch 5/5
 1/63 [..............................] - ETA: 1s - loss: 0.0086 - mean_squared_error: 0.0086 3/63 [>.............................] - ETA: 2s - loss: 0.0070 - mean_squared_error: 0.0070 5/63 [=>............................] - ETA: 1s - loss: 0.0171 - mean_squared_error: 0.0171 7/63 [==>...........................] - ETA: 1s - loss: 0.0141 - mean_squared_error: 0.0141 9/63 [===>..........................] - ETA: 1s - loss: 0.0128 - mean_squared_error: 0.012811/63 [====>.........................] - ETA: 1s - loss: 0.0165 - mean_squared_error: 0.016513/63 [=====>........................] - ETA: 1s - loss: 0.0156 - mean_squared_error: 0.015615/63 [======>.......................] - ETA: 1s - loss: 0.0141 - mean_squared_error: 0.014117/63 [=======>......................] - ETA: 1s - loss: 0.0131 - mean_squared_error: 0.013119/63 [========>.....................] - ETA: 1s - loss: 0.0138 - mean_squared_error: 0.013821/63 [=========>....................] - ETA: 1s - loss: 0.0153 - mean_squared_error: 0.015323/63 [=========>....................] - ETA: 1s - loss: 0.0144 - mean_squared_error: 0.014425/63 [==========>...................] - ETA: 1s - loss: 0.0138 - mean_squared_error: 0.013827/63 [===========>..................] - ETA: 1s - loss: 0.0141 - mean_squared_error: 0.014129/63 [============>.................] - ETA: 1s - loss: 0.0133 - mean_squared_error: 0.013331/63 [=============>................] - ETA: 1s - loss: 0.0130 - mean_squared_error: 0.013033/63 [==============>...............] - ETA: 1s - loss: 0.0170 - mean_squared_error: 0.017035/63 [===============>..............] - ETA: 1s - loss: 0.0166 - mean_squared_error: 0.016637/63 [================>.............] - ETA: 0s - loss: 0.0164 - mean_squared_error: 0.016439/63 [=================>............] - ETA: 0s - loss: 0.0173 - mean_squared_error: 0.017341/63 [==================>...........] - ETA: 0s - loss: 0.0168 - mean_squared_error: 0.016843/63 [===================>..........] - ETA: 0s - loss: 0.0162 - mean_squared_error: 0.016245/63 [====================>.........] - ETA: 0s - loss: 0.0172 - mean_squared_error: 0.017247/63 [=====================>........] - ETA: 0s - loss: 0.0189 - mean_squared_error: 0.018949/63 [======================>.......] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.018851/63 [=======================>......] - ETA: 0s - loss: 0.0184 - mean_squared_error: 0.018453/63 [========================>.....] - ETA: 0s - loss: 0.0180 - mean_squared_error: 0.018055/63 [=========================>....] - ETA: 0s - loss: 0.0188 - mean_squared_error: 0.018857/63 [==========================>...] - ETA: 0s - loss: 0.0186 - mean_squared_error: 0.018659/63 [===========================>..] - ETA: 0s - loss: 0.0182 - mean_squared_error: 0.018261/63 [============================>.] - ETA: 0s - loss: 0.0185 - mean_squared_error: 0.018563/63 [==============================] - ETA: 0s - loss: 0.0192 - mean_squared_error: 0.019263/63 [==============================] - 2s 38ms/step - loss: 0.0192 - mean_squared_error: 0.0192 - val_loss: 0.0201 - val_mean_squared_error: 0.0201
(24992, 1, 5)
Model: "sequential_1199"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 1stConvL (Conv1D)           (None, 24992, 5)          105       
                                                                 
 1stPoolL (AveragePooling1D)  (None, 6248, 5)          0         
                                                                 
 2ndConvL (Conv1D)           (None, 6248, 6)           306       
                                                                 
 2ndPoolL (AveragePooling1D)  (None, 1562, 6)          0         
                                                                 
=================================================================
Total params: 411
Trainable params: 411
Non-trainable params: 0
_________________________________________________________________
Model: "valence_NN"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 inputGSR (InputLayer)          [(None, 24992, 1)]   0           []                               
                                                                                                  
 inputPPG (InputLayer)          [(None, 24992, 1)]   0           []                               
                                                                                                  
 sequential_1198 (Sequential)   (None, 1562, 6)      411         ['inputGSR[0][0]']               
                                                                                                  
 sequential_1199 (Sequential)   (None, 1562, 6)      411         ['inputPPG[0][0]']               
                                                                                                  
 concatenate_599 (Concatenate)  (None, 1562, 12)     0           ['sequential_1198[0][0]',        
                                                                  'sequential_1199[0][0]']        
                                                                                                  
 permute_599 (Permute)          (None, 12, 1562)     0           ['concatenate_599[0][0]']        
                                                                                                  
 flatten_599 (Flatten)          (None, 18744)        0           ['permute_599[0][0]']            
                                                                                                  
 dropout_599 (Dropout)          (None, 18744)        0           ['flatten_599[0][0]']            
                                                                                                  
 dense_599 (Dense)              (None, 1)            18745       ['dropout_599[0][0]']            
                                                                                                  
==================================================================================================
Total params: 19,567
Trainable params: 19,567
Non-trainable params: 0
__________________________________________________________________________________________________
Epoch 1/10
 1/63 [..............................] - ETA: 16s - loss: 0.3466 - binary_accuracy: 1.000013/63 [=====>........................] - ETA: 0s - loss: 0.9794 - binary_accuracy: 0.3077 25/63 [==========>...................] - ETA: 0s - loss: 0.8577 - binary_accuracy: 0.440038/63 [=================>............] - ETA: 0s - loss: 0.7683 - binary_accuracy: 0.552650/63 [======================>.......] - ETA: 0s - loss: 0.7079 - binary_accuracy: 0.600063/63 [==============================] - ETA: 0s - loss: 0.6952 - binary_accuracy: 0.619063/63 [==============================] - 1s 6ms/step - loss: 0.6952 - binary_accuracy: 0.6190 - val_loss: 0.7227 - val_binary_accuracy: 0.5714
Epoch 2/10
 1/63 [..............................] - ETA: 0s - loss: 0.2372 - binary_accuracy: 1.000011/63 [====>.........................] - ETA: 0s - loss: 0.5284 - binary_accuracy: 0.818224/63 [==========>...................] - ETA: 0s - loss: 0.5215 - binary_accuracy: 0.791737/63 [================>.............] - ETA: 0s - loss: 0.5934 - binary_accuracy: 0.729750/63 [======================>.......] - ETA: 0s - loss: 0.6169 - binary_accuracy: 0.660063/63 [==============================] - ETA: 0s - loss: 0.6182 - binary_accuracy: 0.650863/63 [==============================] - 0s 5ms/step - loss: 0.6182 - binary_accuracy: 0.6508 - val_loss: 0.6403 - val_binary_accuracy: 0.7143
Epoch 3/10
 1/63 [..............................] - ETA: 0s - loss: 0.4523 - binary_accuracy: 1.000013/63 [=====>........................] - ETA: 0s - loss: 0.3691 - binary_accuracy: 1.000026/63 [===========>..................] - ETA: 0s - loss: 0.4473 - binary_accuracy: 0.807738/63 [=================>............] - ETA: 0s - loss: 0.4775 - binary_accuracy: 0.789550/63 [======================>.......] - ETA: 0s - loss: 0.5184 - binary_accuracy: 0.740063/63 [==============================] - ETA: 0s - loss: 0.5353 - binary_accuracy: 0.730263/63 [==============================] - 0s 4ms/step - loss: 0.5353 - binary_accuracy: 0.7302 - val_loss: 0.8350 - val_binary_accuracy: 0.5714
Epoch 4/10
 1/63 [..............................] - ETA: 0s - loss: 0.7888 - binary_accuracy: 0.0000e+0013/63 [=====>........................] - ETA: 0s - loss: 0.4560 - binary_accuracy: 0.6923    26/63 [===========>..................] - ETA: 0s - loss: 0.4668 - binary_accuracy: 0.730838/63 [=================>............] - ETA: 0s - loss: 0.4787 - binary_accuracy: 0.736850/63 [======================>.......] - ETA: 0s - loss: 0.4584 - binary_accuracy: 0.760063/63 [==============================] - ETA: 0s - loss: 0.4724 - binary_accuracy: 0.746063/63 [==============================] - 0s 4ms/step - loss: 0.4724 - binary_accuracy: 0.7460 - val_loss: 0.8168 - val_binary_accuracy: 0.7143
Epoch 5/10
 1/63 [..............................] - ETA: 0s - loss: 0.2103 - binary_accuracy: 1.000014/63 [=====>........................] - ETA: 0s - loss: 0.4675 - binary_accuracy: 0.785727/63 [===========>..................] - ETA: 0s - loss: 0.3448 - binary_accuracy: 0.888940/63 [==================>...........] - ETA: 0s - loss: 0.3126 - binary_accuracy: 0.875053/63 [========================>.....] - ETA: 0s - loss: 0.3447 - binary_accuracy: 0.849163/63 [==============================] - 0s 4ms/step - loss: 0.3507 - binary_accuracy: 0.8254 - val_loss: 0.7589 - val_binary_accuracy: 0.7143
Epoch 6/10
 1/63 [..............................] - ETA: 0s - loss: 0.1039 - binary_accuracy: 1.000014/63 [=====>........................] - ETA: 0s - loss: 0.2932 - binary_accuracy: 0.928627/63 [===========>..................] - ETA: 0s - loss: 0.2583 - binary_accuracy: 0.963040/63 [==================>...........] - ETA: 0s - loss: 0.2958 - binary_accuracy: 0.925053/63 [========================>.....] - ETA: 0s - loss: 0.3211 - binary_accuracy: 0.905763/63 [==============================] - 0s 4ms/step - loss: 0.3185 - binary_accuracy: 0.8889 - val_loss: 0.8563 - val_binary_accuracy: 0.5714
Epoch 7/10
 1/63 [..............................] - ETA: 0s - loss: 0.3542 - binary_accuracy: 1.000013/63 [=====>........................] - ETA: 0s - loss: 0.2120 - binary_accuracy: 1.000026/63 [===========>..................] - ETA: 0s - loss: 0.1489 - binary_accuracy: 1.000039/63 [=================>............] - ETA: 0s - loss: 0.1892 - binary_accuracy: 0.923152/63 [=======================>......] - ETA: 0s - loss: 0.2415 - binary_accuracy: 0.884663/63 [==============================] - 0s 4ms/step - loss: 0.2298 - binary_accuracy: 0.9048 - val_loss: 0.9984 - val_binary_accuracy: 0.7143
Epoch 8/10
 1/63 [..............................] - ETA: 0s - loss: 0.0736 - binary_accuracy: 1.000014/63 [=====>........................] - ETA: 0s - loss: 0.2422 - binary_accuracy: 0.928626/63 [===========>..................] - ETA: 0s - loss: 0.2948 - binary_accuracy: 0.884639/63 [=================>............] - ETA: 0s - loss: 0.2520 - binary_accuracy: 0.923153/63 [========================>.....] - ETA: 0s - loss: 0.2401 - binary_accuracy: 0.924563/63 [==============================] - 0s 4ms/step - loss: 0.2156 - binary_accuracy: 0.9365 - val_loss: 1.0073 - val_binary_accuracy: 0.7143
Epoch 9/10
 1/63 [..............................] - ETA: 0s - loss: 0.0759 - binary_accuracy: 1.000014/63 [=====>........................] - ETA: 0s - loss: 0.1477 - binary_accuracy: 1.000027/63 [===========>..................] - ETA: 0s - loss: 0.1564 - binary_accuracy: 0.963039/63 [=================>............] - ETA: 0s - loss: 0.1534 - binary_accuracy: 0.974453/63 [========================>.....] - ETA: 0s - loss: 0.1458 - binary_accuracy: 0.981163/63 [==============================] - 0s 4ms/step - loss: 0.1764 - binary_accuracy: 0.9524 - val_loss: 1.1299 - val_binary_accuracy: 0.7143
Epoch 10/10
 1/63 [..............................] - ETA: 0s - loss: 0.2906 - binary_accuracy: 1.000013/63 [=====>........................] - ETA: 0s - loss: 0.1119 - binary_accuracy: 1.000025/63 [==========>...................] - ETA: 0s - loss: 0.1347 - binary_accuracy: 0.960038/63 [=================>............] - ETA: 0s - loss: 0.1256 - binary_accuracy: 0.973752/63 [=======================>......] - ETA: 0s - loss: 0.1325 - binary_accuracy: 0.980863/63 [==============================] - 0s 4ms/step - loss: 0.1480 - binary_accuracy: 0.9683 - val_loss: 1.4240 - val_binary_accuracy: 0.7143
1/1 [==============================] - ETA: 0s1/1 [==============================] - 0s 61ms/step
predicted [0.00164224 0.9919255  0.98488283 0.9887384  0.04934759 0.9300826
 0.9960024  0.9354866  0.9765247  0.89759225 0.9783017  0.94735056
 0.89118713 0.9919285  0.15103562 0.9952713  0.7496589  0.02788498]
predicted [0 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 1 0]
expected [False False  True  True False  True  True  True  True False False  True
  True  True False  True  True False]
accuracy: 0.8333333333333334
confusion matrix: 
[[ 4  3]
 [ 0 11]]
              precision    recall  f1-score   support

       False       1.00      0.57      0.73         7
        True       0.79      1.00      0.88        11

    accuracy                           0.83        18
   macro avg       0.89      0.79      0.80        18
weighted avg       0.87      0.83      0.82        18

macro avg f1-score: 0.8036363636363637
macro avg (UAR): 0.7857142857142857
Sensitivity:  0.5714285714285714
Specificity:  1.0
g-mean:  0.7559289460184544
-------- Model Performance ----------: 
accuracy:  [0.66666667 0.72222222 0.83333333 0.66666667 0.88888889 0.66666667
 0.77777778 0.72222222 0.72222222 0.83333333]
gmean:  [0.64465837 0.62418778 0.83743579 0.67419986 0.84515425 0.59215653
 0.78954203 0.73854895 0.68376346 0.75592895]
f1_score:  [0.64935065 0.67272727 0.82857143 0.6625     0.875      0.625
 0.775      0.72136223 0.69899666 0.80363636]
UAR:  [0.64935065 0.66883117 0.83766234 0.67532468 0.85714286 0.62337662
 0.79220779 0.74675325 0.69480519 0.78571429]
Cohen Kappa score:  [0.2987013  0.36619718 0.65822785 0.33333333 0.75342466 0.26027397
 0.55555556 0.45783133 0.4        0.61971831]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  24.992
step (sec):  12.496
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  12.496
Number of windows / instances:  88
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.93  0.968 0.965 0.965 0.965 0.476 0.727 0.775 0.662 0.724]
 [0.753 0.87  0.886 0.891 0.871 0.441 0.788 0.797 0.755 0.777]
 [0.856 0.922 0.922 0.871 0.918 0.495 0.72  0.75  0.648 0.715]
 [0.256 0.617 0.706 0.368 0.567 0.    0.5   0.647 0.    0.393]
 [0.74  0.847 0.872 0.825 0.848 0.502 0.742 0.788 0.683 0.743]
 [0.47  0.733 0.75  0.719 0.731 0.07  0.529 0.589 0.443 0.517]
 [0.    0.5   0.614 0.    0.38  0.    0.5   0.647 0.    0.393]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.60915579  0.8075      0.80833333  0.77125171  0.79847042  0.5357764
   0.73571429  0.86527778  0.6551948   0.75895147]
 [ 0.55233737  0.77583333  0.77638889  0.74994513  0.76329559  0.53012821
   0.75583333  0.79583333  0.72330672  0.75738817]
 [ 0.51722352  0.75333333  0.775       0.71172476  0.74380259  0.34001468
   0.67        0.66805556  0.63730232  0.65267316]
 [ 0.51960382  0.7575      0.76111111  0.73937294  0.75164141 -0.06941464
   0.47142857  0.65833333  0.13093073  0.44094322]
 [ 0.504494    0.74666667  0.77222222  0.72885741  0.74856005  0.41312991
   0.71        0.70277778  0.67166851  0.68709235]
 [ 0.56604967  0.785       0.78194444  0.76234536  0.77348485  0.62375275
   0.815       0.80694444  0.79801371  0.80233045]
 [ 0.563002    0.78        0.80555556  0.75892872  0.77679959  0.36822876
   0.685       0.68333333  0.6576622   0.66981602]
 [ 0.76363636  0.86666667  0.89861111  0.84966965  0.87692308  0.76460451
   0.88583333  0.88472222  0.87626105  0.87792097]
 [ 0.4151049   0.70833333  0.77361111  0.5979384   0.69309191  0.28232767
   0.64166667  0.71388889  0.53870083  0.63201299]
 [ 0.1035983   0.5525      0.55555556  0.50464712  0.53840548  0.38776346
   0.69083333  0.69305556  0.67938806  0.68691558]
 [ 0.28495592  0.64083333  0.68194444  0.53608154  0.62094322  0.41107561
   0.69416667  0.74861111  0.59190633  0.67755078]
 [ 0.69265235  0.83166667  0.86527778  0.81448689  0.84305445  0.40246533
   0.70833333  0.71388889  0.64675571  0.68257659]
 [ 0.44318087  0.7275      0.725       0.68824426  0.70343989  0.21966013
   0.60583333  0.625       0.57415754  0.59987013]
 [-0.01172031  0.49285714  0.62222222  0.24222322  0.47396062  0.60740094
   0.8075      0.80138889  0.78506727  0.79268759]
 [ 0.27858272  0.64285714  0.73194444  0.4816716   0.61941059  0.36062121
   0.68666667  0.72777778  0.59967362  0.67350316]
 [ 0.21823775  0.60916667  0.65833333  0.49050375  0.58594544  0.32937063
   0.64166667  0.72638889  0.5279049   0.63347375]
 [ 0.29503892  0.6425      0.67083333  0.5956748   0.62981102  0.36968286
   0.68166667  0.71527778  0.61085576  0.66864552]
 [ 0.00908385  0.51785714  0.84027778  0.09354143  0.4890091   0.4338533
   0.7225      0.72638889  0.71388248  0.7137518 ]
 [ 0.40569982  0.69642857  0.7625      0.66479465  0.69591242  0.33963365
   0.66916667  0.68194444  0.60493074  0.65226634]
 [ 0.35999001  0.675       0.70416667  0.61104     0.66052697  0.32930278
   0.66166667  0.70833333  0.5766713   0.64384615]
 [ 0.13387916  0.56785714  0.71388889  0.385714    0.55638736  0.01108832
   0.50833333  0.51527778  0.44215201  0.49378427]
 [ 0.42916066  0.7125      0.71527778  0.6923429   0.70493146  0.40277065
   0.70333333  0.72777778  0.67887338  0.69628372]
 [ 0.14492979  0.57166667  0.60138889  0.47827659  0.5453663   0.29472457
   0.63928571  0.7625      0.4522441   0.6213499 ]
 [ 0.26323375  0.63        0.625       0.59713745  0.61808081  0.62801619
   0.81        0.81944444  0.79603688  0.80982684]
 [ 0.66050262  0.82666667  0.84027778  0.80225743  0.82313492  0.59870008
   0.8025      0.79722222  0.79503816  0.79597042]
 [ 0.39967962  0.6975      0.70416667  0.66735105  0.68669025  0.39534667
   0.69666667  0.70555556  0.67684017  0.69132395]
 [ 0.55806324  0.76785714  0.84027778  0.68886473  0.76869963  0.5680193
   0.785       0.78472222  0.77685033  0.78022727]
 [ 0.30134374  0.65083333  0.67916667  0.62386029  0.64406926  0.51385529
   0.755       0.77638889  0.71801175  0.74451326]
 [ 0.02607656  0.5075      0.54722222  0.39903046  0.48384754  0.4247031
   0.70583333  0.72638889  0.67229231  0.70179654]
 [ 0.92997186  0.9675      0.96527778  0.96548798  0.96450216  0.47596903
   0.7275      0.775       0.66217076  0.72391359]]
KNN mean:
[0.39789162 0.69699603 0.74009259 0.62310887 0.68607328 0.40975238
 0.70246429 0.73458333 0.64235815 0.69210687]
---------------------------
---------------------------
DT performance:
[[0.48709602 0.765      0.77083333 0.77419048 0.75098568 0.47033152
  0.74642857 0.79861111 0.68112636 0.73231685]
 [0.45484666 0.73583333 0.74027778 0.72482583 0.73229076 0.46270747
  0.7025     0.71666667 0.63481797 0.67857948]
 [0.4380048  0.7525     0.75416667 0.71215118 0.73796176 0.38443445
  0.6625     0.65972222 0.68506075 0.64655844]
 [0.51106497 0.7425     0.75138889 0.74270302 0.73672439 0.13080805
  0.55119048 0.68611111 0.34006896 0.55018482]
 [0.4484208  0.74416667 0.76111111 0.77456302 0.73298535 0.36477696
  0.6275     0.62361111 0.62176169 0.60844156]
 [0.40494109 0.68       0.68333333 0.67116668 0.67087302 0.39145541
  0.7075     0.70138889 0.67547992 0.68670996]
 [0.3001658  0.65916667 0.68333333 0.57347011 0.64884504 0.21999733
  0.5975     0.59861111 0.59600936 0.58223304]
 [0.87272727 0.93452381 0.95416667 0.96551876 0.93794872 0.6474482
  0.85083333 0.85138889 0.82551564 0.84507881]
 [0.35720946 0.65       0.69444444 0.64735032 0.64307692 0.43261738
  0.71666667 0.72777778 0.69891027 0.69968032]
 [0.24641826 0.6475     0.64861111 0.63278773 0.63454906 0.46661427
  0.7        0.70694444 0.70476518 0.69088745]
 [0.29784715 0.63166667 0.66111111 0.62687092 0.62394855 0.31657684
  0.685      0.70694444 0.62773623 0.66550422]
 [0.64977962 0.81833333 0.83333333 0.80555142 0.82057609 0.45416965
  0.75083333 0.74861111 0.70979261 0.72969697]
 [0.19448024 0.595      0.59444444 0.46810682 0.58558442 0.17045565
  0.59666667 0.60138889 0.59343548 0.57587746]
 [0.07335997 0.53095238 0.625      0.41502516 0.52390276 0.31751003
  0.64       0.63333333 0.58999218 0.61973304]
 [0.21037296 0.57738095 0.6375     0.463264   0.56231435 0.2122212
  0.62333333 0.64305556 0.65271951 0.60942835]
 [0.2167523  0.62166667 0.63194444 0.5692673  0.60385115 0.51386447
  0.66166667 0.69166667 0.72798563 0.64036214]
 [0.65018808 0.84083333 0.85277778 0.81676282 0.83693307 0.40539176
  0.68666667 0.69305556 0.71801963 0.67546176]
 [0.10729814 0.60446429 0.83888889 0.19354143 0.57108193 0.42863257
  0.72083333 0.74861111 0.67016906 0.72071429]
 [0.51145442 0.77142857 0.80833333 0.7244025  0.75918748 0.63250505
  0.79166667 0.79166667 0.75972708 0.78514791]
 [0.17096404 0.6        0.63888889 0.44292937 0.57154679 0.14159664
  0.57       0.58194444 0.46019872 0.53763709]
 [0.05519629 0.51190476 0.62777778 0.31393945 0.49024267 0.10618394
  0.53333333 0.53472222 0.52567581 0.51669025]
 [0.42276862 0.6625     0.6625     0.62037439 0.64001526 0.39363009
  0.61416667 0.64305556 0.60571098 0.60709069]
 [0.23888827 0.63083333 0.64722222 0.65117525 0.62561688 0.44173913
  0.77261905 0.81944444 0.80472067 0.76504579]
 [0.39262705 0.71       0.70277778 0.64672908 0.69878788 0.46042395
  0.77583333 0.78333333 0.72586233 0.77174964]
 [0.51112436 0.77166667 0.78472222 0.73120023 0.76958514 0.52018762
  0.7475     0.75138889 0.74150596 0.74612554]
 [0.21919847 0.635      0.63611111 0.58321517 0.62570346 0.3851903
  0.68916667 0.69583333 0.62956432 0.68133838]
 [0.53056653 0.775      0.825      0.66863681 0.7825641  0.60477444
  0.815      0.81666667 0.8058488  0.81140693]
 [0.13784972 0.58166667 0.61388889 0.45968253 0.56551726 0.44059538
  0.69083333 0.69722222 0.70589555 0.67619603]
 [0.28370599 0.62583333 0.65138889 0.61587427 0.61372378 0.25641878
  0.6775     0.68472222 0.61887125 0.67056999]
 [0.75295554 0.87       0.88611111 0.89085137 0.87066739 0.44083171
  0.78833333 0.79722222 0.75514826 0.77671856]]
DT mean:
[0.3716091  0.68924405 0.7200463  0.63087091 0.6789197  0.38713634
 0.68978571 0.70449074 0.66306987 0.67677219]
---------------------------
---------------------------
RF performance:
[[0.66755025 0.8275     0.82916667 0.8364016  0.82170274 0.46793337
  0.64166667 0.77361111 0.58356848 0.6532967 ]
 [0.46919414 0.74666667 0.75972222 0.74391772 0.74327201 0.50972439
  0.77083333 0.78472222 0.68567823 0.76232157]
 [0.38591046 0.71916667 0.74166667 0.67164932 0.71093823 0.21071652
  0.69       0.68194444 0.62110934 0.66960678]
 [0.50148352 0.765      0.77083333 0.81438555 0.76158369 0.12282891
  0.58809524 0.72916667 0.11816999 0.58270604]
 [0.59729509 0.75166667 0.76388889 0.76848855 0.7364233  0.44202054
  0.765      0.76527778 0.76946502 0.75347597]
 [0.47346116 0.6825     0.68333333 0.73293863 0.66937951 0.57592375
  0.7875     0.78055556 0.72655121 0.77378066]
 [0.41936485 0.605      0.6375     0.69571063 0.58961955 0.37570483
  0.725      0.72916667 0.65453288 0.72106061]
 [0.87818182 0.93452381 0.94305556 0.93622944 0.92841825 0.66618313
  0.7925     0.79583333 0.74403265 0.78173882]
 [0.43999667 0.675      0.7375     0.68968342 0.65481019 0.25010989
  0.725      0.78472222 0.56927619 0.71918082]
 [0.34223758 0.6275     0.62638889 0.56019037 0.61721501 0.45477983
  0.71083333 0.71805556 0.68680287 0.70962843]
 [0.39260984 0.62916667 0.66111111 0.52818626 0.6246337  0.65796703
  0.7325     0.75138889 0.74174939 0.72820346]
 [0.6939011  0.89083333 0.89722222 0.84184004 0.88856782 0.4975565
  0.7725     0.78194444 0.72946074 0.76189644]
 [0.32481766 0.7025     0.69444444 0.586769   0.67292652 0.16390769
  0.54833333 0.55555556 0.63185489 0.54059524]
 [0.22063936 0.61190476 0.71527778 0.32440169 0.59333167 0.42836181
  0.7175     0.71527778 0.63267704 0.70834776]
 [0.16496013 0.61904762 0.68194444 0.52567452 0.61311855 0.33923077
  0.65666667 0.69027778 0.40857761 0.65524476]
 [0.31091995 0.58333333 0.60416667 0.40885313 0.5692649  0.59486846
  0.73333333 0.78611111 0.71372147 0.72195971]
 [0.6953537  0.82666667 0.85277778 0.86174772 0.82678211 0.46826454
  0.78083333 0.79305556 0.64497629 0.77545038]
 [0.19479814 0.58660714 0.88472222 0.19354143 0.56856793 0.41990343
  0.74333333 0.78472222 0.63392059 0.74424492]
 [0.60011199 0.84642857 0.86388889 0.57480914 0.83340326 0.41184665
  0.7375     0.75       0.68393629 0.72027417]
 [0.34234266 0.69166667 0.70694444 0.63427783 0.66417999 0.25573181
  0.59666667 0.63611111 0.55950531 0.56353313]
 [0.30513834 0.62261905 0.77222222 0.39210679 0.61123751 0.03301043
  0.5025     0.50833333 0.48534638 0.47005051]
 [0.38645269 0.705      0.70555556 0.69345145 0.69496753 0.33772727
  0.73333333 0.76111111 0.73041951 0.7290959 ]
 [0.27220217 0.70833333 0.725      0.55746206 0.70262821 0.32963485
  0.6952381  0.79444444 0.41106292 0.69003039]
 [0.32109615 0.6775     0.67638889 0.65381849 0.66723304 0.5386322
  0.72416667 0.72361111 0.80380763 0.7177381 ]
 [0.46219512 0.7575     0.77083333 0.72133286 0.74950216 0.57541579
  0.8        0.79722222 0.78589529 0.79234848]
 [0.38334763 0.6375     0.64722222 0.62516875 0.6287987  0.44219512
  0.6925     0.7125     0.65919772 0.68021645]
 [0.59952757 0.77619048 0.81666667 0.81981279 0.76191808 0.79738676
  0.8075     0.80694444 0.77062893 0.79742979]
 [0.0971263  0.53666667 0.57916667 0.35476648 0.50605922 0.4756768
  0.78       0.775      0.7594162  0.76130231]
 [0.36457218 0.67833333 0.72777778 0.64132924 0.65415418 0.32375177
  0.65583333 0.66944444 0.63187699 0.65651515]
 [0.85551282 0.92166667 0.92222222 0.87077924 0.91777722 0.49499255
  0.72       0.75       0.6477216  0.71479076]]
RF mean:
[0.43874337 0.71146627 0.74662037 0.64199081 0.69941383 0.42206625
 0.71088889 0.7362037  0.64083132 0.70186881]
---------------------------
---------------------------
SVM performance:
[[0.00000000e+00 5.00000000e-01 5.33333333e-01 0.00000000e+00
  3.47435897e-01 0.00000000e+00 5.00000000e-01 7.72222222e-01
  0.00000000e+00 4.35714286e-01]
 [1.03950104e-01 5.50000000e-01 6.01388889e-01 1.36602540e-01
  4.32853813e-01 1.98120144e-01 5.87500000e-01 6.81944444e-01
  2.93915759e-01 5.25732601e-01]
 [2.73590734e-01 6.25000000e-01 6.83333333e-01 4.39384685e-01
  5.78256743e-01 4.08176497e-01 7.05000000e-01 7.02777778e-01
  6.75919224e-01 6.88863636e-01]
 [1.38200396e-01 5.67500000e-01 5.91666667e-01 2.37792717e-01
  4.63882784e-01 0.00000000e+00 5.00000000e-01 7.50000000e-01
  0.00000000e+00 4.28214286e-01]
 [4.56330043e-01 7.12500000e-01 7.73611111e-01 5.99480048e-01
  6.96516817e-01 5.05569851e-01 7.55000000e-01 7.51388889e-01
  7.22078072e-01 7.36338384e-01]
 [2.51583342e-01 6.22500000e-01 6.26388889e-01 5.07104472e-01
  5.76447719e-01 6.37550255e-01 8.22500000e-01 8.16666667e-01
  8.12968456e-01 8.14740260e-01]
 [0.00000000e+00 5.00000000e-01 6.36111111e-01 0.00000000e+00
  3.88351648e-01 1.61850312e-01 5.77500000e-01 5.91666667e-01
  2.83956231e-01 4.79953380e-01]
 [4.26324111e-01 6.83333333e-01 8.06944444e-01 5.35660780e-01
  6.86080586e-01 2.49207322e-01 6.10833333e-01 6.95833333e-01
  3.99576098e-01 5.70472860e-01]
 [0.00000000e+00 5.00000000e-01 6.83333333e-01 0.00000000e+00
  4.05714286e-01 0.00000000e+00 5.00000000e-01 6.83333333e-01
  0.00000000e+00 4.05714286e-01]
 [0.00000000e+00 5.00000000e-01 5.11111111e-01 0.00000000e+00
  3.37545788e-01 0.00000000e+00 5.00000000e-01 5.56944444e-01
  0.00000000e+00 3.57509158e-01]
 [0.00000000e+00 5.00000000e-01 6.25000000e-01 0.00000000e+00
  3.84065934e-01 0.00000000e+00 5.00000000e-01 6.13888889e-01
  0.00000000e+00 3.79780220e-01]
 [0.00000000e+00 5.00000000e-01 6.13888889e-01 0.00000000e+00
  3.79780220e-01 0.00000000e+00 5.00000000e-01 6.13888889e-01
  0.00000000e+00 3.79780220e-01]
 [0.00000000e+00 5.00000000e-01 5.80555556e-01 0.00000000e+00
  3.66923077e-01 0.00000000e+00 5.00000000e-01 5.69444444e-01
  0.00000000e+00 3.62637363e-01]
 [0.00000000e+00 5.00000000e-01 7.05555556e-01 0.00000000e+00
  4.13214286e-01 1.79422107e-01 5.87500000e-01 5.90277778e-01
  3.26045259e-01 4.92184482e-01]
 [0.00000000e+00 5.00000000e-01 6.94444444e-01 0.00000000e+00
  4.09464286e-01 0.00000000e+00 5.00000000e-01 6.47222222e-01
  0.00000000e+00 3.92637363e-01]
 [0.00000000e+00 5.00000000e-01 6.13888889e-01 0.00000000e+00
  3.79780220e-01 0.00000000e+00 5.00000000e-01 6.58333333e-01
  0.00000000e+00 3.96923077e-01]
 [0.00000000e+00 5.00000000e-01 6.02777778e-01 0.00000000e+00
  3.75494505e-01 0.00000000e+00 5.00000000e-01 6.02777778e-01
  0.00000000e+00 3.75494505e-01]
 [0.00000000e+00 5.00000000e-01 8.86111111e-01 0.00000000e+00
  4.69803922e-01 0.00000000e+00 5.00000000e-01 6.02777778e-01
  0.00000000e+00 3.75494505e-01]
 [0.00000000e+00 5.00000000e-01 7.16666667e-01 0.00000000e+00
  4.16964286e-01 0.00000000e+00 5.00000000e-01 5.56944444e-01
  0.00000000e+00 3.57509158e-01]
 [0.00000000e+00 5.00000000e-01 6.70833333e-01 0.00000000e+00
  4.01318681e-01 0.00000000e+00 5.00000000e-01 6.25000000e-01
  0.00000000e+00 3.84065934e-01]
 [0.00000000e+00 5.00000000e-01 7.72222222e-01 0.00000000e+00
  4.35714286e-01 0.00000000e+00 5.00000000e-01 5.56944444e-01
  0.00000000e+00 3.57509158e-01]
 [5.74324324e-02 5.27500000e-01 5.34722222e-01 1.50000000e-01
  4.01125541e-01 0.00000000e+00 5.00000000e-01 6.36111111e-01
  0.00000000e+00 3.88351648e-01]
 [0.00000000e+00 5.00000000e-01 5.80555556e-01 0.00000000e+00
  3.66923077e-01 0.00000000e+00 5.00000000e-01 7.50000000e-01
  0.00000000e+00 4.28214286e-01]
 [0.00000000e+00 5.00000000e-01 5.22222222e-01 0.00000000e+00
  3.42490842e-01 3.05673487e-01 6.44166667e-01 6.83333333e-01
  4.85299099e-01 6.03351648e-01]
 [0.00000000e+00 5.00000000e-01 5.56944444e-01 0.00000000e+00
  3.57509158e-01 3.06379254e-04 5.00000000e-01 5.11111111e-01
  1.51975387e-01 3.89588745e-01]
 [0.00000000e+00 5.00000000e-01 5.22222222e-01 0.00000000e+00
  3.42490842e-01 3.85838166e-01 6.81666667e-01 7.15277778e-01
  6.03823293e-01 6.63349983e-01]
 [0.00000000e+00 5.00000000e-01 6.94444444e-01 0.00000000e+00
  4.09464286e-01 5.69449209e-01 7.87500000e-01 7.83333333e-01
  7.77791608e-01 7.80295815e-01]
 [0.00000000e+00 5.00000000e-01 6.25000000e-01 0.00000000e+00
  3.84065934e-01 0.00000000e+00 5.00000000e-01 6.02777778e-01
  0.00000000e+00 3.75494505e-01]
 [0.00000000e+00 5.00000000e-01 6.25000000e-01 0.00000000e+00
  3.84065934e-01 0.00000000e+00 5.00000000e-01 5.56944444e-01
  0.00000000e+00 3.57509158e-01]
 [2.56335983e-01 6.16666667e-01 7.05555556e-01 3.67830390e-01
  5.67124542e-01 0.00000000e+00 5.00000000e-01 6.47222222e-01
  0.00000000e+00 3.92637363e-01]]
SVM mean:
[0.06545824 0.53016667 0.64319444 0.09912852 0.430029   0.12003879
 0.55863889 0.65087963 0.18444495 0.46920208]
---------------------------
---------------------------
GBM performance:
[[ 0.5754548   0.7875      0.79583333  0.72969917  0.7677381   0.36434783
   0.65        0.84027778  0.42426407  0.65359432]
 [ 0.45670681  0.72083333  0.74027778  0.69972183  0.71622128  0.30730184
   0.62333333  0.67083333  0.56426643  0.60488234]
 [ 0.58074578  0.7675      0.79861111  0.7150147   0.7586963   0.30235411
   0.6525      0.65833333  0.58383121  0.62280664]
 [ 0.55860351  0.7775      0.78472222  0.7406929   0.77018593  0.02086957
   0.50833333  0.73888889  0.07071068  0.45589286]
 [ 0.66181645  0.81666667  0.85277778  0.78293397  0.82115884  0.39431443
   0.735       0.72916667  0.69125786  0.71704185]
 [ 0.47962562  0.74        0.73888889  0.70749279  0.7238456   0.55075259
   0.7675      0.75972222  0.75169254  0.75570707]
 [ 0.26309501  0.63166667  0.68194444  0.60259654  0.62612721  0.22189973
   0.5875      0.58888889  0.56704317  0.57664141]
 [ 0.93272727  0.96785714  0.97638889  0.96329269  0.96564103  0.5546571
   0.7625      0.78472222  0.75004596  0.76208097]
 [ 0.17090909  0.575       0.71527778  0.30755937  0.54521978  0.17818182
   0.575       0.71527778  0.22590971  0.5339011 ]
 [ 0.25157766  0.6225      0.62638889  0.59692411  0.60670468  0.41978602
   0.71        0.71805556  0.66466877  0.70144494]
 [ 0.36866445  0.685       0.73888889  0.62956888  0.68881452  0.53408902
   0.75666667  0.79583333  0.70883871  0.7515257 ]
 [ 0.75334928  0.86666667  0.88888889  0.84996695  0.87230769  0.47979758
   0.74166667  0.75972222  0.694058    0.72761738]
 [ 0.40961635  0.7         0.72638889  0.62203685  0.68534632  0.15188728
   0.56583333  0.59166667  0.50199201  0.55223499]
 [ 0.0717316   0.53452381  0.69166667  0.19000565  0.50928114  0.4272361
   0.7275      0.725       0.6792117   0.71483405]
 [ 0.1226087   0.55119048  0.70694444  0.24391576  0.51734432 -0.035338
   0.48666667  0.6         0.11138803  0.41141858]
 [ 0.07405405  0.55        0.64861111  0.21547005  0.47705128  0.43534466
   0.71666667  0.79583333  0.56529451  0.70843989]
 [ 0.66571255  0.83083333  0.84166667  0.78494784  0.82604562  0.31761407
   0.62666667  0.67916667  0.48287552  0.59260573]
 [ 0.06086957  0.54375     0.88611111  0.09354143  0.5027451   0.51482171
   0.7475      0.78472222  0.71565758  0.74980714]
 [ 0.27343823  0.63809524  0.75        0.44721189  0.60236846  0.48939538
   0.73        0.74861111  0.67517912  0.72222777]
 [ 0.18076923  0.59166667  0.68194444  0.41425243  0.55766622  0.20425798
   0.60166667  0.66111111  0.42860606  0.56071262]
 [ 0.04347826  0.51785714  0.77222222  0.07071068  0.46821429  0.10536794
   0.5625      0.5875      0.48690451  0.54678044]
 [ 0.40922809  0.6825      0.68194444  0.65968541  0.66871212  0.37206294
   0.67583333  0.74861111  0.53955168  0.65620463]
 [ 0.20440973  0.6025      0.63333333  0.52833004  0.58679321  0.33503801
   0.65952381  0.81805556  0.48790768  0.67070971]
 [ 0.24901332  0.6025      0.60555556  0.60046188  0.59        0.39192565
   0.69333333  0.70555556  0.66581144  0.68576673]
 [ 0.25703663  0.62416667  0.64722222  0.49763723  0.58960317  0.57152908
   0.795       0.79583333  0.77313921  0.7922583 ]
 [ 0.36828808  0.685       0.69305556  0.60084216  0.65524004  0.35906811
   0.67416667  0.69583333  0.63557467  0.66646659]
 [ 0.53783926  0.75833333  0.85277778  0.7002105   0.78271062  0.6361104
   0.82        0.81666667  0.8024553   0.81069986]
 [ 0.03431046  0.50833333  0.59166667  0.31792644  0.4848668   0.30830522
   0.645       0.675       0.56342035  0.62008492]
 [ 0.25915895  0.625       0.69444444  0.44271028  0.59086081  0.41261954
   0.66583333  0.68333333  0.62201208  0.64952603]
 [ 0.73966866  0.84666667  0.87222222  0.82549869  0.84813437  0.5023119
   0.74166667  0.7875      0.68319082  0.7434965 ]]
GBM mean:
[0.36715025 0.6783869  0.74388889 0.5526953  0.66018816 0.36093032
 0.6735119  0.72199074 0.57055865 0.65724704]
---------------------------
---------------------------
BDDAE performance:
[[-1.93752384e-01  4.02500000e-01  4.05555556e-01  3.49334774e-01
   3.85459100e-01  1.73352138e-01  5.80357143e-01  7.50000000e-01
   4.23500057e-01  5.72292550e-01]
 [ 4.12614908e-01  7.07500000e-01  7.11111111e-01  6.95294382e-01
   7.01985236e-01  4.58776102e-01  7.25974026e-01  7.44444444e-01
   7.11557926e-01  7.24881061e-01]
 [-2.39282786e-02  4.88750000e-01  5.11111111e-01  4.20971503e-01
   4.66777019e-01  3.33333333e-01  6.66666667e-01  6.66666667e-01
   6.58618712e-01  6.62880854e-01]
 [ 2.71818547e-01  6.36250000e-01  6.44444444e-01  6.10103235e-01
   6.26428478e-01  1.76334823e-01  5.87774725e-01  7.11111111e-01
   4.63603303e-01  5.72045868e-01]
 [ 2.92388162e-01  6.53896104e-01  6.50000000e-01  6.42242146e-01
   6.38960087e-01  2.66666667e-01  6.33333333e-01  6.33333333e-01
   5.92631514e-01  6.13415162e-01]
 [ 4.22222222e-01  7.11111111e-01  7.11111111e-01  6.65073736e-01
   6.89616602e-01  2.22044605e-17  5.00000000e-01  5.00000000e-01
   4.54581531e-01  4.77479497e-01]
 [ 2.40681639e-01  6.18181818e-01  6.66666667e-01  5.18282517e-01
   5.97738871e-01  2.66666667e-01  6.33333333e-01  6.33333333e-01
   5.90887621e-01  6.12978219e-01]
 [ 5.59904009e-01  7.62500000e-01  8.22222222e-01  7.30039554e-01
   7.74339142e-01 -4.24178543e-03  4.96753247e-01  5.05555556e-01
   4.82256917e-01  4.89152628e-01]
 [ 3.76601922e-01  6.75000000e-01  7.38888889e-01  6.19797062e-01
   6.72416476e-01  2.34688995e-01  6.00000000e-01  7.16666667e-01
   4.27856732e-01  5.85362827e-01]
 [-4.44444444e-02  4.77777778e-01  4.77777778e-01  4.72125788e-01
   4.74985257e-01  2.75269668e-01  6.38750000e-01  6.44444444e-01
   6.23183940e-01  6.32184867e-01]
 [-8.19521860e-02  4.59740260e-01  5.11111111e-01  3.42333074e-01
   4.40681875e-01  9.93840578e-02  5.48051948e-01  5.77777778e-01
   5.13152109e-01  5.41205905e-01]
 [ 2.65760551e-01  6.33116883e-01  6.50000000e-01  6.21031773e-01
   6.29897809e-01  4.10128317e-01  7.05844156e-01  7.16666667e-01
   6.91627634e-01  7.00138185e-01]
 [-5.32077043e-02  4.72500000e-01  4.77777778e-01  4.54807076e-01
   4.65508021e-01  3.15442950e-01  6.57500000e-01  6.61111111e-01
   6.42022912e-01  6.50538137e-01]
 [-4.65013182e-02  4.77692308e-01  6.27777778e-01  2.37449708e-01
   4.61101743e-01  2.44444444e-01  6.22222222e-01  6.22222222e-01
   6.03686933e-01  6.13703426e-01]
 [ 1.30811911e-01  5.62500000e-01  6.38888889e-01  4.48777371e-01
   5.51111280e-01  3.01762830e-01  6.50000000e-01  6.94444444e-01
   5.96697540e-01  6.42854133e-01]
 [ 1.21186803e-01  5.63636364e-01  5.77777778e-01  5.22573390e-01
   5.44612253e-01  5.12753608e-03  5.00000000e-01  5.88888889e-01
   3.23577818e-01  4.70318830e-01]
 [ 5.05622286e-01  7.66233766e-01  7.55555556e-01  7.56890122e-01
   7.48318846e-01  1.13588577e-01  5.50000000e-01  6.05555556e-01
   4.44639523e-01  5.29039013e-01]
 [ 3.42511264e-01  7.15625000e-01  8.44444444e-01  6.05982445e-01
   6.61839670e-01 -1.95456847e-01  4.01948052e-01  4.27777778e-01
   3.54544287e-01  4.01403814e-01]
 [ 3.75701987e-01  7.02307692e-01  7.38888889e-01  6.84747143e-01
   6.84832259e-01  1.54759508e-01  5.77500000e-01  5.83333333e-01
   5.69322628e-01  5.75091288e-01]
 [ 6.27877934e-02  5.29166667e-01  6.05555556e-01  4.28804346e-01
   5.13913256e-01  9.25856640e-02  5.44805195e-01  6.05555556e-01
   4.06759206e-01  5.19378153e-01]
 [ 1.03493987e-01  5.41071429e-01  6.88888889e-01  3.95376392e-01
   5.46100346e-01  1.99673440e-01  5.98750000e-01  6.05555556e-01
   5.65768330e-01  5.84891594e-01]
 [ 5.55555556e-02  5.27777778e-01  5.27777778e-01  5.09001084e-01
   5.18707496e-01  2.31919650e-01  6.12987013e-01  6.44444444e-01
   5.88655556e-01  6.11245895e-01]
 [ 6.28257912e-02  5.31250000e-01  5.33333333e-01  5.13755639e-01
   5.22367170e-01  2.71168174e-01  6.26043956e-01  7.83333333e-01
   4.81014793e-01  6.22945226e-01]
 [ 2.22222222e-02  5.11111111e-01  5.11111111e-01  4.86398965e-01
   4.99227095e-01  1.15031310e-02  5.06250000e-01  5.11111111e-01
   4.93535142e-01  5.00961710e-01]
 [-6.58531518e-02  4.67500000e-01  4.77777778e-01  4.41223213e-01
   4.59163192e-01  1.22222222e-01  5.61111111e-01  5.61111111e-01
   5.47188554e-01  5.54536513e-01]
 [ 3.33333333e-02  5.16666667e-01  5.16666667e-01  4.71365360e-01
   5.02462574e-01  5.49326432e-02  5.28750000e-01  5.38888889e-01
   4.75551644e-01  5.15267345e-01]
 [ 4.42003069e-01  7.16666667e-01  7.66666667e-01  6.82041668e-01
   7.15366559e-01  5.00000000e-01  7.50000000e-01  7.50000000e-01
   7.36416826e-01  7.44637446e-01]
 [ 8.24973093e-02  5.44805195e-01  5.61111111e-01  5.14321546e-01
   5.32748066e-01  3.96761425e-01  6.89610390e-01  7.22222222e-01
   6.67679279e-01  6.93459779e-01]
 [ 1.58374482e-01  5.75324675e-01  6.11111111e-01  5.38256362e-01
   5.69549844e-01  3.38879823e-01  6.67500000e-01  6.77777778e-01
   6.43613548e-01  6.61149338e-01]
 [ 4.70326348e-01  7.33116883e-01  7.50000000e-01  7.18557597e-01
   7.31214460e-01  7.01836724e-02  5.29166667e-01  5.88888889e-01
   4.43404292e-01  5.17080100e-01]]
BDDAE mean:
[0.17672022 0.58937587 0.6237037  0.5365653  0.577581   0.19732859
 0.59636611 0.63240741 0.54058456 0.58641731]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.53333333 0.         0.3474359  0.
  0.5        0.77222222 0.         0.43571429]
 [0.         0.5        0.55694444 0.         0.35750916 0.
  0.5        0.61388889 0.         0.37978022]
 [0.         0.5        0.58055556 0.         0.36692308 0.
  0.5        0.51111111 0.         0.33754579]
 [0.         0.5        0.53333333 0.         0.3474359  0.
  0.5        0.75       0.         0.42821429]
 [0.         0.5        0.61388889 0.         0.37978022 0.
  0.5        0.52222222 0.         0.34249084]
 [0.         0.5        0.51111111 0.         0.33754579 0.
  0.5        0.52222222 0.         0.34249084]
 [0.         0.5        0.63611111 0.         0.38835165 0.
  0.5        0.52222222 0.         0.34249084]
 [0.         0.5        0.69444444 0.         0.40946429 0.
  0.5        0.61388889 0.         0.37978022]
 [0.         0.5        0.68333333 0.         0.40571429 0.
  0.5        0.68333333 0.         0.40571429]
 [0.         0.5        0.51111111 0.         0.33754579 0.
  0.5        0.55694444 0.         0.35750916]
 [0.         0.5        0.625      0.         0.38406593 0.
  0.5        0.61388889 0.         0.37978022]
 [0.         0.5        0.61388889 0.         0.37978022 0.
  0.5        0.61388889 0.         0.37978022]
 [0.         0.5        0.58055556 0.         0.36692308 0.
  0.5        0.56944444 0.         0.36263736]
 [0.         0.5        0.70555556 0.         0.41321429 0.
  0.5        0.51111111 0.         0.33754579]
 [0.         0.5        0.69444444 0.         0.40946429 0.
  0.5        0.64722222 0.         0.39263736]
 [0.         0.5        0.61388889 0.         0.37978022 0.
  0.5        0.65833333 0.         0.39692308]
 [0.         0.5        0.60277778 0.         0.37549451 0.
  0.5        0.60277778 0.         0.37549451]
 [0.         0.5        0.88611111 0.         0.46980392 0.
  0.5        0.60277778 0.         0.37549451]
 [0.         0.5        0.71666667 0.         0.41696429 0.
  0.5        0.55694444 0.         0.35750916]
 [0.         0.5        0.67083333 0.         0.40131868 0.
  0.5        0.625      0.         0.38406593]
 [0.         0.5        0.77222222 0.         0.43571429 0.
  0.5        0.55694444 0.         0.35750916]
 [0.         0.5        0.51111111 0.         0.33754579 0.
  0.5        0.63611111 0.         0.38835165]
 [0.         0.5        0.58055556 0.         0.36692308 0.
  0.5        0.75       0.         0.42821429]
 [0.         0.5        0.52222222 0.         0.34249084 0.
  0.5        0.55694444 0.         0.35750916]
 [0.         0.5        0.55694444 0.         0.35750916 0.
  0.5        0.52222222 0.         0.34249084]
 [0.         0.5        0.52222222 0.         0.34249084 0.
  0.5        0.55694444 0.         0.35750916]
 [0.         0.5        0.69444444 0.         0.40946429 0.
  0.5        0.51111111 0.         0.33754579]
 [0.         0.5        0.625      0.         0.38406593 0.
  0.5        0.60277778 0.         0.37549451]
 [0.         0.5        0.625      0.         0.38406593 0.
  0.5        0.55694444 0.         0.35750916]
 [0.         0.5        0.61388889 0.         0.37978022 0.
  0.5        0.64722222 0.         0.39263736]]
DUMMY mean:
[0.         0.5        0.61958333 0.         0.38048553 0.
 0.5        0.59888889 0.         0.37294567]
---------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_1_2_55
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.398 0.697 0.74  0.623 0.686 0.41  0.702 0.735 0.642 0.692]
 [0.372 0.689 0.72  0.631 0.679 0.387 0.69  0.704 0.663 0.677]
 [0.439 0.711 0.747 0.642 0.699 0.422 0.711 0.736 0.641 0.702]
 [0.065 0.53  0.643 0.099 0.43  0.12  0.559 0.651 0.184 0.469]
 [0.367 0.678 0.744 0.553 0.66  0.361 0.674 0.722 0.571 0.657]
 [0.177 0.589 0.624 0.537 0.578 0.197 0.596 0.632 0.541 0.586]
 [0.    0.5   0.62  0.    0.38  0.    0.5   0.599 0.    0.373]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.227 0.111 0.096 0.18  0.119 0.168 0.084 0.071 0.137 0.088]
 [0.198 0.1   0.092 0.164 0.106 0.14  0.078 0.076 0.1   0.081]
 [0.188 0.101 0.094 0.176 0.106 0.165 0.074 0.069 0.138 0.076]
 [0.129 0.059 0.089 0.186 0.094 0.193 0.096 0.078 0.276 0.136]
 [0.236 0.116 0.093 0.229 0.13  0.162 0.082 0.071 0.177 0.096]
 [0.202 0.103 0.112 0.127 0.103 0.154 0.077 0.085 0.106 0.08 ]
 [0.    0.    0.085 0.    0.031 0.    0.    0.071 0.    0.027]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 57.  16.  13.  29.  17.  41.  12.  10.  21.  13.]
 [ 53.  15.  13.  26.  16.  36.  11.  11.  15.  12.]
 [ 43.  14.  13.  27.  15.  39.  10.   9.  22.  11.]
 [197.  11.  14. 188.  22. 161.  17.  12. 150.  29.]
 [ 64.  17.  13.  41.  20.  45.  12.  10.  31.  15.]
 [114.  17.  18.  24.  18.  78.  13.  13.  20.  14.]
 [  0.   0.  14.   0.   8.   0.   0.  12.   0.   7.]]
-------------------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_1_2_55
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  24.992
step (sec):  12.496
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  12.496
Number of windows / instances:  88
Elapsed time: 3238.217437597116 minutes
Elapsed time: 53.9702906266186 hours
