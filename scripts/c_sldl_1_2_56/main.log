2024-05-17 03:52:02.631040: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-17 03:52:06.299026: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-17 03:52:15.344052: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  27.0
step (sec):  27.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_56\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

d
[1m 1/30[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1060
[1m 5/30[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2917
[1m 9/30[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9877 - loss: 0.2998
[1m13/30[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9462 - loss: 0.3350
[1m17/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8868 - loss: 0.3740
[1m21/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8522 - loss: 0.3995
[1m25/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8349 - loss: 0.4179
[1m29/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8275 - loss: 0.4290
[1m30/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.8257 - loss: 0.4323 - val_binary_accuracy: 0.5000 - val_loss: 0.6761
Epoch 5/10

[1m 1/30[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1626
[1m 5/30[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1560
[1m 9/30[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9579 - loss: 0.1885
[1m13/30[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9256 - loss: 0.2271
[1m17/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8939 - loss: 0.2762
[1m21/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8749 - loss: 0.3064
[1m25/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8558 - loss: 0.3328
[1m28/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8448 - loss: 0.3478
[1m30/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.8337 - loss: 0.3608 - val_binary_accuracy: 0.7500 - val_loss: 0.4151
Epoch 6/10

[1m 1/30[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.4927
[1m 5/30[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9600 - loss: 0.3880
[1m 9/30[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8488 - loss: 0.4726
[1m13/30[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8143 - loss: 0.4976
[1m17/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8122 - loss: 0.4978
[1m21/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8186 - loss: 0.4866
[1m24/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8249 - loss: 0.4784
[1m28/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8298 - loss: 0.4701
[1m30/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.8332 - loss: 0.4628 - val_binary_accuracy: 0.7500 - val_loss: 0.6534
Epoch 7/10

[1m 1/30[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.0588
[1m 5/30[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7433 - loss: 0.5544
[1m 9/30[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7968 - loss: 0.4920
[1m13/30[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8323 - loss: 0.4388
[1m17/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8565 - loss: 0.3998
[1m20/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8701 - loss: 0.3799
[1m25/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8874 - loss: 0.3598
[1m28/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8955 - loss: 0.3522
[1m30/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9023 - loss: 0.3460 - val_binary_accuracy: 0.7500 - val_loss: 0.5867
Epoch 8/10

[1m 1/30[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1195
[1m 4/30[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 21ms/step - binary_accuracy: 1.0000 - loss: 0.1639
[1m 9/30[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1601
[1m12/30[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9855 - loss: 0.1604
[1m16/30[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9718 - loss: 0.1645
[1m20/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9666 - loss: 0.1694
[1m25/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9645 - loss: 0.1752
[1m28/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9643 - loss: 0.1783
[1m30/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9645 - loss: 0.1816 - val_binary_accuracy: 0.7500 - val_loss: 0.6508
Epoch 9/10

[1m 1/30[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1861
[1m 5/30[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8433 - loss: 0.2897
[1m 8/30[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8478 - loss: 0.2811
[1m12/30[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8664 - loss: 0.2568
[1m16/30[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8825 - loss: 0.2404
[1m21/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8978 - loss: 0.2292
[1m24/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9052 - loss: 0.2265
[1m28/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9133 - loss: 0.2235
[1m30/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9184 - loss: 0.2212 - val_binary_accuracy: 0.7500 - val_loss: 0.7441
Epoch 10/10

[1m 1/30[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1079
[1m 5/30[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9100 - loss: 0.2032
[1m 9/30[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8894 - loss: 0.1999
[1m13/30[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8964 - loss: 0.1882
[1m17/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9055 - loss: 0.1854
[1m21/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9137 - loss: 0.1783
[1m25/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9207 - loss: 0.1735
[1m29/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9266 - loss: 0.1715
[1m30/30[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9292 - loss: 0.1706 - val_binary_accuracy: 0.7500 - val_loss: 0.6626

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
predicted [0.4684568  0.06944828 0.76293516 0.07575748 0.5259919  0.980212
 0.998631   0.39852643 0.9361466 ]
predicted [0 0 1 0 1 1 1 0 1]
expected [ True  True  True False False  True  True False False]
accuracy: 0.5555555555555556
confusion matrix: 
[[2 2]
 [2 3]]
              precision    recall  f1-score   support

       False       0.50      0.50      0.50         4
        True       0.60      0.60      0.60         5

    accuracy                           0.56         9
   macro avg       0.55      0.55      0.55         9
weighted avg       0.56      0.56      0.56         9

macro avg f1-score: 0.55
macro avg (UAR): 0.55
Sensitivity:  0.5
Specificity:  0.6
g-mean:  0.5477225575051661
-------- Model Performance ----------: 
accuracy:  [0.66666667 0.55555556 0.55555556 0.77777778 0.66666667 0.77777778
 0.66666667 0.66666667 0.55555556 0.55555556]
gmean:  [0.63245553 0.4472136  0.4472136  0.70710678 0.5        0.70710678
 0.63245553 0.63245553 0.4472136  0.54772256]
f1_score:  [0.64935065 0.5        0.5        0.75       0.58461538 0.75
 0.64935065 0.64935065 0.5        0.55      ]
UAR:  [0.65  0.525 0.525 0.75  0.625 0.75  0.65  0.65  0.525 0.55 ]
Cohen Kappa score:  [0.30769231 0.05263158 0.05263158 0.52631579 0.27027027 0.52631579
 0.30769231 0.30769231 0.05263158 0.1       ]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  27.008
step (sec):  27.008
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  43
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.962  0.983  0.98   0.982  0.98   0.428  0.725  0.765  0.575  0.687]
 [ 0.828  0.892  0.905  0.782  0.887  0.379  0.667  0.7    0.553  0.643]
 [ 0.678  0.958  0.96   0.799  0.958  0.353  0.667  0.675  0.635  0.625]
 [ 0.     0.5    0.605  0.     0.374  0.     0.5    0.63   0.     0.384]
 [ 0.712  0.908  0.93   0.852  0.896  0.359  0.683  0.73   0.552  0.663]
 [ 0.25   0.62   0.644  0.57   0.608 -0.059  0.458  0.467  0.423  0.442]
 [ 0.     0.5    0.605  0.     0.374  0.     0.5    0.63   0.     0.384]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.36153846  0.68333333  0.69        0.60236034  0.66083333         nan
   0.68333333  0.835       0.38164966  0.63365079]
 [ 0.17857143  0.59166667  0.59        0.52760209  0.57333333  0.38333333
   0.7         0.745       0.51009536  0.65202381]
 [ 0.42062937  0.71666667  0.715       0.59614203  0.66904762  0.23461538
   0.61666667  0.625       0.46153551  0.57416667]
 [ 0.32820513  0.66666667  0.675       0.55151672  0.62916667  0.25151515
   0.625       0.73        0.37071068  0.59464286]
 [ 0.66363636  0.825       0.865       0.75355339  0.81809524  0.42575758
   0.70833333  0.72        0.61128842  0.68404762]
 [ 0.26318681  0.63333333  0.625       0.53140257  0.595       0.58787879
   0.79166667  0.8         0.76902344  0.78619048]
 [ 0.43787879  0.725       0.775       0.58080604  0.69488095  0.24175824
   0.61666667  0.61        0.47366754  0.56666667]
 [ 0.71666667  0.85833333  0.91        0.75773503  0.84404762  0.39714286
   0.71666667  0.68        0.58094011  0.6375    ]
 [ 0.30454545  0.66666667  0.74        0.45472067  0.6122619  -0.01841492
   0.49166667  0.615       0.19082483  0.45130952]
 [ 0.1952381   0.6         0.605       0.46902344  0.56083333  0.19487179
   0.60833333  0.61        0.44614203  0.55416667]
 [ 0.21363636  0.61666667  0.635       0.51783039  0.5952381   0.27575758
   0.64166667  0.67        0.48080604  0.59357143]
 [ 0.65909091  0.825       0.835       0.75355339  0.80571429  0.41941725
   0.71666667  0.72        0.63854107  0.69119048]
 [ 0.33974359  0.68333333  0.695       0.54494897  0.6375      0.32472527
   0.675       0.655       0.54470188  0.61166667]
 [ 0.24941725  0.65        0.645       0.55504434  0.60119048  0.30582751
   0.65        0.655       0.56069189  0.62452381]
 [ 0.20454545  0.59166667  0.725       0.27071068  0.54261905  0.09249417
   0.55        0.58        0.31927053  0.48702381]
 [-0.15460539  0.4         0.45        0.18224619  0.35761905  0.18787879
   0.59166667  0.7         0.32844571  0.55392857]
 [ 0.19010989  0.6         0.635       0.46009536  0.57452381  0.50909091
   0.75833333  0.795       0.59378169  0.7127381 ]
 [        nan  0.75        0.885       0.5         0.71746032  0.27380952
   0.65833333  0.63        0.55721508  0.59833333]
 [ 0.34249417  0.675       0.73        0.52700556  0.64357143  0.0529304
   0.525       0.52        0.42760209  0.49      ]
 [ 0.25454545  0.63333333  0.71        0.41009536  0.58642857  0.1969697
   0.61666667  0.635       0.39174502  0.54440476]
 [        nan  0.6         0.81        0.2         0.54603175  0.26282051
   0.63333333  0.63        0.60092019  0.62166667]
 [ 0.17249417  0.58333333  0.59        0.47366754  0.55619048  0.28787879
   0.65        0.69        0.49174502  0.61071429]
 [ 0.24645688  0.63333333  0.635       0.45092019  0.57190476  0.17575758
   0.59583333  0.745       0.25731322  0.5375    ]
 [ 0.05060606  0.51666667  0.525       0.32307101  0.46047619  0.38333333
   0.69166667  0.745       0.49915638  0.65202381]
 [ 0.30850816  0.66666667  0.705       0.54174502  0.6397619   0.51153846
   0.75833333  0.775       0.55236034  0.695     ]
 [-0.01818182  0.48333333  0.51        0.21213203  0.42        0.07032967
   0.53333333  0.52        0.43998121  0.50166667]
 [ 0.35909091  0.66666667  0.77        0.44142136  0.63845238  0.31318681
   0.65833333  0.66        0.51069189  0.60916667]
 [ 0.11818182  0.54166667  0.605       0.34142136  0.51845238  0.37249417
   0.68333333  0.745       0.51153551  0.65904762]
 [ 0.35454545  0.68333333  0.71        0.51009536  0.63404762  0.41794872
   0.725       0.71        0.71101556  0.7       ]
 [ 0.96153846  0.98333333  0.98        0.98164966  0.98        0.42820513
   0.725       0.765       0.57543135  0.68702381]]
KNN mean:
[0.31151123 0.65833333 0.69916667 0.50075054 0.62282275 0.29520181
 0.65319444 0.68383333 0.49296097 0.61051852]
---------------------------
---------------------------
DT performance:
[[ 0.23311688  0.61666667  0.615       0.61009536  0.60285714 -0.00164502
   0.55416667  0.7         0.1682522   0.47015873]
 [ 0.02554113  0.45833333  0.47        0.34057774  0.41690476  0.11666667
   0.55833333  0.595       0.44391576  0.53285714]
 [ 0.03311688  0.55        0.54        0.42307101  0.50452381 -0.03181818
   0.46666667  0.485       0.37844571  0.42702381]
 [ 0.24615385  0.60833333  0.62        0.44318517  0.5425      0.09004662
   0.56666667  0.65        0.35485474  0.52428571]
 [ 0.52062937  0.75833333  0.765       0.69410536  0.73571429  0.3
   0.63333333  0.63        0.55604779  0.595     ]
 [ 0.37077922  0.7         0.69        0.75387706  0.65880952  0.47820513
   0.75833333  0.76        0.7222274   0.74952381]
 [ 0.33707959  0.69166667  0.705       0.55504434  0.65357143  0.05
   0.55        0.56        0.25295686  0.50083333]
 [ 0.81666667  0.89166667  0.915       0.88164966  0.88119048  0.35677656
   0.65        0.65        0.55504434  0.58952381]
 [ 0.15403263  0.53333333  0.55        0.47163087  0.49952381  0.07484848
   0.53333333  0.6         0.36783039  0.5052381 ]
 [ 0.06491841  0.55833333  0.565       0.32760209  0.49357143  0.23744589
   0.56666667  0.57        0.44915638  0.52357143]
 [ 0.36923077  0.625       0.61        0.70387706  0.5625      0.19941725
   0.65833333  0.675       0.42103434  0.63785714]
 [ 0.49121212  0.775       0.76        0.79614203  0.73619048  0.16282051
   0.6         0.65        0.46009536  0.54785714]
 [ 0.22484848  0.66666667  0.65        0.51783039  0.6002381   0.10714286
   0.51666667  0.535       0.24711971  0.46285714]
 [ 0.25370629  0.58333333  0.56        0.3793659   0.5125      0.10128205
   0.525       0.515       0.32366754  0.46547619]
 [ 0.23275058  0.80833333  0.82        0.61565965  0.75988095  0.11794872
   0.63333333  0.65        0.68256985  0.60833333]
 [-0.14969697  0.46666667  0.465       0.19855986  0.40690476  0.27307692
   0.63333333  0.71        0.39494897  0.6052381 ]
 [ 0.48974359  0.775       0.795       0.67103434  0.74119048  0.30128205
   0.73333333  0.73        0.61305223  0.71785714]
 [        nan  0.6375      0.765       0.28660254  0.51269841  0.33787879
   0.68333333  0.67        0.52760209  0.64452381]
 [ 0.46941725  0.7         0.73        0.73112971  0.65928571  0.26923077
   0.625       0.625       0.55355339  0.6       ]
 [ 0.35454545  0.65833333  0.68        0.55184038  0.63833333  0.01060606
   0.45        0.44        0.21009536  0.38880952]
 [-0.03181818  0.5125      0.645       0.17320508  0.43706349  0.08534799
   0.55        0.555       0.49915638  0.52452381]
 [ 0.22377622  0.56666667  0.565       0.53460652  0.52833333  0.40333333
   0.65833333  0.725       0.48618073  0.62595238]
 [ 0.27554113  0.625       0.635       0.57307101  0.60833333 -0.03658009
   0.57083333  0.65        0.24990186  0.50440476]
 [ 0.01615385  0.53333333  0.53        0.4624557   0.46690476  0.38132201
   0.76666667  0.79        0.6624557   0.75166667]
 [ 0.37671329  0.69166667  0.675       0.59174502  0.65357143  0.36818182
   0.60833333  0.635       0.51213203  0.56690476]
 [ 0.52272727  0.73333333  0.75        0.67426407  0.72428571  0.07380952
   0.55        0.555       0.44378169  0.50416667]
 [ 0.37909091  0.6         0.625       0.5624557   0.53154762  0.28461538
   0.575       0.59        0.45295686  0.52940476]
 [ 0.01645022  0.375       0.455       0.24318517  0.34702381  0.18168831
   0.60833333  0.62        0.42844571  0.55619048]
 [ 0.43916084  0.675       0.655       0.52307101  0.60035714 -0.17738928
   0.5         0.48        0.36153551  0.42857143]
 [ 0.82820513  0.89166667  0.905       0.78164966  0.88666667  0.37948718
   0.66666667  0.7         0.55328053  0.64285714]]
DT mean:
[0.29599286 0.64222222 0.657      0.53575298 0.59676587 0.18316761
 0.59833333 0.62333333 0.44440991 0.55771561]
---------------------------
---------------------------
RF performance:
[[ 5.50000000e-01  7.50000000e-01  7.60000000e-01  7.64492371e-01
   7.30833333e-01  3.81818182e-02  6.33333333e-01  7.70000000e-01
   2.81649658e-01  5.45753968e-01]
 [ 2.33333333e-01  5.75000000e-01  5.75000000e-01  3.98312766e-01
   5.26071429e-01  1.60606061e-01  6.25000000e-01  6.30000000e-01
   5.75431351e-01  5.77142857e-01]
 [ 2.32750583e-01  5.50000000e-01  5.65000000e-01  4.51516719e-01
   5.21666667e-01 -6.53846154e-02  5.08333333e-01  5.05000000e-01
   3.32842712e-01  4.68333333e-01]
 [ 3.55827506e-01  6.75000000e-01  6.75000000e-01  6.13895843e-01
   6.62857143e-01  3.37878788e-01  5.58333333e-01  6.95000000e-01
   2.21034343e-01  5.18452381e-01]
 [ 4.25757576e-01  7.58333333e-01  7.70000000e-01  6.82842712e-01
   7.20238095e-01  5.25757576e-01  6.41666667e-01  6.55000000e-01
   5.19270534e-01  6.26428571e-01]
 [ 3.14285714e-01  7.08333333e-01  7.05000000e-01  6.14626437e-01
   6.75000000e-01  4.83116883e-01  7.58333333e-01  7.55000000e-01
   6.44378220e-01  7.48333333e-01]
 [ 3.49783550e-01  7.16666667e-01  7.45000000e-01  6.14816036e-01
   6.84047619e-01 -9.99000999e-05  5.66666667e-01  5.65000000e-01
   3.82842712e-01  5.18333333e-01]
 [ 6.92207792e-01  9.33333333e-01  9.55000000e-01  7.63299316e-01
   9.22857143e-01  3.53116883e-01  7.66666667e-01  7.45000000e-01
   7.38541068e-01  6.95833333e-01]
 [ 3.61841492e-01  7.08333333e-01  7.40000000e-01  3.84009994e-01
   6.29880952e-01 -1.30303030e-02  5.66666667e-01  5.95000000e-01
   2.07735027e-01  5.23214286e-01]
 [ 5.27972028e-02  4.75000000e-01  4.80000000e-01  5.51516719e-01
   4.44523810e-01 -4.23076923e-02  3.00000000e-01  3.30000000e-01
   3.99156383e-01  2.85238095e-01]
 [ 3.83333333e-01  5.41666667e-01  5.15000000e-01  4.74834823e-01
   4.90238095e-01  3.34848485e-01  6.58333333e-01  6.70000000e-01
   6.23991207e-01  6.19523810e-01]
 [ 5.75757576e-01  7.50000000e-01  7.90000000e-01  7.72227397e-01
   7.24761905e-01  5.70629371e-01  6.58333333e-01  6.70000000e-01
   5.60095363e-01  6.03214286e-01]
 [ 2.30303030e-01  7.33333333e-01  7.10000000e-01  3.69270534e-01
   6.65714286e-01  1.91758242e-01  6.16666667e-01  6.25000000e-01
   5.10095363e-01  5.80000000e-01]
 [ 1.54545455e-01  6.25000000e-01  6.30000000e-01  4.92938076e-01
   5.63333333e-01  1.08874459e-01  5.08333333e-01  5.10000000e-01
   4.69867061e-01  4.76666667e-01]
 [ 7.60839161e-02  5.91666667e-01  6.95000000e-01  2.67830390e-01
   5.60000000e-01  4.04761905e-02  6.33333333e-01  6.30000000e-01
   3.19270534e-01  6.11190476e-01]
 [-2.19696970e-01  4.58333333e-01  4.70000000e-01  1.48559856e-01
   4.22380952e-01  2.16083916e-01  6.41666667e-01  7.00000000e-01
   4.31649658e-01  6.11904762e-01]
 [ 6.32750583e-01  7.66666667e-01  7.95000000e-01  7.80806041e-01
   7.37380952e-01  4.21212121e-01  8.83333333e-01  8.75000000e-01
   5.72227397e-01  8.73333333e-01]
 [            nan  7.12500000e-01  8.10000000e-01  5.00000000e-01
   6.25396825e-01  4.07878788e-01  6.83333333e-01  6.70000000e-01
   4.09251746e-01  6.44523810e-01]
 [ 3.01515152e-01  8.08333333e-01  8.20000000e-01  5.23071014e-01
   7.76904762e-01  1.84615385e-01  6.75000000e-01  6.80000000e-01
   5.23667542e-01  6.51666667e-01]
 [ 4.50000000e-01  6.33333333e-01  6.95000000e-01  4.78769370e-01
   6.09047619e-01  9.69696970e-02  5.75000000e-01  5.90000000e-01
   3.25565417e-01  5.19166667e-01]
 [            nan  4.79166667e-01  6.95000000e-01  8.66025404e-02
   4.03531746e-01  4.00699301e-01  7.16666667e-01  7.20000000e-01
   5.69023444e-01  7.07857143e-01]
 [ 5.38461538e-02  5.16666667e-01  5.15000000e-01  5.56891410e-01
   5.05000000e-01  2.33333333e-01  6.08333333e-01  6.65000000e-01
   4.25431351e-01  5.53571429e-01]
 [ 2.99783550e-01  6.58333333e-01  6.95000000e-01  3.74834823e-01
   6.19166667e-01  2.90995671e-01  5.33333333e-01  6.50000000e-01
   4.38962877e-01  4.70119048e-01]
 [ 3.14871795e-01  5.91666667e-01  5.80000000e-01  5.01516719e-01
   5.55000000e-01  4.45238095e-01  7.33333333e-01  7.65000000e-01
   6.64492371e-01  7.07380952e-01]
 [ 4.59090909e-01  6.75000000e-01  7.15000000e-01  5.34009994e-01
   6.59523810e-01  5.59090909e-01  7.33333333e-01  7.55000000e-01
   4.32842712e-01  6.98928571e-01]
 [ 5.66083916e-01  5.75000000e-01  5.95000000e-01  4.95545501e-01
   5.58571429e-01  1.71212121e-01  5.66666667e-01  5.60000000e-01
   4.62455699e-01  5.24761905e-01]
 [ 3.09160839e-01  7.16666667e-01  7.55000000e-01  5.80806041e-01
   6.93690476e-01  1.84848485e-01  6.91666667e-01  7.05000000e-01
   4.52956863e-01  6.54166667e-01]
 [-1.00000000e-01  3.91666667e-01  4.45000000e-01  1.20710678e-01
   3.50238095e-01  2.28904429e-01  6.75000000e-01  7.00000000e-01
   4.78445705e-01  6.16428571e-01]
 [ 3.00000000e-01  6.25000000e-01  6.80000000e-01  5.10095363e-01
   6.02261905e-01  1.91391941e-01  5.91666667e-01  5.95000000e-01
   5.61535507e-01  5.40000000e-01]
 [ 6.78205128e-01  9.58333333e-01  9.60000000e-01  7.99156383e-01
   9.58333333e-01  3.53333333e-01  6.66666667e-01  6.75000000e-01
   6.34606521e-01  6.25357143e-01]]
RF mean:
[0.32265068 0.65527778 0.6845     0.50692686 0.61994841 0.24700766
 0.6325     0.65516667 0.47231054 0.59322751]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.53        0.          0.34583333         nan
   0.55        0.79        0.1         0.49047619]
 [ 0.          0.5         0.555       0.          0.35535714  0.
   0.5         0.63        0.          0.38392857]
 [ 0.          0.5         0.555       0.          0.35535714  0.12857143
   0.56666667  0.55        0.15773503  0.4202381 ]
 [ 0.11608392  0.55833333  0.55        0.15236034  0.41809524  0.
   0.5         0.705       0.          0.4125    ]
 [ 0.          0.5         0.63        0.          0.38392857  0.24978355
   0.625       0.625       0.39831277  0.54619048]
 [ 0.09010989  0.55        0.53        0.13938469  0.4002381   0.
   0.5         0.53        0.          0.34583333]
 [ 0.          0.5         0.655       0.          0.39345238 -0.02619048
   0.48333333  0.47        0.05773503  0.33690476]
 [ 0.          0.5         0.705       0.          0.4125      0.
   0.5         0.63        0.          0.38392857]
 [ 0.          0.5         0.655       0.          0.39345238  0.
   0.5         0.68        0.          0.40297619]
 [ 0.11608392  0.55833333  0.55        0.15236034  0.41809524  0.
   0.5         0.555       0.          0.35535714]
 [ 0.          0.5         0.605       0.          0.37440476  0.
   0.5         0.605       0.          0.37440476]
 [ 0.          0.5         0.58        0.          0.36488095  0.
   0.5         0.605       0.          0.37440476]
 [ 0.          0.5         0.605       0.          0.37440476  0.
   0.5         0.555       0.          0.35535714]
 [ 0.          0.5         0.63        0.          0.38392857 -0.1030303
   0.45        0.45        0.          0.30714286]
 [ 0.          0.5         0.705       0.          0.4125      0.
   0.5         0.63        0.          0.38392857]
 [ 0.          0.5         0.605       0.          0.37440476  0.
   0.5         0.655       0.          0.39345238]
 [ 0.          0.5         0.58        0.          0.36488095  0.
   0.5         0.605       0.          0.37440476]
 [        nan  0.7         0.865       0.4         0.66190476  0.
   0.5         0.58        0.          0.36488095]
 [ 0.          0.5         0.705       0.          0.4125      0.
   0.5         0.53        0.          0.34583333]
 [ 0.          0.5         0.655       0.          0.39345238  0.
   0.5         0.63        0.          0.38392857]
 [        nan  0.55        0.79        0.1         0.49047619  0.0452381
   0.525       0.51        0.11547005  0.37857143]
 [ 0.00128205  0.5         0.49        0.09855986  0.3602381   0.
   0.5         0.655       0.          0.39345238]
 [ 0.          0.5         0.63        0.          0.38392857  0.
   0.5         0.725       0.          0.41944444]
 [ 0.          0.5         0.58        0.          0.36488095  0.
   0.5         0.58        0.          0.36488095]
 [ 0.          0.5         0.605       0.          0.37440476  0.06153846
   0.53333333  0.53        0.08164966  0.37940476]
 [ 0.          0.5         0.555       0.          0.35535714  0.
   0.5         0.53        0.          0.34583333]
 [ 0.          0.5         0.705       0.          0.4125      0.03916084
   0.51666667  0.51        0.11153551  0.37809524]
 [ 0.          0.5         0.655       0.          0.39345238  0.
   0.5         0.63        0.          0.38392857]
 [ 0.          0.5         0.605       0.          0.37440476  0.
   0.5         0.58        0.          0.36488095]
 [ 0.          0.5         0.605       0.          0.37440476  0.
   0.5         0.63        0.          0.38392857]]
SVM mean:
[0.01155571 0.51388889 0.62233333 0.03475551 0.39592063 0.01362316
 0.50833333 0.59633333 0.03408127 0.38428307]
---------------------------
---------------------------
GBM performance:
[[ 0.35909091  0.70833333  0.71        0.69378169  0.70285714         nan
   0.625       0.795       0.2         0.57242063]
 [ 0.16190476  0.54166667  0.545       0.33080604  0.49083333  0.2
   0.58333333  0.67        0.36986706  0.55869048]
 [ 0.19941725  0.625       0.63        0.43400999  0.57285714 -0.08776224
   0.45        0.465       0.24142136  0.40369048]
 [ 0.0506993   0.55        0.55        0.36389584  0.49952381  0.01363636
   0.55833333  0.685       0.23938469  0.5025    ]
 [ 0.62062937  0.81666667  0.84        0.75472067  0.79857143  0.33787879
   0.66666667  0.675       0.61902344  0.65619048]
 [ 0.39220779  0.7         0.69        0.59057774  0.65880952  0.48275058
   0.74166667  0.745       0.71364875  0.73119048]
 [ 0.24978355  0.69166667  0.7         0.53854107  0.65285714  0.16868132
   0.58333333  0.59        0.44378169  0.53583333]
 [ 0.86153846  0.93333333  0.955       0.83938469  0.92285714  0.30766234
   0.71666667  0.735       0.53316638  0.66857143]
 [ 0.22454545  0.65        0.715       0.44915638  0.60607143 -0.02969697
   0.46666667  0.585       0.1         0.39880952]
 [ 0.1469697   0.575       0.585       0.32760209  0.5252381   0.05714286
   0.55        0.55        0.41986706  0.515     ]
 [ 0.30128205  0.70833333  0.71        0.53256985  0.64916667  0.25128205
   0.63333333  0.65        0.48020951  0.59119048]
 [ 0.62121212  0.81666667  0.815       0.79293808  0.80119048  0.16153846
   0.61666667  0.67        0.35472067  0.55452381]
 [ 0.12151515  0.58333333  0.625       0.37760209  0.52821429  0.02472527
   0.5         0.505       0.30472067  0.445     ]
 [ 0.15        0.58333333  0.63        0.38080604  0.54        0.13333333
   0.55833333  0.56        0.34915638  0.53190476]
 [ 0.12820513  0.56666667  0.725       0.08164966  0.50261905  0.08939394
   0.51666667  0.6         0.24142136  0.47333333]
 [-0.18333333  0.4         0.465       0.07071068  0.32833333  0.18275058
   0.6         0.695       0.4124557   0.54261905]
 [ 0.53275058  0.76666667  0.795       0.63080604  0.74071429  0.32575758
   0.65833333  0.695       0.59057774  0.61357143]
 [        nan  0.65        0.815       0.2         0.54761905  0.30275058
   0.70833333  0.69        0.54174502  0.64619048]
 [-0.01969697  0.55        0.685       0.18164966  0.48166667  0.27517483
   0.64166667  0.645       0.51449237  0.6052381 ]
 [ 0.28484848  0.65833333  0.705       0.48400999  0.61511905 -0.1030303
   0.50833333  0.535       0.33938469  0.46440476]
 [-0.03333333  0.48333333  0.715       0.          0.41428571  0.26773227
   0.63333333  0.63        0.61069189  0.62119048]
 [ 0.07377622  0.55833333  0.56        0.40295686  0.54619048  0.36666667
   0.68333333  0.77        0.41009536  0.63107143]
 [ 0.18333333  0.6         0.655       0.37071068  0.55702381  0.02272727
   0.52916667  0.645       0.23896288  0.46845238]
 [ 0.01491841  0.5         0.515       0.38460652  0.45607143  0.36132201
   0.68333333  0.695       0.5722274   0.65119048]
 [ 0.26666667  0.65833333  0.72        0.47071068  0.62452381  0.13181818
   0.56666667  0.585       0.40355339  0.53      ]
 [ 0.45909091  0.75        0.77        0.62426407  0.71654762  0.08787879
   0.51666667  0.53        0.25689141  0.4575    ]
 [ 0.34242424  0.61666667  0.695       0.49174502  0.56559524  0.18461538
   0.59166667  0.61        0.45295686  0.53833333]
 [-0.19848485  0.41666667  0.54        0.07071068  0.37        0.19835498
   0.60833333  0.645       0.43998121  0.55761905]
 [ 0.33787879  0.68333333  0.755       0.44142136  0.63464286  0.05897436
   0.50833333  0.525       0.36389584  0.4575    ]
 [ 0.71153846  0.90833333  0.93        0.85236034  0.89619048  0.35850816
   0.68333333  0.73        0.55151672  0.66309524]]
GBM mean:
[0.25384064 0.64166667 0.6915     0.43882348 0.59820635 0.17698508
 0.59625    0.63683333 0.41032725 0.55289418]
---------------------------
---------------------------
BDDAE performance:
[[ 0.06249154  0.5325      0.52222222  0.49938393  0.50993146  0.11166008
   0.55357143  0.72222222  0.32686821  0.54096154]
 [ 0.2600385   0.6325      0.63333333  0.59773042  0.61476024  0.30161838
   0.66666667  0.65555556  0.61926815  0.63151487]
 [ 0.14841511  0.575       0.58888889  0.50771026  0.55583167  0.63850802
   0.8175      0.82222222  0.80520794  0.81542569]
 [-0.03458646  0.4825      0.47777778  0.46323972  0.46982684  0.19064935
   0.59166667  0.65555556  0.52250022  0.58747253]
 [ 0.17129518  0.59166667  0.6         0.50672801  0.5627306   0.33950247
   0.6775      0.65555556  0.60260652  0.63640776]
 [ 0.15242929  0.58        0.58888889  0.48423416  0.55237096  0.17755166
   0.5925      0.56666667  0.45940323  0.52956876]
 [-0.08013986  0.46666667  0.56666667  0.2136522   0.43141858  0.0188228
   0.5125      0.52222222  0.43090406  0.48894855]
 [ 0.1304995   0.575       0.64444444  0.38936367  0.53731518 -0.02344655
   0.49166667  0.54444444  0.35673613  0.47470779]
 [ 0.33895105  0.65833333  0.74444444  0.49102672  0.63625375  0.39363636
   0.68333333  0.77777778  0.5060096   0.66148352]
 [ 0.02438404  0.51        0.52222222  0.47644196  0.5037987   0.1868409
   0.59        0.61111111  0.52141739  0.57651238]
 [ 0.02680359  0.5125      0.53333333  0.40386199  0.48460317 -0.17957814
   0.41        0.43333333  0.23829451  0.36676768]
 [ 0.15636493  0.58        0.58888889  0.51514405  0.55989094  0.36061954
   0.68        0.67777778  0.66466552  0.67063853]
 [ 0.2204543   0.6125      0.61111111  0.60235581  0.6053824   0.50983552
   0.7575      0.75555556  0.73755906  0.74672799]
 [ 0.13081918  0.575       0.57777778  0.46278748  0.53427323  0.19939065
   0.5975      0.61111111  0.54217288  0.58521118]
 [ 0.02902098  0.51666667  0.61111111  0.28690257  0.48154845  0.2521978
   0.63333333  0.63333333  0.61821996  0.61132395]
 [-0.01356905  0.495       0.52222222  0.30002685  0.44415307 -0.099001
   0.45        0.52222222  0.29825002  0.4332018 ]
 [ 0.10628976  0.5525      0.56666667  0.50390522  0.54217532  0.24885436
   0.6175      0.64444444  0.55466567  0.60194306]
 [ 0.12951715  0.58125     0.72222222  0.35226202  0.52900776 -0.21410397
   0.3925      0.4         0.33164962  0.37807554]
 [ 0.11108891  0.55833333  0.58888889  0.48931838  0.53279609  0.04700835
   0.5225      0.51111111  0.47543413  0.49907648]
 [ 0.14603397  0.56666667  0.62222222  0.48038674  0.54857531 -0.08328671
   0.45833333  0.55555556  0.22023591  0.43050949]
 [-0.00393197  0.48571429  0.58888889  0.34342904  0.47371503  0.03230522
   0.5175      0.53333333  0.45565907  0.50172799]
 [-0.04508165  0.4775      0.47777778  0.41388155  0.46029582  0.13377622
   0.55833333  0.63333333  0.47690965  0.55493007]
 [-0.16888112  0.41666667  0.45555556  0.31841833  0.40371379  0.61090909
   0.78333333  0.85555556  0.69980371  0.78818681]
 [ 0.09237484  0.545       0.55555556  0.52048222  0.53666667 -0.07698282
   0.4625      0.47777778  0.41004506  0.45155483]
 [ 0.01759094  0.51        0.53333333  0.38923038  0.47722777  0.11445944
   0.5625      0.55555556  0.51778909  0.54382395]
 [-0.13215931  0.435       0.44444444  0.36126433  0.41762266 -0.06213834
   0.4675      0.47777778  0.44687056  0.46159812]
 [ 0.59224775  0.79166667  0.82222222  0.7718324   0.79213037  0.45626854
   0.7375      0.72222222  0.71854531  0.71882395]
 [-0.30283716  0.35        0.43333333  0.12761424  0.33364885  0.41874126
   0.69166667  0.77777778  0.59093577  0.68897103]
 [ 0.22994145  0.6175      0.63333333  0.49370376  0.5886741   0.68144959
   0.8375      0.84444444  0.82979969  0.83882395]
 [ 0.25038735  0.62        0.64444444  0.57009435  0.60826673 -0.05931319
   0.45833333  0.46666667  0.42296155  0.44246753]]
BDDAE mean:
[0.09154176 0.5467877  0.58074074 0.44454709 0.52428685 0.1875585
 0.5924246  0.62074074 0.51337961 0.57524624]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.53       0.         0.34583333        nan
  0.55       0.79       0.1        0.49047619]
 [0.         0.5        0.555      0.         0.35535714 0.
  0.5        0.63       0.         0.38392857]
 [0.         0.5        0.555      0.         0.35535714 0.
  0.5        0.47       0.         0.31904762]
 [0.         0.5        0.47       0.         0.31904762 0.
  0.5        0.705      0.         0.4125    ]
 [0.         0.5        0.63       0.         0.38392857 0.
  0.5        0.51       0.         0.33690476]
 [0.         0.5        0.47       0.         0.31904762 0.
  0.5        0.53       0.         0.34583333]
 [0.         0.5        0.655      0.         0.39345238 0.
  0.5        0.47       0.         0.31904762]
 [0.         0.5        0.705      0.         0.4125     0.
  0.5        0.63       0.         0.38392857]
 [0.         0.5        0.655      0.         0.39345238 0.
  0.5        0.68       0.         0.40297619]
 [0.         0.5        0.51       0.         0.33690476 0.
  0.5        0.555      0.         0.35535714]
 [0.         0.5        0.605      0.         0.37440476 0.
  0.5        0.605      0.         0.37440476]
 [0.         0.5        0.58       0.         0.36488095 0.
  0.5        0.605      0.         0.37440476]
 [0.         0.5        0.605      0.         0.37440476 0.
  0.5        0.555      0.         0.35535714]
 [0.         0.5        0.63       0.         0.38392857 0.
  0.5        0.47       0.         0.31904762]
 [0.         0.5        0.705      0.         0.4125     0.
  0.5        0.63       0.         0.38392857]
 [0.         0.5        0.605      0.         0.37440476 0.
  0.5        0.655      0.         0.39345238]
 [0.         0.5        0.58       0.         0.36488095 0.
  0.5        0.605      0.         0.37440476]
 [       nan 0.7        0.865      0.4        0.66190476 0.
  0.5        0.58       0.         0.36488095]
 [0.         0.5        0.705      0.         0.4125     0.
  0.5        0.53       0.         0.34583333]
 [0.         0.5        0.655      0.         0.39345238 0.
  0.5        0.63       0.         0.38392857]
 [       nan 0.55       0.79       0.1        0.49047619 0.
  0.5        0.51       0.         0.33690476]
 [0.         0.5        0.47       0.         0.31904762 0.
  0.5        0.655      0.         0.39345238]
 [0.         0.5        0.63       0.         0.38392857 0.
  0.5        0.725      0.         0.41944444]
 [0.         0.5        0.58       0.         0.36488095 0.
  0.5        0.58       0.         0.36488095]
 [0.         0.5        0.605      0.         0.37440476 0.
  0.5        0.47       0.         0.31904762]
 [0.         0.5        0.555      0.         0.35535714 0.
  0.5        0.53       0.         0.34583333]
 [0.         0.5        0.705      0.         0.4125     0.
  0.5        0.47       0.         0.31904762]
 [0.         0.5        0.655      0.         0.39345238 0.
  0.5        0.63       0.         0.38392857]
 [0.         0.5        0.605      0.         0.37440476 0.
  0.5        0.58       0.         0.36488095]
 [0.         0.5        0.605      0.         0.37440476 0.
  0.5        0.63       0.         0.38392857]]
DUMMY mean:
[0.         0.50833333 0.61566667 0.01666667 0.38583333 0.
 0.50166667 0.58716667 0.00333333 0.3683664 ]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_56
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.31  0.656 0.689 0.508 0.621 0.297 0.653 0.682 0.491 0.61 ]
 [0.296 0.642 0.653 0.544 0.6   0.178 0.595 0.622 0.442 0.555]
 [0.323 0.66  0.68  0.522 0.627 0.236 0.628 0.652 0.471 0.587]
 [0.012 0.506 0.611 0.02  0.384 0.013 0.506 0.593 0.03  0.381]
 [0.25  0.639 0.686 0.438 0.596 0.172 0.591 0.629 0.413 0.549]
 [0.092 0.547 0.581 0.445 0.524 0.188 0.592 0.621 0.513 0.575]
 [0.    0.5   0.603 0.    0.373 0.    0.5   0.583 0.    0.365]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.227 0.117 0.117 0.168 0.126 0.147 0.074 0.074 0.127 0.079]
 [0.228 0.121 0.118 0.177 0.132 0.156 0.077 0.084 0.138 0.089]
 [0.217 0.128 0.129 0.173 0.134 0.18  0.104 0.102 0.131 0.107]
 [0.034 0.018 0.056 0.049 0.02  0.058 0.029 0.066 0.082 0.041]
 [0.251 0.129 0.116 0.222 0.142 0.146 0.075 0.077 0.14  0.084]
 [0.164 0.081 0.085 0.123 0.084 0.241 0.12  0.122 0.154 0.124]
 [0.    0.    0.068 0.    0.027 0.    0.    0.074 0.    0.029]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 73.  18.  17.  33.  20.  49.  11.  11.  26.  13.]
 [ 77.  19.  18.  33.  22.  88.  13.  14.  31.  16.]
 [ 67.  19.  19.  33.  21.  76.  17.  16.  28.  18.]
 [284.   4.   9. 244.   5. 448.   6.  11. 274.  11.]
 [100.  20.  17.  51.  24.  85.  13.  12.  34.  15.]
 [179.  15.  15.  28.  16. 128.  20.  20.  30.  22.]
 [  0.   0.  11.   0.   7.   0.   0.  13.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_56
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  27.008
step (sec):  27.008
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  43
Elapsed time: 731.6763622403145 minutes
Elapsed time: 12.194606037338575 hours
