2024-05-17 16:04:05.602005: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-17 16:04:09.401642: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-17 16:04:18.216399: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  27.0
step (sec):  20.25
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  6.75
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_57\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - loss: 0.0153 - mean_squared_error: 0.0153 - val_loss: 0.0132 - val_mean_squared_error: 0.0132
(27008, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 27008, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 6752, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 6752, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1688, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 27008, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 27008, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 1688, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 1688, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 1688, 12)  â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 1688)  â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 20256)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 20256)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     20,257 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 21,079 (82.34 KB)
 Trainable params: 21,079 (82.34 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/39[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m48s[0m 1s/step - binary_accuracy: 1.0000 - loss: 0.5233
[1m 5/39[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.5202
[1m 9/39[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9008 - loss: 0.5564
[1m13/39[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8503 - loss: 0.5742
[1m17/39[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8215 - loss: 0.5797
[1m21/39[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7971 - loss: 0.5839
[1m25/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7769 - loss: 0.5864
[1m29/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7575 - loss: 0.5885
[1m33/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7426 - loss: 0.5910
[1m37/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7293 - loss: 0.5979
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 22ms/step - binary_accuracy: 0.7192 - loss: 0.6039 - val_binary_accuracy: 0.8000 - val_loss: 0.5375
Epoch 2/10

[1m 1/39[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7080
[1m 5/39[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.4533 - loss: 0.5792    
[1m 9/39[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5206 - loss: 0.6024
[1m13/39[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5207 - loss: 0.6482
[1m17/39[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5419 - loss: 0.6659
[1m20/39[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5607 - loss: 0.6707
[1m24/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5819 - loss: 0.6720
[1m28/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5973 - loss: 0.6743
[1m32/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6097 - loss: 0.6727
[1m35/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6163 - loss: 0.6734
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6242 - loss: 0.6733
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.6259 - loss: 0.6731 - val_binary_accuracy: 0.8000 - val_loss: 0.5085
Epoch 3/10

[1m 1/39[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.2533
[1m 4/39[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 21ms/step - binary_accuracy: 0.5833 - loss: 0.4807
[1m 8/39[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.5452 - loss: 0.5275
[1m12/39[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5455 - loss: 0.5373
[1m16/39[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5724 - loss: 0.5271
[1m20/39[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6012 - loss: 0.5193
[1m24/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6231 - loss: 0.5161
[1m28/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6353 - loss: 0.5223
[1m32/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6429 - loss: 0.5286
[1m36/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6496 - loss: 0.5339
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.6559 - loss: 0.5368 - val_binary_accuracy: 0.8000 - val_loss: 0.5009
Epoch 4/10

[1m 1/39[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 36ms/step - binary_accuracy: 0.0000e+00 - loss: 0.8046
[1m 5/39[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.4533 - loss: 0.5888    
[1m 9/39[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5627 - loss: 0.5681
[1m13/39[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5833 - loss: 0.5743
[1m16/39[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5987 - loss: 0.5738
[1m21/39[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6266 - loss: 0.5725
[1m25/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6454 - loss: 0.5703
[1m28/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6583 - loss: 0.5666
[1m32/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6723 - loss: 0.5605
[1m36/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6822 - loss: 0.5557
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.6878 - loss: 0.5555 - val_binary_accuracy: 0.8000 - val_loss: 0.4735
Epoch 5/10

[1m 1/39[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.3915
[1m 5/39[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7433 - loss: 0.4909
[1m 9/39[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7968 - loss: 0.4548
[1m13/39[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8323 - loss: 0.4311
[1m17/39[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8494 - loss: 0.4177
[1m21/39[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8585 - loss: 0.4117
[1m25/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8608 - loss: 0.4100
[1m29/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8562 - loss: 0.4135
[1m33/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8516 - loss: 0.4161
[1m37/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8478 - loss: 0.4176
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.8457 - loss: 0.4189 - val_binary_accuracy: 1.0000 - val_loss: 0.5156
Epoch 6/10

[1m 1/39[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.0374
[1m 5/39[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1645
[1m 9/39[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2179
[1m13/39[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9807 - loss: 0.2551
[1m17/39[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9700 - loss: 0.2727
[1m21/39[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9561 - loss: 0.2952
[1m24/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9490 - loss: 0.3032
[1m29/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9323 - loss: 0.3235
[1m33/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9194 - loss: 0.3366
[1m36/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9111 - loss: 0.3448
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9004 - loss: 0.3536 - val_binary_accuracy: 0.8000 - val_loss: 0.4511
Epoch 7/10

[1m 1/39[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.3492
[1m 5/39[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5467 - loss: 0.5350
[1m 8/39[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.5537 - loss: 0.5258
[1m11/39[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - binary_accuracy: 0.5931 - loss: 0.5100
[1m15/39[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.6327 - loss: 0.4864
[1m19/39[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6560 - loss: 0.4685
[1m22/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.6704 - loss: 0.4584
[1m26/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6897 - loss: 0.4432
[1m30/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7065 - loss: 0.4294
[1m33/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7161 - loss: 0.4225
[1m37/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7285 - loss: 0.4135
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.7372 - loss: 0.4077 - val_binary_accuracy: 0.8000 - val_loss: 0.4146
Epoch 8/10

[1m 1/39[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2083
[1m 5/39[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2043
[1m 8/39[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9457 - loss: 0.2612
[1m12/39[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9317 - loss: 0.2983
[1m16/39[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9189 - loss: 0.3216
[1m20/39[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9134 - loss: 0.3318
[1m24/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9130 - loss: 0.3352
[1m27/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9141 - loss: 0.3341
[1m32/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9161 - loss: 0.3311
[1m36/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9158 - loss: 0.3294
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9163 - loss: 0.3272 - val_binary_accuracy: 0.8000 - val_loss: 0.4401
Epoch 9/10

[1m 1/39[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3044
[1m 5/39[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2205
[1m 9/39[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2608
[1m13/39[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2739
[1m17/39[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9889 - loss: 0.2764
[1m21/39[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9812 - loss: 0.2740
[1m25/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9774 - loss: 0.2719
[1m28/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9733 - loss: 0.2709
[1m32/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9684 - loss: 0.2703
[1m36/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9655 - loss: 0.2697
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9637 - loss: 0.2679 - val_binary_accuracy: 0.8000 - val_loss: 0.4438
Epoch 10/10

[1m 1/39[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.0577
[1m 5/39[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9600 - loss: 0.1031
[1m 9/39[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9172 - loss: 0.1815
[1m13/39[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9033 - loss: 0.2050
[1m17/39[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8884 - loss: 0.2209
[1m21/39[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8803 - loss: 0.2264
[1m25/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8789 - loss: 0.2276
[1m29/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8806 - loss: 0.2278
[1m33/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8835 - loss: 0.2259
[1m37/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8862 - loss: 0.2244
[1m39/39[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.8870 - loss: 0.2242 - val_binary_accuracy: 0.8000 - val_loss: 0.4315

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
predicted [0.45627642 0.47479245 0.8728876  0.40722212 0.24472536 0.6094713
 0.36935592 0.8877741  0.20954563 0.66127956 0.49397466]
predicted [0 0 1 0 0 1 0 1 0 1 0]
expected [ True  True False False  True  True  True  True False  True False]
accuracy: 0.5454545454545454
confusion matrix: 
[[3 1]
 [4 3]]
              precision    recall  f1-score   support

       False       0.43      0.75      0.55         4
        True       0.75      0.43      0.55         7

    accuracy                           0.55        11
   macro avg       0.59      0.59      0.55        11
weighted avg       0.63      0.55      0.55        11

macro avg f1-score: 0.5454545454545454
macro avg (UAR): 0.5892857142857143
Sensitivity:  0.75
Specificity:  0.42857142857142855
g-mean:  0.5669467095138409
-------- Model Performance ----------: 
accuracy:  [0.63636364 0.54545455 0.63636364 0.54545455 0.54545455 0.54545455
 0.63636364 0.54545455 0.45454545 0.54545455]
gmean:  [0.65465367 0.53452248 0.5976143  0.53452248 0.42257713 0.56694671
 0.5976143  0.42257713 0.37796447 0.56694671]
f1_score:  [0.63333333 0.52991453 0.60714286 0.52991453 0.47619048 0.54545455
 0.60714286 0.47619048 0.41071429 0.54545455]
UAR:  [0.66071429 0.53571429 0.60714286 0.53571429 0.48214286 0.58928571
 0.60714286 0.48214286 0.41071429 0.58928571]
Cohen Kappa score:  [ 0.29032258  0.06779661  0.21428571  0.06779661 -0.03773585  0.15384615
  0.21428571 -0.03773585 -0.17857143  0.15384615]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  27.008
step (sec):  20.256
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  6.752
Number of windows / instances:  55
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.933  0.975  0.967  0.973  0.966  0.605  0.796  0.82   0.773  0.795]
 [ 0.64   0.842  0.853  0.822  0.832  0.244  0.667  0.697  0.577  0.648]
 [ 0.746  0.867  0.877  0.856  0.842  0.528  0.762  0.81   0.786  0.754]
 [ 0.     0.5    0.633  0.     0.388  0.     0.5    0.653  0.     0.394]
 [ 0.725  0.858  0.873  0.835  0.855  0.349  0.65   0.7    0.573  0.635]
 [ 0.091  0.55   0.564  0.528  0.536 -0.081  0.454  0.509  0.353  0.445]
 [ 0.     0.5    0.633  0.     0.388  0.     0.5    0.653  0.     0.394]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.35227273  0.67083333  0.70666667  0.56107029  0.65202381  0.31168831
   0.6625      0.85        0.35731322  0.62265512]
 [ 0.3959374   0.7         0.68333333  0.62802663  0.66404762  0.40881618
   0.6875      0.74        0.6079758   0.68349206]
 [ 0.49242424  0.74166667  0.76333333  0.61462644  0.70654762  0.39941725
   0.7         0.70666667  0.67993309  0.6927381 ]
 [ 0.51190476  0.75833333  0.76333333  0.68818807  0.73285714 -0.03214286
   0.4875      0.66        0.12247449  0.44      ]
 [ 0.57012987  0.775       0.80666667  0.71128842  0.77103175  0.39652015
   0.70833333  0.69333333  0.67016999  0.67880952]
 [ 0.47985348  0.74166667  0.73666667  0.70461121  0.72702381  0.61095571
   0.80833333  0.81666667  0.74006135  0.78571429]
 [ 0.38484848  0.69166667  0.73        0.57844571  0.67666667  0.50619381
   0.75833333  0.75        0.73888547  0.74309524]
 [ 0.83549784  0.9125      0.93        0.89873457  0.91460317  0.47461538
   0.75        0.73333333  0.7372018   0.72880952]
 [ 0.40292208  0.7         0.75        0.5806825   0.67269841  0.10842491
   0.55416667  0.69        0.26884873  0.5159127 ]
 [ 0.06959707  0.54166667  0.53        0.48947341  0.5197619   0.31282051
   0.65833333  0.65666667  0.61505098  0.64261905]
 [-0.045       0.475       0.55        0.26380104  0.45277778  0.44465534
   0.70833333  0.78        0.53520305  0.68230159]
 [ 0.56363636  0.78333333  0.80666667  0.70080717  0.75928571  0.30658508
   0.65416667  0.67333333  0.58293337  0.6347619 ]
 [ 0.4532967   0.73333333  0.71666667  0.69279928  0.70285714  0.29047619
   0.65        0.64        0.61628937  0.62630952]
 [ 0.07288545  0.5375      0.62        0.30907703  0.5072619   0.36796537
   0.68333333  0.69333333  0.61447163  0.66464286]
 [ 0.09220779  0.53333333  0.65        0.26123724  0.50301587  0.18430736
   0.59583333  0.67666667  0.38575892  0.57015873]
 [ 0.13293373  0.55833333  0.57666667  0.4725718   0.53888889  0.32077922
   0.6625      0.73333333  0.46944525  0.62829365]
 [ 0.34818182  0.67083333  0.69        0.5698524   0.64595238  0.38372461
   0.69166667  0.73333333  0.58142797  0.67190476]
 [        nan  0.73        0.88        0.48944272  0.69858586  0.44234765
   0.725       0.71333333  0.70184038  0.7052381 ]
 [ 0.45201465  0.72916667  0.8         0.61917239  0.71285714  0.28939394
   0.64583333  0.67        0.54349395  0.62214286]
 [ 0.359329    0.67083333  0.73333333  0.56645964  0.66210317  0.01244589
   0.51666667  0.58        0.26039363  0.45884921]
 [-0.08493506  0.445       0.71        0.          0.41059163  0.13113553
   0.56666667  0.56666667  0.50033226  0.53964286]
 [ 0.11565102  0.55833333  0.56666667  0.39140062  0.50988095  0.36819014
   0.67916667  0.70666667  0.63522845  0.67757937]
 [ 0.17786713  0.58333333  0.58666667  0.50822541  0.55297619  0.15357143
   0.59        0.77        0.25067996  0.53684343]
 [-0.23041958  0.38333333  0.39666667  0.22734997  0.3452381   0.43989344
   0.71666667  0.74666667  0.59578903  0.68765873]
 [ 0.22380952  0.60833333  0.63        0.42876937  0.55619048  0.34638695
   0.675       0.68666667  0.59223208  0.65559524]
 [ 0.27062937  0.63333333  0.64333333  0.60553999  0.62488095  0.33534799
   0.675       0.67        0.56529451  0.63178571]
 [ 0.479329    0.73333333  0.8         0.6591282   0.72876984  0.43216783
   0.71666667  0.71        0.64849377  0.68857143]
 [ 0.20746753  0.6         0.65        0.47544187  0.58484127  0.43311688
   0.72083333  0.76        0.55443293  0.67607143]
 [ 0.01309524  0.50833333  0.63666667  0.11897227  0.43111111  0.19339827
   0.59166667  0.66        0.34003594  0.53940476]
 [ 0.93333333  0.975       0.96666667  0.97320508  0.96571429  0.60504329
   0.79583333  0.82        0.77252566  0.79543651]]
KNN mean:
[0.31140348 0.65611111 0.70033333 0.52628002 0.63103475 0.33260806
 0.66786111 0.70955556 0.54280723 0.64090127]
---------------------------
---------------------------
DT performance:
[[ 0.5469697   0.71666667  0.73333333  0.82138734  0.71321429  0.14287879
   0.6575      0.78333333  0.31223255  0.62295455]
 [ 0.33058608  0.68333333  0.67333333  0.58674793  0.64869048  0.49035132
   0.75416667  0.76        0.73130443  0.74015873]
 [ 0.30524476  0.7125      0.71333333  0.69131106  0.69416667  0.19067599
   0.59166667  0.61        0.45269259  0.57214286]
 [ 0.45710956  0.7         0.69        0.69132594  0.68214286 -0.06136364
   0.45        0.58        0.29318517  0.41853175]
 [ 0.40946387  0.7625      0.77666667  0.67894409  0.75857143  0.36483516
   0.7         0.68666667  0.66791052  0.67404762]
 [ 0.37459207  0.675       0.68666667  0.59779637  0.65607143  0.53787879
   0.76666667  0.77666667  0.74659976  0.7622619 ]
 [ 0.18428904  0.63333333  0.68        0.47048899  0.60071429  0.16095571
   0.58333333  0.57333333  0.50543053  0.55369048]
 [ 0.72121212  0.9375      0.95        0.88660254  0.9384127   0.32881119
   0.69583333  0.69666667  0.60847694  0.65738095]
 [ 0.25489344  0.67916667  0.69        0.624886    0.61813492  0.24565435
   0.6375      0.66666667  0.50937529  0.61361111]
 [ 0.07004662  0.53333333  0.53        0.44350883  0.50452381  0.44652015
   0.66666667  0.66        0.65636287  0.63666667]
 [-0.09091575  0.42916667  0.41666667  0.29165022  0.3752381   0.40272894
   0.67916667  0.69333333  0.66803051  0.65146825]
 [ 0.52121212  0.75416667  0.77333333  0.66504825  0.71873016  0.4745671
   0.70833333  0.70333333  0.64506896  0.6777381 ]
 [ 0.14423077  0.64583333  0.64666667  0.57423667  0.62761905  0.13461538
   0.55        0.56666667  0.54720844  0.52214286]
 [ 0.16439394  0.56666667  0.67333333  0.42628549  0.5259127   0.31714952
   0.59166667  0.60333333  0.56623544  0.57035714]
 [ 0.37042271  0.65        0.69333333  0.60604779  0.59206349  0.14822677
   0.62083333  0.64333333  0.50967355  0.59063492]
 [ 0.04024309  0.46666667  0.49666667  0.31783039  0.45051587  0.14154179
   0.60416667  0.65333333  0.47150733  0.57126984]
 [ 0.76168831  0.89583333  0.90666667  0.86740858  0.89515873  0.35017483
   0.7         0.71        0.62502362  0.68392857]
 [        nan  0.77        0.89333333  0.4         0.71979798  0.38308025
   0.725       0.72333333  0.66910358  0.70813492]
 [ 0.44978355  0.7         0.72666667  0.65907703  0.67869048  0.36766234
   0.71666667  0.71666667  0.68483539  0.7       ]
 [ 0.27142857  0.7         0.68333333  0.45731322  0.62988095  0.11818681
   0.57916667  0.59333333  0.40276392  0.53063492]
 [-0.05201299  0.43        0.57        0.17071068  0.43416667  0.15201465
   0.6         0.59        0.45175166  0.56654762]
 [ 0.12835498  0.61666667  0.62        0.39022474  0.58392857  0.43407093
   0.725       0.74666667  0.69840757  0.72043651]
 [ 0.2409324   0.62916667  0.60666667  0.59085526  0.59214286  0.21214286
   0.5625      0.70666667  0.29817035  0.53962121]
 [ 0.18228438  0.625       0.63        0.58019736  0.58559524  0.40298868
   0.65833333  0.65333333  0.62506621  0.57968254]
 [ 0.27488178  0.64583333  0.64333333  0.64156674  0.62373016  0.39615385
   0.75833333  0.75333333  0.71815406  0.74404762]
 [ 0.5455711   0.74166667  0.73666667  0.66706691  0.72488095  0.15502831
   0.58333333  0.58        0.45648448  0.54809524]
 [ 0.10702445  0.54583333  0.58333333  0.37163087  0.50325397  0.2514319
   0.6         0.6         0.56816265  0.57452381]
 [-0.0473976   0.49166667  0.54666667  0.34567957  0.46230159  0.20151515
   0.65416667  0.66        0.50831689  0.64416667]
 [ 0.16162837  0.57083333  0.6         0.43431878  0.54912698  0.16940559
   0.59583333  0.6         0.56703291  0.5802381 ]
 [ 0.63982684  0.84166667  0.85333333  0.82180559  0.83198413  0.2438961
   0.66666667  0.69666667  0.57658375  0.64753968]]
DT mean:
[0.2919996  0.65833333 0.68077778 0.55906511 0.63064538 0.27679265
 0.64608333 0.66622222 0.5580384  0.6200885 ]
---------------------------
---------------------------
RF performance:
[[0.58242424 0.8125     0.83333333 0.81090689 0.81607143 0.32383117
  0.6275     0.79333333 0.3601534  0.59239899]
 [0.43952714 0.7        0.69333333 0.53045305 0.66702381 0.44995837
  0.71666667 0.75333333 0.55007658 0.72015873]
 [0.46095571 0.59166667 0.58333333 0.72166679 0.56285714 0.44312354
  0.59166667 0.59666667 0.60006798 0.58214286]
 [0.4955711  0.70833333 0.70333333 0.7386212  0.66714286 0.06244589
  0.55416667 0.67       0.35731322 0.53313492]
 [0.34450383 0.71666667 0.74333333 0.64657436 0.69884921 0.50128205
  0.725      0.71333333 0.68315779 0.70952381]
 [0.41282051 0.65       0.65333333 0.59174502 0.6452381  0.54675325
  0.76666667 0.78333333 0.6651397  0.76464286]
 [0.39065934 0.625      0.66666667 0.49525776 0.5943254  0.27322677
  0.64166667 0.62       0.54924511 0.60416667]
 [0.84621212 0.8625     0.90666667 0.81855046 0.85746032 0.48443889
  0.71666667 0.73333333 0.54747271 0.70968254]
 [0.46385281 0.69583333 0.73       0.6306825  0.66972222 0.26686647
  0.50833333 0.56666667 0.57586368 0.4590873 ]
 [0.07600733 0.49166667 0.47333333 0.61553804 0.44285714 0.08663004
  0.59166667 0.6        0.5645725  0.5677381 ]
 [0.11978355 0.575      0.60666667 0.34915638 0.53369048 0.49303197
  0.8        0.81666667 0.66639025 0.79944444]
 [0.68653846 0.79583333 0.79666667 0.91258977 0.77107143 0.34691142
  0.62916667 0.65333333 0.61621197 0.61535714]
 [0.06556777 0.65       0.65666667 0.65216405 0.61309524 0.15887446
  0.5        0.52       0.59518036 0.48571429]
 [0.09480519 0.54166667 0.60333333 0.46389584 0.52845238 0.3959374
  0.76666667 0.76666667 0.72457914 0.75833333]
 [0.04830317 0.62083333 0.67333333 0.33194792 0.56468254 0.11327006
  0.54583333 0.59       0.42133261 0.52107143]
 [0.12706127 0.6125     0.63333333 0.40667328 0.57313492 0.33008658
  0.59166667 0.65666667 0.3767073  0.55956349]
 [0.83787879 0.89166667 0.90666667 0.88080604 0.89515873 0.38649684
  0.74166667 0.76333333 0.64908364 0.72571429]
 [       nan 0.77       0.89333333 0.58944272 0.71979798 0.42393939
  0.65       0.66666667 0.71124694 0.60214286]
 [0.2746337  0.74166667 0.78666667 0.62453656 0.73412698 0.3309324
  0.68333333 0.70666667 0.66107029 0.66940476]
 [0.43668831 0.73333333 0.77       0.67267461 0.69801587 0.17464349
  0.53333333 0.62       0.2526586  0.49765873]
 [0.12772727 0.43       0.69       0.07071068 0.40563131 0.06132201
  0.46666667 0.46333333 0.54543604 0.4447619 ]
 [0.31132201 0.64166667 0.62666667 0.45386846 0.6052381  0.36623377
  0.68333333 0.71       0.68346464 0.67496032]
 [0.16153846 0.59583333 0.60333333 0.6506037  0.56833333 0.16547619
  0.5525     0.72666667 0.41213203 0.51468254]
 [0.42740593 0.625      0.63333333 0.42449901 0.60285714 0.56831502
  0.775      0.78       0.69236695 0.74690476]
 [0.30295538 0.70833333 0.72333333 0.53522845 0.69547619 0.51608392
  0.71666667 0.72333333 0.60433479 0.69833333]
 [0.08310023 0.68333333 0.68333333 0.48364485 0.6677381  0.35337995
  0.60833333 0.62       0.63695827 0.56904762]
 [0.41036963 0.775      0.80666667 0.75049838 0.76503968 0.43916084
  0.71666667 0.72666667 0.5811383  0.70678571]
 [0.05638528 0.4625     0.50666667 0.24348343 0.41071429 0.14751082
  0.65833333 0.69333333 0.47543135 0.61642857]
 [0.27344322 0.525      0.6        0.56639025 0.48396825 0.09469697
  0.64166667 0.67333333 0.57316167 0.60630952]
 [0.74577922 0.86666667 0.87666667 0.8564696  0.8418254  0.52820346
  0.7625     0.81       0.78638085 0.75376984]]
RF mean:
[0.34840762 0.67       0.70211111 0.583976   0.64331987 0.32776878
 0.64877778 0.68388889 0.57061095 0.62696886]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.56666667  0.          0.36083333  0.
   0.5         0.8         0.          0.4440404 ]
 [ 0.          0.5         0.55        0.          0.35416667  0.
   0.5         0.63333333  0.          0.3875    ]
 [ 0.          0.5         0.56666667  0.          0.36083333  0.19010989
   0.6         0.58333333  0.37925643  0.5127381 ]
 [ 0.          0.5         0.53        0.          0.3452381   0.
   0.5         0.71333333  0.          0.41527778]
 [ 0.          0.5         0.61666667  0.          0.38083333  0.55128205
   0.78333333  0.77333333  0.76026422  0.76535714]
 [ 0.09487179  0.55        0.53        0.18164966  0.42047619  0.59465534
   0.8         0.79666667  0.77051439  0.78392857]
 [ 0.          0.5         0.63333333  0.          0.3875      0.
   0.5         0.53        0.          0.3452381 ]
 [ 0.          0.5         0.69333333  0.          0.40833333  0.
   0.5         0.6         0.          0.37416667]
 [ 0.          0.5         0.67333333  0.          0.40138889  0.
   0.5         0.69333333  0.          0.40833333]
 [-0.01969697  0.49166667  0.47        0.05773503  0.33928571  0.
   0.5         0.55        0.          0.35416667]
 [ 0.          0.5         0.65333333  0.          0.39444444  0.
   0.5         0.63333333  0.          0.3875    ]
 [ 0.          0.5         0.6         0.          0.37416667  0.
   0.5         0.6         0.          0.37416667]
 [ 0.          0.5         0.58333333  0.          0.3675      0.
   0.5         0.55        0.          0.35416667]
 [ 0.          0.5         0.69333333  0.          0.40833333  0.13333333
   0.56666667  0.55        0.21547005  0.44047619]
 [ 0.          0.5         0.69333333  0.          0.40833333  0.
   0.5         0.65333333  0.          0.39444444]
 [ 0.          0.5         0.6         0.          0.37416667  0.
   0.5         0.65333333  0.          0.39444444]
 [ 0.          0.5         0.58333333  0.          0.3675      0.
   0.5         0.6         0.          0.37416667]
 [        nan  0.7         0.89666667  0.4         0.67171717  0.
   0.5         0.6         0.          0.37416667]
 [ 0.          0.5         0.69333333  0.          0.40833333  0.
   0.5         0.56666667  0.          0.36083333]
 [ 0.          0.5         0.67333333  0.          0.40138889  0.
   0.5         0.65333333  0.          0.39444444]
 [ 0.          0.5         0.8         0.          0.4440404   0.
   0.5         0.53        0.          0.3452381 ]
 [ 0.12857143  0.56666667  0.55        0.15773503  0.41964286  0.
   0.5         0.63333333  0.          0.3875    ]
 [ 0.          0.5         0.6         0.          0.37416667  0.
   0.5         0.76666667  0.          0.43313131]
 [ 0.          0.5         0.55        0.          0.35416667  0.
   0.5         0.56666667  0.          0.36083333]
 [ 0.          0.5         0.6         0.          0.37416667  0.11818182
   0.55833333  0.55        0.17071068  0.42857143]
 [ 0.          0.5         0.55        0.          0.35416667  0.
   0.5         0.55        0.          0.35416667]
 [ 0.          0.5         0.69333333  0.          0.40833333 -0.03636364
   0.48333333  0.51        0.          0.33630952]
 [ 0.          0.5         0.63333333  0.          0.3875      0.
   0.5         0.61666667  0.          0.38083333]
 [ 0.          0.5         0.65333333  0.          0.39444444  0.
   0.5         0.6         0.          0.37416667]
 [ 0.          0.5         0.63333333  0.          0.3875      0.
   0.5         0.65333333  0.          0.39444444]]
SVM mean:
[0.00702573 0.51027778 0.62544444 0.02657066 0.39443001 0.05170663
 0.52638889 0.62366667 0.07654053 0.4144917 ]
---------------------------
---------------------------
GBM performance:
[[ 0.51818182  0.75        0.77        0.73889406  0.74821429  0.25714286
   0.625       0.85333333  0.27071068  0.59191919]
 [ 0.33744589  0.65833333  0.65        0.56090609  0.60202381  0.40454545
   0.69166667  0.73666667  0.59831277  0.68285714]
 [ 0.56363636  0.77916667  0.79        0.74299474  0.76785714  0.21015651
   0.575       0.57666667  0.5101755   0.54952381]
 [ 0.46260406  0.725       0.72333333  0.67043426  0.70940476 -0.05714286
   0.475       0.68        0.          0.40194444]
 [ 0.64577922  0.8125      0.85        0.7311043   0.80634921  0.46996337
   0.73333333  0.72333333  0.70336066  0.71440476]
 [ 0.23872794  0.64166667  0.63333333  0.57732923  0.6125      0.65909091
   0.825       0.83666667  0.80184038  0.82178571]
 [ 0.28827006  0.65416667  0.69666667  0.55417532  0.63884921  0.16868132
   0.55833333  0.54666667  0.49306355  0.51738095]
 [ 0.73290043  0.8625      0.89333333  0.84015593  0.86857143  0.4469697
   0.68333333  0.71333333  0.58407525  0.65392857]
 [ 0.32301032  0.67916667  0.71        0.62183099  0.6318254   0.195005
   0.55        0.64666667  0.4395594   0.52444444]
 [ 0.07169497  0.55833333  0.55666667  0.48493019  0.535       0.17435897
   0.64166667  0.64333333  0.53778973  0.61333333]
 [-0.1784965   0.4125      0.50333333  0.15236034  0.3840873   0.48571429
   0.725       0.76666667  0.62514361  0.7084127 ]
 [ 0.44941725  0.725       0.75333333  0.53080604  0.67809524  0.42575758
   0.7         0.71        0.71853831  0.67071429]
 [ 0.11470196  0.55833333  0.59666667  0.41318629  0.4997619   0.15
   0.55833333  0.57666667  0.47353734  0.52869048]
 [ 0.30097403  0.6375      0.78333333  0.38284271  0.6159127   0.31678322
   0.7         0.69666667  0.58458578  0.68119048]
 [ 0.3991342   0.6625      0.76        0.4026586   0.6447619  -0.06904762
   0.44583333  0.58333333  0.05773503  0.36551587]
 [-0.17272727  0.4         0.47333333  0.          0.31738095  0.28192641
   0.60833333  0.69333333  0.36986706  0.57174603]
 [ 0.76168831  0.875       0.89        0.8624557   0.87801587  0.39140859
   0.67083333  0.71        0.57543135  0.66396825]
 [        nan  0.79        0.91333333  0.5         0.77535354  0.4863037
   0.70833333  0.73        0.65900822  0.68813492]
 [ 0.04318182  0.53333333  0.65666667  0.27978771  0.47809524  0.47099567
   0.74166667  0.76        0.68316638  0.71833333]
 [ 0.14844322  0.62083333  0.69666667  0.36359758  0.58416667  0.17056277
   0.59583333  0.66666667  0.29873457  0.51873016]
 [ 0.03454545  0.5275      0.78333333  0.08660254  0.47033189  0.11190476
   0.55833333  0.55666667  0.50975096  0.5447619 ]
 [ 0.254329    0.625       0.62        0.45200378  0.58642857  0.33458874
   0.65833333  0.69333333  0.507881    0.64246032]
 [ 0.37690643  0.69166667  0.71        0.57019073  0.65547619 -0.03636364
   0.475       0.71        0.05        0.42582973]
 [ 0.03018648  0.50833333  0.52333333  0.38795313  0.47166667  0.39047619
   0.69166667  0.69333333  0.54426016  0.64166667]
 [ 0.29647852  0.6625      0.68666667  0.58774359  0.65277778  0.56608392
   0.78333333  0.78666667  0.72219807  0.77607143]
 [ 0.50979021  0.75        0.76        0.69502555  0.7425      0.32948718
   0.63333333  0.64        0.5101755   0.59583333]
 [ 0.19383117  0.5875      0.70666667  0.39584377  0.57619048  0.21608392
   0.625       0.64        0.49056915  0.59916667]
 [ 0.03441558  0.54166667  0.63666667  0.19915638  0.48468254  0.32528139
   0.625       0.68        0.44915638  0.57595238]
 [ 0.07835498  0.52083333  0.62        0.21897227  0.43202381  0.24388112
   0.58333333  0.61333333  0.456495    0.53452381]
 [ 0.72491508  0.85833333  0.87333333  0.83520305  0.85460317  0.34865801
   0.65        0.7         0.57336928  0.63507937]]
GBM mean:
[0.2959421  0.65363889 0.70733333 0.49463816 0.62343025 0.29564191
 0.63652778 0.68544444 0.49328304 0.60527682]
---------------------------
---------------------------
BDDAE performance:
[[-0.03240693  0.48333333  0.48181818  0.47045715  0.47552392  0.30630188
   0.68333333  0.8         0.53276358  0.64293516]
 [ 0.21858226  0.61166667  0.60909091  0.58837762  0.59808719  0.46099974
   0.74821429  0.72727273  0.73618279  0.72104812]
 [ 0.15369556  0.57666667  0.59090909  0.51450961  0.56216575  0.34417709
   0.67333333  0.67272727  0.67058384  0.67058275]
 [ 0.12087594  0.56        0.56363636  0.5265537   0.54532912  0.13335432
   0.57083333  0.66363636  0.45473663  0.55317586]
 [ 0.49688479  0.75892857  0.75454545  0.74102414  0.74042888  0.191471
   0.59833333  0.58181818  0.55149916  0.5658855 ]
 [ 0.42269194  0.70833333  0.71818182  0.67557383  0.69870879 -0.01554063
   0.49333333  0.49090909  0.44426285  0.47507548]
 [ 0.17178474  0.58571429  0.63636364  0.50740511  0.57301821 -0.01038705
   0.495       0.50909091  0.41469038  0.4696355 ]
 [ 0.23172234  0.62291667  0.70909091  0.50633849  0.60386905  0.06456093
   0.5375      0.55454545  0.48241829  0.52709402]
 [ 0.23815226  0.61428571  0.67272727  0.52758451  0.60242423  0.45499411
   0.70416667  0.82727273  0.5649501   0.70467738]
 [-0.0042503   0.49833333  0.50909091  0.44156686  0.48358738  0.07356433
   0.535       0.54545455  0.50817298  0.52709402]
 [ 0.10581793  0.55        0.59090909  0.50326814  0.54821429  0.15831032
   0.57142857  0.64545455  0.42240685  0.55481326]
 [ 0.07292505  0.54464286  0.59090909  0.42643096  0.51821078  0.30069148
   0.66428571  0.65454545  0.65196289  0.64238373]
 [ 0.08621851  0.54333333  0.54545455  0.53689424  0.5405439   0.33925776
   0.67333333  0.66363636  0.64236099  0.65254912]
 [-0.01012232  0.49375     0.53636364  0.43893947  0.47329293  0.29018574
   0.64833333  0.64545455  0.6267866   0.63551587]
 [-0.099827    0.42708333  0.5         0.28090395  0.4031391   0.32886433
   0.675       0.68181818  0.61910045  0.64857753]
 [ 0.01097298  0.50535714  0.47272727  0.40112175  0.4463499   0.24078202
   0.60892857  0.7         0.4113273   0.57598389]
 [ 0.27531605  0.635       0.65454545  0.53701799  0.6047233   0.19795531
   0.58928571  0.65454545  0.52650883  0.58566212]
 [ 0.1240779   0.625       0.80909091  0.34446445  0.53927736  0.27594639
   0.64464286  0.66363636  0.61069927  0.62704656]
 [-0.01727335  0.49583333  0.55454545  0.37691062  0.46913919  0.04869915
   0.52666667  0.52727273  0.488884    0.50829226]
 [ 0.0344814   0.5125      0.56363636  0.39649419  0.49158419 -0.17649943
   0.41607143  0.48181818  0.22724594  0.39214591]
 [-0.10186104  0.44444444  0.66363636  0.12009598  0.42797054  0.1365557
   0.56833333  0.57272727  0.53616841  0.55383117]
 [ 0.03120187  0.515       0.52727273  0.45943448  0.50220613 -0.07937414
   0.46071429  0.51818182  0.32778326  0.44273044]
 [ 0.12766832  0.56071429  0.59090909  0.5061192   0.55007979  0.27356181
   0.61458333  0.77272727  0.42060211  0.61028896]
 [-0.19008695  0.40333333  0.4         0.35085888  0.38194084  0.23008102
   0.61333333  0.62727273  0.57413865  0.6010076 ]
 [ 0.12257917  0.56607143  0.56363636  0.55356298  0.549392   -0.03021986
   0.485       0.49090909  0.46415711  0.4768809 ]
 [-0.01648455  0.49166667  0.50909091  0.40795382  0.46669789  0.04378005
   0.52166667  0.53636364  0.44100751  0.49633752]
 [ 0.47662627  0.74166667  0.79090909  0.68889047  0.73174118  0.52615482
   0.77        0.75454545  0.73614727  0.74497946]
 [ 0.49550043  0.73928571  0.76363636  0.72237002  0.74109307  0.54481941
   0.75714286  0.8         0.73500261  0.76886905]
 [ 0.25279466  0.62142857  0.65454545  0.59539161  0.61856907  0.28354011
   0.63214286  0.68181818  0.5920689   0.63242773]
 [ 0.09081364  0.55        0.56363636  0.52759394  0.53614524 -0.08145431
   0.45357143  0.50909091  0.35340815  0.44522409]]
BDDAE mean:
[0.12963572 0.56620966 0.6030303  0.48913694 0.54744844 0.19517111
 0.59778373 0.63181818 0.52560092 0.58175837]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.56666667 0.         0.36083333 0.
  0.5        0.8        0.         0.4440404 ]
 [0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.56666667 0.         0.36083333 0.
  0.5        0.45       0.         0.30952381]
 [0.         0.5        0.53       0.         0.3452381  0.
  0.5        0.71333333 0.         0.41527778]
 [0.         0.5        0.61666667 0.         0.38083333 0.
  0.5        0.51       0.         0.33630952]
 [0.         0.5        0.45       0.         0.30952381 0.
  0.5        0.51       0.         0.33630952]
 [0.         0.5        0.63333333 0.         0.3875     0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.69333333 0.         0.40833333 0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.67333333 0.         0.40138889 0.
  0.5        0.69333333 0.         0.40833333]
 [0.         0.5        0.51       0.         0.33630952 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.65333333 0.         0.39444444 0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.6        0.         0.37416667 0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.58333333 0.         0.3675     0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.69333333 0.         0.40833333 0.
  0.5        0.51       0.         0.33630952]
 [0.         0.5        0.69333333 0.         0.40833333 0.
  0.5        0.65333333 0.         0.39444444]
 [0.         0.5        0.6        0.         0.37416667 0.
  0.5        0.65333333 0.         0.39444444]
 [0.         0.5        0.58333333 0.         0.3675     0.
  0.5        0.6        0.         0.37416667]
 [       nan 0.7        0.89666667 0.4        0.67171717 0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.69333333 0.         0.40833333 0.
  0.5        0.56666667 0.         0.36083333]
 [0.         0.5        0.67333333 0.         0.40138889 0.
  0.5        0.65333333 0.         0.39444444]
 [0.         0.5        0.8        0.         0.4440404  0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.45       0.         0.30952381 0.
  0.5        0.63333333 0.         0.3875    ]
 [0.         0.5        0.6        0.         0.37416667 0.
  0.5        0.76666667 0.         0.43313131]
 [0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.56666667 0.         0.36083333]
 [0.         0.5        0.6        0.         0.37416667 0.
  0.5        0.45       0.         0.30952381]
 [0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.69333333 0.         0.40833333 0.
  0.5        0.53       0.         0.3452381 ]
 [0.         0.5        0.63333333 0.         0.3875     0.
  0.5        0.61666667 0.         0.38083333]
 [0.         0.5        0.65333333 0.         0.39444444 0.
  0.5        0.6        0.         0.37416667]
 [0.         0.5        0.63333333 0.         0.3875     0.
  0.5        0.65333333 0.         0.39444444]]
DUMMY mean:
[0.         0.50666667 0.62077778 0.01333333 0.38696176 0.
 0.5        0.59688889 0.         0.37135281]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_57
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.311 0.654 0.694 0.528 0.629 0.329 0.666 0.709 0.537 0.639]
 [0.292 0.654 0.673 0.565 0.628 0.273 0.643 0.664 0.554 0.617]
 [0.348 0.667 0.696 0.584 0.641 0.324 0.649 0.684 0.566 0.628]
 [0.007 0.504 0.616 0.014 0.385 0.053 0.527 0.624 0.079 0.416]
 [0.296 0.649 0.7   0.494 0.618 0.289 0.634 0.684 0.488 0.602]
 [0.13  0.566 0.603 0.489 0.547 0.195 0.598 0.632 0.526 0.582]
 [0.    0.5   0.611 0.    0.377 0.    0.5   0.597 0.    0.371]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.258 0.13  0.116 0.212 0.14  0.152 0.075 0.063 0.163 0.084]
 [0.222 0.121 0.112 0.176 0.128 0.14  0.071 0.064 0.125 0.077]
 [0.232 0.116 0.107 0.193 0.126 0.159 0.092 0.087 0.123 0.1  ]
 [0.029 0.015 0.068 0.044 0.025 0.149 0.076 0.081 0.204 0.104]
 [0.252 0.122 0.109 0.225 0.141 0.174 0.089 0.075 0.193 0.103]
 [0.173 0.089 0.098 0.128 0.092 0.183 0.092 0.1   0.122 0.093]
 [0.    0.    0.076 0.    0.03  0.    0.    0.083 0.    0.032]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 83.  20.  17.  40.  22.  46.  11.   9.  30.  13.]
 [ 76.  18.  17.  31.  20.  51.  11.  10.  23.  12.]
 [ 67.  17.  15.  33.  20.  49.  14.  13.  22.  16.]
 [413.   3.  11. 321.   6. 279.  14.  13. 258.  25.]
 [ 85.  19.  16.  46.  23.  60.  14.  11.  40.  17.]
 [133.  16.  16.  26.  17.  94.  15.  16.  23.  16.]
 [  0.   0.  12.   0.   8.   0.   0.  14.   0.   9.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_57
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  27.008
step (sec):  20.256
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  6.752
Number of windows / instances:  55
Elapsed time: 930.2862499435743 minutes
Elapsed time: 15.504770832392904 hours
