2024-05-18 07:34:44.848363: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-18 07:34:48.649423: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-18 07:34:57.987922: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  27.0
step (sec):  13.5
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  13.5
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_58\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

inary_accuracy: 0.7387 - loss: 0.4907
[1m48/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7327 - loss: 0.4974
[1m52/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7258 - loss: 0.5044
[1m56/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7205 - loss: 0.5091
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.7186 - loss: 0.5108 - val_binary_accuracy: 0.7143 - val_loss: 0.5073
Epoch 6/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 0.0000e+00 - loss: 0.7416
[1m 5/57[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5433 - loss: 0.4154    
[1m 9/57[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6857 - loss: 0.3315
[1m13/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7554 - loss: 0.2971
[1m17/57[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7977 - loss: 0.2794
[1m21/57[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8166 - loss: 0.2774
[1m25/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8274 - loss: 0.2781
[1m29/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8249 - loss: 0.2926
[1m32/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8182 - loss: 0.3049
[1m36/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8102 - loss: 0.3172
[1m40/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8032 - loss: 0.3277
[1m45/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7982 - loss: 0.3376
[1m48/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7962 - loss: 0.3424
[1m52/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7947 - loss: 0.3472
[1m56/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7930 - loss: 0.3518
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 16ms/step - binary_accuracy: 0.7917 - loss: 0.3547 - val_binary_accuracy: 0.8571 - val_loss: 0.5142
Epoch 7/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3537
[1m 4/57[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 21ms/step - binary_accuracy: 1.0000 - loss: 0.3114
[1m 8/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 1.0000 - loss: 0.2547
[1m11/57[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - binary_accuracy: 1.0000 - loss: 0.2394
[1m15/57[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9956 - loss: 0.2314
[1m19/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9816 - loss: 0.2389
[1m23/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9686 - loss: 0.2462
[1m26/57[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9584 - loss: 0.2561
[1m30/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9476 - loss: 0.2692
[1m34/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9366 - loss: 0.2821
[1m37/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9290 - loss: 0.2906
[1m41/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9186 - loss: 0.3005
[1m45/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9105 - loss: 0.3086
[1m49/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9041 - loss: 0.3153
[1m54/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8973 - loss: 0.3234
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.8933 - loss: 0.3277 - val_binary_accuracy: 0.7143 - val_loss: 0.6042
Epoch 8/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.4453
[1m 4/57[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 21ms/step - binary_accuracy: 1.0000 - loss: 0.3565
[1m 8/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 1.0000 - loss: 0.3032
[1m12/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 1.0000 - loss: 0.2741
[1m16/57[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 1.0000 - loss: 0.2557
[1m20/57[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9891 - loss: 0.2636
[1m24/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9835 - loss: 0.2647
[1m28/57[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9739 - loss: 0.2683
[1m32/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9649 - loss: 0.2710
[1m36/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9583 - loss: 0.2744
[1m40/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9521 - loss: 0.2796
[1m43/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9488 - loss: 0.2823
[1m47/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9457 - loss: 0.2844
[1m52/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9421 - loss: 0.2866
[1m56/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9377 - loss: 0.2904
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9350 - loss: 0.2928 - val_binary_accuracy: 0.7143 - val_loss: 0.6337
Epoch 9/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.1955
[1m 5/57[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2398
[1m 8/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9665 - loss: 0.2975
[1m12/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9456 - loss: 0.3463
[1m15/57[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9285 - loss: 0.3618
[1m19/57[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9016 - loss: 0.3812
[1m23/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8782 - loss: 0.3975
[1m27/57[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8570 - loss: 0.4130
[1m31/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8448 - loss: 0.4200
[1m35/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8386 - loss: 0.4212
[1m39/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8340 - loss: 0.4208
[1m43/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8315 - loss: 0.4183
[1m47/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8309 - loss: 0.4144
[1m51/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8303 - loss: 0.4113
[1m55/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8304 - loss: 0.4077
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.8309 - loss: 0.4047 - val_binary_accuracy: 0.5714 - val_loss: 0.6825
Epoch 10/10

[1m 1/57[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.4741
[1m 6/57[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 13ms/step - binary_accuracy: 0.7583 - loss: 0.4555
[1m10/57[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 14ms/step - binary_accuracy: 0.8071 - loss: 0.3853
[1m13/57[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8323 - loss: 0.3516
[1m17/57[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8565 - loss: 0.3158
[1m21/57[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8740 - loss: 0.2896
[1m25/57[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8874 - loss: 0.2691
[1m29/57[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8904 - loss: 0.2643
[1m34/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - binary_accuracy: 0.8910 - loss: 0.2617
[1m38/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 15ms/step - binary_accuracy: 0.8909 - loss: 0.2612
[1m41/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8904 - loss: 0.2615
[1m45/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8889 - loss: 0.2625
[1m49/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8872 - loss: 0.2635
[1m53/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8855 - loss: 0.2649
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8837 - loss: 0.2666
[1m57/57[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 16ms/step - binary_accuracy: 0.8830 - loss: 0.2673 - val_binary_accuracy: 0.5714 - val_loss: 0.7621

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
predicted [0.0505892  0.91728354 0.6927736  0.19648099 0.8957275  0.07269181
 0.81828356 0.03152968 0.5583155  0.8231439  0.09641788 0.49127522
 0.0842839  0.87097967 0.41900787 0.93169266]
predicted [0 1 1 0 1 0 1 0 1 1 0 0 0 1 0 1]
expected [False  True  True False  True  True  True False  True  True False False
 False  True  True  True]
accuracy: 0.875
confusion matrix: 
[[6 0]
 [2 8]]
              precision    recall  f1-score   support

       False       0.75      1.00      0.86         6
        True       1.00      0.80      0.89        10

    accuracy                           0.88        16
   macro avg       0.88      0.90      0.87        16
weighted avg       0.91      0.88      0.88        16

macro avg f1-score: 0.873015873015873
macro avg (UAR): 0.9
Sensitivity:  1.0
Specificity:  0.8
g-mean:  0.8944271909999159
-------- Model Performance ----------: 
accuracy:  [0.5    0.75   0.5625 0.625  0.5    0.5625 0.6875 0.625  0.4375 0.875 ]
gmean:  [0.4472136  0.73029674 0.48304589 0.51639778 0.5        0.48304589
 0.63245553 0.51639778 0.4472136  0.89442719]
f1_score:  [0.46666667 0.73333333 0.51515152 0.56363636 0.49206349 0.51515152
 0.65367965 0.56363636 0.43529412 0.87301587]
UAR:  [0.46666667 0.73333333 0.51666667 0.56666667 0.5        0.51666667
 0.65       0.56666667 0.48333333 0.9       ]
Cohen Kappa score:  [-0.06666667  0.46666667  0.03448276  0.14285714  0.          0.03448276
  0.31034483  0.14285714 -0.02857143  0.75      ]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  27.008
step (sec):  13.504
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  13.504
Number of windows / instances:  80
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.95  0.98  0.975 0.979 0.975 0.601 0.788 0.838 0.767 0.797]
 [0.84  0.92  0.925 0.896 0.919 0.649 0.82  0.85  0.816 0.817]
 [0.815 0.97  0.962 0.873 0.962 0.577 0.818 0.85  0.824 0.822]
 [0.115 0.55  0.662 0.173 0.469 0.    0.5   0.662 0.    0.398]
 [0.865 0.93  0.938 0.926 0.932 0.606 0.788 0.838 0.741 0.799]
 [0.179 0.59  0.612 0.565 0.581 0.078 0.539 0.606 0.455 0.524]
 [0.    0.5   0.625 0.    0.385 0.    0.5   0.662 0.    0.398]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.66833333  0.83916667  0.8375      0.83475241  0.8331746   0.29238095
   0.64404762  0.825       0.40471404  0.63032967]
 [ 0.45        0.72916667  0.725       0.71161339  0.71588745  0.42287546
   0.70833333  0.7625      0.57917834  0.68134532]
 [ 0.54120879  0.75916667  0.7875      0.73274161  0.76271284  0.325
   0.6625      0.65        0.58710092  0.62871573]
 [ 0.4647619   0.73        0.7375      0.70881732  0.72513709  0.09809524
   0.54666667  0.7         0.25754515  0.50634699]
 [ 0.68700711  0.83        0.8625      0.80117801  0.83560606  0.44666667
   0.72333333  0.725       0.69398954  0.70993506]
 [ 0.58928571  0.79416667  0.8         0.77999786  0.79080808  0.59642857
   0.79666667  0.8         0.73665075  0.7778355 ]
 [ 0.51404008  0.75333333  0.8         0.64214144  0.7347669   0.34921569
   0.675       0.675       0.64427349  0.65920635]
 [ 0.78774725  0.89        0.9125      0.87247483  0.88981907  0.66413165
   0.83833333  0.8375      0.82585717  0.82781385]
 [ 0.19289377  0.59666667  0.7125      0.39365733  0.57441891 -0.01452381
   0.49666667  0.6         0.29998581  0.46687091]
 [ 0.525       0.76333333  0.7625      0.74364775  0.75365079  0.1797619
   0.595       0.6         0.52445605  0.56944444]
 [ 0.33591898  0.66833333  0.725       0.50459631  0.6311655   0.34694139
   0.66666667  0.725       0.54452132  0.64939949]
 [ 0.61914997  0.80333333  0.8375      0.72732141  0.7904601   0.55428571
   0.76        0.8         0.74093842  0.77454545]
 [ 0.4341058   0.71583333  0.725       0.69884911  0.70910534  0.35794118
   0.69083333  0.675       0.63921292  0.6605267 ]
 [ 0.20263736  0.60166667  0.7         0.44069768  0.5808658   0.475
   0.7375      0.7375      0.7106713   0.72611111]
 [ 0.14830281  0.56833333  0.6375      0.46159984  0.55133311  0.2856044
   0.64833333  0.6875      0.54949817  0.62391608]
 [ 0.09800366  0.55        0.6         0.42012959  0.53336219  0.30784493
   0.645       0.7125      0.48459133  0.61785548]
 [ 0.55549451  0.78333333  0.8         0.73037007  0.77026696  0.44858974
   0.71666667  0.75        0.68238517  0.71508658]
 [ 0.13142857  0.61607143  0.8625      0.19258201  0.54520147  0.54659664
   0.78333333  0.775       0.75963739  0.7626912 ]
 [ 0.34989011  0.66333333  0.7375      0.58497496  0.65571096  0.46703297
   0.73166667  0.75        0.684258    0.72613997]
 [ 0.41973138  0.69666667  0.7625      0.57558964  0.67913198  0.22681319
   0.61166667  0.675       0.49683053  0.597336  ]
 [-0.03904762  0.4952381   0.625       0.25031704  0.45922078  0.11722689
   0.56166667  0.5625      0.51924249  0.54479076]
 [ 0.40761905  0.70333333  0.7125      0.67909933  0.6947114   0.56620879
   0.785       0.8125      0.75759833  0.77648574]
 [ 0.37190476  0.68916667  0.7         0.66847325  0.67907648  0.23761905
   0.61666667  0.75        0.41606644  0.59746254]
 [ 0.24095238  0.62333333  0.625       0.57968466  0.60113997  0.43489011
   0.7175      0.725       0.68164936  0.70323232]
 [ 0.47880952  0.735       0.7625      0.66772067  0.7222422   0.675
   0.8375      0.8375      0.82872338  0.83484127]
 [ 0.21428571  0.60916667  0.6125      0.52756893  0.57656566  0.34960784
   0.68083333  0.675       0.66964841  0.66809524]
 [ 0.64774725  0.81333333  0.875       0.74023888  0.81216339  0.63088235
   0.81583333  0.8125      0.80928718  0.81111111]
 [ 0.19191123  0.59333333  0.625       0.48848398  0.56899545  0.58367378
   0.79        0.8125      0.75978393  0.78354257]
 [ 0.36078647  0.6825      0.6875      0.64941893  0.66296537  0.36997414
   0.68583333  0.7         0.63162361  0.67026696]
 [ 0.95        0.98        0.975       0.97888544  0.97460317  0.60131868
   0.78833333  0.8375      0.76690381  0.79706294]]
KNN mean:
[0.417997   0.70921032 0.75083333 0.62625412 0.69380897 0.3981028
 0.69857937 0.73291667 0.62289409 0.68327811]
---------------------------
---------------------------
DT performance:
[[ 0.62857143  0.78083333  0.7875      0.7518447   0.77169553  0.2156044
   0.67380952  0.8         0.3904865   0.63040293]
 [ 0.5147619   0.7575      0.7625      0.73980993  0.75474026  0.37239927
   0.76166667  0.775       0.70095922  0.74360029]
 [ 0.52928571  0.70916667  0.725       0.73717957  0.70886003  0.27368421
   0.60833333  0.6         0.49468156  0.57838384]
 [ 0.50833333  0.70583333  0.7         0.72519457  0.6918254   0.08666667
   0.50166667  0.6125      0.40846548  0.48689977]
 [ 0.62512821  0.83333333  0.85        0.85234642  0.83275613  0.54666667
   0.76083333  0.7625      0.73084128  0.75738095]
 [ 0.44960784  0.77833333  0.775       0.72422392  0.76580808  0.525
   0.73666667  0.7375      0.66311992  0.71619048]
 [ 0.26137686  0.615       0.6375      0.59537045  0.57902431  0.23627451
   0.64416667  0.6375      0.58489745  0.62746032]
 [ 0.88131868  0.94166667  0.9625      0.93270845  0.9444289   0.56750916
   0.79416667  0.8125      0.79418151  0.78984127]
 [ 0.40666667  0.67833333  0.7625      0.53879947  0.65511822  0.59131868
   0.75666667  0.775       0.78769902  0.73918415]
 [ 0.07016807  0.5575      0.5625      0.54106293  0.54818903  0.15833333
   0.58166667  0.575       0.56458105  0.54958874]
 [ 0.20836565  0.595       0.6375      0.47929894  0.58387446  0.46142857
   0.74        0.775       0.73313931  0.72882229]
 [ 0.64714286  0.79666667  0.8125      0.81174942  0.79165945  0.56833333
   0.76666667  0.775       0.73789282  0.76023088]
 [ 0.22584249  0.66        0.675       0.62150307  0.64867965  0.29603641
   0.61916667  0.6         0.54907089  0.58132035]
 [ 0.20869963  0.56166667  0.6375      0.49856194  0.54265013  0.6
   0.7125      0.7125      0.72583257  0.70452381]
 [ 0.28822344  0.68333333  0.75        0.4957955   0.67177045  0.35958199
   0.68166667  0.7125      0.5887643   0.67608891]
 [ 0.18205417  0.57166667  0.6125      0.46003057  0.54315851  0.44498779
   0.745       0.775       0.67293179  0.73229437]
 [ 0.68595238  0.83083333  0.8375      0.80176828  0.82444444  0.62001832
   0.78833333  0.8         0.72220033  0.77471861]
 [-0.02285714  0.525       0.7875      0.1         0.48835165  0.294234
   0.64083333  0.65        0.64021392  0.62471861]
 [ 0.45668498  0.74666667  0.7875      0.71474154  0.73393939  0.48238095
   0.74416667  0.7625      0.73624134  0.7408658 ]
 [ 0.18036464  0.58        0.6375      0.3891361   0.55307193  0.13101056
   0.59166667  0.6125      0.5032157   0.57191919]
 [ 0.1209707   0.51309524  0.65        0.26242737  0.48045954  0.05428571
   0.49        0.4875      0.45543817  0.4602886 ]
 [ 0.27127451  0.6575      0.65        0.6121306   0.63334776  0.5103663
   0.71166667  0.7375      0.75630621  0.70393939]
 [ 0.04042448  0.58166667  0.6         0.49670731  0.5644733   0.23142857
   0.66666667  0.725       0.59142088  0.63222777]
 [ 0.18333333  0.58        0.575       0.5650865   0.5550938   0.46722689
   0.76166667  0.7625      0.73671211  0.75271284]
 [ 0.55674603  0.78        0.7875      0.79800983  0.76408369  0.475
   0.7         0.7         0.70460528  0.6882684 ]
 [ 0.59666667  0.78583333  0.7875      0.76441992  0.78071429  0.2001634
   0.6725      0.6625      0.53341281  0.64437951]
 [ 0.34627451  0.67166667  0.725       0.58936803  0.64052614  0.57142857
   0.74166667  0.75        0.70948061  0.73674603]
 [ 0.33616462  0.67666667  0.6875      0.65650049  0.66992063  0.40040616
   0.70333333  0.7125      0.66268612  0.6895671 ]
 [ 0.3491058   0.63916667  0.65        0.61679214  0.63189755  0.24960784
   0.5875      0.6         0.5788984   0.58016595]
 [ 0.83952381  0.92        0.925       0.89603716  0.9188456   0.64928571
   0.82        0.85        0.81621783  0.8170918 ]]
DT mean:
[0.38587254 0.69046429 0.72458333 0.62562017 0.67578028 0.38802227
 0.69015476 0.70833333 0.64248648 0.6739941 ]
---------------------------
---------------------------
RF performance:
[[0.62127451 0.85666667 0.8625     0.85023966 0.8531746  0.53047619
  0.70595238 0.8375     0.38984018 0.72490842]
 [0.53489011 0.78666667 0.7875     0.68170522 0.76650794 0.29896574
  0.71       0.7375     0.58094813 0.68988289]
 [0.37001401 0.72916667 0.75       0.70069712 0.73059885 0.30833333
  0.73833333 0.7375     0.68056368 0.73404762]
 [0.49960784 0.75166667 0.75       0.71642823 0.73874459 0.09835165
  0.56166667 0.6875     0.44333104 0.55435897]
 [0.64925986 0.81333333 0.85       0.80225854 0.82181818 0.52166667
  0.71083333 0.7125     0.70342356 0.69810967]
 [0.49666667 0.69916667 0.7        0.58954832 0.68968254 0.57142857
  0.83583333 0.8375     0.75041351 0.81690476]
 [0.40681319 0.73333333 0.75       0.74756087 0.71362915 0.32421569
  0.66166667 0.65       0.65221247 0.63968254]
 [0.87785714 0.915      0.9375     0.90380083 0.91718504 0.68965201
  0.7575     0.775      0.79918383 0.74929293]
 [0.25538462 0.66166667 0.725      0.55648257 0.64055944 0.21858974
  0.56666667 0.6125     0.60429566 0.54885115]
 [0.35254902 0.60916667 0.6125     0.56798419 0.60279221 0.26214286
  0.5625     0.575      0.58673781 0.54958874]
 [0.32822344 0.59166667 0.6375     0.56757508 0.55866356 0.40380952
  0.74666667 0.775      0.72480883 0.75106061]
 [0.82285714 0.82666667 0.825      0.85873763 0.81602453 0.53854557
  0.77333333 0.8        0.78899892 0.77284271]
 [0.34497414 0.67333333 0.7125     0.60177925 0.65452214 0.34785714
  0.71333333 0.7125     0.63175293 0.69753247]
 [0.18627451 0.62833333 0.7        0.42791697 0.60480186 0.525
  0.725      0.725      0.67056346 0.71888889]
 [0.47501832 0.61166667 0.7        0.56962832 0.59924964 0.36045033
  0.625      0.675      0.574484   0.61335997]
 [0.18144689 0.59333333 0.6375     0.67995379 0.5772422  0.36029627
  0.74       0.7875     0.65039927 0.72879343]
 [0.72441392 0.83       0.85       0.77318254 0.82975469 0.53666667
  0.8425     0.8375     0.84576183 0.82850649]
 [0.23142857 0.54464286 0.8125     0.39258201 0.4959707  0.3797619
  0.75416667 0.775      0.75882633 0.75450938]
 [0.38608059 0.74       0.8125     0.55819715 0.73395271 0.56142857
  0.7675     0.775      0.67619243 0.75029582]
 [0.37224054 0.655      0.75       0.6562921  0.63214785 0.30417582
  0.64666667 0.6875     0.46609706 0.63463203]
 [0.09809524 0.52857143 0.6625     0.12909944 0.53162171 0.04333333
  0.52166667 0.5375     0.43760901 0.51235209]
 [0.365      0.7575     0.7625     0.80586771 0.75007215 0.4660989
  0.74166667 0.775      0.71172213 0.72157842]
 [0.24984594 0.70833333 0.7125     0.56505684 0.696443   0.35333333
  0.70833333 0.7625     0.51083001 0.65965368]
 [0.35166667 0.65333333 0.6625     0.69785003 0.64383838 0.59642857
  0.73       0.7375     0.81176206 0.70893773]
 [0.58595238 0.7125     0.7375     0.71248996 0.68009768 0.525
  0.8375     0.8375     0.84212084 0.83404762]
 [0.265      0.68833333 0.6875     0.62959783 0.67904762 0.37040616
  0.70833333 0.7125     0.62764878 0.68426407]
 [0.52309524 0.74166667 0.7875     0.63984729 0.70856144 0.57166667
  0.8375     0.8375     0.78264741 0.83444444]
 [0.18018638 0.54333333 0.5875     0.50548019 0.52336219 0.54187424
  0.79       0.8125     0.66685408 0.76748751]
 [0.23284076 0.635      0.675      0.53263365 0.624329   0.33529627
  0.70666667 0.7125     0.47212559 0.7004329 ]
 [0.8147619  0.97       0.9625     0.87294946 0.96190476 0.57666667
  0.81833333 0.85       0.82401089 0.82194028]]
RF mean:
[0.42612398 0.70630159 0.74666667 0.64311409 0.69254335 0.41739728
 0.71817063 0.74291667 0.65553886 0.70670628]
---------------------------
---------------------------
SVM performance:
[[0.         0.5        0.55       0.         0.35384615 0.
  0.5        0.8        0.         0.44380952]
 [0.03846154 0.51666667 0.55       0.05773503 0.37692308 0.
  0.5        0.6375     0.         0.38901099]
 [0.23681319 0.60833333 0.6625     0.35485474 0.54177156 0.45
  0.72583333 0.725      0.66410923 0.7034127 ]
 [0.20679487 0.60083333 0.6125     0.42745639 0.53681818 0.
  0.5        0.725      0.         0.41978022]
 [0.37362637 0.66666667 0.75       0.50970948 0.64783217 0.5
  0.75083333 0.75       0.71447453 0.73320346]
 [0.06428571 0.53166667 0.55       0.16287702 0.41309357 0.67142857
  0.83416667 0.8375     0.80952165 0.82499278]
 [0.         0.5        0.65       0.         0.39340659 0.14285714
  0.56666667 0.5875     0.20660059 0.4560373 ]
 [0.29538462 0.625      0.7625     0.38533712 0.6010989  0.
  0.5        0.6125     0.         0.37948718]
 [0.         0.5        0.7        0.         0.41098901 0.
  0.5        0.6875     0.         0.40659341]
 [0.         0.5        0.525      0.         0.34358974 0.
  0.5        0.55       0.         0.35384615]
 [0.         0.5        0.6625     0.         0.3978022  0.
  0.5        0.625      0.         0.38461538]
 [0.         0.5        0.625      0.         0.38461538 0.
  0.5        0.625      0.         0.38461538]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.5625     0.         0.35897436]
 [0.         0.5        0.6875     0.         0.40659341 0.45
  0.725      0.725      0.70540219 0.71674603]
 [0.         0.5        0.6875     0.         0.40659341 0.
  0.5        0.65       0.         0.39340659]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.65       0.         0.39340659]
 [0.         0.5        0.575      0.         0.36410256 0.
  0.5        0.6        0.         0.37435897]
 [       nan 0.55       0.8875     0.1        0.52       0.
  0.5        0.6        0.         0.37435897]
 [0.         0.5        0.7125     0.         0.41538462 0.
  0.5        0.575      0.         0.36410256]
 [0.         0.5        0.675      0.         0.4021978  0.
  0.5        0.65       0.         0.39340659]
 [0.         0.5        0.775      0.         0.43619048 0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.5375     0.         0.34871795 0.
  0.5        0.6625     0.         0.3978022 ]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.75       0.         0.42857143]
 [0.         0.5        0.5375     0.         0.34871795 0.26785714
  0.62916667 0.65       0.36329932 0.54937063]
 [0.         0.5        0.5875     0.         0.36923077 0.375
  0.6875     0.6875     0.64805787 0.66891775]
 [0.         0.5        0.525      0.         0.34358974 0.01428571
  0.50666667 0.5625     0.05163978 0.37687646]
 [0.         0.5        0.7        0.         0.41098901 0.56428571
  0.78166667 0.7875     0.76836421 0.77858586]
 [0.         0.5        0.625      0.         0.38461538 0.
  0.5        0.625      0.         0.38461538]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.5875     0.         0.36923077]
 [0.11538462 0.55       0.6625     0.17320508 0.46923077 0.
  0.5        0.6625     0.         0.3978022 ]]
SVM mean:
[0.04588796 0.52163889 0.63916667 0.07237249 0.41751254 0.11452381
 0.55691667 0.65583333 0.16438231 0.46478425]
---------------------------
---------------------------
GBM performance:
[[ 0.63238095  0.82583333  0.8375      0.80023487  0.81945166  0.25142857
   0.61071429  0.8375      0.31213203  0.60322344]
 [ 0.34309524  0.6825      0.6875      0.65477662  0.67680375  0.2809707
   0.655       0.7         0.52907584  0.62255078]
 [ 0.55        0.765       0.8         0.69286485  0.7579132   0.22368421
   0.63833333  0.6375      0.5783886   0.62010823]
 [ 0.425       0.7125      0.7125      0.69429064  0.70365079  0.00358974
   0.49833333  0.7         0.07071068  0.44175824]
 [ 0.71727106  0.84666667  0.875       0.82845571  0.85457431  0.52166667
   0.73583333  0.7375      0.74255992  0.71930014]
 [ 0.42460784  0.71583333  0.7125      0.70893366  0.70325397  0.62166667
   0.81166667  0.8125      0.75339607  0.79111111]
 [ 0.23728938  0.61666667  0.7         0.43098856  0.57917416  0.27810458
   0.62916667  0.6125      0.57664936  0.60365079]
 [ 0.86285714  0.9         0.9375      0.89600777  0.89325341  0.63238095
   0.83166667  0.8375      0.79804622  0.82499278]
 [ 0.29142857  0.62166667  0.75        0.41763766  0.58661339  0.1407326
   0.53166667  0.6625      0.23849973  0.48610057]
 [ 0.32731092  0.70333333  0.7         0.68142748  0.69080808  0.25393773
   0.6175      0.625       0.53328389  0.59204906]
 [ 0.32336996  0.66166667  0.75        0.48804045  0.64504829  0.45157509
   0.71333333  0.775       0.58256375  0.69449883]
 [ 0.71369963  0.84666667  0.875       0.76961843  0.85272727  0.65655678
   0.83        0.8625      0.78678629  0.83575758]
 [ 0.20148675  0.59666667  0.6375      0.50911908  0.57772561  0.09728507
   0.5575      0.5625      0.53851827  0.54253968]
 [ 0.29750916  0.62        0.7375      0.40717706  0.61936064  0.325
   0.675       0.675       0.66514304  0.67095238]
 [-0.00461538  0.49833333  0.675       0.05773503  0.42527473  0.46906593
   0.705       0.775       0.59056255  0.68553003]
 [ 0.05954212  0.54        0.6125      0.24834603  0.47883894  0.37252747
   0.66666667  0.75        0.4865044   0.64265734]
 [ 0.70679487  0.83416667  0.85        0.83431267  0.83402597  0.48430403
   0.73666667  0.775       0.65265324  0.72091464]
 [        nan  0.52142857  0.8375      0.1         0.5047619   0.39438806
   0.70333333  0.7125      0.65251223  0.68749639]
 [ 0.20014652  0.59833333  0.75        0.30393441  0.56201132  0.47880952
   0.7475      0.775       0.68420352  0.73193917]
 [ 0.18173382  0.54166667  0.625       0.36651052  0.51419913  0.22373626
   0.56833333  0.675       0.32556542  0.53595738]
 [-0.02        0.49166667  0.7625      0.          0.43179487  0.16833333
   0.57166667  0.5875      0.50316567  0.54561883]
 [ 0.28889356  0.6475      0.65        0.56727216  0.61534632  0.37787546
   0.68333333  0.75        0.55789054  0.66434898]
 [ 0.16084249  0.5925      0.625       0.46677544  0.55932734  0.10761905
   0.56666667  0.725       0.29981012  0.54558775]
 [ 0.16601056  0.56833333  0.575       0.53719218  0.55515873  0.52016807
   0.77666667  0.775       0.74161592  0.76247475]
 [ 0.28956044  0.63166667  0.6875      0.51876806  0.61188145  0.525
   0.75        0.75        0.74208973  0.74761905]
 [ 0.59642857  0.79583333  0.8         0.7314696   0.78048341  0.19254902
   0.5975      0.6         0.53091523  0.57077922]
 [ 0.48547619  0.72833333  0.8         0.61999033  0.71640582  0.575
   0.7875      0.7875      0.77495067  0.78285714]
 [ 0.16087912  0.57333333  0.65        0.39423567  0.54298368  0.34690476
   0.65666667  0.6875      0.60583989  0.64293484]
 [ 0.3217033   0.67583333  0.7375      0.45649452  0.66484848  0.3819888
   0.69916667  0.7         0.68363541  0.68287157]
 [ 0.86452381  0.93        0.9375      0.92577171  0.93154401  0.60571429
   0.78833333  0.8375      0.74095674  0.79897436]]
GBM mean:
[0.37259402 0.67613095 0.74291667 0.53694604 0.65630815 0.36541878
 0.67802381 0.72333333 0.57595417 0.65990517]
---------------------------
---------------------------
BDDAE performance:
[[-1.74700110e-01  4.12698413e-01  4.12500000e-01  3.89803192e-01
   4.06359035e-01  8.76662673e-02  5.38461538e-01  7.50000000e-01
   3.14992987e-01  5.26353289e-01]
 [ 1.88843747e-01  5.92857143e-01  6.06250000e-01  5.67704865e-01
   5.87033496e-01  4.11953815e-01  7.13333333e-01  7.12500000e-01
   7.04457649e-01  7.00361313e-01]
 [ 1.37445702e-01  5.69841270e-01  5.75000000e-01  5.46224700e-01
   5.59665628e-01  5.37500000e-01  7.68750000e-01  7.68750000e-01
   7.59702491e-01  7.65553996e-01]
 [ 1.25000000e-01  5.62500000e-01  5.62500000e-01  5.32371536e-01
   5.47728611e-01  9.68142968e-02  5.54166667e-01  6.81250000e-01
   3.83707895e-01  5.26595562e-01]
 [ 2.76515024e-01  6.45000000e-01  6.56250000e-01  5.91986934e-01
   6.25215954e-01  1.62500000e-01  5.81250000e-01  5.81250000e-01
   5.47460672e-01  5.65289871e-01]
 [ 4.37500000e-01  7.18750000e-01  7.18750000e-01  7.00962151e-01
   7.11004511e-01  6.25000000e-02  5.31250000e-01  5.31250000e-01
   5.04369061e-01  5.18439930e-01]
 [-6.61436711e-02  4.70000000e-01  5.25000000e-01  3.60808918e-01
   4.54601292e-01  7.38887174e-02  5.37301587e-01  5.43750000e-01
   5.10554274e-01  5.25812038e-01]
 [ 2.03016902e-01  5.94545455e-01  6.75000000e-01  5.23622764e-01
   5.96345693e-01 -1.90339992e-02  4.91666667e-01  5.43750000e-01
   3.96842890e-01  4.73282219e-01]
 [ 2.99955396e-01  6.43636364e-01  7.12500000e-01  5.99498587e-01
   6.43082270e-01  2.71603474e-01  6.21818182e-01  7.12500000e-01
   5.31255677e-01  6.18960124e-01]
 [ 7.50000000e-02  5.37500000e-01  5.37500000e-01  5.22560911e-01
   5.30348864e-01  2.83963134e-01  6.39682540e-01  6.50000000e-01
   6.26518772e-01  6.37833869e-01]
 [ 5.65811052e-02  5.22727273e-01  6.06250000e-01  4.27761389e-01
   5.12725268e-01  1.40606724e-01  5.68333333e-01  6.18750000e-01
   5.08942240e-01  5.58575451e-01]
 [ 3.16397997e-03  5.01666667e-01  5.43750000e-01  4.61539449e-01
   4.96252133e-01  2.70557311e-01  6.38333333e-01  6.56250000e-01
   6.20450025e-01  6.31812661e-01]
 [ 2.94633282e-01  6.49206349e-01  6.50000000e-01  6.36282756e-01
   6.41631440e-01  2.54145092e-01  6.29365079e-01  6.31250000e-01
   5.96222520e-01  6.12569036e-01]
 [ 2.31124858e-01  6.09090909e-01  6.87500000e-01  5.35764375e-01
   6.08315704e-01  2.00000000e-01  6.00000000e-01  6.00000000e-01
   5.77678482e-01  5.89085180e-01]
 [-1.35310262e-01  4.30909091e-01  4.87500000e-01  3.39803405e-01
   4.19102197e-01  2.85799285e-01  6.48333333e-01  6.56250000e-01
   6.26584356e-01  6.33268186e-01]
 [ 2.51983206e-01  6.25000000e-01  6.43750000e-01  5.87851413e-01
   6.09093722e-01  3.27324885e-02  5.18333333e-01  5.93750000e-01
   3.17747143e-01  4.84921764e-01]
 [ 5.26703987e-01  7.62698413e-01  7.68750000e-01  7.51214211e-01
   7.59693713e-01  2.15566860e-01  6.00000000e-01  6.50000000e-01
   5.47984231e-01  5.95401549e-01]
 [ 2.37390480e-01  6.60714286e-01  8.18750000e-01  4.68078140e-01
   6.07882066e-01 -1.00753868e-01  4.51666667e-01  4.81250000e-01
   3.95584141e-01  4.42961513e-01]
 [ 1.97715793e-01  5.99090909e-01  6.43750000e-01  5.63604503e-01
   5.87527974e-01  3.87985492e-01  6.94444444e-01  6.93750000e-01
   6.82119777e-01  6.87015814e-01]
 [-1.39351483e-01  4.26363636e-01  5.18750000e-01  2.76584594e-01
   4.13638817e-01  6.44851445e-02  5.28333333e-01  5.93750000e-01
   4.13187105e-01  5.05963891e-01]
 [ 1.35742036e-01  5.54166667e-01  7.18750000e-01  3.95341321e-01
   5.54260834e-01  1.25000000e-02  5.06250000e-01  5.06250000e-01
   4.51714077e-01  4.87806845e-01]
 [-3.75189327e-04  5.00000000e-01  5.00000000e-01  4.93474682e-01
   4.94927704e-01 -9.91892658e-02  4.52727273e-01  5.62500000e-01
   2.62136540e-01  4.37694586e-01]
 [ 1.19872997e-01  5.59523810e-01  5.68750000e-01  5.45357893e-01
   5.55758206e-01  4.78787879e-01  6.95833333e-01  8.43750000e-01
   6.21253711e-01  7.27785104e-01]
 [-1.02284232e-01  4.47619048e-01  4.50000000e-01  4.33672354e-01
   4.41403736e-01  2.25216891e-01  6.11904762e-01  6.18750000e-01
   5.96253281e-01  6.06244584e-01]
 [ 6.01686574e-02  5.30952381e-01  5.43750000e-01  4.71344190e-01
   5.14686678e-01  1.25000000e-01  5.62500000e-01  5.62500000e-01
   5.29767431e-01  5.47194421e-01]
 [-5.00000000e-02  4.75000000e-01  4.75000000e-01  4.57546587e-01
   4.66789534e-01  1.58590578e-01  5.77777778e-01  5.87500000e-01
   5.56735496e-01  5.71986459e-01]
 [ 3.42009371e-01  6.61818182e-01  7.37500000e-01  6.15212172e-01
   6.63662231e-01  3.75000000e-01  6.87500000e-01  6.87500000e-01
   6.61622241e-01  6.76162533e-01]
 [-9.27652177e-02  4.51666667e-01  4.81250000e-01  4.07520547e-01
   4.45584976e-01  4.15647844e-01  7.05000000e-01  7.31250000e-01
   6.81152953e-01  7.00696515e-01]
 [ 2.44424426e-01  6.25000000e-01  6.31250000e-01  6.06404252e-01
   6.11602139e-01  3.86570387e-01  6.88095238e-01  7.06250000e-01
   6.57589497e-01  6.82736393e-01]
 [ 1.78645320e-01  5.90000000e-01  6.12500000e-01  5.65049400e-01
   5.81162889e-01  7.82183108e-02  5.39090909e-01  6.06250000e-01
   4.55256648e-01  5.24314854e-01]]
BDDAE mean:
[0.1287502  0.56435143 0.60229167 0.51249841 0.55490291 0.1958941
 0.59605009 0.63541667 0.53466148 0.58542265]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.55       0.         0.35384615 0.
  0.5        0.8        0.         0.44380952]
 [0.         0.5        0.5375     0.         0.34871795 0.
  0.5        0.6375     0.         0.38901099]
 [0.         0.5        0.575      0.         0.36410256 0.
  0.5        0.4875     0.         0.32727273]
 [0.         0.5        0.525      0.         0.34358974 0.
  0.5        0.725      0.         0.41978022]
 [0.         0.5        0.625      0.         0.38461538 0.
  0.5        0.5125     0.         0.33846154]
 [0.         0.5        0.525      0.         0.34358974 0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.65       0.         0.39340659 0.
  0.5        0.5375     0.         0.34871795]
 [0.         0.5        0.6875     0.         0.40659341 0.
  0.5        0.6125     0.         0.37948718]
 [0.         0.5        0.7        0.         0.41098901 0.
  0.5        0.6875     0.         0.40659341]
 [0.         0.5        0.525      0.         0.34358974 0.
  0.5        0.55       0.         0.35384615]
 [0.         0.5        0.6625     0.         0.3978022  0.
  0.5        0.625      0.         0.38461538]
 [0.         0.5        0.625      0.         0.38461538 0.
  0.5        0.625      0.         0.38461538]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.5625     0.         0.35897436]
 [0.         0.5        0.6875     0.         0.40659341 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.6875     0.         0.40659341 0.
  0.5        0.65       0.         0.39340659]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.65       0.         0.39340659]
 [0.         0.5        0.575      0.         0.36410256 0.
  0.5        0.6        0.         0.37435897]
 [       nan 0.55       0.8875     0.1        0.52       0.
  0.5        0.6        0.         0.37435897]
 [0.         0.5        0.7125     0.         0.41538462 0.
  0.5        0.575      0.         0.36410256]
 [0.         0.5        0.675      0.         0.4021978  0.
  0.5        0.65       0.         0.39340659]
 [0.         0.5        0.775      0.         0.43619048 0.
  0.5        0.525      0.         0.34358974]
 [0.         0.5        0.5375     0.         0.34871795 0.
  0.5        0.6625     0.         0.3978022 ]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.75       0.         0.42857143]
 [0.         0.5        0.5375     0.         0.34871795 0.
  0.5        0.55       0.         0.35384615]
 [0.         0.5        0.5875     0.         0.36923077 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.525      0.         0.34358974 0.
  0.5        0.5625     0.         0.35897436]
 [0.         0.5        0.7        0.         0.41098901 0.
  0.5        0.4875     0.         0.32727273]
 [0.         0.5        0.625      0.         0.38461538 0.
  0.5        0.625      0.         0.38461538]
 [0.         0.5        0.6125     0.         0.37948718 0.
  0.5        0.5875     0.         0.36923077]
 [0.         0.5        0.625      0.         0.38461538 0.
  0.5        0.6625     0.         0.3978022 ]]
DUMMY mean:
[0.         0.50166667 0.62416667 0.00333333 0.38381441 0.
 0.5        0.60083333 0.         0.37333955]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_58
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.418 0.709 0.751 0.626 0.694 0.398 0.699 0.733 0.623 0.683]
 [0.386 0.69  0.725 0.626 0.676 0.388 0.69  0.708 0.642 0.674]
 [0.426 0.706 0.747 0.643 0.693 0.417 0.718 0.743 0.656 0.707]
 [0.046 0.521 0.631 0.071 0.414 0.118 0.559 0.658 0.17  0.468]
 [0.373 0.681 0.74  0.552 0.662 0.364 0.677 0.724 0.573 0.659]
 [0.129 0.564 0.602 0.512 0.555 0.196 0.596 0.635 0.535 0.585]
 [0.    0.5   0.615 0.    0.379 0.    0.5   0.601 0.    0.373]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.219 0.106 0.093 0.174 0.116 0.173 0.086 0.074 0.147 0.094]
 [0.229 0.111 0.098 0.181 0.12  0.171 0.082 0.084 0.114 0.088]
 [0.203 0.107 0.089 0.159 0.112 0.15  0.084 0.078 0.127 0.086]
 [0.099 0.044 0.072 0.147 0.075 0.208 0.104 0.078 0.291 0.145]
 [0.24  0.118 0.094 0.227 0.134 0.177 0.091 0.08  0.177 0.101]
 [0.173 0.088 0.099 0.105 0.087 0.164 0.08  0.082 0.123 0.085]
 [0.    0.    0.068 0.    0.026 0.    0.    0.079 0.    0.03 ]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 52.  15.  12.  28.  17.  43.  12.  10.  24.  14.]
 [ 59.  16.  14.  29.  18.  44.  12.  12.  18.  13.]
 [ 48.  15.  12.  25.  16.  36.  12.  10.  19.  12.]
 [216.   8.  11. 206.  18. 176.  19.  12. 171.  31.]
 [ 64.  17.  13.  41.  20.  49.  13.  11.  31.  15.]
 [134.  16.  16.  20.  16.  84.  13.  13.  23.  15.]
 [  0.   0.  11.   0.   7.   0.   0.  13.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_58
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  27.008
step (sec):  13.504
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  13.504
Number of windows / instances:  80
Elapsed time: 1341.5654958486557 minutes
Elapsed time: 22.359424930810928 hours
