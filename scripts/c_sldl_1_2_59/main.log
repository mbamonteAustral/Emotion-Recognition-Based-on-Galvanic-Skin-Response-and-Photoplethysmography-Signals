2024-05-20 10:06:42.775457: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-20 10:06:46.366657: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-20 10:06:55.518450: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  29.0
step (sec):  29.0
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_59\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:


Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 28992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 7248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 7248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1812, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 28992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 28992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 1812, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 1812, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 1812, 12)  â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 1812)  â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 21744)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 21744)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     21,745 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 22,567 (88.15 KB)
 Trainable params: 22,567 (88.15 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/28[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m33s[0m 1s/step - binary_accuracy: 1.0000 - loss: 0.6515
[1m 5/28[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7433 - loss: 0.8622
[1m 9/28[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6940 - loss: 0.8852
[1m13/28[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7071 - loss: 0.8476
[1m16/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7119 - loss: 0.8282
[1m20/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7261 - loss: 0.7933
[1m23/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7322 - loss: 0.7769
[1m26/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7339 - loss: 0.7669
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 25ms/step - binary_accuracy: 0.7290 - loss: 0.7648 - val_binary_accuracy: 0.2500 - val_loss: 1.0191
Epoch 2/10

[1m 1/28[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.1095
[1m 5/28[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2085
[1m 8/28[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9844 - loss: 0.2458
[1m12/28[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9505 - loss: 0.2896
[1m16/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9201 - loss: 0.3201
[1m20/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8905 - loss: 0.3563
[1m23/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8732 - loss: 0.3778
[1m27/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8499 - loss: 0.4036
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 19ms/step - binary_accuracy: 0.8381 - loss: 0.4170 - val_binary_accuracy: 0.5000 - val_loss: 0.7971
Epoch 3/10

[1m 1/28[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.5446
[1m 5/28[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8033 - loss: 0.5379
[1m 9/28[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6703 - loss: 0.6156
[1m13/28[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5903 - loss: 0.6669
[1m17/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5764 - loss: 0.6767
[1m20/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5742 - loss: 0.6759
[1m23/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5744 - loss: 0.6722
[1m27/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5750 - loss: 0.6743
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.5772 - loss: 0.6734 - val_binary_accuracy: 0.5000 - val_loss: 1.0278
Epoch 4/10

[1m 1/28[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 31ms/step - binary_accuracy: 0.0000e+00 - loss: 1.0451
[1m 4/28[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 21ms/step - binary_accuracy: 0.3333 - loss: 0.8452    
[1m 8/28[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.3952 - loss: 0.7997
[1m12/28[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.4684 - loss: 0.7397
[1m15/28[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.5174 - loss: 0.7016
[1m19/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5707 - loss: 0.6650
[1m23/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6110 - loss: 0.6333
[1m27/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6395 - loss: 0.6091
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.6521 - loss: 0.5981 - val_binary_accuracy: 0.5000 - val_loss: 0.9145
Epoch 5/10

[1m 1/28[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.6174
[1m 4/28[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 21ms/step - binary_accuracy: 1.0000 - loss: 0.5135
[1m 7/28[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 21ms/step - binary_accuracy: 0.9272 - loss: 0.4950
[1m11/28[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - binary_accuracy: 0.9066 - loss: 0.4608
[1m15/28[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8917 - loss: 0.4466
[1m18/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8870 - loss: 0.4349
[1m22/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8809 - loss: 0.4269
[1m26/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8740 - loss: 0.4226
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 19ms/step - binary_accuracy: 0.8721 - loss: 0.4179 - val_binary_accuracy: 0.5000 - val_loss: 0.7706
Epoch 6/10

[1m 1/28[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.4158
[1m 5/28[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.3358
[1m 9/28[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.2863
[1m13/28[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9941 - loss: 0.2637
[1m17/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9620 - loss: 0.2764
[1m20/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9440 - loss: 0.2858
[1m24/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9275 - loss: 0.2929
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9163 - loss: 0.2987
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.9142 - loss: 0.3002 - val_binary_accuracy: 0.5000 - val_loss: 0.6898
Epoch 7/10

[1m 1/28[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2646
[1m 5/28[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9100 - loss: 0.3102
[1m 9/28[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8894 - loss: 0.3401
[1m13/28[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8964 - loss: 0.3369
[1m16/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8908 - loss: 0.3349
[1m20/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8909 - loss: 0.3252
[1m24/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8942 - loss: 0.3153
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8985 - loss: 0.3088
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.8996 - loss: 0.3073 - val_binary_accuracy: 0.5000 - val_loss: 0.8812
Epoch 8/10

[1m 1/28[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.0324
[1m 5/28[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9600 - loss: 0.1629
[1m 8/28[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9207 - loss: 0.2198
[1m12/28[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9150 - loss: 0.2454
[1m16/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9189 - loss: 0.2533
[1m19/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9229 - loss: 0.2522
[1m23/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9244 - loss: 0.2544
[1m27/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9239 - loss: 0.2543
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9242 - loss: 0.2540 - val_binary_accuracy: 0.5000 - val_loss: 0.8407
Epoch 9/10

[1m 1/28[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1486
[1m 5/28[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1722
[1m 9/28[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1494
[1m12/28[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9771 - loss: 0.1546
[1m16/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9655 - loss: 0.1523
[1m20/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9564 - loss: 0.1528
[1m24/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9488 - loss: 0.1540
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9453 - loss: 0.1541
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9448 - loss: 0.1543 - val_binary_accuracy: 0.5000 - val_loss: 0.9869
Epoch 10/10

[1m 1/28[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.1501
[1m 5/28[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1271
[1m 9/28[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1327
[1m13/28[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1344
[1m17/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 1.0000 - loss: 0.1332
[1m21/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1340
[1m25/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.1341
[1m28/28[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 1.0000 - loss: 0.1325 - val_binary_accuracy: 0.5000 - val_loss: 0.8370

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 94ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 94ms/step
predicted [0.7867977  0.08491761 0.0512603  0.19481406 0.89881146 0.35270086
 0.6208931  0.09336407]
predicted [1 0 0 0 1 0 1 0]
expected [False False  True  True  True  True  True False]
accuracy: 0.5
confusion matrix: 
[[2 1]
 [3 2]]
              precision    recall  f1-score   support

       False       0.40      0.67      0.50         3
        True       0.67      0.40      0.50         5

    accuracy                           0.50         8
   macro avg       0.53      0.53      0.50         8
weighted avg       0.57      0.50      0.50         8

macro avg f1-score: 0.5
macro avg (UAR): 0.5333333333333333
Sensitivity:  0.6666666666666666
Specificity:  0.4
g-mean:  0.5163977794943222
-------- Model Performance ----------: 
accuracy:  [0.75  0.5   0.875 0.375 0.5   0.375 0.875 0.75  0.625 0.5  ]
gmean:  [0.73029674 0.51639778 0.81649658 0.36514837 0.51639778 0.36514837
 0.89442719 0.57735027 0.51639778 0.51639778]
f1_score:  [0.73333333 0.5        0.85454545 0.36507937 0.5        0.36507937
 0.87301587 0.66666667 0.56363636 0.5       ]
UAR:  [0.73333333 0.53333333 0.83333333 0.36666667 0.53333333 0.36666667
 0.9        0.66666667 0.56666667 0.53333333]
Cohen Kappa score:  [ 0.46666667  0.05882353  0.71428571 -0.25        0.05882353 -0.25
  0.75        0.38461538  0.14285714  0.05882353]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  28.992
step (sec):  28.992
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  40
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.75  0.9   0.875 0.886 0.867 0.42  0.733 0.7   0.633 0.67 ]
 [0.667 0.825 0.825 0.784 0.803 0.27  0.65  0.65  0.505 0.606]
 [0.65  0.917 0.9   0.613 0.897 0.32  0.658 0.625 0.405 0.58 ]
 [0.    0.5   0.65  0.    0.39  0.    0.5   0.625 0.    0.381]
 [0.617 0.825 0.825 0.784 0.803 0.3   0.592 0.6   0.584 0.536]
 [0.213 0.603 0.612 0.581 0.592 0.094 0.55  0.588 0.443 0.527]
 [0.    0.5   0.65  0.    0.39  0.    0.5   0.625 0.    0.381]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.1         0.55        0.575       0.33284271  0.49285714  0.37
   0.70833333  0.775       0.49174502  0.64142857]
 [ 0.4         0.70833333  0.675       0.60236034  0.65666667  0.36666667
   0.69166667  0.775       0.49378169  0.6552381 ]
 [ 0.6         0.8         0.825       0.68284271  0.76952381  0.4
   0.7         0.7         0.63284271  0.68333333]
 [ 0.2         0.59166667  0.625       0.43284271  0.55952381 -0.03333333
   0.475       0.6         0.18164966  0.43809524]
 [ 0.7         0.85        0.85        0.79142136  0.83        0.25
   0.63333333  0.65        0.49378169  0.58952381]
 [ 0.1         0.53333333  0.55        0.31213203  0.48        0.36666667
   0.69166667  0.7         0.60591373  0.66666667]
 [ 0.08333333  0.54166667  0.625       0.30236034  0.50238095  0.05
   0.525       0.525       0.44142136  0.50666667]
 [ 0.75        0.875       0.925       0.77071068  0.85904762  0.57
   0.80833333  0.775       0.77458773  0.76333333]
 [ 0.36666667  0.7         0.75        0.50472067  0.64571429 -0.06666667
   0.46666667  0.625       0.07071068  0.40761905]
 [ 0.65        0.825       0.825       0.79497475  0.81333333  0.1
   0.55        0.55        0.39142136  0.50333333]
 [ 0.21666667  0.59166667  0.625       0.41213203  0.56        0.45
   0.73333333  0.775       0.52307101  0.67238095]
 [ 0.7         0.85833333  0.875       0.79378169  0.83619048  0.4
   0.70833333  0.725       0.58400999  0.66619048]
 [ 0.17        0.6         0.6         0.46009536  0.54285714  0.1
   0.55833333  0.55        0.53164966  0.54333333]
 [-0.1         0.45        0.6         0.05        0.38809524  0.47
   0.74166667  0.725       0.66128842  0.7       ]
 [ 0.12333333  0.55        0.625       0.28164966  0.49238095  0.41666667
   0.725       0.75        0.60472067  0.68952381]
 [-0.04333333  0.45833333  0.5         0.29142136  0.43285714  0.1
   0.56666667  0.675       0.23400999  0.48761905]
 [ 0.4         0.70833333  0.75        0.53520305  0.65904762 -0.06666667
   0.46666667  0.575       0.12071068  0.4052381 ]
 [        nan  0.75        0.875       0.5         0.71428571  0.32
   0.675       0.65        0.61364875  0.63333333]
 [ 0.23333333  0.625       0.725       0.39378169  0.58857143  0.15
   0.56666667  0.575       0.41213203  0.53333333]
 [ 0.13333333  0.56666667  0.725       0.24142136  0.52761905  0.05
   0.54166667  0.525       0.16329932  0.39619048]
 [-0.13333333  0.45833333  0.65        0.          0.39047619  0.3
   0.65833333  0.65        0.60236034  0.63666667]
 [ 0.21666667  0.60833333  0.625       0.42071068  0.56333333  0.26666667
   0.625       0.7         0.42071068  0.59571429]
 [ 0.44        0.75833333  0.7         0.67255106  0.67333333         nan
   0.53333333  0.75        0.1         0.47619048]
 [-0.08333333  0.45        0.475       0.26213203  0.41666667  0.45
   0.73333333  0.775       0.61449237  0.70238095]
 [ 0.31666667  0.65833333  0.675       0.50236034  0.61666667  0.5
   0.74166667  0.75        0.65355339  0.72      ]
 [ 0.27        0.63333333  0.625       0.44057774  0.56333333  0.25
   0.63333333  0.625       0.47307101  0.57      ]
 [ 0.3         0.65833333  0.8         0.35236034  0.60380952  0.25
   0.625       0.625       0.51213203  0.58666667]
 [ 0.25        0.63333333  0.675       0.45236034  0.59238095  0.45
   0.725       0.8         0.51213203  0.68190476]
 [ 0.5         0.76666667  0.75        0.67543135  0.72       -0.13333333
   0.43333333  0.5         0.1         0.36571429]
 [ 0.75        0.9         0.875       0.88637033  0.86666667  0.42
   0.73333333  0.7         0.63316638  0.67      ]]
KNN mean:
[0.29689655 0.65527778 0.69916667 0.47171829 0.61192063 0.2591954
 0.6325     0.66916667 0.45493381 0.58625397]
---------------------------
---------------------------
DT performance:
[[ 2.20000000e-01  6.50000000e-01  6.25000000e-01  5.99156383e-01
   5.93333333e-01  2.16666667e-01  5.66666667e-01  6.25000000e-01
   3.91745021e-01  5.02380952e-01]
 [ 3.50000000e-01  5.58333333e-01  5.75000000e-01  4.52360336e-01
   5.23333333e-01  1.83333333e-01  6.16666667e-01  6.75000000e-01
   4.44105357e-01  5.52380952e-01]
 [ 4.00000000e-02  5.08333333e-01  5.00000000e-01  4.40577739e-01
   4.19523810e-01  3.00000000e-01  6.00000000e-01  6.00000000e-01
   4.32842712e-01  5.56666667e-01]
 [ 5.00000000e-02  5.00000000e-01  4.75000000e-01  3.73071014e-01
   4.50000000e-01  1.00000000e-01  5.50000000e-01  6.50000000e-01
   2.81649658e-01  5.15714286e-01]
 [ 4.90000000e-01  7.58333333e-01  7.25000000e-01  7.30806041e-01
   7.16666667e-01  0.00000000e+00  5.41666667e-01  5.50000000e-01
   3.93781692e-01  4.83333333e-01]
 [ 3.40000000e-01  6.58333333e-01  6.25000000e-01  5.98312766e-01
   6.13333333e-01  4.00000000e-01  6.41666667e-01  6.50000000e-01
   6.04720672e-01  5.90000000e-01]
 [ 3.33333333e-03  5.41666667e-01  6.25000000e-01  2.15470054e-01
   4.91904762e-01 -1.00000000e-01  5.08333333e-01  5.00000000e-01
   3.73071014e-01  4.90000000e-01]
 [ 5.50000000e-01  7.83333333e-01  8.75000000e-01  6.81649658e-01
   7.44761905e-01  4.16666667e-01  6.58333333e-01  6.75000000e-01
   6.32842712e-01  6.26666667e-01]
 [ 3.83333333e-01  6.83333333e-01  7.50000000e-01  4.75431351e-01
   6.59047619e-01  2.36666667e-01  6.83333333e-01  7.25000000e-01
   4.00000000e-01  6.39047619e-01]
 [ 2.00000000e-01  5.75000000e-01  5.75000000e-01  5.70710678e-01
   5.40000000e-01 -1.50000000e-01  5.16666667e-01  5.00000000e-01
   3.28445705e-01  4.26666667e-01]
 [ 3.66666667e-02  4.33333333e-01  4.75000000e-01  3.28445705e-01
   3.66190476e-01  4.16666667e-01  7.25000000e-01  7.50000000e-01
   4.84009994e-01  6.92857143e-01]
 [ 5.70000000e-01  8.16666667e-01  8.00000000e-01  6.62455699e-01
   7.86666667e-01  3.16666667e-01  6.66666667e-01  7.00000000e-01
   5.23071014e-01  6.46190476e-01]
 [-1.66666667e-01  4.16666667e-01  4.00000000e-01  3.89384685e-01
   3.70000000e-01 -2.00000000e-01  4.25000000e-01  4.50000000e-01
   4.02360336e-01  3.96190476e-01]
 [-1.13333333e-01  4.50000000e-01  5.00000000e-01  1.89384685e-01
   4.02857143e-01  1.70000000e-01  6.16666667e-01  6.00000000e-01
   5.96142029e-01  5.83333333e-01]
 [ 6.66666667e-03  5.00000000e-01  6.00000000e-01  3.23071014e-01
   4.75238095e-01  2.22044605e-17  5.08333333e-01  5.50000000e-01
   2.02360336e-01  4.66666667e-01]
 [ 3.33333333e-02  5.58333333e-01  5.75000000e-01  3.80806041e-01
   5.30000000e-01  1.83333333e-01  6.08333333e-01  6.50000000e-01
   3.34009994e-01  5.69523810e-01]
 [ 7.00000000e-01  8.50000000e-01  8.75000000e-01  8.00000000e-01
   8.42857143e-01  2.66666667e-01  6.41666667e-01  6.50000000e-01
   6.28445705e-01  5.96666667e-01]
 [            nan  6.08333333e-01  6.50000000e-01  3.00000000e-01
   5.08571429e-01  5.00000000e-01  6.83333333e-01  7.00000000e-01
   4.91421356e-01  6.43333333e-01]
 [ 1.33333333e-01  5.91666667e-01  7.00000000e-01  2.63299316e-01
   5.31428571e-01  1.16666667e-01  5.83333333e-01  6.00000000e-01
   3.91421356e-01  5.56190476e-01]
 [ 5.66666667e-01  8.41666667e-01  8.50000000e-01  6.81649658e-01
   8.30000000e-01  1.86666667e-01  6.08333333e-01  6.00000000e-01
   5.30806041e-01  5.56666667e-01]
 [-3.33333333e-02  5.00000000e-01  6.25000000e-01  1.57735027e-01
   4.28095238e-01  6.50000000e-01  7.75000000e-01  8.00000000e-01
   6.82842712e-01  7.62857143e-01]
 [ 6.50000000e-01  8.16666667e-01  8.00000000e-01  7.76624405e-01
   7.86666667e-01  3.60000000e-01  6.75000000e-01  6.75000000e-01
   5.99480048e-01  6.36666667e-01]
 [ 9.00000000e-02  5.58333333e-01  5.50000000e-01  3.36180732e-01
   4.42380952e-01  3.83333333e-01  7.25000000e-01  7.50000000e-01
   3.44948974e-01  6.45714286e-01]
 [ 1.00000000e-01  5.75000000e-01  5.75000000e-01  5.69867061e-01
   5.26190476e-01  3.50000000e-01  7.25000000e-01  7.25000000e-01
   6.33166377e-01  6.82857143e-01]
 [ 3.50000000e-01  6.83333333e-01  7.00000000e-01  5.43781692e-01
   6.70000000e-01  3.86666667e-01  7.00000000e-01  6.75000000e-01
   5.40577739e-01  6.33333333e-01]
 [ 4.50000000e-01  7.58333333e-01  7.50000000e-01  6.90577739e-01
   7.23333333e-01  1.00000000e-01  6.50000000e-01  6.50000000e-01
   4.35203049e-01  6.23333333e-01]
 [ 3.16666667e-01  6.58333333e-01  7.00000000e-01  5.15659652e-01
   6.05714286e-01  3.00000000e-01  6.83333333e-01  6.75000000e-01
   5.55913727e-01  6.63333333e-01]
 [-2.00000000e-01  4.66666667e-01  5.25000000e-01  2.00000000e-01
   4.60000000e-01  8.33333333e-02  5.66666667e-01  6.50000000e-01
   3.41421356e-01  5.39047619e-01]
 [ 5.33333333e-02  5.83333333e-01  5.75000000e-01  3.80806041e-01
   5.10000000e-01  2.03333333e-01  5.33333333e-01  5.50000000e-01
   5.01516719e-01  4.89523810e-01]
 [ 6.66666667e-01  8.25000000e-01  8.25000000e-01  7.84009994e-01
   8.03333333e-01  2.70000000e-01  6.50000000e-01  6.50000000e-01
   5.04720672e-01  6.06190476e-01]]
DT mean:
[0.23574713 0.62361111 0.64666667 0.48037617 0.57838095 0.22155556
 0.62111111 0.64       0.46690147 0.57911111]
---------------------------
---------------------------
RF performance:
[[ 4.00000000e-01  8.00000000e-01  8.00000000e-01  5.73071014e-01
   7.90000000e-01  4.66666667e-01  6.83333333e-01  7.00000000e-01
   4.39384685e-01  5.91904762e-01]
 [ 2.50000000e-01  6.16666667e-01  6.00000000e-01  5.54720672e-01
   5.80000000e-01  2.66666667e-01  5.50000000e-01  6.00000000e-01
   2.78445705e-01  4.72380952e-01]
 [ 2.00000000e-01  5.50000000e-01  5.50000000e-01  4.77602088e-01
   5.00000000e-01  3.00000000e-01  6.00000000e-01  6.00000000e-01
   4.41421356e-01  5.50000000e-01]
 [ 1.16666667e-01  5.58333333e-01  5.25000000e-01  4.04720672e-01
   4.76666667e-01 -4.00000000e-02  5.83333333e-01  6.50000000e-01
   2.52360336e-01  5.45714286e-01]
 [ 4.20000000e-01  8.16666667e-01  8.00000000e-01  6.62455699e-01
   7.86666667e-01  3.50000000e-01  5.66666667e-01  5.75000000e-01
   4.32842712e-01  4.99523810e-01]
 [ 1.70000000e-01  6.50000000e-01  6.75000000e-01  5.35203049e-01
   6.30000000e-01  4.16666667e-01  6.66666667e-01  6.50000000e-01
   7.25431351e-01  6.33333333e-01]
 [ 8.66666667e-02  6.08333333e-01  6.75000000e-01  4.84009994e-01
   5.72857143e-01  2.20000000e-01  6.08333333e-01  6.00000000e-01
   6.35203049e-01  5.93333333e-01]
 [ 7.16666667e-01  9.16666667e-01  9.25000000e-01  8.81649658e-01
   9.06666667e-01  4.20000000e-01  7.58333333e-01  7.25000000e-01
   5.73071014e-01  6.96666667e-01]
 [ 1.86666667e-01  7.66666667e-01  8.00000000e-01  2.60095363e-01
   7.15714286e-01 -2.00000000e-01  6.58333333e-01  7.50000000e-01
   3.02360336e-01  6.18571429e-01]
 [ 1.50000000e-01  5.50000000e-01  5.50000000e-01  4.24264069e-01
   5.06666667e-01  0.00000000e+00  5.25000000e-01  5.25000000e-01
   2.91421356e-01  4.83333333e-01]
 [ 2.00000000e-01  5.66666667e-01  6.00000000e-01  2.91421356e-01
   5.36666667e-01  4.16666667e-01  7.50000000e-01  7.75000000e-01
   5.57081009e-01  7.02857143e-01]
 [ 5.70000000e-01  7.75000000e-01  7.75000000e-01  7.62455699e-01
   7.16190476e-01  5.50000000e-01  6.75000000e-01  7.00000000e-01
   5.36370331e-01  6.39523810e-01]
 [ 2.50000000e-01  5.00000000e-01  5.00000000e-01  4.81649658e-01
   4.70000000e-01  1.66666667e-02  5.00000000e-01  5.25000000e-01
   4.43781692e-01  4.76190476e-01]
 [-3.33333333e-02  5.83333333e-01  6.75000000e-01  3.49156383e-01
   5.41904762e-01  4.00000000e-01  6.33333333e-01  6.00000000e-01
   6.56891410e-01  5.80000000e-01]
 [ 6.66666667e-02  5.66666667e-01  6.50000000e-01  1.52360336e-01
   5.42380952e-01  1.83333333e-01  4.58333333e-01  5.25000000e-01
   4.63299316e-01  4.43333333e-01]
 [ 8.33333333e-02  4.50000000e-01  5.00000000e-01  1.70710678e-01
   4.09523810e-01  4.00000000e-01  7.25000000e-01  7.50000000e-01
   4.15659652e-01  6.75714286e-01]
 [ 7.50000000e-01  8.58333333e-01  8.75000000e-01  8.20710678e-01
   8.36190476e-01  5.20000000e-01  7.50000000e-01  7.50000000e-01
   6.91421356e-01  7.10000000e-01]
 [            nan  6.25000000e-01  7.50000000e-01  2.00000000e-01
   5.23809524e-01  4.16666667e-01  5.58333333e-01  6.00000000e-01
   6.32842712e-01  5.06190476e-01]
 [ 2.22044605e-17  7.16666667e-01  7.75000000e-01  5.44948974e-01
   6.75238095e-01  1.50000000e-01  6.08333333e-01  6.00000000e-01
   4.52360336e-01  5.73333333e-01]
 [ 4.50000000e-01  6.83333333e-01  7.75000000e-01  6.52360336e-01
   6.28571429e-01  1.16666667e-01  5.33333333e-01  5.25000000e-01
   3.10095363e-01  4.29523810e-01]
 [            nan  5.41666667e-01  7.25000000e-01  1.81649658e-01
   4.49523810e-01  6.66666667e-02  6.75000000e-01  7.00000000e-01
   4.90577739e-01  6.09523810e-01]
 [ 4.50000000e-01  7.08333333e-01  7.00000000e-01  6.93781692e-01
   6.83333333e-01  3.66666667e-01  6.16666667e-01  6.75000000e-01
   4.64492371e-01  5.85714286e-01]
 [ 1.20000000e-01  5.75000000e-01  5.50000000e-01  4.80806041e-01
   4.80000000e-01             nan  6.50000000e-01  7.75000000e-01
   4.63299316e-01  5.94285714e-01]
 [-3.33333333e-02  4.50000000e-01  4.75000000e-01  2.41421356e-01
   4.03333333e-01  5.16666667e-01  7.83333333e-01  8.00000000e-01
   7.04720672e-01  7.59523810e-01]
 [ 3.66666667e-01  6.91666667e-01  7.25000000e-01  4.69867061e-01
   6.66190476e-01  3.00000000e-01  7.41666667e-01  7.25000000e-01
   4.73071014e-01  6.86666667e-01]
 [ 4.00000000e-01  5.91666667e-01  6.00000000e-01  3.02360336e-01
   5.40000000e-01  5.00000000e-02  5.66666667e-01  5.50000000e-01
   5.85203049e-01  5.00000000e-01]
 [ 4.66666667e-01  7.25000000e-01  8.00000000e-01  3.81649658e-01
   6.81904762e-01  4.50000000e-01  5.58333333e-01  5.50000000e-01
   3.93781692e-01  4.80000000e-01]
 [-1.50000000e-01  4.33333333e-01  5.25000000e-01  3.34009994e-01
   3.78571429e-01  1.50000000e-01  6.00000000e-01  6.50000000e-01
   4.23071014e-01  5.45238095e-01]
 [-1.63333333e-01  5.58333333e-01  5.50000000e-01  3.28445705e-01
   5.00000000e-01  1.16666667e-01  6.50000000e-01  6.75000000e-01
   1.89384685e-01  6.16190476e-01]
 [ 6.50000000e-01  9.16666667e-01  9.00000000e-01  6.13299316e-01
   8.96666667e-01  3.20000000e-01  6.58333333e-01  6.25000000e-01
   4.04720672e-01  5.80000000e-01]]
RF mean:
[0.255      0.645      0.6775     0.45702857 0.60084127 0.26574713
 0.62972222 0.64833333 0.47080224 0.57928571]
---------------------------
---------------------------
SVM performance:
[[0.         0.5        0.525      0.         0.34285714 0.
  0.5        0.725      0.         0.41904762]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.675      0.         0.4       ]
 [0.         0.5        0.6        0.         0.37142857 0.5
  0.75       0.75       0.68284271 0.73333333]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.725      0.         0.41904762]
 [0.         0.5        0.65       0.         0.39047619 0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.675      0.         0.4        0.05
  0.53333333 0.525      0.08164966 0.37333333]
 [0.         0.5        0.725      0.         0.41904762 0.
  0.5        0.575      0.         0.36190476]
 [0.         0.5        0.675      0.         0.4        0.
  0.5        0.675      0.         0.4       ]
 [0.35       0.675      0.675      0.64497475 0.66333333 0.
  0.5        0.525      0.         0.34285714]
 [0.         0.5        0.625      0.         0.38095238 0.
  0.5        0.65       0.         0.39047619]
 [0.         0.5        0.65       0.         0.39047619 0.
  0.5        0.625      0.         0.38095238]
 [0.         0.5        0.575      0.         0.36190476 0.1
  0.55       0.55       0.1        0.4       ]
 [0.         0.5        0.675      0.         0.4        0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.7        0.         0.40952381 0.
  0.5        0.65       0.         0.39047619]
 [0.         0.5        0.6        0.         0.37142857 0.
  0.5        0.675      0.         0.4       ]
 [0.         0.5        0.575      0.         0.36190476 0.
  0.5        0.625      0.         0.38095238]
 [       nan 0.75       0.875      0.5        0.71428571 0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.675      0.         0.4        0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.7        0.         0.40952381 0.
  0.5        0.6        0.         0.37142857]
 [       nan 0.55       0.775      0.1        0.48571429 0.05
  0.53333333 0.525      0.08164966 0.37333333]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.6        0.         0.37142857]
 [0.         0.5        0.625      0.         0.38095238        nan
  0.55       0.775      0.1        0.48571429]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.6        0.         0.37142857]
 [0.         0.5        0.6        0.         0.37142857 0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.55       0.         0.35238095 0.1
  0.55       0.55       0.1        0.4       ]
 [0.         0.5        0.725      0.         0.41904762 0.1
  0.55       0.55       0.1        0.4       ]
 [0.         0.5        0.65       0.         0.39047619 0.
  0.5        0.65       0.         0.39047619]
 [0.         0.5        0.6        0.         0.37142857 0.
  0.5        0.6        0.         0.37142857]
 [0.         0.5        0.65       0.         0.39047619 0.
  0.5        0.625      0.         0.38095238]]
SVM mean:
[0.0125     0.51583333 0.63666667 0.04149916 0.40369841 0.03103448
 0.51722222 0.61083333 0.04153807 0.39409524]
---------------------------
---------------------------
GBM performance:
[[ 3.20000000e-01  6.16666667e-01  6.00000000e-01  4.69867061e-01
   5.70000000e-01  1.50000000e-01  6.08333333e-01  7.25000000e-01
   2.63299316e-01  5.54285714e-01]
 [ 3.00000000e-01  6.08333333e-01  6.00000000e-01  5.43781692e-01
   5.80000000e-01  1.66666667e-01  5.41666667e-01  6.25000000e-01
   2.52360336e-01  4.85714286e-01]
 [ 1.56666667e-01  5.83333333e-01  6.00000000e-01  3.93781692e-01
   5.22857143e-01  5.00000000e-02  5.25000000e-01  5.25000000e-01
   4.82842712e-01  4.83333333e-01]
 [ 4.00000000e-02  5.66666667e-01  5.50000000e-01  2.78445705e-01
   4.90000000e-01 -1.16666667e-01  4.50000000e-01  6.50000000e-01
   8.16496581e-02  3.90476190e-01]
 [ 6.70000000e-01  7.91666667e-01  8.00000000e-01  7.12132034e-01
   7.73333333e-01 -3.33333333e-02  5.16666667e-01  5.25000000e-01
   3.43781692e-01  4.70000000e-01]
 [ 4.40000000e-01  7.08333333e-01  6.75000000e-01  6.27602088e-01
   6.70000000e-01  3.00000000e-01  6.66666667e-01  6.50000000e-01
   4.75431351e-01  5.86666667e-01]
 [ 2.22044605e-17  5.33333333e-01  6.75000000e-01  2.00000000e-01
   4.80952381e-01 -1.00000000e-01  4.33333333e-01  4.25000000e-01
   2.73071014e-01  3.83333333e-01]
 [ 4.50000000e-01  7.08333333e-01  8.25000000e-01  4.52360336e-01
   6.60952381e-01  4.50000000e-01  6.50000000e-01  6.75000000e-01
   5.12132034e-01  5.96190476e-01]
 [ 3.66666667e-01  7.33333333e-01  8.00000000e-01  5.23071014e-01
   6.81904762e-01  5.00000000e-02  5.25000000e-01  6.75000000e-01
   1.70710678e-01  4.70476190e-01]
 [ 1.50000000e-01  5.75000000e-01  5.75000000e-01  4.41421356e-01
   5.60000000e-01  2.00000000e-02  4.66666667e-01  4.50000000e-01
   3.28445705e-01  4.16666667e-01]
 [ 3.33333333e-02  5.08333333e-01  5.75000000e-01  2.20710678e-01
   4.82380952e-01  2.66666667e-01  6.66666667e-01  7.00000000e-01
   4.13299316e-01  6.26190476e-01]
 [ 5.50000000e-01  7.83333333e-01  8.00000000e-01  6.23071014e-01
   7.29523810e-01  3.16666667e-01  6.75000000e-01  7.00000000e-01
   5.25431351e-01  6.26190476e-01]
 [ 6.66666667e-02  5.58333333e-01  5.75000000e-01  4.00000000e-01
   5.20000000e-01  1.00000000e-01  5.75000000e-01  5.75000000e-01
   4.50000000e-01  5.66666667e-01]
 [ 3.33333333e-02  4.66666667e-01  6.25000000e-01  5.00000000e-02
   3.97619048e-01  2.70000000e-01  7.00000000e-01  6.75000000e-01
   6.46142029e-01  6.50000000e-01]
 [ 1.90000000e-01  5.41666667e-01  7.00000000e-01  2.70710678e-01
   4.83809524e-01 -3.33333333e-02  5.00000000e-01  6.00000000e-01
   8.16496581e-02  4.01904762e-01]
 [ 6.66666667e-02  5.50000000e-01  5.75000000e-01  4.04720672e-01
   4.82857143e-01  1.16666667e-01  5.66666667e-01  6.75000000e-01
   1.81649658e-01  4.78095238e-01]
 [ 7.50000000e-01  8.75000000e-01  9.00000000e-01  8.20710678e-01
   8.66190476e-01  2.16666667e-01  6.41666667e-01  7.00000000e-01
   4.41421356e-01  6.12857143e-01]
 [            nan  6.83333333e-01  8.00000000e-01  3.00000000e-01
   6.38095238e-01  2.66666667e-01  6.58333333e-01  6.75000000e-01
   5.03553391e-01  6.16666667e-01]
 [ 1.16666667e-01  5.58333333e-01  6.75000000e-01  3.41421356e-01
   4.91428571e-01  2.16666667e-01  5.83333333e-01  6.00000000e-01
   5.03553391e-01  5.70000000e-01]
 [ 5.66666667e-01  7.83333333e-01  8.25000000e-01  6.70710678e-01
   7.69523810e-01  1.70000000e-01  6.16666667e-01  6.25000000e-01
   3.39384685e-01  5.06190476e-01]
 [ 3.33333333e-02  5.58333333e-01  7.50000000e-01  1.00000000e-01
   4.76190476e-01  5.50000000e-01  8.00000000e-01  8.25000000e-01
   6.62132034e-01  7.69523810e-01]
 [ 5.00000000e-01  7.41666667e-01  7.25000000e-01  7.35203049e-01
   7.06666667e-01  2.00000000e-01  5.91666667e-01  6.50000000e-01
   3.20710678e-01  5.29047619e-01]
 [ 2.00000000e-01  6.08333333e-01  6.50000000e-01  2.89384685e-01
   5.29047619e-01             nan  6.50000000e-01  7.75000000e-01
   3.63299316e-01  5.94285714e-01]
 [ 2.00000000e-01  5.75000000e-01  5.75000000e-01  4.32842712e-01
   5.19523810e-01  3.50000000e-01  7.00000000e-01  7.00000000e-01
   6.07081009e-01  6.52857143e-01]
 [ 3.66666667e-01  6.50000000e-01  7.00000000e-01  5.35203049e-01
   6.22857143e-01  3.70000000e-01  6.75000000e-01  6.50000000e-01
   6.42938076e-01  6.23333333e-01]
 [ 4.50000000e-01  7.08333333e-01  7.00000000e-01  6.14492371e-01
   6.43333333e-01  1.66666667e-02  5.58333333e-01  5.75000000e-01
   3.82842712e-01  5.20000000e-01]
 [ 2.00000000e-01  6.41666667e-01  7.75000000e-01  4.52360336e-01
   5.94285714e-01  1.50000000e-01  6.08333333e-01  6.00000000e-01
   4.35203049e-01  5.36666667e-01]
 [-5.00000000e-02  4.50000000e-01  5.50000000e-01  1.00000000e-01
   3.92380952e-01 -5.00000000e-02  4.75000000e-01  6.00000000e-01
   1.91421356e-01  4.45238095e-01]
 [-1.66666667e-02  4.66666667e-01  5.00000000e-01  2.41421356e-01
   4.26666667e-01  8.66666667e-02  5.83333333e-01  6.00000000e-01
   3.80806041e-01  5.19523810e-01]
 [ 6.16666667e-01  8.25000000e-01  8.25000000e-01  7.84009994e-01
   8.03333333e-01  3.00000000e-01  5.91666667e-01  6.00000000e-01
   5.84009994e-01  5.36190476e-01]]
GBM mean:
[0.26781609 0.63194444 0.68333333 0.43431454 0.58552381 0.16609195
 0.59333333 0.63416667 0.39467512 0.54061905]
---------------------------
---------------------------
BDDAE performance:
[[-2.25000000e-01  3.87500000e-01  3.87500000e-01  2.29538514e-01
   3.50339105e-01  3.52380952e-02  5.33333333e-01  6.50000000e-01
   3.22227201e-01  5.02484182e-01]
 [ 1.50000000e-01  5.75000000e-01  5.75000000e-01  5.37982829e-01
   5.57222222e-01  2.07899897e-01  6.16666667e-01  5.87500000e-01
   5.60056269e-01  5.72857143e-01]
 [-1.33333333e-02  4.93333333e-01  5.25000000e-01  4.17685668e-01
   4.86522367e-01  2.25000000e-01  6.12500000e-01  6.12500000e-01
   5.83960923e-01  5.98982684e-01]
 [-5.00000000e-02  4.75000000e-01  4.75000000e-01  4.23088976e-01
   4.54300144e-01 -1.53968254e-02  5.00000000e-01  5.75000000e-01
   3.10557627e-01  4.60011100e-01]
 [ 3.88646617e-01  7.06666667e-01  7.00000000e-01  6.67489631e-01
   6.74805195e-01  5.00000000e-02  5.25000000e-01  5.25000000e-01
   4.76131096e-01  5.09256854e-01]
 [ 2.50000000e-02  5.12500000e-01  5.12500000e-01  3.81315209e-01
   4.64761905e-01 -1.75000000e-01  4.12500000e-01  4.12500000e-01
   1.71957879e-01  3.42568543e-01]
 [-2.92531782e-01  3.60000000e-01  4.25000000e-01  1.15262608e-01
   3.26212121e-01  3.75000000e-01  6.87500000e-01  6.87500000e-01
   6.59326219e-01  6.74379509e-01]
 [ 4.47619048e-01  7.25000000e-01  8.12500000e-01  6.21253714e-01
   7.17902098e-01  2.48043525e-01  6.20000000e-01  6.50000000e-01
   5.87661202e-01  6.13203463e-01]
 [ 1.76849817e-01  5.76666667e-01  6.62500000e-01  3.85036272e-01
   5.46620047e-01  2.53846154e-01  6.13333333e-01  7.00000000e-01
   4.15869322e-01  5.80233100e-01]
 [ 5.00000000e-02  5.25000000e-01  5.25000000e-01  4.26380288e-01
   4.92525253e-01  4.00000000e-01  7.00000000e-01  7.00000000e-01
   6.71574359e-01  6.87539683e-01]
 [ 5.68095238e-01  7.80000000e-01  8.00000000e-01  7.66099653e-01
   7.80779221e-01 -2.19780220e-03  5.00000000e-01  6.00000000e-01
   1.39384685e-01  4.33240093e-01]
 [-4.19381599e-02  4.86666667e-01  5.25000000e-01  3.11420320e-01
   4.56839827e-01  5.28263305e-01  7.73333333e-01  7.75000000e-01
   7.59212905e-01  7.59754690e-01]
 [ 4.84593838e-03  5.03333333e-01  5.12500000e-01  4.61496200e-01
   4.90266955e-01  3.50000000e-01  6.75000000e-01  6.75000000e-01
   5.85221010e-01  6.40808081e-01]
 [-3.44981685e-01  3.33333333e-01  3.75000000e-01  1.44481750e-01
   3.11341991e-01  2.75000000e-01  6.37500000e-01  6.37500000e-01
   6.21810664e-01  6.30396825e-01]
 [ 3.84126984e-02  5.33333333e-01  5.25000000e-01  4.61149622e-01
   4.78932179e-01  9.94446902e-03  5.06666667e-01  5.00000000e-01
   4.50267702e-01  4.76204906e-01]
 [ 1.49066785e-01  5.73333333e-01  5.75000000e-01  5.44309996e-01
   5.52806638e-01 -3.26007326e-02  4.86666667e-01  5.75000000e-01
   1.62472393e-01  4.19580420e-01]
 [ 2.57675807e-01  6.36666667e-01  6.12500000e-01  5.84588054e-01
   5.97712843e-01  4.71794872e-02  5.20000000e-01  6.00000000e-01
   3.15110747e-01  4.86620047e-01]
 [ 1.19141122e-01  6.50000000e-01  6.87500000e-01  4.61274902e-01
   5.19933400e-01  1.25000000e-01  5.62500000e-01  5.62500000e-01
   5.30036898e-01  5.47063492e-01]
 [ 2.39926740e-03  5.06666667e-01  5.50000000e-01  3.88843803e-01
   4.89350649e-01 -7.50000000e-02  4.62500000e-01  4.62500000e-01
   3.74616479e-01  4.37922078e-01]
 [ 4.67619048e-01  7.50000000e-01  8.25000000e-01  5.85993313e-01
   7.05814186e-01 -2.09249084e-01  4.00000000e-01  4.50000000e-01
   2.32636365e-01  3.79163059e-01]
 [ 2.86666667e-01  6.50000000e-01  7.00000000e-01  5.83438904e-01
   6.21918082e-01 -5.00000000e-02  4.75000000e-01  4.75000000e-01
   3.83560420e-01  4.44538240e-01]
 [-2.00000000e-01  4.00000000e-01  4.00000000e-01  2.91421356e-01
   3.69668110e-01  1.05659341e-01  5.53333333e-01  6.00000000e-01
   4.20141662e-01  5.32438672e-01]
 [-1.35117432e-01  4.43333333e-01  4.87500000e-01  2.66662776e-01
   4.15230880e-01  1.60000000e-01  5.66666667e-01  7.75000000e-01
   2.12132034e-01  5.34065934e-01]
 [ 5.00000000e-02  5.25000000e-01  5.25000000e-01  4.84077028e-01
   5.12142857e-01  2.74864253e-01  6.43333333e-01  6.62500000e-01
   5.76207288e-01  6.30238095e-01]
 [ 1.42121669e-01  5.70000000e-01  5.62500000e-01  5.36191131e-01
   5.43333333e-01  7.50000000e-02  5.37500000e-01  5.37500000e-01
   4.82197114e-01  5.18189033e-01]
 [ 1.75000000e-01  5.87500000e-01  5.87500000e-01  5.66274141e-01
   5.77936508e-01  2.25000000e-01  6.12500000e-01  6.12500000e-01
   5.81564401e-01  5.98650794e-01]
 [ 4.57142857e-01  7.41666667e-01  7.87500000e-01  6.83958380e-01
   7.16796537e-01  5.25000000e-01  7.62500000e-01  7.62500000e-01
   7.46543248e-01  7.56587302e-01]
 [ 2.33333333e-01  6.16666667e-01  6.37500000e-01  5.82632786e-01
   6.12467532e-01  1.15036630e-01  5.56666667e-01  6.12500000e-01
   4.26185563e-01  5.42510823e-01]
 [ 3.22128852e-04  5.10000000e-01  5.37500000e-01  4.08278824e-01
   4.92229437e-01  4.67156863e-01  7.43333333e-01  7.37500000e-01
   7.35040819e-01  7.28174603e-01]
 [ 2.13489550e-01  6.03333333e-01  6.12500000e-01  5.81445865e-01
   5.92135642e-01  9.43104934e-02  5.50000000e-01  5.87500000e-01
   4.43179577e-01  5.26695527e-01]]
BDDAE mean:
[0.10335151 0.55791667 0.58083333 0.46330242 0.53029491 0.1537666
 0.57819444 0.61       0.46456147 0.5521453 ]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.475      0.         0.32       0.
  0.5        0.725      0.         0.41904762]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.675      0.         0.4       ]
 [0.         0.5        0.6        0.         0.37142857 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.725      0.         0.41904762]
 [0.         0.5        0.65       0.         0.39047619 0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.675      0.         0.4        0.
  0.5        0.525      0.         0.34285714]
 [0.         0.5        0.725      0.         0.41904762 0.
  0.5        0.575      0.         0.36190476]
 [0.         0.5        0.675      0.         0.4        0.
  0.5        0.675      0.         0.4       ]
 [0.         0.5        0.5        0.         0.33333333 0.
  0.5        0.525      0.         0.34285714]
 [0.         0.5        0.625      0.         0.38095238 0.
  0.5        0.65       0.         0.39047619]
 [0.         0.5        0.65       0.         0.39047619 0.
  0.5        0.625      0.         0.38095238]
 [0.         0.5        0.575      0.         0.36190476 0.
  0.5        0.475      0.         0.32      ]
 [0.         0.5        0.675      0.         0.4        0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.7        0.         0.40952381 0.
  0.5        0.65       0.         0.39047619]
 [0.         0.5        0.6        0.         0.37142857 0.
  0.5        0.675      0.         0.4       ]
 [0.         0.5        0.575      0.         0.36190476 0.
  0.5        0.625      0.         0.38095238]
 [       nan 0.75       0.875      0.5        0.71428571 0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.675      0.         0.4        0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.7        0.         0.40952381 0.
  0.5        0.6        0.         0.37142857]
 [       nan 0.55       0.775      0.1        0.48571429 0.
  0.5        0.525      0.         0.34285714]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.6        0.         0.37142857]
 [0.         0.5        0.625      0.         0.38095238        nan
  0.55       0.775      0.1        0.48571429]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.6        0.         0.37142857]
 [0.         0.5        0.6        0.         0.37142857 0.
  0.5        0.55       0.         0.35238095]
 [0.         0.5        0.55       0.         0.35238095 0.
  0.5        0.475      0.         0.32      ]
 [0.         0.5        0.725      0.         0.41904762 0.
  0.5        0.525      0.         0.34285714]
 [0.         0.5        0.65       0.         0.39047619 0.
  0.5        0.65       0.         0.39047619]
 [0.         0.5        0.6        0.         0.37142857 0.
  0.5        0.6        0.         0.37142857]
 [0.         0.5        0.65       0.         0.39047619 0.
  0.5        0.625      0.         0.38095238]]
DUMMY mean:
[0.         0.51       0.62916667 0.02       0.39193651 0.
 0.50166667 0.59666667 0.00333333 0.37149206]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_59
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.292 0.648 0.693 0.464 0.606 0.257 0.635 0.667 0.462 0.589]
 [0.236 0.624 0.647 0.487 0.581 0.212 0.619 0.638 0.466 0.577]
 [0.26  0.652 0.678 0.476 0.614 0.268 0.63  0.644 0.464 0.58 ]
 [0.013 0.506 0.623 0.024 0.39  0.031 0.516 0.61  0.039 0.393]
 [0.27  0.631 0.68  0.444 0.586 0.163 0.589 0.628 0.392 0.536]
 [0.103 0.558 0.581 0.463 0.53  0.154 0.578 0.61  0.465 0.552]
 [0.    0.5   0.615 0.    0.377 0.    0.5   0.594 0.    0.369]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.257 0.132 0.115 0.219 0.141 0.195 0.101 0.089 0.187 0.11 ]
 [0.26  0.132 0.128 0.19  0.145 0.185 0.079 0.079 0.119 0.082]
 [0.245 0.135 0.13  0.189 0.144 0.191 0.084 0.083 0.141 0.088]
 [0.066 0.033 0.06  0.122 0.058 0.097 0.049 0.064 0.131 0.07 ]
 [0.229 0.114 0.106 0.207 0.126 0.164 0.085 0.081 0.163 0.089]
 [0.223 0.113 0.121 0.153 0.115 0.19  0.095 0.094 0.175 0.106]
 [0.    0.    0.067 0.    0.026 0.    0.    0.069 0.    0.027]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 88.  20.  17.  47.  23.  76.  16.  13.  40.  19.]
 [110.  21.  20.  39.  25.  87.  13.  12.  26.  14.]
 [ 94.  21.  19.  40.  23.  71.  13.  13.  30.  15.]
 [509.   7.  10. 511.  15. 308.   9.  10. 332.  18.]
 [ 85.  18.  16.  47.  22. 101.  14.  13.  42.  17.]
 [216.  20.  21.  33.  22. 124.  16.  15.  38.  19.]
 [  0.   0.  11.   0.   7.   0.   0.  12.   0.   7.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_59
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  28.992
step (sec):  28.992
overlap:  True
perc. of overlap:  0.0
overlap duration (sec):  0.0
Number of windows / instances:  40
Elapsed time: 795.0132077813148 minutes
Elapsed time: 13.25022012968858 hours
