2024-05-20 23:22:06.167464: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-20 23:22:09.972193: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-20 23:22:19.134769: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  29.0
step (sec):  21.75
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  7.25
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_60\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:


[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - loss: 0.0111 - mean_squared_error: 0.0111
[1m 4/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 21ms/step - loss: 0.0175 - mean_squared_error: 0.0175
[1m 8/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0165 - mean_squared_error: 0.0165
[1m11/37[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - loss: 0.0160 - mean_squared_error: 0.0160
[1m14/37[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - loss: 0.0161 - mean_squared_error: 0.0161
[1m17/37[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 20ms/step - loss: 0.0160 - mean_squared_error: 0.0160
[1m21/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - loss: 0.0164 - mean_squared_error: 0.0164
[1m25/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0169 - mean_squared_error: 0.0169
[1m28/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 19ms/step - loss: 0.0171 - mean_squared_error: 0.0171
[1m32/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 19ms/step - loss: 0.0172 - mean_squared_error: 0.0172
[1m36/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 18ms/step - loss: 0.0173 - mean_squared_error: 0.0173
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 19ms/step - loss: 0.0174 - mean_squared_error: 0.0174 - val_loss: 0.0145 - val_mean_squared_error: 0.0145
(28992, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 28992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 7248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 7248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1812, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 28992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 28992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 1812, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 1812, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 1812, 12)  â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 1812)  â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 21744)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 21744)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     21,745 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 22,567 (88.15 KB)
 Trainable params: 22,567 (88.15 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m45s[0m 1s/step - binary_accuracy: 0.0000e+00 - loss: 0.8805
[1m 5/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5033 - loss: 0.6701   
[1m 8/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.4775 - loss: 0.8236
[1m12/37[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.4521 - loss: 0.9320
[1m15/37[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.4470 - loss: 0.9486
[1m19/37[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.4640 - loss: 0.9360
[1m22/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.4743 - loss: 0.9274
[1m26/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.4893 - loss: 0.9164
[1m30/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.4989 - loss: 0.9195
[1m34/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5080 - loss: 0.9199
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5164 - loss: 0.9149
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 23ms/step - binary_accuracy: 0.5191 - loss: 0.9129 - val_binary_accuracy: 0.8000 - val_loss: 0.5952
Epoch 2/10

[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 34ms/step - binary_accuracy: 0.0000e+00 - loss: 1.9134
[1m 4/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 21ms/step - binary_accuracy: 0.3333 - loss: 1.2535    
[1m 8/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.5080 - loss: 0.9961
[1m12/37[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5612 - loss: 0.9244
[1m16/37[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6015 - loss: 0.8755
[1m19/37[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6208 - loss: 0.8475
[1m23/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6321 - loss: 0.8227
[1m27/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6402 - loss: 0.8042
[1m30/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6452 - loss: 0.7917
[1m34/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6543 - loss: 0.7728
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.6624 - loss: 0.7575 - val_binary_accuracy: 0.8000 - val_loss: 0.6673
Epoch 3/10

[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.4127
[1m 5/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9600 - loss: 0.4362
[1m 9/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8021 - loss: 0.6754
[1m13/37[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7233 - loss: 0.7600
[1m16/37[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6999 - loss: 0.7753
[1m20/37[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6897 - loss: 0.7777
[1m24/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6894 - loss: 0.7686
[1m28/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6934 - loss: 0.7545
[1m32/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6989 - loss: 0.7380
[1m35/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7012 - loss: 0.7279
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.7018 - loss: 0.7204 - val_binary_accuracy: 0.8000 - val_loss: 0.6349
Epoch 4/10

[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1346
[1m 5/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 20ms/step - binary_accuracy: 0.7533 - loss: 0.4122
[1m 9/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7417 - loss: 0.4345
[1m13/37[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7672 - loss: 0.4222
[1m16/37[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7738 - loss: 0.4209
[1m20/37[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7756 - loss: 0.4249
[1m24/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7833 - loss: 0.4243
[1m28/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7888 - loss: 0.4245
[1m31/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7899 - loss: 0.4241
[1m35/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7875 - loss: 0.4290
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.7827 - loss: 0.4341 - val_binary_accuracy: 0.8000 - val_loss: 0.6507
Epoch 5/10

[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0650
[1m 5/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7433 - loss: 0.2802
[1m 9/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7705 - loss: 0.3061
[1m13/37[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7871 - loss: 0.3304
[1m16/37[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7894 - loss: 0.3484
[1m20/37[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7990 - loss: 0.3621
[1m23/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8074 - loss: 0.3666
[1m27/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8185 - loss: 0.3707
[1m31/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8233 - loss: 0.3749
[1m34/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8255 - loss: 0.3775
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.8294 - loss: 0.3777 - val_binary_accuracy: 0.8000 - val_loss: 0.6807
Epoch 6/10

[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2673
[1m 5/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8433 - loss: 0.3044
[1m 8/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8321 - loss: 0.3048
[1m12/37[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8010 - loss: 0.3525
[1m16/37[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7867 - loss: 0.3723
[1m20/37[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7751 - loss: 0.3899
[1m24/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7755 - loss: 0.3968
[1m28/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7766 - loss: 0.3997
[1m31/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7788 - loss: 0.3985
[1m35/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7811 - loss: 0.3967
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.7797 - loss: 0.3968 - val_binary_accuracy: 0.6000 - val_loss: 0.6809
Epoch 7/10

[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3934
[1m 4/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 21ms/step - binary_accuracy: 1.0000 - loss: 0.3358
[1m 8/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 1.0000 - loss: 0.2853
[1m12/37[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9855 - loss: 0.2683
[1m16/37[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9637 - loss: 0.2633
[1m19/37[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9462 - loss: 0.2732
[1m23/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9252 - loss: 0.2901
[1m27/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9130 - loss: 0.3021
[1m31/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9067 - loss: 0.3076
[1m35/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9003 - loss: 0.3128
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.8960 - loss: 0.3158 - val_binary_accuracy: 0.8000 - val_loss: 0.6519
Epoch 8/10

[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.1222
[1m 5/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5867 - loss: 0.5078
[1m 9/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6229 - loss: 0.4506
[1m13/37[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6579 - loss: 0.4124
[1m17/37[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6926 - loss: 0.3804
[1m21/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7146 - loss: 0.3648
[1m24/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7285 - loss: 0.3545
[1m28/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7377 - loss: 0.3491
[1m32/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7459 - loss: 0.3454
[1m36/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7547 - loss: 0.3408
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.7591 - loss: 0.3383 - val_binary_accuracy: 0.4000 - val_loss: 0.7323
Epoch 9/10

[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.2249
[1m 5/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7433 - loss: 0.3803
[1m 9/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7968 - loss: 0.3227
[1m12/37[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8247 - loss: 0.2964
[1m16/37[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8512 - loss: 0.2797
[1m20/37[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8701 - loss: 0.2654
[1m24/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8843 - loss: 0.2555
[1m27/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8901 - loss: 0.2518
[1m31/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8955 - loss: 0.2481
[1m35/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9006 - loss: 0.2445
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9042 - loss: 0.2429 - val_binary_accuracy: 0.4000 - val_loss: 0.7172
Epoch 10/10

[1m 1/37[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.0571
[1m 5/37[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 1.0000 - loss: 0.0773
[1m 9/37[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9394 - loss: 0.1307
[1m13/37[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9310 - loss: 0.1611
[1m17/37[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9320 - loss: 0.1708
[1m21/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.9351 - loss: 0.1720
[1m25/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9387 - loss: 0.1711
[1m29/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9409 - loss: 0.1728
[1m33/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9385 - loss: 0.1788
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9360 - loss: 0.1851
[1m37/37[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 17ms/step - binary_accuracy: 0.9356 - loss: 0.1863 - val_binary_accuracy: 0.4000 - val_loss: 0.8195

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 125ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 125ms/step
predicted [0.33939782 0.3683171  0.79851294 0.86635524 0.8367875  0.12899607
 0.04999618 0.67940736 0.26796433 0.7959816  0.92414   ]
predicted [0 0 1 1 1 0 0 1 0 1 1]
expected [ True False  True False  True False  True False  True  True  True]
accuracy: 0.5454545454545454
confusion matrix: 
[[2 2]
 [3 4]]
              precision    recall  f1-score   support

       False       0.40      0.50      0.44         4
        True       0.67      0.57      0.62         7

    accuracy                           0.55        11
   macro avg       0.53      0.54      0.53        11
weighted avg       0.57      0.55      0.55        11

macro avg f1-score: 0.5299145299145299
macro avg (UAR): 0.5357142857142857
Sensitivity:  0.5
Specificity:  0.5714285714285714
g-mean:  0.5345224838248488
-------- Model Performance ----------: 
accuracy:  [0.72727273 0.63636364 0.45454545 0.72727273 0.36363636 0.63636364
 0.45454545 0.63636364 0.45454545 0.54545455]
gmean:  [0.73192505 0.5976143  0.37796447 0.65465367 0.32732684 0.5976143
 0.         0.46291005 0.46291005 0.53452248]
f1_score:  [0.71794872 0.60714286 0.41071429 0.68571429 0.34188034 0.60714286
 0.3125     0.54166667 0.45       0.52991453]
UAR:  [0.73214286 0.60714286 0.41071429 0.67857143 0.33928571 0.60714286
 0.35714286 0.55357143 0.46428571 0.53571429]
Cohen Kappa score:  [ 0.44067797  0.21428571 -0.17857143  0.37735849 -0.30508475  0.21428571
 -0.32        0.12       -0.06451613  0.06779661]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  28.992
step (sec):  21.744
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  7.248
Number of windows / instances:  53
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[ 0.89   0.954  0.943  0.95   0.943  0.349  0.683  0.72   0.583  0.658]
 [ 0.756  0.846  0.867  0.821  0.845  0.331  0.725  0.713  0.662  0.691]
 [ 0.719  0.846  0.873  0.767  0.835  0.367  0.733  0.753  0.614  0.715]
 [ 0.     0.5    0.62   0.     0.382  0.     0.5    0.64   0.     0.389]
 [ 0.749  0.888  0.907  0.869  0.889  0.251  0.638  0.677  0.514  0.584]
 [ 0.057  0.529  0.564  0.475  0.52  -0.022  0.493  0.545  0.342  0.461]
 [ 0.     0.5    0.62   0.     0.382  0.     0.5    0.64   0.     0.389]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[ 0.53729604  0.76666667  0.78666667  0.70184038  0.7522619   0.16461039
   0.575       0.75        0.29873457  0.55337302]
 [ 0.48531469  0.75        0.74        0.7292025   0.73202381  0.38686314
   0.69166667  0.73666667  0.5455455   0.66253968]
 [ 0.47121212  0.73333333  0.77        0.52844571  0.68035714  0.42004662
   0.70833333  0.72        0.63028609  0.68392857]
 [ 0.32987013  0.66666667  0.67333333  0.59339581  0.6425      0.09577922
   0.5375      0.71666667  0.23194792  0.51718254]
 [ 0.61900599  0.79583333  0.83333333  0.76715097  0.80281746  0.41773227
   0.71666667  0.69666667  0.63600519  0.6725    ]
 [ 0.36996337  0.69166667  0.67666667  0.66839759  0.66952381  0.48952714
   0.75        0.74666667  0.72824942  0.7352381 ]
 [ 0.3539627   0.6875      0.73333333  0.53896288  0.65599206  0.27835498
   0.63333333  0.63666667  0.49998785  0.59083333]
 [ 0.70623377  0.8625      0.89        0.79873457  0.84126984  0.56297036
   0.8         0.77333333  0.77100145  0.7652381 ]
 [ 0.1466034   0.57916667  0.67666667  0.33938469  0.54579365  0.25941558
   0.6125      0.72333333  0.44407996  0.61130952]
 [ 0.28498168  0.65        0.64666667  0.59980371  0.63404762  0.19067599
   0.59166667  0.6         0.48424138  0.56011905]
 [ 0.3539627   0.67083333  0.70333333  0.53854107  0.6425      0.47792208
   0.725       0.83333333  0.55355339  0.71301587]
 [ 0.60032468  0.7875      0.83666667  0.71479063  0.78503968  0.50625874
   0.7625      0.77333333  0.66595792  0.73142857]
 [ 0.26829837  0.64166667  0.65        0.56454317  0.62238095  0.35747586
   0.68333333  0.67666667  0.65409126  0.66321429]
 [ 0.05779221  0.53333333  0.61        0.31855046  0.50535714  0.51073926
   0.75833333  0.75666667  0.73517372  0.7452381 ]
 [ 0.01082251  0.51666667  0.63        0.21504825  0.46400794  0.26229604
   0.62916667  0.66333333  0.56524837  0.61678571]
 [ 0.05712621  0.53333333  0.55666667  0.34855986  0.49154762  0.22175325
   0.6125      0.72        0.36944525  0.57531746]
 [ 0.54025974  0.76666667  0.77666667  0.7462761   0.76039683  0.2030303
   0.6         0.64333333  0.43854107  0.57571429]
 [        nan  0.65        0.83333333  0.3         0.60174242  0.41333666
   0.7125      0.69666667  0.66048428  0.68154762]
 [ 0.32264069  0.67083333  0.72333333  0.47969291  0.62063492  0.33986014
   0.66666667  0.68        0.60017745  0.64797619]
 [ 0.31980519  0.6625      0.72666667  0.53284271  0.64400794  0.1396337
   0.57916667  0.62333333  0.36783039  0.52833333]
 [ 0.06454545  0.5425      0.74        0.18660254  0.50452381  0.01253746
   0.50833333  0.48666667  0.37615335  0.45357143]
 [ 0.22016317  0.60833333  0.62        0.55352406  0.59511905  0.39898435
   0.69583333  0.73333333  0.62429999  0.68420635]
 [ 0.26774892  0.62916667  0.64333333  0.58247757  0.61896825  0.20454545
   0.6125      0.78        0.28660254  0.5634127 ]
 [-0.10324675  0.45        0.46333333  0.30165079  0.41666667  0.49941725
   0.75        0.75666667  0.72268513  0.73904762]
 [ 0.31701632  0.6625      0.68333333  0.54863643  0.63380952  0.53290043
   0.76666667  0.76666667  0.72019073  0.75142857]
 [ 0.17660673  0.59166667  0.59        0.48961607  0.55583333  0.30374625
   0.65833333  0.66666667  0.52590982  0.61916667]
 [ 0.63668831  0.825       0.87        0.71855046  0.7984127   0.58974359
   0.8         0.79666667  0.78920914  0.79095238]
 [ 0.1961039   0.60416667  0.67333333  0.38926114  0.56388889  0.48686314
   0.73333333  0.77666667  0.65767754  0.72503968]
 [ 0.1505994   0.57083333  0.6         0.48829754  0.56087302  0.21831502
   0.61666667  0.63333333  0.43770101  0.56035714]
 [ 0.88974359  0.95416667  0.94333333  0.94990186  0.94285714  0.3492008
   0.68333333  0.72        0.58287864  0.65849206]]
KNN mean:
[0.33280846 0.6685     0.71       0.54108941 0.6428385  0.34315118
 0.67236111 0.70944444 0.55346301 0.6458836 ]
---------------------------
---------------------------
DT performance:
[[ 0.38722944  0.65        0.65333333  0.52152141  0.62940476  0.28311688
   0.5875      0.71666667  0.43194792  0.53869048]
 [ 0.37890443  0.65        0.66        0.59854771  0.62642857  0.32758075
   0.71666667  0.75666667  0.57048899  0.71130952]
 [ 0.15495837  0.55        0.56333333  0.35101499  0.48166667  0.14350649
   0.575       0.58666667  0.52506621  0.5447619 ]
 [ 0.44080919  0.75833333  0.75666667  0.64352987  0.73904762 -0.11096257
   0.4375      0.56666667  0.11123724  0.40015873]
 [ 0.58000333  0.76666667  0.77666667  0.7409014   0.75698413  0.56586747
   0.775       0.77        0.64065787  0.76238095]
 [ 0.21901432  0.65        0.65333333  0.60433479  0.63607143  0.39080919
   0.68333333  0.68        0.71883072  0.66345238]
 [ 0.41774892  0.7375      0.78666667  0.59873457  0.70968254  0.13583084
   0.63333333  0.62666667  0.46010395  0.59595238]
 [ 0.77835498  0.8875      0.92666667  0.8280239   0.88126984  0.31095571
   0.67083333  0.68333333  0.63886473  0.6577381 ]
 [ 0.13896104  0.5         0.59333333  0.34741798  0.49257937  0.06655844
   0.53333333  0.62666667  0.46213203  0.5418254 ]
 [-0.03846154  0.38333333  0.38        0.35545127  0.31107143 -0.02943723
   0.5         0.50666667  0.37724909  0.45107143]
 [ 0.1006993   0.5625      0.58666667  0.49079194  0.55428571  0.23165168
   0.70833333  0.71666667  0.43256985  0.6784127 ]
 [ 0.75790043  0.85        0.87333333  0.78841752  0.84662698  0.31301032
   0.64166667  0.66333333  0.53769745  0.6143254 ]
 [ 0.05712621  0.525       0.54333333  0.36935067  0.48416667  0.13568099
   0.53333333  0.53666667  0.37176494  0.50083333]
 [ 0.02127706  0.54583333  0.57333333  0.26504825  0.45611111  0.14267399
   0.58333333  0.58666667  0.34115708  0.53380952]
 [ 0.23004496  0.63333333  0.64        0.56691738  0.5947619   0.13856144
   0.58333333  0.60666667  0.50925175  0.5475    ]
 [ 0.06095571  0.525       0.54666667  0.43433366  0.49702381  0.17660673
   0.5875      0.64        0.48490479  0.57253968]
 [ 0.80363636  0.875       0.88666667  0.88284271  0.87142857  0.2979021
   0.68333333  0.70333333  0.56481604  0.66952381]
 [        nan  0.695       0.79333333  0.38660254  0.61886724  0.3992008
   0.70416667  0.7         0.69124581  0.66428571]
 [ 0.23688811  0.61666667  0.69333333  0.53402052  0.60710317  0.15431235
   0.50833333  0.53        0.51840618  0.4927381 ]
 [ 0.15238095  0.54583333  0.62666667  0.33080604  0.5172619  -0.06713287
   0.475       0.52333333  0.32454708  0.43952381]
 [-0.06727273  0.48        0.64        0.17320508  0.44974026  0.15887446
   0.61666667  0.64        0.49721703  0.61369048]
 [ 0.32773893  0.66666667  0.66        0.62515849  0.655       0.31985348
   0.6375      0.66        0.54410536  0.6122619 ]
 [-0.08392774  0.56666667  0.58        0.4195942   0.54059524  0.16168831
   0.625       0.72        0.34783978  0.60277778]
 [ 0.2029304   0.50833333  0.51666667  0.39519251  0.46333333  0.5006993
   0.74166667  0.72        0.65454513  0.69869048]
 [ 0.33799534  0.68333333  0.68        0.60116372  0.64452381  0.44941725
   0.725       0.73333333  0.67944603  0.72190476]
 [ 0.35979021  0.75        0.75        0.69434889  0.74452381  0.4039627
   0.7         0.71666667  0.68088618  0.67059524]
 [ 0.28766234  0.63333333  0.68666667  0.51986706  0.59464286  0.49407259
   0.76666667  0.76333333  0.70184038  0.74107143]
 [ 0.05396104  0.525       0.55333333  0.43359871  0.48392857  0.49090909
   0.77916667  0.77666667  0.68566078  0.75357143]
 [-0.10454545  0.54166667  0.56666667  0.17908837  0.47869048  0.25188811
   0.65        0.67666667  0.53056251  0.61214286]
 [ 0.75597736  0.84583333  0.86666667  0.82120906  0.84539683  0.33080087
   0.725       0.71333333  0.6616226   0.69107143]]
DT mean:
[0.27409453 0.63694444 0.66711111 0.51670117 0.60707395 0.25228199
 0.63625    0.66155556 0.52322218 0.6099537 ]
---------------------------
---------------------------
RF performance:
[[ 0.5039627   0.83333333  0.83333333  0.7787987   0.82428571  0.39480519
   0.625       0.75        0.37978771  0.6       ]
 [ 0.37004662  0.70833333  0.71666667  0.52734997  0.6747619   0.30095571
   0.66666667  0.69333333  0.58680266  0.63825397]
 [ 0.26738262  0.6125      0.62        0.57388174  0.56738095  0.46550117
   0.65        0.66        0.50973881  0.64130952]
 [ 0.16553447  0.70833333  0.69333333  0.47781629  0.6802381  -0.06071429
   0.375       0.52666667  0.25731322  0.38210317]
 [ 0.56370296  0.79583333  0.83333333  0.70181498  0.80281746  0.31247086
   0.74166667  0.74        0.73600519  0.71738095]
 [ 0.33403263  0.74166667  0.73333333  0.67615335  0.72047619  0.61528472
   0.78333333  0.77333333  0.71834365  0.74916667]
 [ 0.35151515  0.70833333  0.72666667  0.52556542  0.68357143  0.22359307
   0.63333333  0.64        0.53399785  0.61833333]
 [ 0.79714286  0.8875      0.92666667  0.85731322  0.88126984  0.51550117
   0.7         0.71333333  0.64821462  0.69619048]
 [ 0.20948218  0.5625      0.67333333  0.53094011  0.53742063  0.14190222
   0.44166667  0.54666667  0.2026586   0.43035714]
 [ 0.21388611  0.48333333  0.48333333  0.4628333   0.44238095  0.26403596
   0.54166667  0.56        0.60778227  0.51630952]
 [ 0.35151515  0.62083333  0.64666667  0.57792575  0.60880952  0.34355644
   0.7125      0.79666667  0.65151672  0.69396825]
 [ 0.6271645   0.775       0.79333333  0.7811043   0.76710317  0.1470363
   0.64166667  0.68333333  0.49440362  0.6218254 ]
 [ 0.25619381  0.6         0.61666667  0.59223208  0.5822619   0.27169497
   0.625       0.63333333  0.63362411  0.60238095]
 [ 0.08961039  0.56666667  0.64666667  0.39349395  0.5497619   0.27099567
   0.61666667  0.62666667  0.48568152  0.55285714]
 [ 0.05909091  0.62083333  0.69666667  0.29813805  0.58579365 -0.10477855
   0.50833333  0.54333333  0.4293659   0.47059524]
 [ 0.25972028  0.65833333  0.67        0.4365044   0.61488095  0.12812188
   0.72083333  0.77333333  0.51128842  0.71678571]
 [ 0.75818182  0.85833333  0.86666667  0.91213203  0.85357143  0.32575758
   0.6625      0.69666667  0.54747271  0.63107143]
 [        nan  0.635       0.79666667  0.4         0.58934343  0.35201465
   0.7125      0.72333333  0.63857699  0.70261905]
 [ 0.36363636  0.70833333  0.79666667  0.59162148  0.67988095  0.35524476
   0.70833333  0.71666667  0.65042098  0.68238095]
 [ 0.37911255  0.62916667  0.70333333  0.53752273  0.6081746   0.13297683
   0.575       0.58666667  0.38050778  0.52369048]
 [ 0.13311688  0.465       0.67666667  0.17320508  0.43160534  0.12798868
   0.45833333  0.48666667  0.67043426  0.4327381 ]
 [ 0.30656011  0.65        0.66        0.63532852  0.63178571  0.23758575
   0.64583333  0.66        0.5395594   0.63154762]
 [ 0.1976024   0.5625      0.58333333  0.46848358  0.54107143  0.21168831
   0.6         0.73666667  0.49391576  0.56944444]
 [ 0.11608392  0.54166667  0.55333333  0.59828344  0.53        0.52645688
   0.775       0.77        0.73888547  0.74464286]
 [ 0.17905428  0.6125      0.62        0.55140725  0.59190476  0.31318681
   0.73333333  0.74666667  0.66935067  0.73142857]
 [ 0.15970696  0.64166667  0.66333333  0.57163946  0.62035714  0.35221445
   0.65833333  0.65666667  0.61614671  0.62321429]
 [ 0.39480519  0.65        0.70333333  0.57586368  0.61626984  0.52972028
   0.725       0.71666667  0.72708569  0.70488095]
 [ 0.09216617  0.45416667  0.50333333  0.14783978  0.41051587  0.51171329
   0.6875      0.71666667  0.63008597  0.6727381 ]
 [ 0.06535132  0.54583333  0.56        0.47320508  0.52575397  0.47554113
   0.57083333  0.59333333  0.39763884  0.53059524]
 [ 0.71861472  0.84583333  0.87333333  0.76740858  0.83515873  0.36742424
   0.73333333  0.75333333  0.61423003  0.71468254]]
RF mean:
[0.3201371  0.65611111 0.69566667 0.55319354 0.63295358 0.3016492
 0.64097222 0.674      0.55669454 0.6181164 ]
---------------------------
---------------------------
SVM performance:
[[0.         0.5        0.57       0.         0.3625     0.
  0.5        0.76       0.         0.43111111]
 [0.         0.5        0.55       0.         0.35357143 0.
  0.5        0.62       0.         0.3825    ]
 [0.         0.5        0.58666667 0.         0.36916667 0.01666667
  0.50833333 0.53       0.05773503 0.36547619]
 [0.         0.5        0.55       0.         0.35357143 0.
  0.5        0.76       0.         0.43111111]
 [0.         0.5        0.62       0.         0.3825     0.45887446
  0.725      0.73333333 0.5909014  0.68119048]
 [0.         0.5        0.53       0.         0.34464286 0.2969697
  0.64166667 0.66       0.46698677 0.59      ]
 [0.         0.5        0.66       0.         0.39638889 0.07832168
  0.53333333 0.51       0.22307101 0.41809524]
 [0.         0.5        0.7        0.         0.41027778 0.
  0.5        0.60333333 0.         0.37583333]
 [0.         0.5        0.68       0.         0.40333333 0.
  0.5        0.68       0.         0.40333333]
 [0.19370629 0.59166667 0.57       0.28224619 0.47809524 0.10454545
  0.55       0.53       0.24391576 0.43690476]
 [0.         0.5        0.58666667 0.         0.36916667 0.
  0.5        0.66       0.         0.39638889]
 [0.         0.5        0.64       0.         0.38944444 0.
  0.5        0.62       0.         0.3825    ]
 [0.         0.5        0.57       0.         0.3625     0.
  0.5        0.55       0.         0.35357143]
 [0.         0.5        0.7        0.         0.41027778 0.
  0.5        0.53       0.         0.34464286]
 [0.         0.5        0.68       0.         0.40333333 0.
  0.5        0.62       0.         0.3825    ]
 [0.         0.5        0.60333333 0.         0.37583333 0.
  0.5        0.68       0.         0.40333333]
 [0.         0.5        0.60333333 0.         0.37583333 0.
  0.5        0.58666667 0.         0.36916667]
 [       nan 0.7        0.89       0.4        0.66969697 0.
  0.5        0.58666667 0.         0.36916667]
 [0.         0.5        0.7        0.         0.41027778 0.
  0.5        0.55       0.         0.35357143]
 [0.         0.5        0.7        0.         0.41027778 0.
  0.5        0.64       0.         0.38944444]
 [0.         0.5        0.79333333 0.         0.4420202  0.
  0.5        0.55       0.         0.35357143]
 [0.         0.5        0.53       0.         0.34464286 0.
  0.5        0.62       0.         0.3825    ]
 [0.         0.5        0.60333333 0.         0.37583333 0.
  0.5        0.76       0.         0.43111111]
 [0.         0.5        0.53       0.         0.34464286 0.
  0.5        0.57       0.         0.3625    ]
 [0.         0.5        0.58666667 0.         0.36916667 0.
  0.5        0.53       0.         0.34464286]
 [0.         0.5        0.53       0.         0.34464286 0.
  0.5        0.55       0.         0.35357143]
 [0.         0.5        0.7        0.         0.41027778 0.18311688
  0.59166667 0.57       0.22844571 0.4577381 ]
 [0.         0.5        0.66       0.         0.39638889 0.
  0.5        0.62       0.         0.3825    ]
 [0.         0.5        0.60333333 0.         0.37583333 0.
  0.5        0.58666667 0.         0.36916667]
 [0.         0.5        0.62       0.         0.3825     0.
  0.5        0.64       0.         0.38944444]]
SVM mean:
[0.00667953 0.50972222 0.62822222 0.02274154 0.39388793 0.03794983
 0.51833333 0.61355556 0.06036852 0.40288624]
---------------------------
---------------------------
GBM performance:
[[ 0.46305361  0.73333333  0.75333333  0.63449706  0.7097619   0.15714286
   0.5875      0.79333333  0.23194792  0.55527778]
 [ 0.32703963  0.65833333  0.67666667  0.6135307   0.63761905  0.4465368
   0.71666667  0.77        0.56986706  0.69456349]
 [ 0.21305361  0.625       0.66        0.41783039  0.57619048  0.23426573
   0.61666667  0.64333333  0.51447163  0.59511905]
 [ 0.20481185  0.60833333  0.60333333  0.46888465  0.54916667  0.01071429
   0.4625      0.7         0.          0.40829365]
 [ 0.61327006  0.79166667  0.83        0.76364875  0.8002381   0.53216783
   0.75        0.73666667  0.64898084  0.71357143]
 [ 0.21388611  0.65        0.64        0.62776236  0.63190476  0.35747586
   0.7         0.69666667  0.65409126  0.68107143]
 [ 0.31363636  0.6625      0.72666667  0.48575892  0.62551587  0.2980353
   0.675       0.69333333  0.49923652  0.6402381 ]
 [ 0.77835498  0.8875      0.92666667  0.8280239   0.88126984  0.32480852
   0.64583333  0.67        0.59863643  0.62047619]
 [ 0.12954545  0.5625      0.71333333  0.17071068  0.49507937 -0.06461039
   0.4375      0.54333333  0.06123724  0.39833333]
 [-0.16603397  0.44166667  0.43666667  0.26117036  0.40452381  0.01794872
   0.53333333  0.52666667  0.42843356  0.48547619]
 [ 0.26327006  0.59583333  0.62666667  0.46698677  0.57107143  0.53165168
   0.73333333  0.79333333  0.59234155  0.71833333]
 [ 0.74837662  0.85        0.87333333  0.8306825   0.84662698  0.26528472
   0.62916667  0.68666667  0.44614203  0.59968254]
 [ 0.20454545  0.625       0.64        0.50770214  0.6072619   0.12472527
   0.58333333  0.58333333  0.38996047  0.51797619]
 [-0.03636364  0.4875      0.63666667  0.15731322  0.43892857  0.21586747
   0.59166667  0.61        0.47557401  0.54809524]
 [ 0.10811688  0.55        0.7         0.2026586   0.49563492  0.09396936
   0.55        0.59        0.46852172  0.5072619 ]
 [-0.13636364  0.43333333  0.49333333  0.10773503  0.36047619  0.14393939
   0.57083333  0.64333333  0.40181498  0.54714286]
 [ 0.89454545  0.95        0.94666667  0.91213203  0.94285714  0.14848485
   0.5875      0.63        0.36499158  0.54440476]
 [        nan  0.645       0.81333333  0.3         0.59479798  0.34582751
   0.68333333  0.68666667  0.54898084  0.63678571]
 [ 0.31168831  0.65        0.76        0.39142136  0.60642857  0.22191142
   0.625       0.64333333  0.55983968  0.60845238]
 [ 0.06266234  0.5625      0.68        0.36039363  0.55119048  0.03732101
   0.53333333  0.62        0.26927053  0.48920635]
 [-0.095       0.4525      0.71666667  0.          0.41573232  0.3492008
   0.66666667  0.69666667  0.54459242  0.63452381]
 [ 0.32147852  0.66666667  0.66        0.604112    0.63952381  0.41439394
   0.69583333  0.75333333  0.5819991   0.67928571]
 [-0.05997669  0.5         0.55        0.26009536  0.45130952  0.00714286
   0.6125      0.74        0.17071068  0.58424603]
 [-0.06430236  0.49166667  0.49666667  0.32296155  0.46238095  0.55524476
   0.74166667  0.73333333  0.70740467  0.7075    ]
 [ 0.13531469  0.59166667  0.62333333  0.46733118  0.55928571  0.51550117
   0.73333333  0.74666667  0.59730931  0.73142857]
 [ 0.52972028  0.75833333  0.77666667  0.73266717  0.75738095  0.34638695
   0.675       0.69666667  0.57388174  0.65095238]
 [ 0.37337662  0.69583333  0.76333333  0.57236146  0.66944444  0.50792541
   0.74166667  0.74333333  0.70555214  0.72083333]
 [ 0.02435065  0.51666667  0.61666667  0.14142136  0.44111111  0.63638695
   0.8         0.80666667  0.75689141  0.78809524]
 [-0.05651016  0.45833333  0.52666667  0.08164966  0.35214286  0.25887446
   0.64583333  0.68        0.40361005  0.59333333]
 [ 0.74898435  0.8875      0.90666667  0.86944525  0.88920635  0.25108225
   0.6375      0.67666667  0.51378282  0.58444444]]
GBM mean:
[0.25408729 0.63297222 0.69244444 0.4520296  0.59880207 0.27618692
 0.63875    0.68444444 0.47600247 0.60614683]
---------------------------
---------------------------
BDDAE performance:
[[-0.03530503  0.485       0.49090909  0.4141665   0.46124847 -0.12688756
   0.43541667  0.57272727  0.20365589  0.425838  ]
 [ 0.15525422  0.57833333  0.58181818  0.53057023  0.56466894  0.33774661
   0.67857143  0.67272727  0.66221426  0.65743423]
 [ 0.14640088  0.57333333  0.58181818  0.54916651  0.56549895 -0.04938198
   0.47666667  0.48181818  0.3657211   0.44676962]
 [-0.05629202  0.47166667  0.47272727  0.32705594  0.42402788 -0.06946699
   0.46666667  0.61818182  0.15606602  0.42718542]
 [ 0.03103742  0.5125      0.50909091  0.44638738  0.47834097  0.2137521
   0.61        0.59090909  0.54199503  0.56582695]
 [ 0.31738875  0.65666667  0.67272727  0.60101816  0.63748141  0.01128066
   0.505       0.49090909  0.42597689  0.46653902]
 [-0.06599805  0.475       0.53636364  0.23020948  0.41924607  0.2944725
   0.64833333  0.64545455  0.61072699  0.62800311]
 [ 0.1875102   0.59583333  0.7         0.48313496  0.58687092  0.04622655
   0.52321429  0.53636364  0.47546894  0.50697216]
 [ 0.31035407  0.64107143  0.72727273  0.48367907  0.6206746   0.20851064
   0.5875      0.7         0.35        0.54490196]
 [ 0.09079133  0.545       0.54545455  0.53138371  0.53792485  0.41157755
   0.705       0.70909091  0.68357434  0.69729853]
 [ 0.02801382  0.515       0.52727273  0.37172878  0.47410878  0.03872822
   0.51785714  0.61818182  0.25030509  0.46891106]
 [ 0.01352121  0.50714286  0.53636364  0.42738308  0.49192943  0.3387456
   0.67857143  0.67272727  0.66643753  0.65955905]
 [ 0.09785885  0.55        0.54545455  0.52962413  0.53730325  0.34359069
   0.675       0.66363636  0.65064218  0.65633561]
 [ 0.0524755   0.52708333  0.64545455  0.35343987  0.5141991   0.02360322
   0.51166667  0.51818182  0.48193991  0.4997472 ]
 [-0.12889199  0.43214286  0.48181818  0.33545933  0.4205762   0.08905616
   0.53928571  0.56363636  0.50678984  0.53523657]
 [ 0.05985002  0.51785714  0.56363636  0.41673677  0.49254817  0.08501967
   0.53571429  0.62727273  0.32843376  0.50890289]
 [ 0.17426675  0.5875      0.61818182  0.52224828  0.57172644  0.10016479
   0.54833333  0.56363636  0.46767902  0.52464286]
 [ 0.38602022  0.78        0.84545455  0.64637571  0.67792607  0.03637543
   0.51666667  0.52727273  0.47227872  0.4992513 ]
 [ 0.09358543  0.55416667  0.60909091  0.50116634  0.53898935  0.09422315
   0.54833333  0.54545455  0.52775414  0.53599373]
 [ 0.40783219  0.69791667  0.77272727  0.66405138  0.69991363 -0.03534646
   0.47678571  0.54545455  0.32447699  0.4552618 ]
 [-0.11344714  0.45277778  0.64545455  0.13689656  0.43141733  0.05836402
   0.53        0.52727273  0.49561006  0.51112998]
 [-0.18407401  0.40666667  0.40909091  0.38681745  0.39794789  0.16220273
   0.58214286  0.64545455  0.43709917  0.56219538]
 [-0.16450879  0.41428571  0.44545455  0.34994559  0.40043262  0.54051408
   0.73333333  0.85454545  0.6391884   0.75517716]
 [-0.16297607  0.41833333  0.41818182  0.3544657   0.39893801  0.37275131
   0.68333333  0.69090909  0.6719461   0.68249334]
 [ 0.25457998  0.625       0.63636364  0.6019838   0.61947525  0.1993923
   0.6         0.6         0.57872724  0.58918831]
 [ 0.06008973  0.53        0.53636364  0.45272058  0.50326382  0.16350334
   0.58        0.59090909  0.52528846  0.56738914]
 [ 0.44495462  0.71875     0.77272727  0.69823225  0.718324    0.40077767
   0.70666667  0.69090909  0.65952816  0.6750383 ]
 [-0.12858897  0.43571429  0.5         0.27332222  0.41541317  0.49738907
   0.73392857  0.79090909  0.67768578  0.73353857]
 [ 0.03249723  0.51785714  0.53636364  0.49293915  0.50636197  0.32806351
   0.66166667  0.67272727  0.64362211  0.65885226]
 [ 0.05662322  0.52857143  0.56363636  0.47474412  0.52046245 -0.02165975
   0.49285714  0.54545455  0.34230606  0.46116255]]
BDDAE mean:
[0.07869412 0.54170569 0.58090909 0.45290177 0.520908   0.16977629
 0.5829504  0.61575758 0.49410461 0.5635592 ]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.57       0.         0.3625     0.
  0.5        0.76       0.         0.43111111]
 [0.         0.5        0.55       0.         0.35357143 0.
  0.5        0.62       0.         0.3825    ]
 [0.         0.5        0.58666667 0.         0.36916667 0.
  0.5        0.53       0.         0.34464286]
 [0.         0.5        0.55       0.         0.35357143 0.
  0.5        0.76       0.         0.43111111]
 [0.         0.5        0.62       0.         0.3825     0.
  0.5        0.53       0.         0.34464286]
 [0.         0.5        0.53       0.         0.34464286 0.
  0.5        0.53       0.         0.34464286]
 [0.         0.5        0.66       0.         0.39638889 0.
  0.5        0.51       0.         0.33571429]
 [0.         0.5        0.7        0.         0.41027778 0.
  0.5        0.60333333 0.         0.37583333]
 [0.         0.5        0.68       0.         0.40333333 0.
  0.5        0.68       0.         0.40333333]
 [0.         0.5        0.51       0.         0.33571429 0.
  0.5        0.43       0.         0.3       ]
 [0.         0.5        0.58666667 0.         0.36916667 0.
  0.5        0.66       0.         0.39638889]
 [0.         0.5        0.64       0.         0.38944444 0.
  0.5        0.62       0.         0.3825    ]
 [0.         0.5        0.57       0.         0.3625     0.
  0.5        0.55       0.         0.35357143]
 [0.         0.5        0.7        0.         0.41027778 0.
  0.5        0.53       0.         0.34464286]
 [0.         0.5        0.68       0.         0.40333333 0.
  0.5        0.62       0.         0.3825    ]
 [0.         0.5        0.60333333 0.         0.37583333 0.
  0.5        0.68       0.         0.40333333]
 [0.         0.5        0.60333333 0.         0.37583333 0.
  0.5        0.58666667 0.         0.36916667]
 [       nan 0.7        0.89       0.4        0.66969697 0.
  0.5        0.58666667 0.         0.36916667]
 [0.         0.5        0.7        0.         0.41027778 0.
  0.5        0.55       0.         0.35357143]
 [0.         0.5        0.7        0.         0.41027778 0.
  0.5        0.64       0.         0.38944444]
 [0.         0.5        0.79333333 0.         0.4420202  0.
  0.5        0.55       0.         0.35357143]
 [0.         0.5        0.53       0.         0.34464286 0.
  0.5        0.62       0.         0.3825    ]
 [0.         0.5        0.60333333 0.         0.37583333 0.
  0.5        0.76       0.         0.43111111]
 [0.         0.5        0.53       0.         0.34464286 0.
  0.5        0.57       0.         0.3625    ]
 [0.         0.5        0.58666667 0.         0.36916667 0.
  0.5        0.53       0.         0.34464286]
 [0.         0.5        0.53       0.         0.34464286 0.
  0.5        0.55       0.         0.35357143]
 [0.         0.5        0.7        0.         0.41027778 0.
  0.5        0.43       0.         0.3       ]
 [0.         0.5        0.66       0.         0.39638889 0.
  0.5        0.62       0.         0.3825    ]
 [0.         0.5        0.60333333 0.         0.37583333 0.
  0.5        0.58666667 0.         0.36916667]
 [0.         0.5        0.62       0.         0.3825     0.
  0.5        0.64       0.         0.38944444]]
DUMMY mean:
[0.         0.50666667 0.62622222 0.01333333 0.3891419  0.
 0.5        0.59444444 0.         0.37022751]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_60
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.333 0.669 0.706 0.549 0.644 0.341 0.671 0.71  0.55  0.645]
 [0.274 0.635 0.663 0.521 0.607 0.247 0.634 0.66  0.517 0.608]
 [0.32  0.657 0.692 0.558 0.634 0.3   0.639 0.672 0.554 0.615]
 [0.007 0.503 0.619 0.01  0.384 0.039 0.519 0.614 0.062 0.404]
 [0.254 0.633 0.688 0.457 0.599 0.274 0.637 0.684 0.473 0.605]
 [0.079 0.542 0.581 0.453 0.521 0.17  0.583 0.616 0.494 0.564]
 [0.    0.5   0.617 0.    0.379 0.    0.5   0.595 0.    0.37 ]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.226 0.112 0.104 0.178 0.119 0.15  0.077 0.07  0.151 0.085]
 [0.26  0.128 0.123 0.187 0.143 0.169 0.091 0.077 0.139 0.096]
 [0.205 0.115 0.107 0.172 0.123 0.169 0.098 0.083 0.135 0.099]
 [0.035 0.017 0.066 0.052 0.03  0.103 0.05  0.073 0.146 0.071]
 [0.29  0.14  0.127 0.254 0.16  0.187 0.086 0.072 0.182 0.095]
 [0.172 0.091 0.105 0.127 0.09  0.177 0.086 0.085 0.149 0.094]
 [0.    0.    0.069 0.    0.026 0.    0.    0.083 0.    0.033]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 68.  17.  15.  32.  18.  44.  11.  10.  27.  13.]
 [ 95.  20.  19.  36.  24.  68.  14.  12.  27.  16.]
 [ 64.  18.  15.  31.  19.  56.  15.  12.  24.  16.]
 [524.   3.  11. 534.   8. 262.  10.  12. 234.  18.]
 [114.  22.  18.  56.  27.  68.  13.  11.  38.  16.]
 [219.  17.  18.  28.  17. 104.  15.  14.  30.  17.]
 [  0.   0.  11.   0.   7.   0.   0.  14.   0.   9.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_60
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  28.992
step (sec):  21.744
overlap:  True
perc. of overlap:  25.0
overlap duration (sec):  7.248
Number of windows / instances:  53
Elapsed time: 1045.5494699041049 minutes
Elapsed time: 17.42582449840175 hours
