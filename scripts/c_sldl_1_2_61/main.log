2024-05-21 16:48:02.088971: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-21 16:48:05.983653: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.
2024-05-21 16:48:15.396120: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
corriendo en PC gamer:
Window size (sec):  29.0
step (sec):  14.5
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  14.5
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
C:\Users\Javier\Dropbox\c_sldl_1_2_61\functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:


[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - loss: 0.0018 - mean_squared_error: 0.0018
[1m 5/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 20ms/step - loss: 0.0074 - mean_squared_error: 0.0074
[1m 9/54[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0071 - mean_squared_error: 0.0071
[1m11/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - loss: 0.0072 - mean_squared_error: 0.0072
[1m14/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - loss: 0.0080 - mean_squared_error: 0.0080
[1m18/54[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0097 - mean_squared_error: 0.0097
[1m22/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0108 - mean_squared_error: 0.0108
[1m25/54[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0112 - mean_squared_error: 0.0112
[1m29/54[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0117 - mean_squared_error: 0.0117
[1m32/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0123 - mean_squared_error: 0.0123
[1m36/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0131 - mean_squared_error: 0.0131
[1m39/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0136 - mean_squared_error: 0.0136
[1m43/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0140 - mean_squared_error: 0.0140
[1m47/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 18ms/step - loss: 0.0143 - mean_squared_error: 0.0143
[1m50/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 18ms/step - loss: 0.0146 - mean_squared_error: 0.0146
[1m53/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 18ms/step - loss: 0.0148 - mean_squared_error: 0.0148
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 19ms/step - loss: 0.0150 - mean_squared_error: 0.0150 - val_loss: 0.0202 - val_mean_squared_error: 0.0202
(28992, 1, 5)
Model: "sequential_1199"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)                    â”‚ Output Shape           â”‚       Param # â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stConvL (Conv1D)               â”‚ (None, 28992, 5)       â”‚           105 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 1stPoolL (AveragePooling1D)     â”‚ (None, 7248, 5)        â”‚             0 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndConvL (Conv1D)               â”‚ (None, 7248, 6)        â”‚           306 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ 2ndPoolL (AveragePooling1D)     â”‚ (None, 1812, 6)        â”‚             0 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 2,209 (8.63 KB)
 Trainable params: 411 (1.61 KB)
 Non-trainable params: 0 (0.00 B)
 Optimizer params: 1,798 (7.03 KB)
Model: "valence_NN"
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Layer (type)        â”‚ Output Shape      â”‚    Param # â”‚ Connected to      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputGSR            â”‚ (None, 28992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ inputPPG            â”‚ (None, 28992, 1)  â”‚          0 â”‚ -                 â”‚
â”‚ (InputLayer)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1198     â”‚ (None, 1812, 6)   â”‚        411 â”‚ inputGSR[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ sequential_1199     â”‚ (None, 1812, 6)   â”‚        411 â”‚ inputPPG[0][0]    â”‚
â”‚ (Sequential)        â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ concatenate_599     â”‚ (None, 1812, 12)  â”‚          0 â”‚ sequential_1198[â€¦ â”‚
â”‚ (Concatenate)       â”‚                   â”‚            â”‚ sequential_1199[â€¦ â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ permute_599         â”‚ (None, 12, 1812)  â”‚          0 â”‚ concatenate_599[â€¦ â”‚
â”‚ (Permute)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ flatten_599         â”‚ (None, 21744)     â”‚          0 â”‚ permute_599[0][0] â”‚
â”‚ (Flatten)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dropout_599         â”‚ (None, 21744)     â”‚          0 â”‚ flatten_599[0][0] â”‚
â”‚ (Dropout)           â”‚                   â”‚            â”‚                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ dense_599 (Dense)   â”‚ (None, 1)         â”‚     21,745 â”‚ dropout_599[0][0] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
 Total params: 22,567 (88.15 KB)
 Trainable params: 22,567 (88.15 KB)
 Non-trainable params: 0 (0.00 B)
Epoch 1/10

[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1:18[0m 1s/step - binary_accuracy: 1.0000 - loss: 0.3836
[1m 4/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 21ms/step - binary_accuracy: 1.0000 - loss: 0.4041
[1m 8/54[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9457 - loss: 0.4953
[1m12/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8996 - loss: 0.5697
[1m16/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8646 - loss: 0.6041
[1m20/54[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.8483 - loss: 0.6158
[1m23/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8424 - loss: 0.6186
[1m27/54[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8325 - loss: 0.6276
[1m31/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8148 - loss: 0.6404
[1m34/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8035 - loss: 0.6476
[1m38/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7924 - loss: 0.6536
[1m42/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7858 - loss: 0.6565
[1m46/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7761 - loss: 0.6642
[1m50/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7676 - loss: 0.6709
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7619 - loss: 0.6740
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 21ms/step - binary_accuracy: 0.7605 - loss: 0.6746 - val_binary_accuracy: 0.5000 - val_loss: 0.8079
Epoch 2/10

[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.2072
[1m 4/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 21ms/step - binary_accuracy: 0.9375 - loss: 0.3093
[1m 8/54[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7766 - loss: 0.5452
[1m12/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7319 - loss: 0.5839
[1m16/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7295 - loss: 0.5774
[1m20/54[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7402 - loss: 0.5647
[1m23/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7465 - loss: 0.5601
[1m27/54[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7550 - loss: 0.5533
[1m30/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7622 - loss: 0.5461
[1m34/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7640 - loss: 0.5501
[1m37/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7644 - loss: 0.5540
[1m41/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7621 - loss: 0.5612
[1m45/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7592 - loss: 0.5674
[1m48/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7548 - loss: 0.5722
[1m52/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7478 - loss: 0.5784
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.7432 - loss: 0.5826 - val_binary_accuracy: 0.6667 - val_loss: 0.7015
Epoch 3/10

[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.3350
[1m 4/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 21ms/step - binary_accuracy: 0.9375 - loss: 0.4736
[1m 7/54[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 21ms/step - binary_accuracy: 0.8187 - loss: 0.6329
[1m11/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - binary_accuracy: 0.7682 - loss: 0.6961
[1m14/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - binary_accuracy: 0.7411 - loss: 0.7285
[1m18/54[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7279 - loss: 0.7339
[1m22/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7240 - loss: 0.7252
[1m25/54[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - binary_accuracy: 0.7205 - loss: 0.7158
[1m29/54[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7188 - loss: 0.7028
[1m32/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7210 - loss: 0.6930
[1m36/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7230 - loss: 0.6808
[1m40/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7209 - loss: 0.6733
[1m44/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7206 - loss: 0.6649
[1m48/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7223 - loss: 0.6563
[1m52/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7250 - loss: 0.6472
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.7268 - loss: 0.6404 - val_binary_accuracy: 0.6667 - val_loss: 0.8468
Epoch 4/10

[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 0.0000e+00 - loss: 1.4961
[1m 5/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5433 - loss: 0.7660    
[1m 9/54[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6436 - loss: 0.6478
[1m13/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.6933 - loss: 0.5919
[1m16/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7093 - loss: 0.5800
[1m19/54[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7200 - loss: 0.5715
[1m23/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7324 - loss: 0.5568
[1m27/54[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7429 - loss: 0.5442
[1m31/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7542 - loss: 0.5305
[1m34/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7617 - loss: 0.5212
[1m38/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7673 - loss: 0.5125
[1m42/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7718 - loss: 0.5050
[1m46/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7760 - loss: 0.4990
[1m50/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7791 - loss: 0.4936
[1m53/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7807 - loss: 0.4912
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.7813 - loss: 0.4904 - val_binary_accuracy: 0.6667 - val_loss: 0.8194
Epoch 5/10

[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 0.0000e+00 - loss: 1.5087
[1m 5/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.1567 - loss: 1.1701    
[1m 8/54[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.2557 - loss: 1.0571
[1m12/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.3608 - loss: 0.9539
[1m16/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.4085 - loss: 0.8976
[1m20/54[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.4508 - loss: 0.8489
[1m24/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.4904 - loss: 0.8050
[1m28/54[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5254 - loss: 0.7648
[1m31/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5477 - loss: 0.7391
[1m35/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.5695 - loss: 0.7112
[1m39/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.5863 - loss: 0.6914
[1m43/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6018 - loss: 0.6719
[1m47/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6151 - loss: 0.6537
[1m50/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6247 - loss: 0.6403
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - binary_accuracy: 0.6367 - loss: 0.6240
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.6392 - loss: 0.6205 - val_binary_accuracy: 0.6667 - val_loss: 0.8256
Epoch 6/10

[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.0405
[1m 5/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 20ms/step - binary_accuracy: 0.7033 - loss: 0.2975
[1m 8/54[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 20ms/step - binary_accuracy: 0.7060 - loss: 0.3123
[1m12/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7076 - loss: 0.3454
[1m16/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7287 - loss: 0.3433
[1m20/54[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7425 - loss: 0.3397
[1m24/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7557 - loss: 0.3374
[1m27/54[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7657 - loss: 0.3348
[1m30/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7754 - loss: 0.3325
[1m34/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7873 - loss: 0.3296
[1m38/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7981 - loss: 0.3277
[1m41/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8038 - loss: 0.3269
[1m45/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8110 - loss: 0.3258
[1m49/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8165 - loss: 0.3251
[1m53/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8190 - loss: 0.3265
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.8195 - loss: 0.3272 - val_binary_accuracy: 0.6667 - val_loss: 0.5493
Epoch 7/10

[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.1884
[1m 4/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 21ms/step - binary_accuracy: 0.8542 - loss: 0.2315
[1m 7/54[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 21ms/step - binary_accuracy: 0.8235 - loss: 0.2340
[1m11/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - binary_accuracy: 0.8100 - loss: 0.2353
[1m15/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8010 - loss: 0.2613
[1m19/54[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7946 - loss: 0.2764
[1m23/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7919 - loss: 0.2853
[1m27/54[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7894 - loss: 0.2909
[1m30/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7897 - loss: 0.2923
[1m34/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7919 - loss: 0.2925
[1m37/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7930 - loss: 0.2934
[1m40/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7938 - loss: 0.2941
[1m43/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7948 - loss: 0.2952
[1m46/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7966 - loss: 0.2959
[1m50/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7985 - loss: 0.2967
[1m53/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7990 - loss: 0.2980
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.7996 - loss: 0.2987 - val_binary_accuracy: 0.8333 - val_loss: 0.5004
Epoch 8/10

[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 39ms/step - binary_accuracy: 1.0000 - loss: 0.1954
[1m 4/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - binary_accuracy: 1.0000 - loss: 0.1754
[1m 7/54[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 19ms/step - binary_accuracy: 1.0000 - loss: 0.2022
[1m11/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9744 - loss: 0.2210
[1m14/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9468 - loss: 0.2340
[1m17/54[0m [32mâ”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9269 - loss: 0.2410
[1m20/54[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.9062 - loss: 0.2496
[1m23/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8947 - loss: 0.2520
[1m27/54[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8870 - loss: 0.2524
[1m31/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8841 - loss: 0.2516
[1m35/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8837 - loss: 0.2512
[1m39/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8847 - loss: 0.2514
[1m43/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8853 - loss: 0.2519
[1m47/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8857 - loss: 0.2525
[1m51/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8868 - loss: 0.2521
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8878 - loss: 0.2511
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.8882 - loss: 0.2507 - val_binary_accuracy: 0.6667 - val_loss: 0.7977
Epoch 9/10

[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m2s[0m 47ms/step - binary_accuracy: 1.0000 - loss: 0.0074
[1m 5/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7433 - loss: 0.3771
[1m 9/54[0m [32mâ”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7284 - loss: 0.3923
[1m12/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7278 - loss: 0.3903
[1m16/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7438 - loss: 0.3705
[1m20/54[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7625 - loss: 0.3484
[1m24/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7798 - loss: 0.3284
[1m28/54[0m [32mâ”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.7950 - loss: 0.3137
[1m31/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8052 - loss: 0.3028
[1m35/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8172 - loss: 0.2899
[1m39/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8244 - loss: 0.2836
[1m43/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8273 - loss: 0.2810
[1m47/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8307 - loss: 0.2789
[1m51/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8345 - loss: 0.2772
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.8383 - loss: 0.2757 - val_binary_accuracy: 0.8333 - val_loss: 0.4145
Epoch 10/10

[1m 1/54[0m [37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m1s[0m 31ms/step - binary_accuracy: 1.0000 - loss: 0.0233
[1m 5/54[0m [32mâ”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 16ms/step - binary_accuracy: 0.7433 - loss: 0.2634
[1m 8/54[0m [32mâ”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.7853 - loss: 0.2605
[1m12/54[0m [32mâ”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8247 - loss: 0.2389
[1m15/54[0m [32mâ”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 18ms/step - binary_accuracy: 0.8455 - loss: 0.2298
[1m19/54[0m [32mâ”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8659 - loss: 0.2200
[1m23/54[0m [32mâ”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8811 - loss: 0.2107
[1m26/54[0m [32mâ”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.8902 - loss: 0.2043
[1m30/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9002 - loss: 0.1991
[1m34/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9083 - loss: 0.1950
[1m38/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9151 - loss: 0.1910
[1m41/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9194 - loss: 0.1883
[1m44/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9233 - loss: 0.1861
[1m48/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”â”â”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9279 - loss: 0.1830
[1m52/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37mâ”[0m [1m0s[0m 17ms/step - binary_accuracy: 0.9320 - loss: 0.1812
[1m54/54[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m1s[0m 18ms/step - binary_accuracy: 0.9347 - loss: 0.1802 - val_binary_accuracy: 0.8333 - val_loss: 0.4291

[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
[1m1/1[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m0s[0m 109ms/step
predicted [0.09806072 0.8646991  0.9732269  0.90747195 0.45353758 0.33969462
 0.30574915 0.9779319  0.05915186 0.9444627  0.20188658 0.00451802
 0.01719326 0.9084212  0.2672423  0.256852  ]
predicted [0 1 1 1 0 0 0 1 0 1 0 0 0 1 0 0]
expected [False False False  True  True  True  True False  True  True  True False
 False  True  True  True]
accuracy: 0.375
confusion matrix: 
[[3 3]
 [7 3]]
              precision    recall  f1-score   support

       False       0.30      0.50      0.38         6
        True       0.50      0.30      0.38        10

    accuracy                           0.38        16
   macro avg       0.40      0.40      0.38        16
weighted avg       0.42      0.38      0.38        16

macro avg f1-score: 0.375
macro avg (UAR): 0.4
Sensitivity:  0.5
Specificity:  0.3
g-mean:  0.3872983346207417
-------- Model Performance ----------: 
accuracy:  [0.6875 0.4375 0.4375 0.5625 0.6875 0.4375 0.5    0.5625 0.4375 0.375 ]
gmean:  [0.68313005 0.31622777 0.40824829 0.54772256 0.63245553 0.4472136
 0.51639778 0.54772256 0.31622777 0.38729833]
f1_score:  [0.67611336 0.37662338 0.41700405 0.5465587  0.65367965 0.43529412
 0.5        0.5465587  0.37662338 0.375     ]
UAR:  [0.68333333 0.38333333 0.41666667 0.55       0.65       0.45
 0.53333333 0.55       0.38333333 0.4       ]
Cohen Kappa score:  [ 0.35483871 -0.24137931 -0.16129032  0.09677419  0.31034483 -0.09090909
  0.05882353  0.09677419 -0.24137931 -0.17647059]
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  28.992
step (sec):  14.496
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  14.496
Number of windows / instances:  76
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.975 0.99  0.988 0.989 0.987 0.436 0.718 0.745 0.665 0.707]
 [0.786 0.893 0.907 0.909 0.891 0.534 0.728 0.746 0.736 0.726]
 [0.888 0.872 0.895 0.894 0.877 0.348 0.672 0.718 0.699 0.67 ]
 [0.    0.5   0.632 0.    0.387 0.    0.5   0.646 0.    0.392]
 [0.788 0.876 0.895 0.851 0.876 0.409 0.692 0.748 0.616 0.687]
 [0.001 0.5   0.512 0.48  0.49  0.167 0.583 0.638 0.422 0.548]
 [0.    0.5   0.632 0.    0.387 0.    0.5   0.646 0.    0.392]]
last participant performance loaded in numpy array
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
Saving performance metrics in csv files...
Performance metrics already saved in csv files
---------------------------
KNN performance:
[[0.54254658 0.76916667 0.78035714 0.7530871  0.76715729 0.32882353
  0.65       0.81785714 0.44123148 0.64463037]
 [0.53171906 0.77       0.76428571 0.75669939 0.75956349 0.46203745
  0.725      0.76785714 0.59692983 0.69682207]
 [0.50510823 0.74916667 0.78035714 0.66123188 0.72727828 0.398
  0.7        0.69642857 0.69135914 0.69365079]
 [0.54492754 0.77166667 0.77678571 0.75879105 0.76765873 0.23374822
  0.61333333 0.7125     0.46533807 0.59571928]
 [0.60304076 0.78833333 0.84285714 0.71945403 0.78914141 0.53850639
  0.77583333 0.76785714 0.74694084 0.75691919]
 [0.53033333 0.76666667 0.76607143 0.75540219 0.76015873 0.66028428
  0.82916667 0.83035714 0.814163   0.82456349]
 [0.3606318  0.67833333 0.7375     0.57026688 0.66338384 0.31672464
  0.65833333 0.66428571 0.61681205 0.64914141]
 [0.81882353 0.9        0.94464286 0.84142136 0.89994172 0.74324589
  0.8775     0.87142857 0.86393052 0.86606782]
 [0.2547619  0.62833333 0.73571429 0.40944493 0.60417249 0.33907994
  0.66833333 0.7375     0.51255175 0.64224942]
 [0.29652991 0.65       0.63928571 0.62913467 0.63349206 0.25205929
  0.625      0.63035714 0.58987563 0.61119048]
 [0.18431897 0.6        0.65892857 0.46627104 0.58010101 0.473741
  0.72583333 0.79107143 0.60911226 0.7169031 ]
 [0.75264739 0.86916667 0.89285714 0.85147538 0.87212121 0.59731247
  0.79333333 0.82321429 0.72864156 0.78244422]
 [0.3648977  0.68583333 0.6875     0.66879555 0.67563492 0.2634359
  0.63583333 0.63392857 0.60605676 0.61861111]
 [0.16127451 0.595      0.66607143 0.47270846 0.57502886 0.55838907
  0.775      0.78928571 0.7497218  0.77154762]
 [0.2063784  0.61333333 0.68392857 0.44643175 0.58656566 0.37998137
  0.6975     0.70178571 0.68173846 0.68136364]
 [0.19277592 0.58416667 0.6375     0.48700181 0.56978355 0.36538462
  0.67       0.74821429 0.55716131 0.66282051]
 [0.51536975 0.76416667 0.75892857 0.75570161 0.75390693 0.37881588
  0.69083333 0.7125     0.6275533  0.67688312]
 [       nan 0.6        0.89642857 0.2        0.57230769 0.39422679
  0.69666667 0.70535714 0.67147896 0.68799423]
 [0.42024395 0.71333333 0.76607143 0.6331686  0.69483405 0.56858931
  0.7775     0.79285714 0.7597907  0.77878788]
 [0.17993213 0.59333333 0.69642857 0.38658647 0.55561272 0.24851268
  0.62166667 0.68392857 0.48071937 0.60583694]
 [0.12809524 0.56666667 0.74821429 0.26199777 0.54626041 0.14172464
  0.57083333 0.57678571 0.54880905 0.56238095]
 [0.25464903 0.6275     0.64464286 0.57025776 0.61575397 0.58892041
  0.79166667 0.80357143 0.76415964 0.7828355 ]
 [0.27173743 0.6475     0.65535714 0.56006399 0.61703297 0.45289855
  0.72166667 0.83928571 0.55865935 0.70576313]
 [0.29315385 0.65       0.64642857 0.62451834 0.63493506 0.641
  0.82333333 0.82678571 0.7660935  0.80374459]
 [0.59125864 0.79416667 0.80357143 0.77814562 0.7909127  0.71366667
  0.85833333 0.85714286 0.84657436 0.85242063]
 [0.33854515 0.67083333 0.67321429 0.62586368 0.64973665 0.39355556
  0.7        0.69285714 0.67871286 0.68539683]
 [0.59193277 0.78333333 0.86785714 0.66671844 0.77795038 0.55156522
  0.775      0.775      0.76816458 0.77253968]
 [0.13788734 0.56       0.57142857 0.46545431 0.52766595 0.39520548
  0.68666667 0.7375     0.60115085 0.67667222]
 [0.19745521 0.60166667 0.61785714 0.53876685 0.58382035 0.16462297
  0.585      0.59642857 0.50809956 0.55830808]
 [0.975      0.99       0.9875     0.98944272 0.98730159 0.43589761
  0.71833333 0.74464286 0.66450332 0.70695527]]
KNN mean:
[0.40503366 0.69938889 0.74428571 0.61014345 0.68464049 0.43266519
 0.71458333 0.74428571 0.65053446 0.70237212]
---------------------------
---------------------------
DT performance:
[[ 0.46559552  0.73416667  0.7375      0.70669043  0.72214286  0.55926309
   0.79666667  0.84464286  0.70400716  0.76679071]
 [ 0.49799379  0.77166667  0.77857143  0.72825217  0.76665945  0.58505465
   0.81166667  0.81428571  0.77775257  0.79180375]
 [ 0.48557143  0.73333333  0.74285714  0.7012003   0.72604257  0.27418415
   0.65416667  0.64464286  0.62032716  0.63746032]
 [ 0.1430303   0.61666667  0.60535714  0.6092227   0.58949856  0.08408964
   0.555       0.67678571  0.44340117  0.53132035]
 [ 0.53350374  0.795       0.80535714  0.77288896  0.79392496  0.47189855
   0.71166667  0.70535714  0.71834948  0.68366522]
 [ 0.441       0.69583333  0.69642857  0.72502006  0.69178571  0.45695095
   0.7525      0.7625      0.76731044  0.74345238]
 [ 0.15302553  0.575       0.6125      0.60689134  0.55953102  0.30382051
   0.64166667  0.64285714  0.58564282  0.62261905]
 [ 0.87124589  0.88166667  0.91607143  0.90380083  0.86914141  0.6133035
   0.80333333  0.79107143  0.83535016  0.78547619]
 [ 0.19431405  0.58166667  0.65892857  0.48880851  0.57578588  0.59525243
   0.76833333  0.78035714  0.70985116  0.75753247]
 [ 0.33330769  0.68333333  0.66964286  0.60999722  0.65972222  0.313
   0.67083333  0.67321429  0.63070046  0.65413781]
 [ 0.16260452  0.555       0.58392857  0.43503226  0.52941558  0.48882102
   0.71416667  0.73392857  0.7395326   0.69999278]
 [ 0.73718085  0.8725      0.87857143  0.84805002  0.86402237  0.48349795
   0.74916667  0.76607143  0.72895727  0.73950216]
 [ 0.373       0.66833333  0.67678571  0.64393929  0.66138889  0.133
   0.59083333  0.59821429  0.53845093  0.5818254 ]
 [-0.07397328  0.515       0.60892857  0.24258992  0.46693917  0.42728571
   0.67333333  0.67678571  0.66929011  0.66632395]
 [ 0.29705376  0.66        0.70892857  0.59468164  0.65056943  0.37156457
   0.67916667  0.70535714  0.65319304  0.67007937]
 [ 0.15623824  0.5775      0.61607143  0.52494872  0.55346154  0.39743697
   0.77        0.81428571  0.57971806  0.76871573]
 [ 0.76246499  0.8425      0.84464286  0.83002012  0.83769481  0.45528986
   0.74833333  0.7625      0.70996973  0.74349206]
 [ 0.10551821  0.61785714  0.8         0.1         0.51605228  0.47509837
   0.74833333  0.75        0.71807461  0.73742063]
 [ 0.42886494  0.775       0.80178571  0.68837998  0.74962121  0.56626219
   0.75083333  0.75892857  0.72257607  0.73257937]
 [ 0.42863284  0.67333333  0.75178571  0.57304467  0.64123515  0.36450374
   0.70333333  0.70714286  0.67087061  0.69089105]
 [ 0.03294118  0.49        0.65714286  0.27784327  0.47316683  0.0993913
   0.575       0.57857143  0.54510337  0.56809524]
 [ 0.38924487  0.75833333  0.775       0.69860483  0.7565873   0.39767201
   0.71416667  0.71964286  0.65572831  0.69551587]
 [ 0.48283352  0.73166667  0.75178571  0.71603463  0.73419913  0.34263995
   0.59666667  0.7         0.48321782  0.58885947]
 [ 0.26897993  0.625       0.62678571  0.54956741  0.61186508  0.4665942
   0.75333333  0.75357143  0.71952023  0.73937951]
 [ 0.38412722  0.765       0.77142857  0.6373865   0.75714286  0.60823188
   0.81666667  0.81785714  0.8361391   0.81452381]
 [ 0.58482051  0.75        0.75        0.73785692  0.74194444  0.28902899
   0.67166667  0.67321429  0.62487463  0.6606746 ]
 [ 0.53352941  0.785       0.84285714  0.67590827  0.78057609  0.57648718
   0.76666667  0.76071429  0.7514291   0.74993506]
 [ 0.20651931  0.5775      0.61964286  0.53112595  0.52702214  0.63608881
   0.82916667  0.82857143  0.78941211  0.81531746]
 [ 0.26758398  0.635       0.64107143  0.62170703  0.61616883  0.27081643
   0.62416667  0.62321429  0.63051749  0.61142857]
 [ 0.78610303  0.8925      0.90714286  0.908953    0.89076479  0.53393375
   0.72833333  0.74642857  0.73558714  0.72632035]]
DT mean:
[0.3810952  0.6945119  0.72791667 0.62294823 0.67713575 0.42134874
 0.71230556 0.72702381 0.67649516 0.69917102]
---------------------------
---------------------------
RF performance:
[[ 0.64764935  0.74833333  0.76071429  0.74719629  0.73787157  0.34956522
   0.74        0.80892857  0.35328486  0.71373515]
 [ 0.705       0.73833333  0.7375      0.68498929  0.7257684   0.53849942
   0.69        0.70357143  0.6589835   0.65927323]
 [ 0.55855678  0.74083333  0.75        0.69548965  0.7322583   0.50982051
   0.8         0.78928571  0.66185917  0.78281746]
 [ 0.38192754  0.72        0.72142857  0.7522485   0.71015873  0.43338936
   0.62333333  0.72857143  0.36612448  0.62737096]
 [ 0.55572768  0.7875      0.81428571  0.75653641  0.79243506  0.56066667
   0.79166667  0.7875      0.79632517  0.77683983]
 [ 0.53556522  0.70416667  0.70178571  0.67690978  0.69507937  0.58024476
   0.80833333  0.81428571  0.80113083  0.79876984]
 [ 0.48040373  0.76333333  0.79107143  0.60490117  0.74655123  0.38733333
   0.66666667  0.66607143  0.67896205  0.65293651]
 [ 0.9302521   0.96666667  0.97321429  0.99128709  0.96424242  0.54400789
   0.7875      0.80535714  0.80034625  0.75535909]
 [ 0.28292609  0.69333333  0.7375      0.54377559  0.6734596   0.4159944
   0.63        0.68928571  0.70448732  0.59520202]
 [ 0.29983191  0.79166667  0.7875      0.63752273  0.78123016  0.24015385
   0.675       0.66785714  0.57069245  0.63884921]
 [ 0.02900841  0.56166667  0.61964286  0.40969975  0.5369733   0.58063241
   0.815       0.83928571  0.70106242  0.8094228 ]
 [ 0.75137409  0.87583333  0.89464286  0.88751492  0.87197691  0.62442577
   0.6575      0.69464286  0.65764258  0.64628066]
 [ 0.29517205  0.635       0.65535714  0.56770379  0.61411089  0.05448718
   0.61166667  0.61785714  0.56438088  0.60390332]
 [-0.00522592  0.575       0.67857143  0.35846705  0.55958042  0.45228205
   0.68583333  0.69107143  0.68304235  0.6816811 ]
 [ 0.05103336  0.5         0.60178571  0.3763429   0.47880619  0.34873799
   0.6775      0.72857143  0.57270569  0.66202214]
 [ 0.22070732  0.58333333  0.60714286  0.58968415  0.57416667  0.47546373
   0.715       0.79285714  0.57463484  0.71621212]
 [ 0.85294118  0.91083333  0.90892857  0.8730722   0.90579365  0.44507171
   0.77916667  0.7875      0.78590936  0.77370851]
 [        nan  0.6         0.84285714  0.1         0.55587912  0.41674459
   0.7125      0.72321429  0.75151414  0.70795455]
 [ 0.46284657  0.77333333  0.83214286  0.80124847  0.75952214  0.56502277
   0.68583333  0.70714286  0.71121519  0.67849206]
 [ 0.29885154  0.71666667  0.7875      0.51691006  0.68704129  0.12906355
   0.6425      0.6875      0.39318849  0.62852092]
 [ 0.05616756  0.55        0.72678571  0.20597108  0.56794872  0.21666667
   0.55        0.55535714  0.64499034  0.53111111]
 [ 0.33837762  0.60666667  0.62142857  0.5864769   0.58163781  0.53205175
   0.80416667  0.81607143  0.74634297  0.79038961]
 [ 0.32008159  0.67166667  0.69821429  0.69890103  0.65563492  0.38194617
   0.58        0.725       0.47722156  0.54996836]
 [ 0.31113043  0.6625      0.66071429  0.66801999  0.63717172  0.527
   0.72416667  0.72857143  0.7578116   0.71630952]
 [ 0.57707161  0.79666667  0.81964286  0.77084565  0.79580808  0.60301932
   0.76666667  0.76071429  0.73952829  0.75198413]
 [ 0.3970474   0.65        0.65535714  0.71917239  0.62676046  0.29136232
   0.74833333  0.75        0.63346464  0.73191919]
 [ 0.54528011  0.8         0.81964286  0.55666177  0.7832684   0.58256522
   0.82916667  0.83035714  0.77934531  0.82555556]
 [ 0.2138796   0.60666667  0.6125      0.41170338  0.57517316  0.50734802
   0.79833333  0.80535714  0.69927702  0.79035714]
 [ 0.25891266  0.67416667  0.68214286  0.52423345  0.66938312  0.34659552
   0.49333333  0.49642857  0.63706815  0.47484127]
 [ 0.88766046  0.87166667  0.89464286  0.89371817  0.87660173  0.34841094
   0.67166667  0.71785714  0.69899837  0.66984848]]
RF mean:
[0.42207442 0.70919444 0.7464881  0.62024012 0.69574312 0.43295244
 0.70536111 0.73053571 0.65338468 0.69138786]
---------------------------
---------------------------
SVM performance:
[[0.         0.5        0.54107143 0.         0.35058275 0.
  0.5        0.76428571 0.         0.43278388]
 [0.11988446 0.55416667 0.59464286 0.22320508 0.45815851 0.
  0.5        0.63214286 0.         0.38682984]
 [0.         0.5        0.59107143 0.         0.37109557 0.50756522
  0.75416667 0.74642857 0.68632434 0.72690476]
 [0.         0.5        0.55357143 0.         0.35571096 0.
  0.5        0.73571429 0.         0.42380952]
 [0.11538462 0.55       0.68392857 0.17320508 0.47674825 0.56656522
  0.78583333 0.78392857 0.74393216 0.76345599]
 [0.03636364 0.51666667 0.54285714 0.10773503 0.39106061 0.59749407
  0.79583333 0.80357143 0.77189802 0.79047619]
 [0.         0.5        0.66071429 0.         0.3974359  0.35552991
  0.67916667 0.6625     0.59513421 0.63865079]
 [0.         0.5        0.71071429 0.         0.41501832 0.15714286
  0.57333333 0.64107143 0.21493909 0.48298368]
 [0.         0.5        0.71071429 0.         0.41501832 0.
  0.5        0.67321429 0.         0.4018315 ]
 [0.07272727 0.53333333 0.54285714 0.11547005 0.39636364 0.
  0.5        0.52857143 0.         0.34545455]
 [0.         0.5        0.66071429 0.         0.3974359  0.
  0.5        0.64642857 0.         0.39213287]
 [0.         0.5        0.64642857 0.         0.39213287 0.
  0.5        0.63214286 0.         0.38682984]
 [0.         0.5        0.56607143 0.         0.36083916 0.
  0.5        0.55357143 0.         0.35571096]
 [0.         0.5        0.71071429 0.         0.41501832 0.
  0.5        0.54107143 0.         0.35058275]
 [0.         0.5        0.69821429 0.         0.41062271 0.
  0.5        0.63214286 0.         0.38682984]
 [0.         0.5        0.60357143 0.         0.37622378 0.
  0.5        0.68571429 0.         0.40622711]
 [0.         0.5        0.57857143 0.         0.36596737 0.
  0.5        0.60357143 0.         0.37622378]
 [       nan 0.6        0.89642857 0.2        0.57230769 0.
  0.5        0.57857143 0.         0.36596737]
 [0.         0.5        0.69821429 0.         0.41062271 0.
  0.5        0.55357143 0.         0.35571096]
 [0.         0.5        0.71071429 0.         0.41501832 0.
  0.5        0.63214286 0.         0.38682984]
 [0.         0.5        0.77857143 0.         0.43727106 0.
  0.5        0.52857143 0.         0.34545455]
 [0.         0.5        0.54107143 0.         0.35058275 0.
  0.5        0.61785714 0.         0.38152681]
 [0.         0.5        0.60357143 0.         0.37622378 0.
  0.5        0.75       0.         0.4282967 ]
 [0.         0.5        0.51428571 0.         0.33909091 0.32741715
  0.65416667 0.6875     0.4865044  0.6060101 ]
 [0.         0.5        0.57857143 0.         0.36596737 0.02005929
  0.50833333 0.51607143 0.1510363  0.3919697 ]
 [0.         0.5        0.52857143 0.         0.34545455 0.43051948
  0.70833333 0.72678571 0.63475855 0.68351371]
 [0.         0.5        0.71071429 0.         0.41501832 0.26636232
  0.63333333 0.63571429 0.60698621 0.62132395]
 [0.         0.5        0.63214286 0.         0.38682984 0.
  0.5        0.61785714 0.         0.38152681]
 [0.         0.5        0.60357143 0.         0.37622378 0.
  0.5        0.56607143 0.         0.36083916]
 [0.         0.5        0.63214286 0.         0.38682984 0.
  0.5        0.64642857 0.         0.39213287]]
SVM mean:
[0.01187448 0.50847222 0.63416667 0.02732051 0.39742913 0.10762185
 0.55308333 0.64410714 0.16305044 0.45829401]
---------------------------
---------------------------
GBM performance:
[[ 0.55806061  0.7775      0.7875      0.73972871  0.7641811   0.35215686
   0.61666667  0.81964286  0.30597108  0.58867799]
 [ 0.60222567  0.78666667  0.79285714  0.75447062  0.77947691  0.48821715
   0.765       0.79107143  0.67320693  0.74770396]
 [ 0.47384927  0.7275      0.76607143  0.64488941  0.71486846  0.41767001
   0.7125      0.70892857  0.69404885  0.70115079]
 [ 0.41060163  0.705       0.70892857  0.61442652  0.68762626 -0.06019608
   0.47166667  0.67142857  0.06324555  0.42279221]
 [ 0.67841676  0.84        0.88035714  0.78989795  0.84808081  0.46646377
   0.73666667  0.73035714  0.68598719  0.70821789]
 [ 0.34548718  0.6875      0.68392857  0.65419328  0.67789683  0.51639394
   0.75666667  0.76607143  0.77185668  0.75015873]
 [ 0.1930877   0.59        0.69285714  0.37703577  0.56651515  0.26294129
   0.63333333  0.62857143  0.59301999  0.61706349]
 [ 0.76168067  0.875       0.91964286  0.90341913  0.87085082  0.66010313
   0.83083333  0.84642857  0.7483421   0.81167222]
 [ 0.19623282  0.59666667  0.7125      0.44082882  0.5987296   0.27218918
   0.62166667  0.72678571  0.43632646  0.59752914]
 [ 0.3017094   0.65416667  0.6375      0.56142992  0.6159127   0.33336364
   0.66666667  0.67142857  0.5913517   0.64699495]
 [-0.07418875  0.49333333  0.59464286  0.16652511  0.45285548  0.52211639
   0.75166667  0.80178571  0.66362868  0.75550505]
 [ 0.78610303  0.88583333  0.90714286  0.84943994  0.89076479  0.33358083
   0.64916667  0.69821429  0.56044618  0.64288961]
 [ 0.19836364  0.6         0.62678571  0.46212163  0.56601565  0.14556591
   0.57166667  0.58035714  0.51755635  0.5487987 ]
 [ 0.21003232  0.58333333  0.7375      0.31462644  0.54898934  0.29928571
   0.64833333  0.65535714  0.65261114  0.63743506]
 [ 0.09339367  0.55666667  0.72142857  0.27334092  0.51275225  0.2305507
   0.61416667  0.66428571  0.51869557  0.6016181 ]
 [-0.00988577  0.47833333  0.56428571  0.10163978  0.39447552  0.21274725
   0.615       0.72678571  0.38782051  0.59522145]
 [ 0.73560163  0.8675      0.86964286  0.85718034  0.86406566  0.39664282
   0.725       0.75        0.69425735  0.7212482 ]
 [        nan  0.59285714  0.88392857  0.2         0.56849817  0.50021325
   0.73916667  0.74642857  0.73631447  0.73496032]
 [ 0.47326546  0.72333333  0.80357143  0.59873355  0.70861305  0.62402409
   0.80666667  0.82678571  0.73797978  0.79253247]
 [ 0.16386555  0.57833333  0.73571429  0.33723579  0.53978355  0.25626609
   0.61583333  0.69642857  0.44331645  0.59270396]
 [ 0.00666667  0.5         0.75        0.07071068  0.46053114  0.14548718
   0.575       0.57678571  0.50677531  0.55195887]
 [ 0.49902676  0.7         0.71964286  0.69207177  0.67760823  0.38226775
   0.67666667  0.72142857  0.60375006  0.6688961 ]
 [ 0.41902409  0.70083333  0.74821429  0.57667424  0.67873932  0.26621849
   0.58333333  0.75357143  0.32760209  0.56178655]
 [ 0.29192754  0.67083333  0.67678571  0.61175359  0.66305556  0.54786871
   0.76583333  0.76428571  0.75150891  0.75087302]
 [ 0.41438763  0.68916667  0.71071429  0.58440071  0.65821789  0.62713043
   0.85416667  0.85535714  0.8149501   0.85095238]
 [ 0.65049407  0.82083333  0.83035714  0.79939243  0.81833333  0.32969565
   0.69416667  0.69642857  0.64351122  0.67616522]
 [ 0.37288515  0.67        0.77678571  0.44715948  0.66543124  0.47787291
   0.74166667  0.73571429  0.70483888  0.72746032]
 [ 0.14685814  0.55833333  0.64642857  0.29728981  0.50434732  0.52209937
   0.75083333  0.7875      0.67492809  0.73861666]
 [ 0.18394642  0.58666667  0.64107143  0.44177928  0.56535548  0.2763147
   0.62083333  0.64107143  0.60517645  0.62227633]
 [ 0.78796639  0.87583333  0.89464286  0.85113775  0.87621934  0.40912091
   0.69166667  0.74821429  0.61608194  0.68664336]]
GBM mean:
[0.37486501 0.67906746 0.74738095 0.53378444 0.6579597  0.3738124
 0.68341667 0.72625    0.59083687 0.6683501 ]
---------------------------
---------------------------
BDDAE performance:
[[-1.37207222e-01  4.28571429e-01  4.37500000e-01  3.72841412e-01
   4.11317230e-01  6.51592852e-02  5.37500000e-01  6.81250000e-01
   3.69482120e-01  5.25823397e-01]
 [-2.84321692e-03  4.96825397e-01  5.00000000e-01  4.82747426e-01
   4.89964773e-01  3.40337691e-01  6.73333333e-01  6.87500000e-01
   6.45052741e-01  6.58455832e-01]
 [ 1.08553284e-01  5.52380952e-01  5.62500000e-01  5.31350033e-01
   5.46089301e-01  3.00000000e-01  6.50000000e-01  6.50000000e-01
   6.32115368e-01  6.41881204e-01]
 [ 3.85420530e-01  6.92063492e-01  7.00000000e-01  6.76676327e-01
   6.87620177e-01  3.39860140e-02  5.16666667e-01  6.62500000e-01
   3.56118040e-01  5.12334342e-01]
 [ 4.90324419e-01  7.58333333e-01  7.43750000e-01  7.44634279e-01
   7.35906119e-01  1.49353147e-01  5.76984127e-01  5.68750000e-01
   5.71619094e-01  5.67990196e-01]
 [ 2.75000000e-01  6.37500000e-01  6.37500000e-01  5.87421782e-01
   6.13707024e-01 -3.00058760e-01  3.45238095e-01  3.31250000e-01
   2.47194801e-01  3.04709100e-01]
 [-1.78021575e-01  4.17272727e-01  5.06250000e-01  2.30497485e-01
   4.02131496e-01  1.12500000e-01  5.56250000e-01  5.56250000e-01
   5.40125539e-01  5.48466453e-01]
 [ 4.50521480e-01  7.10000000e-01  7.81250000e-01  6.72859898e-01
   7.19091054e-01  2.53764929e-01  6.26984127e-01  6.25000000e-01
   5.90002539e-01  6.13254885e-01]
 [ 4.24084383e-01  6.93636364e-01  7.81250000e-01  6.37512128e-01
   7.01723089e-01  2.53758693e-01  6.08181818e-01  7.31250000e-01
   4.76377820e-01  6.04622711e-01]
 [ 2.50000000e-01  6.25000000e-01  6.25000000e-01  6.00161517e-01
   6.13200561e-01  4.87500000e-01  7.43750000e-01  7.43750000e-01
   7.25694887e-01  7.35913899e-01]
 [ 1.45095371e-01  5.75454545e-01  6.56250000e-01  4.28128261e-01
   5.52121785e-01  1.50768775e-01  5.73333333e-01  6.25000000e-01
   4.47844015e-01  5.48194087e-01]
 [ 1.76951452e-01  5.88333333e-01  6.18750000e-01  5.62564367e-01
   5.84203009e-01  3.81374292e-01  7.00000000e-01  7.00000000e-01
   6.82601056e-01  6.82581469e-01]
 [-2.04920443e-02  4.90476190e-01  4.87500000e-01  4.71948463e-01
   4.78727957e-01  2.35659796e-01  6.23015873e-01  6.18750000e-01
   5.86038786e-01  6.00918919e-01]
 [-8.84194067e-02  4.64545455e-01  5.93750000e-01  1.94235879e-01
   4.35335341e-01  4.25818581e-01  7.15873016e-01  7.12500000e-01
   7.09119931e-01  7.08704057e-01]
 [-1.02515077e-01  4.45454545e-01  5.37500000e-01  2.81824390e-01
   4.24329617e-01  1.98466176e-01  6.00000000e-01  6.12500000e-01
   5.88914417e-01  5.93256159e-01]
 [ 4.66024494e-02  5.28333333e-01  5.31250000e-01  4.33867926e-01
   4.85161271e-01  8.19422667e-02  5.40000000e-01  6.37500000e-01
   3.72789469e-01  5.15235071e-01]
 [ 5.22479255e-01  7.66666667e-01  7.62500000e-01  7.53231590e-01
   7.56232844e-01  2.19323124e-01  6.05000000e-01  6.31250000e-01
   5.57643958e-01  5.89612899e-01]
 [ 4.46249045e-01  7.42857143e-01  8.87500000e-01  6.28470584e-01
   7.17797707e-01 -3.80896370e-02  4.78571429e-01  4.81250000e-01
   4.55321195e-01  4.67564222e-01]
 [ 2.91017167e-01  6.55454545e-01  6.68750000e-01  6.41743198e-01
   6.36330460e-01  2.92290843e-01  6.50000000e-01  6.43750000e-01
   6.35759863e-01  6.38012546e-01]
 [ 1.97317846e-01  5.90000000e-01  7.06250000e-01  4.14118096e-01
   5.76721694e-01 -3.25253528e-02  4.81666667e-01  5.31250000e-01
   3.86820668e-01  4.61887828e-01]
 [ 8.13986014e-02  5.37500000e-01  6.81250000e-01  3.44777408e-01
   5.30952217e-01  3.75000000e-02  5.18750000e-01  5.18750000e-01
   4.90088185e-01  5.04902170e-01]
 [-5.36165895e-02  4.75396825e-01  4.93750000e-01  4.09892624e-01
   4.60425892e-01  1.00489315e-01  5.50000000e-01  5.87500000e-01
   5.01138860e-01  5.36974342e-01]
 [ 1.10049115e-01  5.60000000e-01  5.87500000e-01  4.99539216e-01
   5.45132606e-01  3.38632479e-01  6.45833333e-01  7.93750000e-01
   5.06284898e-01  6.55683141e-01]
 [-1.62500000e-01  4.18750000e-01  4.18750000e-01  3.52519072e-01
   3.95682381e-01  2.46833003e-01  6.24603175e-01  6.31250000e-01
   6.10869963e-01  6.19001146e-01]
 [-2.23491972e-02  4.89682540e-01  4.93750000e-01  4.65222771e-01
   4.78649233e-01  1.25000000e-01  5.62500000e-01  5.62500000e-01
   5.42088836e-01  5.53235426e-01]
 [ 2.50000000e-01  6.25000000e-01  6.25000000e-01  5.96702105e-01
   6.11801698e-01  1.97169462e-01  5.97619048e-01  6.06250000e-01
   5.83837529e-01  5.93683114e-01]
 [ 4.93292399e-01  7.46363636e-01  7.93750000e-01  7.11211472e-01
   7.39846434e-01  4.00000000e-01  7.00000000e-01  7.00000000e-01
   6.73423670e-01  6.88414248e-01]
 [ 1.42022113e-02  5.08333333e-01  5.56250000e-01  4.31345442e-01
   4.96393746e-01  1.68866397e-01  5.81666667e-01  6.31250000e-01
   5.09399063e-01  5.73714600e-01]
 [ 2.53372612e-01  6.26666667e-01  6.25000000e-01  5.85865366e-01
   6.00599692e-01  3.71735791e-01  6.83333333e-01  6.93750000e-01
   6.75925291e-01  6.84203516e-01]
 [ 6.12683136e-04  5.00000000e-01  5.12500000e-01  4.80264423e-01
   4.90345534e-01  1.67153439e-01  5.83333333e-01  6.37500000e-01
   4.22389889e-01  5.48138586e-01]]
BDDAE mean:
[0.15481933 0.57822842 0.61708333 0.5074725  0.56391806 0.19215699
 0.59499958 0.62645833 0.53640275 0.58257899]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.54107143 0.         0.35058275 0.
  0.5        0.76428571 0.         0.43278388]
 [0.         0.5        0.55357143 0.         0.35571096 0.
  0.5        0.63214286 0.         0.38682984]
 [0.         0.5        0.59107143 0.         0.37109557 0.
  0.5        0.51428571 0.         0.33909091]
 [0.         0.5        0.55357143 0.         0.35571096 0.
  0.5        0.73571429 0.         0.42380952]
 [0.         0.5        0.64642857 0.         0.39213287 0.
  0.5        0.54107143 0.         0.35058275]
 [0.         0.5        0.52857143 0.         0.34545455 0.
  0.5        0.54107143 0.         0.35058275]
 [0.         0.5        0.66071429 0.         0.3974359  0.
  0.5        0.47142857 0.         0.32      ]
 [0.         0.5        0.71071429 0.         0.41501832 0.
  0.5        0.59107143 0.         0.37109557]
 [0.         0.5        0.71071429 0.         0.41501832 0.
  0.5        0.67321429 0.         0.4018315 ]
 [0.         0.5        0.51428571 0.         0.33909091 0.
  0.5        0.52857143 0.         0.34545455]
 [0.         0.5        0.66071429 0.         0.3974359  0.
  0.5        0.64642857 0.         0.39213287]
 [0.         0.5        0.64642857 0.         0.39213287 0.
  0.5        0.63214286 0.         0.38682984]
 [0.         0.5        0.56607143 0.         0.36083916 0.
  0.5        0.55357143 0.         0.35571096]
 [0.         0.5        0.71071429 0.         0.41501832 0.
  0.5        0.54107143 0.         0.35058275]
 [0.         0.5        0.69821429 0.         0.41062271 0.
  0.5        0.63214286 0.         0.38682984]
 [0.         0.5        0.60357143 0.         0.37622378 0.
  0.5        0.68571429 0.         0.40622711]
 [0.         0.5        0.57857143 0.         0.36596737 0.
  0.5        0.60357143 0.         0.37622378]
 [       nan 0.6        0.89642857 0.2        0.57230769 0.
  0.5        0.57857143 0.         0.36596737]
 [0.         0.5        0.69821429 0.         0.41062271 0.
  0.5        0.55357143 0.         0.35571096]
 [0.         0.5        0.71071429 0.         0.41501832 0.
  0.5        0.63214286 0.         0.38682984]
 [0.         0.5        0.77857143 0.         0.43727106 0.
  0.5        0.52857143 0.         0.34545455]
 [0.         0.5        0.54107143 0.         0.35058275 0.
  0.5        0.61785714 0.         0.38152681]
 [0.         0.5        0.60357143 0.         0.37622378 0.
  0.5        0.75       0.         0.4282967 ]
 [0.         0.5        0.51428571 0.         0.33909091 0.
  0.5        0.55357143 0.         0.35571096]
 [0.         0.5        0.57857143 0.         0.36596737 0.
  0.5        0.51428571 0.         0.33909091]
 [0.         0.5        0.52857143 0.         0.34545455 0.
  0.5        0.54107143 0.         0.35058275]
 [0.         0.5        0.71071429 0.         0.41501832 0.
  0.5        0.51428571 0.         0.33909091]
 [0.         0.5        0.63214286 0.         0.38682984 0.
  0.5        0.61785714 0.         0.38152681]
 [0.         0.5        0.60357143 0.         0.37622378 0.
  0.5        0.56607143 0.         0.36083916]
 [0.         0.5        0.63214286 0.         0.38682984 0.
  0.5        0.64642857 0.         0.39213287]]
DUMMY mean:
[0.         0.50333333 0.63011905 0.00666667 0.3877644  0.
 0.5        0.59672619 0.         0.37197863]
---------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_61
-------------------------------------
----- RESULTS from .csv files ------
-------------------------------------
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.405 0.703 0.739 0.624 0.689 0.434 0.715 0.746 0.65  0.703]
 [0.381 0.695 0.728 0.623 0.677 0.421 0.712 0.727 0.676 0.699]
 [0.422 0.713 0.743 0.638 0.701 0.434 0.705 0.731 0.65  0.691]
 [0.012 0.505 0.625 0.021 0.391 0.111 0.555 0.646 0.169 0.461]
 [0.375 0.682 0.743 0.545 0.661 0.369 0.681 0.726 0.586 0.666]
 [0.155 0.578 0.617 0.507 0.564 0.192 0.595 0.626 0.536 0.583]
 [0.    0.5   0.621 0.    0.381 0.    0.5   0.597 0.    0.372]]
-------------------------------------------------------------
Standard Deviation of each metric:
[[0.216 0.105 0.098 0.159 0.114 0.158 0.08  0.077 0.114 0.082]
 [0.226 0.107 0.094 0.179 0.118 0.15  0.074 0.068 0.094 0.074]
 [0.25  0.111 0.098 0.176 0.116 0.143 0.085 0.079 0.123 0.088]
 [0.032 0.014 0.07  0.056 0.032 0.194 0.096 0.079 0.277 0.136]
 [0.245 0.12  0.092 0.23  0.136 0.16  0.086 0.071 0.162 0.092]
 [0.211 0.106 0.115 0.145 0.109 0.161 0.081 0.087 0.116 0.086]
 [0.    0.    0.072 0.    0.027 0.    0.    0.074 0.    0.029]]
-------------------------------------------------------------
std_dev/mean [percentage]:
[[ 53.  15.  13.  25.  17.  36.  11.  10.  18.  12.]
 [ 59.  15.  13.  29.  17.  36.  10.   9.  14.  11.]
 [ 59.  16.  13.  28.  17.  33.  12.  11.  19.  13.]
 [269.   3.  11. 262.   8. 174.  17.  12. 164.  29.]
 [ 65.  18.  12.  42.  21.  43.  13.  10.  28.  14.]
 [136.  18.  19.  29.  19.  84.  14.  14.  22.  15.]
 [  0.   0.  12.   0.   7.   0.   0.  12.   0.   8.]]
-------------------------------------
Current folder: C:\Users\Javier\Dropbox\c_sldl_1_2_61
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  28.992
step (sec):  14.496
overlap:  True
perc. of overlap:  50.0
overlap duration (sec):  14.496
Number of windows / instances:  76
Elapsed time: 1480.9840208093326 minutes
Elapsed time: 24.683067013488877 hours
