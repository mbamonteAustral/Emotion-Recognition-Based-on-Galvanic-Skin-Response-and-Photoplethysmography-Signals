2024-05-22 16:20:00.994113: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:20:01.046129: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:20:01.046367: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:20:01.046735: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-22 16:20:01.047393: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:20:01.047562: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:20:01.047717: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:20:01.488338: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:20:01.488550: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:20:01.488714: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:20:01.488848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
Window size (sec):  11.0
step (sec):  5.5
overlap:  True
perc. of overlap:  50.0
Nearest multiple of 16 to 11000 is: 11008
Nearest multiple of 16 to 5500 is: 5504
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:



Use `df.loc[row_indexer, "col"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

Use `df.loc[row_indexer, "col"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

Use `df.loc[row_indexer, "col"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

Use `df.loc[row_indexer, "col"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

Use `df.loc[row_indexer, "col"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

Use `df.loc[row_indexer, "col"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

Use `df.loc[row_indexer, "col"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_12/functions.py:821: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
----- Extracting features...
/usr/local/lib/python3.10/dist-packages/heartpy/analysis.py:522: UserWarning: Short signal.

---------Warning:---------
too few peak-peak intervals for (reliable) frequency domain measure computation, frequency output measures are still computed but treat them with caution!

HF is usually computed over a minimum of 1 minute of good signal. LF is usually computed over a minimum of 2 minutes of good signal.VLF is usually computed over a minimum of 5 minutes of good signal.The LF/HF ratio is usually computed over minimum 24 hours, although an absolute minimum of 5 min has also been suggested.

For more info see: 
Shaffer, F., Ginsberg, J.P. (2017), An Overview of Heart Rate Variability Metrics and Norms.

Task Force of Pacing and Electrophysiology (1996), Heart Rate Variability, in: European Heart Journal, vol.17, issue 3, pp354-381

This warning will not repeat
  warnings.warn(msg, UserWarning)
Class Imbalance test (arousal)....
Multiclass distribution
Class=0, n=76 (34.703%)
Class=1, n=143 (65.297%)
Class Imbalance test (valence)....
Multiclass distribution
Class=0, n=84 (38.356%)
Class=1, n=135 (61.644%)
------------- Evaluating model --------------
------------- Evaluating model --------------
------------- Evaluating model --------------
------------- Evaluating model --------------
------------- Evaluating model --------------
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  11.008
step (sec):  5.504
overlap:  True
perc. of overlap:  50.0
Number of windows / instances:  219
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.463 0.732 0.744 0.72  0.727 0.41  0.7   0.735 0.68  0.701]
 [0.562 0.785 0.795 0.776 0.782 0.514 0.763 0.79  0.73  0.759]
 [0.496 0.744 0.772 0.73  0.746 0.47  0.716 0.758 0.628 0.72 ]
 [0.    0.5   0.616 0.    0.381 0.    0.5   0.653 0.    0.395]
 [0.334 0.65  0.726 0.539 0.634 0.099 0.54  0.671 0.273 0.487]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.5   0.616 0.    0.381 0.    0.5   0.653 0.    0.395]]
participant performance loaded
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
---------------------------
KNN performance:
[[0.29049377 0.64462121 0.64848485 0.63799181 0.64274846 0.38912195
  0.69223039 0.79437229 0.61212232 0.68515957]
 [0.27044592 0.63623932 0.63463203 0.634193   0.63252648 0.46344582
  0.72471001 0.7530303  0.70741    0.72813791]
 [0.45717919 0.72264957 0.7538961  0.68243429 0.71823443 0.41169911
  0.70568182 0.70692641 0.69605992 0.70177229]
 [0.36239226 0.68136364 0.68073593 0.67597174 0.67829345 0.60298429
  0.79939394 0.80324675 0.78905709 0.79829605]
 [0.27868449 0.63379121 0.67575758 0.5841846  0.62653188 0.20366972
  0.60181818 0.60238095 0.59099578 0.59699832]
 [0.58980437 0.77333333 0.83593074 0.748182   0.79104823 0.40872393
  0.70509768 0.72186147 0.69085563 0.70090642]
 [0.28650782 0.63220238 0.71277056 0.58187163 0.63387087 0.34410979
  0.66154762 0.73939394 0.59407586 0.66659723]
 [0.43378223 0.71772727 0.71666667 0.70978447 0.71322464 0.2400688
  0.62015152 0.62099567 0.6123804  0.6159694 ]
 [0.06056834 0.52861722 0.58051948 0.45340456 0.52164626 0.5107126
  0.74729853 0.77597403 0.72715373 0.75096253]
 [0.61446879 0.80678419 0.81774892 0.80218724 0.8060231  0.49027546
  0.74321581 0.76753247 0.72579428 0.74171961]
 [0.22276637 0.60950855 0.62099567 0.60135104 0.60923561 0.38348186
  0.68980769 0.6991342  0.68009462 0.68855722]
 [       nan        nan        nan        nan        nan        nan
         nan        nan        nan        nan]
 [0.2071874  0.60142857 0.68419913 0.51038904 0.59394206 0.23387786
  0.61857143 0.6534632  0.56588501 0.61158234]
 [0.48506719 0.73659188 0.75779221 0.72374873 0.73981571 0.39432677
  0.69434524 0.74480519 0.65194809 0.68908208]
 [0.63432574 0.8167094  0.82142857 0.81194898 0.81541745 0.38482727
  0.68643926 0.71255411 0.66941309 0.6890367 ]
 [0.06822529 0.52653509 0.83549784 0.16107063 0.5131675  0.39563718
  0.69946581 0.7034632  0.69340692 0.69534224]
 [0.263276   0.625      0.71255411 0.54287348 0.61573481 0.44958786
  0.72608974 0.72619048 0.71783557 0.72089931]
 [0.3794474  0.67785714 0.7530303  0.62228607 0.67978543 0.40043082
  0.69384158 0.73073593 0.67437762 0.69724145]
 [0.06833739 0.52660539 0.73484848 0.29842491 0.51757146 0.12244557
  0.56136364 0.56147186 0.54017649 0.55107485]
 [0.18494561 0.59204545 0.59415584 0.58548474 0.59002389 0.24831876
  0.61940476 0.67099567 0.58021748 0.61548583]
 [0.15726502 0.57940171 0.59393939 0.55709068 0.57278097 0.16863661
  0.58432598 0.7030303  0.46017406 0.57764982]
 [0.47041127 0.7359188  0.73896104 0.73049726 0.73312494 0.63387298
  0.81681818 0.81709957 0.81392401 0.81588306]
 [0.1469797  0.57325758 0.57965368 0.55908315 0.56886211 0.19734535
  0.59886752 0.60281385 0.59403972 0.59689978]
 [0.52359876 0.75717262 0.80367965 0.74310124 0.76036956 0.63204857
  0.81454545 0.81709957 0.80524447 0.81304331]
 [0.06414643 0.53422619 0.58961039 0.44985472 0.52167077 0.48767441
  0.73955281 0.76320346 0.71918185 0.7397379 ]
 [0.20578818 0.59935134 0.64458874 0.55126047 0.59402403 0.22887532
  0.61444444 0.62099567 0.5959821  0.60757329]
 [0.46330747 0.73203602 0.74393939 0.7200685  0.72722665 0.41012042
  0.69964286 0.73528139 0.68042027 0.70059869]]
KNN mean:
[0.31497701 0.65388366 0.70253913 0.60302842 0.65065003 0.37831996
 0.686872   0.71338661 0.66108563 0.68446951]
---------------------------
---------------------------
DT performance:
[[ 0.30914259  0.64916667  0.6534632   0.64338884  0.64474179  0.41583452
   0.70926471  0.78030303  0.70969736  0.70649535]
 [ 0.28300986  0.63638889  0.64458874  0.61533012  0.63413144  0.36187773
   0.68199023  0.70757576  0.62632208  0.67813452]
 [ 0.46066656  0.73707265  0.74393939  0.74501166  0.73179579  0.49849021
   0.72712121  0.72597403  0.72165133  0.72311582]
 [ 0.21346065  0.63530303  0.63506494  0.61922512  0.63079498  0.48829964
   0.75886364  0.75822511  0.74707384  0.75685756]
 [ 0.28616204  0.66265263  0.67575758  0.63881895  0.65574318  0.21290323
   0.65181818  0.6525974   0.60433847  0.64896481]
 [ 0.48210769  0.74857143  0.77619048  0.7273521   0.74390421  0.41295264
   0.73237179  0.73961039  0.73675267  0.7270648 ]
 [ 0.16811298  0.60360119  0.68528139  0.538567    0.59914651  0.30294726
   0.66985119  0.72164502  0.6083165   0.66453894]
 [ 0.31422031  0.66719697  0.66666667  0.65876163  0.66204977  0.26388662
   0.65477273  0.65714286  0.63474372  0.65057659]
 [-0.02090412  0.51883394  0.53961039  0.47745627  0.51005697  0.52001258
   0.76177503  0.78095238  0.74766919  0.7645961 ]
 [ 0.48742127  0.72674756  0.7452381   0.73143146  0.7262964   0.49681831
   0.74278846  0.76233766  0.72313775  0.74375662]
 [ 0.23602969  0.57442308  0.59350649  0.58916175  0.57071731  0.21632533
   0.63720085  0.64393939  0.63673226  0.63201051]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.2217818   0.57357143  0.62056277  0.57790116  0.56648386  0.25261176
   0.62970238  0.66233766  0.62805957  0.62592683]
 [ 0.42838243  0.70367063  0.71623377  0.6756484   0.69990082  0.27175118
   0.64190476  0.65714286  0.62316004  0.62748008]
 [ 0.61713535  0.76378205  0.77142857  0.77039093  0.76304754  0.51721319
   0.75577686  0.76233766  0.72952962  0.74868287]
 [ 0.24476515  0.59261696  0.79458874  0.50634223  0.5953326   0.42162325
   0.66378205  0.67489177  0.68057992  0.66230806]
 [ 0.26002067  0.62        0.67510823  0.53987072  0.60870538  0.44531999
   0.7141453   0.71688312  0.71079301  0.71007043]
 [ 0.32021694  0.66559524  0.69372294  0.66847198  0.65571479  0.3502071
   0.66788004  0.70324675  0.63568467  0.66976952]
 [ 0.147996    0.59057598  0.72575758  0.48544865  0.57076195  0.03481454
   0.53598485  0.53528139  0.55340854  0.53215537]
 [ 0.17961494  0.58090909  0.57987013  0.59687635  0.57594244  0.23710337
   0.58642857  0.62077922  0.59270384  0.5803516 ]
 [ 0.08112637  0.52702991  0.53896104  0.55278141  0.5194051   0.18402848
   0.6120098   0.67987013  0.56335047  0.59840656]
 [ 0.44092502  0.72933761  0.73571429  0.73841396  0.72843576  0.50636364
   0.75727273  0.75779221  0.79617599  0.75485763]
 [ 0.07852298  0.57962121  0.58528139  0.54314478  0.57614289  0.20178907
   0.6082265   0.61277056  0.59996828  0.60384234]
 [ 0.43596369  0.72604167  0.75844156  0.68961766  0.716877    0.54143789
   0.78901515  0.78982684  0.77438951  0.78777297]
 [ 0.04145541  0.50809524  0.5530303   0.47614368  0.5070497   0.24698925
   0.61050061  0.63030303  0.59385093  0.60068743]
 [ 0.13065683  0.56802503  0.58852814  0.5595067   0.56508566  0.14596093
   0.56694444  0.57056277  0.54590596  0.56423423]
 [ 0.56231435  0.78469933  0.79480519  0.77643908  0.78237514  0.51353883
   0.76255952  0.79004329  0.730398    0.75916828]]
DT mean:
[0.28501183 0.64128959 0.67274392 0.62082702 0.63617842 0.34850387
 0.67807506 0.6959374  0.66363052 0.67391638]
---------------------------
---------------------------
RF performance:
[[0.48358147 0.69659091 0.69805195 0.72523741 0.69335035 0.45031877
  0.69699755 0.80411255 0.70000767 0.70347912]
 [0.20342208 0.69570513 0.7034632  0.63773899 0.69274837 0.35465457
  0.73041056 0.75800866 0.75519303 0.73627246]
 [0.39314177 0.74339896 0.75800866 0.71520102 0.74226412 0.44318182
  0.71666667 0.71709957 0.6852433  0.71366579]
 [0.39857254 0.68628788 0.68593074 0.69317106 0.68290794 0.63304087
  0.79939394 0.7995671  0.77733677 0.79587283]
 [0.27752404 0.68080739 0.7025974  0.65275585 0.6790087  0.25181818
  0.64272727 0.64372294 0.69997924 0.6390294 ]
 [0.60930023 0.7147619  0.77142857 0.76318028 0.72512135 0.46604686
  0.73171551 0.74458874 0.76748506 0.72677441]
 [0.28974106 0.6371131  0.68095238 0.58890189 0.63538589 0.29603691
  0.68491071 0.74393939 0.63611536 0.68927587]
 [0.43387298 0.65386364 0.65281385 0.67154581 0.64989749 0.39134064
  0.6305303  0.63008658 0.61622298 0.62345522]
 [0.09349099 0.60599054 0.63982684 0.53787657 0.60061303 0.53130544
  0.72008547 0.74437229 0.70776597 0.71937564]
 [0.60983208 0.79991606 0.80887446 0.78129311 0.7977285  0.531674
  0.70302198 0.71666667 0.72519431 0.69691697]
 [0.21422238 0.58641026 0.60735931 0.5456485  0.58157857 0.23120089
  0.62839744 0.63528139 0.62449936 0.62222176]
 [       nan        nan        nan        nan        nan        nan
         nan        nan        nan        nan]
 [0.33028323 0.65952381 0.71666667 0.53616919 0.65380963 0.196119
  0.64488095 0.68506494 0.53077292 0.64478636]
 [0.44392452 0.6794185  0.69891775 0.71143871 0.67996424 0.42228249
  0.65827381 0.69848485 0.64805408 0.65073564]
 [0.63850784 0.83429487 0.83571429 0.81978685 0.82867047 0.57070674
  0.7583486  0.77640693 0.7795122  0.75947666]
 [0.13632364 0.56710526 0.84025974 0.20062774 0.55412525 0.42535654
  0.73653846 0.7487013  0.67993032 0.735133  ]
 [0.27460644 0.61738095 0.68961039 0.59327214 0.60497197 0.43771721
  0.71183761 0.71709957 0.71455143 0.70705373]
 [0.33367167 0.67666667 0.72186147 0.60232099 0.67692976 0.17479556
  0.61678114 0.65281385 0.61177087 0.61383518]
 [0.04201107 0.50125    0.69350649 0.34844085 0.4829449  0.23438152
  0.55863636 0.55757576 0.5471214  0.54895479]
 [0.1995145  0.55787879 0.55735931 0.60367581 0.55396534 0.30425342
  0.62785714 0.67099567 0.64614386 0.62950322]
 [0.20870148 0.57027778 0.58528139 0.5608868  0.56080678 0.10003818
  0.5351348  0.68939394 0.39107736 0.51885933]
 [0.3595625  0.72089744 0.72575758 0.67293018 0.71430882 0.55237698
  0.78545455 0.78528139 0.76298539 0.78446448]
 [0.23374463 0.61583333 0.61623377 0.60070351 0.61064614 0.34653526
  0.68651709 0.69437229 0.66329752 0.68565899]
 [0.42036165 0.73452381 0.79047619 0.68595467 0.73914768 0.48673188
  0.80636364 0.80800866 0.75440916 0.80513988]
 [0.05904303 0.51928571 0.57575758 0.50388754 0.51306656 0.39329515
  0.77863248 0.78138528 0.70293788 0.77590014]
 [0.08946456 0.61221001 0.64437229 0.5796836  0.61191384 0.28107337
  0.64111111 0.6521645  0.59453006 0.63384182]
 [0.49624985 0.74355922 0.77164502 0.7303433  0.74578321 0.47045067
  0.71619048 0.75844156 0.62820435 0.72022689]]
RF mean:
[0.3181797  0.65811353 0.69895105 0.61779509 0.65429457 0.3837205
 0.69028522 0.71590909 0.66732084 0.68768883]
---------------------------
---------------------------
SVM performance:
[[ 0.27804999  0.63530303  0.65238095  0.57492025  0.61515348  0.
   0.5         0.75800866  0.          0.43109749]
 [-0.01643058  0.49138889  0.54805195  0.09635735  0.37770212  0.01271142
   0.50508242  0.61212121  0.07071068  0.40040234]
 [ 0.1947042   0.58462302  0.67099567  0.36429393  0.53173399  0.42475839
   0.71257576  0.71233766  0.70431776  0.70882065]
 [ 0.31219398  0.65515152  0.65800866  0.62867004  0.64433323  0.34419974
   0.66704545  0.68506494  0.58296835  0.63655014]
 [ 0.          0.5         0.60735931  0.          0.3777591   0.16070118
   0.58045455  0.57987013  0.55762466  0.56944837]
 [ 0.55840928  0.74285714  0.83549784  0.69207319  0.76929617  0.21952932
   0.59822955  0.66645022  0.48872334  0.57619186]
 [ 0.          0.5         0.69415584  0.          0.40966098  0.
   0.5         0.68961039  0.          0.40809625]
 [ 0.17425796  0.58681818  0.58874459  0.54432557  0.5672888   0.25506807
   0.62469697  0.63917749  0.58852031  0.61376902]
 [ 0.          0.5         0.61645022  0.          0.38125117  0.44602646
   0.70433455  0.75822511  0.64553687  0.70579627]
 [ 0.69327508  0.84906899  0.85411255  0.84597184  0.84577     0.1567803
   0.56666667  0.67142857  0.29777705  0.50438699]
 [ 0.          0.5         0.57532468  0.          0.36510313  0.
   0.5         0.57077922  0.          0.36325439]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.          0.5         0.68030303  0.          0.40486486  0.
   0.5         0.64393939  0.          0.39165165]
 [ 0.          0.5         0.5982684   0.          0.37426704  0.07060347
   0.52767857  0.68051948  0.14874468  0.4514092 ]
 [ 0.66652063  0.82963675  0.84004329  0.82336045  0.83166146  0.
   0.5         0.5982684   0.          0.37426704]
 [ 0.          0.5         0.8538961   0.          0.46054409  0.17934961
   0.58728632  0.62554113  0.49423308  0.56444156]
 [ 0.          0.5         0.68506494  0.          0.40653153  0.43220639
   0.71465812  0.72164502  0.70238054  0.71181721]
 [ 0.          0.5         0.67575758  0.          0.40321321  0.
   0.5         0.63008658  0.          0.38648926]
 [ 0.          0.5         0.76709957  0.          0.43406646  0.
   0.5         0.50692641  0.          0.33633578]
 [ 0.04985443  0.52507576  0.52987013  0.36836906  0.45838125  0.
   0.5         0.65757576  0.          0.39660661]
 [ 0.          0.5         0.57532468  0.          0.36510313  0.
   0.5         0.73528139  0.          0.42367509]
 [ 0.27845242  0.62938034  0.67142857  0.54184344  0.60747549  0.60703414
   0.80363636  0.8034632   0.80137032  0.80270657]
 [ 0.01081081  0.505       0.53874459  0.03162278  0.35826092  0.0076082
   0.50378205  0.54329004  0.20493431  0.41398406]
 [ 0.          0.5         0.69415584  0.          0.40966098  0.48877794
   0.74462121  0.74415584  0.73835127  0.74172835]
 [ 0.          0.5         0.64393939  0.          0.39165165  0.19295414
   0.58682082  0.65800866  0.4279431   0.55129666]
 [ 0.          0.5         0.61190476  0.          0.37950514  0.
   0.5         0.54805195  0.          0.3540107 ]
 [ 0.          0.5         0.61645022  0.          0.38125117  0.
   0.5         0.6530303   0.          0.39495495]]
SVM mean:
[0.1230807  0.55901168 0.66474359 0.21199261 0.48274964 0.15378111
 0.57413728 0.65741758 0.28669755 0.50819956]
---------------------------
---------------------------
GBM performance:
[[ 0.36330395  0.67901515  0.68896104  0.65331634  0.67208558  0.32057829
   0.62747549  0.80844156  0.45659792  0.63613473]
 [ 0.30957057  0.65448718  0.66298701  0.62458949  0.64815489  0.3420221
   0.65960775  0.71255411  0.60657794  0.65762173]
 [ 0.47009593  0.72526709  0.76709957  0.67865855  0.72267597  0.52521596
   0.76295455  0.76255411  0.76378596  0.7594998 ]
 [ 0.29402182  0.64242424  0.64415584  0.62400141  0.63405879  0.59692116
   0.79424242  0.79458874  0.78925799  0.79210284]
 [ 0.34522114  0.6541743   0.71190476  0.58697809  0.64440485  0.33408072
   0.66272727  0.66212121  0.63396208  0.64903131]
 [ 0.5239886   0.72571429  0.81731602  0.67602779  0.74719946  0.36705434
   0.66829212  0.72619048  0.60463444  0.66614951]
 [ 0.11690703  0.54735119  0.7034632   0.30570206  0.51533745  0.07771687
   0.52738095  0.69415584  0.1801512   0.47244606]
 [ 0.36100702  0.68121212  0.68030303  0.6811463   0.67602515  0.37330055
   0.68795455  0.69848485  0.66006199  0.68270479]
 [-0.01341658  0.49996184  0.58939394  0.26276226  0.44683437  0.54938449
   0.76208791  0.79437229  0.7391409   0.76903948]
 [ 0.45692829  0.72635836  0.75367965  0.70228912  0.72521822  0.3720064
   0.66894078  0.73506494  0.59984764  0.66766701]
 [ 0.19335081  0.5842735   0.62554113  0.5153097   0.57058115  0.1634954
   0.57965812  0.62532468  0.46300475  0.54657818]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.1467945   0.56238095  0.70324675  0.31425148  0.52771793  0.11738773
   0.54732143  0.67142857  0.30108351  0.49588235]
 [ 0.27952804  0.63510379  0.69393939  0.54981908  0.62661004  0.26516581
   0.62238095  0.72597403  0.52818729  0.6176677 ]
 [ 0.63986965  0.82213675  0.83051948  0.81002927  0.82205459  0.63209244
   0.81913156  0.82207792  0.81084877  0.8137265 ]
 [ 0.15259935  0.55972222  0.86774892  0.22320508  0.55881933  0.47427347
   0.72606838  0.75779221  0.69420085  0.72810401]
 [ 0.02395969  0.50928571  0.68051948  0.11375182  0.43934035  0.48647348
   0.74104701  0.7491342   0.72514316  0.73792553]
 [ 0.19244852  0.58535714  0.71233766  0.40487146  0.55369726  0.28366144
   0.62400794  0.71688312  0.47704887  0.60457722]
 [-0.00819672  0.49705882  0.76255411  0.04472136  0.43258197  0.15405765
   0.57727273  0.57640693  0.56087039  0.56986103]
 [ 0.19487446  0.59742424  0.59372294  0.58057869  0.58743697  0.35151923
   0.65654762  0.74415584  0.57945091  0.65745495]
 [ 0.20686323  0.59465812  0.64393939  0.46035067  0.56083891 -0.01690141
   0.49375     0.72619048  0.          0.42054565]
 [ 0.39197995  0.6890812   0.71277056  0.66159503  0.68787513  0.5237858
   0.76181818  0.76212121  0.75307716  0.75896216]
 [ 0.13028281  0.56333333  0.57965368  0.48782157  0.53519512  0.22758016
   0.61373932  0.61731602  0.58633445  0.59988051]
 [ 0.22105373  0.58833333  0.74437229  0.37137256  0.56440369  0.57812169
   0.78848485  0.78982684  0.77946597  0.78594433]
 [ 0.0040959   0.50892857  0.64372294  0.10850713  0.42446545  0.3533303
   0.66578907  0.69891775  0.6455146   0.66723167]
 [ 0.16063351  0.56598748  0.6530303   0.39300296  0.52277972  0.29896225
   0.65        0.66645022  0.61874958  0.64309102]
 [ 0.33355971  0.65044261  0.72619048  0.53899424  0.63355644  0.09852164
   0.54017857  0.67142857  0.27340701  0.48743976]]
GBM mean:
[0.24966634 0.61728744 0.6997336  0.47590975 0.59538265 0.34037723
 0.66264844 0.71961372 0.57040021 0.64951038]
---------------------------
---------------------------
BDDAE performance:
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
BDDAE mean:
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.53874459 0.         0.35006127 0.
  0.5        0.75800866 0.         0.43109749]
 [0.         0.5        0.56168831 0.         0.35955691 0.
  0.5        0.61190476 0.         0.37950514]
 [0.         0.5        0.60735931 0.         0.3777591  0.
  0.5        0.50692641 0.         0.33633578]
 [0.         0.5        0.51601732 0.         0.34025735 0.
  0.5        0.52965368 0.         0.34613971]
 [0.         0.5        0.60735931 0.         0.3777591  0.
  0.5        0.49761905 0.         0.33225806]
 [0.         0.5        0.68030303 0.         0.40486486 0.
  0.5        0.60735931 0.         0.3777591 ]
 [0.         0.5        0.69415584 0.         0.40966098 0.
  0.5        0.68961039 0.         0.40809625]
 [0.         0.5        0.51147186 0.         0.33829657 0.
  0.5        0.54329004 0.         0.35202206]
 [0.         0.5        0.61645022 0.         0.38125117 0.
  0.5        0.60281385 0.         0.37601307]
 [0.         0.5        0.62099567 0.         0.3829972  0.
  0.5        0.62099567 0.         0.3829972 ]
 [0.         0.5        0.57532468 0.         0.36510313 0.
  0.5        0.57077922 0.         0.36325439]
 [0.         0.5        0.63463203 0.         0.38823529 0.
  0.5        0.52510823 0.         0.34417892]
 [0.         0.5        0.68030303 0.         0.40486486 0.
  0.5        0.64393939 0.         0.39165165]
 [0.         0.5        0.5982684  0.         0.37426704 0.
  0.5        0.66212121 0.         0.39825826]
 [0.         0.5        0.58441558 0.         0.36880061 0.
  0.5        0.5982684  0.         0.37426704]
 [0.         0.5        0.8538961  0.         0.46054409 0.
  0.5        0.58896104 0.         0.37064935]
 [0.         0.5        0.68506494 0.         0.40653153 0.
  0.5        0.5525974  0.         0.35585943]
 [0.         0.5        0.67575758 0.         0.40321321 0.
  0.5        0.63008658 0.         0.38648926]
 [0.         0.5        0.76709957 0.         0.43406646 0.
  0.5        0.50692641 0.         0.33633578]
 [0.         0.5        0.51147186 0.         0.33829657 0.
  0.5        0.65757576 0.         0.39660661]
 [0.         0.5        0.57532468 0.         0.36510313 0.
  0.5        0.73528139 0.         0.42367509]
 [0.         0.5        0.57077922 0.         0.36325439 0.
  0.5        0.49761905 0.         0.33225806]
 [0.         0.5        0.53419913 0.         0.34810049 0.
  0.5        0.5525974  0.         0.35585943]
 [0.         0.5        0.69415584 0.         0.40966098 0.
  0.5        0.50692641 0.         0.33633578]
 [0.         0.5        0.64393939 0.         0.39165165 0.
  0.5        0.5982684  0.         0.37426704]
 [0.         0.5        0.61190476 0.         0.37950514 0.
  0.5        0.54805195 0.         0.3540107 ]
 [0.         0.5        0.61645022 0.         0.38125117 0.
  0.5        0.6530303  0.         0.39495495]]
DUMMY mean:
[0.         0.5        0.62101972 0.         0.38166349 0.
 0.5        0.59245631 0.         0.3707828 ]
---------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_3_1_12
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  11.008
step (sec):  5.504
overlap:  True
perc. of overlap:  50.0
Number of windows / instances:  219
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.315 0.654 0.703 0.603 0.651 0.378 0.687 0.713 0.661 0.684]
 [0.285 0.641 0.673 0.621 0.636 0.349 0.678 0.696 0.664 0.674]
 [0.318 0.658 0.699 0.618 0.654 0.384 0.69  0.716 0.667 0.688]
 [0.123 0.559 0.665 0.212 0.483 0.154 0.574 0.657 0.287 0.508]
 [0.25  0.617 0.7   0.476 0.595 0.34  0.663 0.72  0.57  0.65 ]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.5   0.621 0.    0.382 0.    0.5   0.592 0.    0.371]]
Elapsed time: 3.686195107301076 minutes
Elapsed time: 0.0614365851216846 hours
