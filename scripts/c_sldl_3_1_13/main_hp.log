2024-05-22 16:23:45.102850: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:23:45.155200: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:23:45.155431: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:23:45.155799: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-22 16:23:45.156387: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:23:45.156557: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:23:45.156711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:23:45.557546: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:23:45.557765: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:23:45.557927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:23:45.558064: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
Window size (sec):  16.0
step (sec):  16.0
overlap:  True
perc. of overlap:  0.0
Nearest multiple of 16 to 16000 is: 16000
Nearest multiple of 16 to 16000 is: 16000
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_13/functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

ndas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_13/functions.py:821: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_13/functions.py:821: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

Use `df.loc[row_indexer, "col"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_13/functions.py:821: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_13/functions.py:821: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

df["col"][row_indexer] = value

Use `df.loc[row_indexer, "col"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy

  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_13/functions.py:821: SettingWithCopyWarning: 
A value is trying to be set on a copy of a slice from a DataFrame

See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy
  combined_df_copy[labels[label]].iloc[index] = combined_df_copy[labels[label]].iloc[index].apply(map_labels_bipartite, args=(number_thresholds, threshold, L, H))
----- Extracting features...
/usr/local/lib/python3.10/dist-packages/heartpy/analysis.py:522: UserWarning: Short signal.

---------Warning:---------
too few peak-peak intervals for (reliable) frequency domain measure computation, frequency output measures are still computed but treat them with caution!

HF is usually computed over a minimum of 1 minute of good signal. LF is usually computed over a minimum of 2 minutes of good signal.VLF is usually computed over a minimum of 5 minutes of good signal.The LF/HF ratio is usually computed over minimum 24 hours, although an absolute minimum of 5 min has also been suggested.

For more info see: 
Shaffer, F., Ginsberg, J.P. (2017), An Overview of Heart Rate Variability Metrics and Norms.

Task Force of Pacing and Electrophysiology (1996), Heart Rate Variability, in: European Heart Journal, vol.17, issue 3, pp354-381

This warning will not repeat
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:280: UserWarning: 
The maximal number of iterations maxit (set to 20 by the program)
allowed for finding a smoothing spline with fp=s has been reached: s
too small.
There is an approximation returned but the corresponding weighted sum
of squared residuals does not satisfy the condition abs(fp-s)/s < tol.
  warnings.warn(message)
Class Imbalance test (arousal)....
Multiclass distribution
Class=0, n=27 (35.526%)
Class=1, n=49 (64.474%)
Class Imbalance test (valence)....
Multiclass distribution
Class=0, n=29 (38.158%)
Class=1, n=47 (61.842%)
------------- Evaluating model --------------
------------- Evaluating model --------------
------------- Evaluating model --------------
------------- Evaluating model --------------
------------- Evaluating model --------------
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  16.0
step (sec):  16.0
overlap:  True
perc. of overlap:  0.0
Number of windows / instances:  76
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.352 0.667 0.714 0.598 0.662 0.24  0.626 0.659 0.567 0.611]
 [0.197 0.608 0.646 0.572 0.602 0.261 0.675 0.72  0.581 0.663]
 [0.183 0.622 0.65  0.443 0.614 0.376 0.585 0.643 0.39  0.555]
 [0.    0.5   0.618 0.    0.382 0.    0.5   0.646 0.    0.392]
 [0.05  0.527 0.595 0.296 0.478 0.214 0.602 0.698 0.385 0.567]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.5   0.618 0.    0.382 0.    0.5   0.646 0.    0.392]]
participant performance loaded
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
---------------------------
KNN performance:
[[ 0.07534632  0.53833333  0.55357143  0.47762632  0.52240981  0.20218487
   0.60333333  0.72142857  0.41693458  0.58574592]
 [-0.06960606  0.46583333  0.46964286  0.38040189  0.43997114  0.26550851
   0.62166667  0.70357143  0.4757973   0.60948052]
 [ 0.09818027  0.5525      0.57142857  0.43011569  0.52244589  0.41889855
   0.70833333  0.71071429  0.69659976  0.705     ]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.37395173  0.67666667  0.7375      0.60085562  0.67363636  0.27536232
   0.6375      0.63392857  0.51639025  0.59194444]
 [ 0.4180827   0.70833333  0.71071429  0.68385534  0.69876984  0.54108059
   0.77166667  0.77321429  0.75512072  0.76263348]
 [ 0.21811189  0.60666667  0.63571429  0.51414722  0.58880952  0.24679487
   0.625       0.61964286  0.51419411  0.58304113]
 [ 0.5259944   0.74        0.82857143  0.6191717   0.74022145  0.22348623
   0.6125      0.61964286  0.55037446  0.59164502]
 [ 0.26457014  0.62666667  0.71428571  0.44111754  0.59090909  0.15179487
   0.57166667  0.71071429  0.31948653  0.54153846]
 [ 0.19200334  0.6         0.59464286  0.56064282  0.58206349  0.19679487
   0.60416667  0.59464286  0.56809991  0.58115079]
 [ 0.12300898  0.56166667  0.56428571  0.48205375  0.53634921  0.4449634
   0.70666667  0.76071429  0.62487006  0.70277972]
 [ 0.6279142   0.81166667  0.82857143  0.78553922  0.80638528  0.09454647
   0.54833333  0.56607143  0.47828607  0.52508852]
 [-0.09020943  0.45666667  0.48571429  0.28744338  0.41811688  0.32109928
   0.65833333  0.66428571  0.60336531  0.63401515]
 [ 0.21280805  0.61166667  0.64821429  0.54335343  0.59315657 -0.26059339
   0.37333333  0.40714286  0.19925987  0.34551587]
 [ 0.2122075   0.59833333  0.70178571  0.4012019   0.57748918  0.19099758
   0.60333333  0.67321429  0.38590936  0.55004884]
 [ 0.32067436  0.655       0.66785714  0.62920089  0.64727994  0.25347554
   0.61333333  0.7         0.47716718  0.61236597]
 [ 0.57713043  0.79333333  0.78928571  0.7817158   0.78349206  0.19411968
   0.5925      0.625       0.49594004  0.57648268]
 [ 0.27333333  0.63333333  0.85892857  0.3352604   0.61692308  0.35191554
   0.68166667  0.67142857  0.65995165  0.66365079]
 [ 0.39396574  0.68833333  0.74464286  0.60860797  0.69165945  0.34657545
   0.67333333  0.68214286  0.62034207  0.66043651]
 [ 0.32654816  0.65833333  0.72678571  0.52908078  0.63336247  0.0984451
   0.54916667  0.62321429  0.35604304  0.51666334]
 [ 0.1         0.55        0.76428571  0.17071068  0.51336996 -0.18335897
   0.40833333  0.4125      0.24304129  0.3567316 ]
 [ 0.24995916  0.625       0.63214286  0.54417239  0.60580447  0.54517794
   0.77333333  0.79107143  0.74589562  0.76454906]
 [ 0.0527577   0.525       0.5375      0.4676198   0.50925685 -0.07705882
   0.46        0.64107143  0.12779528  0.43476024]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.2840524   0.64333333  0.64642857  0.62969258  0.63666667  0.43926219
   0.71666667  0.725       0.6993565   0.71365079]
 [ 0.21905034  0.61083333  0.61964286  0.47610301  0.562886    0.13839646
   0.57166667  0.56428571  0.54492099  0.55214286]
 [ 0.21052252  0.58833333  0.68571429  0.45012225  0.58775502  0.616
   0.80833333  0.80714286  0.79029744  0.79968254]
 [ 0.16517917  0.58        0.61964286  0.47825923  0.5580303   0.63621723
   0.81166667  0.83035714  0.79627245  0.81380592]
 [ 0.0734488   0.535       0.5875      0.3933848   0.52396465  0.0973147
   0.55083333  0.56428571  0.44418087  0.51710678]
 [ 0.35163959  0.6675      0.71428571  0.59782452  0.66161977  0.24037169
   0.62583333  0.65892857  0.56661044  0.61113997]]
KNN mean:
[0.2421652  0.61815476 0.66568878 0.5106886  0.60081444 0.25034903
 0.624375   0.6591199  0.52401797 0.60367132]
---------------------------
---------------------------
DT performance:
[[ 0.29701167  0.6425      0.66071429  0.53384511  0.63170635  0.23097177
   0.66166667  0.74107143  0.51847836  0.65069347]
 [ 0.05276084  0.53916667  0.54107143  0.49189089  0.52702381  0.42742175
   0.75666667  0.76785714  0.68101186  0.74694084]
 [ 0.01695827  0.495       0.5         0.46651052  0.47595238  0.42769565
   0.74166667  0.74285714  0.71084392  0.73412698]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.42217595  0.72333333  0.72321429  0.68732023  0.70554834  0.27705929
   0.62916667  0.63214286  0.53154407  0.60714286]
 [ 0.14739936  0.59583333  0.59642857  0.5459108   0.58055556  0.11000178
   0.54666667  0.55178571  0.5334937   0.52948413]
 [ 0.20438539  0.56833333  0.575       0.55071376  0.54743506  0.10691787
   0.56666667  0.5625      0.52398377  0.54936508]
 [ 0.27906701  0.67833333  0.69821429  0.5940936   0.64804113  0.21399284
   0.64833333  0.64821429  0.58086931  0.62920635]
 [ 0.04916921  0.54833333  0.6125      0.49215651  0.5252381   0.13600779
   0.55833333  0.63571429  0.4770048   0.5470202 ]
 [ 0.52769565  0.80833333  0.80535714  0.72333817  0.80297619  0.08366667
   0.55833333  0.55714286  0.59045927  0.54595238]
 [ 0.08198315  0.5575      0.5625      0.49221382  0.54238095  0.4114429
   0.71416667  0.73035714  0.71555993  0.70102092]
 [ 0.40814476  0.72166667  0.74107143  0.63379421  0.69553391  0.22997914
   0.58583333  0.61428571  0.44991991  0.56139805]
 [ 0.40321601  0.69333333  0.69642857  0.59449217  0.6790873   0.04950427
   0.485       0.47857143  0.43879874  0.45450938]
 [ 0.12372432  0.62333333  0.67142857  0.49067849  0.60640693 -0.0952767
   0.46333333  0.46071429  0.35652306  0.43736652]
 [ 0.08216721  0.51        0.58035714  0.48703963  0.49382062 -0.03228236
   0.49666667  0.51607143  0.26326352  0.46161255]
 [ 0.16944422  0.61416667  0.63035714  0.52422003  0.59272727  0.26678241
   0.66        0.71607143  0.48271461  0.63559885]
 [ 0.27821783  0.65583333  0.64464286  0.60967471  0.62904762  0.32154604
   0.68333333  0.69107143  0.58897303  0.64851371]
 [-0.00190476  0.5702381   0.77857143  0.16454972  0.55879121  0.30437065
   0.70583333  0.71428571  0.58888152  0.69150794]
 [ 0.29329563  0.66166667  0.725       0.62350967  0.65654346  0.27975626
   0.64416667  0.65357143  0.61066029  0.63830808]
 [ 0.05546649  0.57166667  0.63214286  0.57614977  0.54292929  0.20519104
   0.58916667  0.60892857  0.58431187  0.56733405]
 [ 0.02370789  0.53333333  0.62857143  0.44239299  0.51602176 -0.05602564
   0.44166667  0.44464286  0.40370234  0.42033189]
 [ 0.03433333  0.47916667  0.47142857  0.45313666  0.45246032  0.41023321
   0.71833333  0.73035714  0.70475936  0.70166667]
 [ 0.16602778  0.53083333  0.53928571  0.48221504  0.51279221 -0.22966874
   0.38        0.525       0.10477226  0.36914696]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.23923825  0.6075      0.60714286  0.51859655  0.59587302  0.43926219
   0.70416667  0.7125      0.6756037   0.70222222]
 [ 0.19149884  0.57083333  0.58214286  0.44183982  0.55231962  0.12088889
   0.6225      0.61607143  0.63099689  0.6102381 ]
 [ 0.09037044  0.56833333  0.63035714  0.41068075  0.5480303   0.4380303
   0.70833333  0.70892857  0.66322222  0.70547619]
 [ 0.06387279  0.59666667  0.63392857  0.49009791  0.57575397  0.4117818
   0.69916667  0.71071429  0.6321124   0.68934343]
 [ 0.24323238  0.64166667  0.65892857  0.52712584  0.61772727  0.26549673
   0.61        0.60357143  0.59083831  0.58059524]
 [ 0.19718487  0.60833333  0.64642857  0.57229428  0.60199134  0.26086027
   0.675       0.71964286  0.58082859  0.6633189 ]]
DT mean:
[0.18356588 0.60411565 0.63475765 0.52216006 0.58623983 0.21484315
 0.61622024 0.63552296 0.54336184 0.59926578]
---------------------------
---------------------------
RF performance:
[[ 0.31336232  0.59666667  0.60535714  0.62914324  0.58769841  0.18313725
   0.56333333  0.6875      0.30624098  0.53614552]
 [ 0.10847544  0.585       0.58392857  0.52993358  0.56096681  0.25352941
   0.64333333  0.67321429  0.58029506  0.6337013 ]
 [ 0.12236677  0.59583333  0.60714286  0.64192158  0.5772619   0.55823188
   0.7625      0.76785714  0.65766773  0.75492063]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.26695238  0.67583333  0.68214286  0.61807083  0.65239899  0.38578744
   0.7         0.6875      0.61015161  0.67944444]
 [ 0.14172464  0.63333333  0.62857143  0.59489246  0.6222619   0.3162924
   0.66833333  0.66428571  0.64892146  0.64898268]
 [ 0.15320794  0.67666667  0.68214286  0.58213647  0.66423882  0.18858454
   0.53333333  0.5375      0.53307158  0.51314935]
 [ 0.52001401  0.71333333  0.78928571  0.65945506  0.70331002  0.36594463
   0.6975      0.69821429  0.52865148  0.66354618]
 [ 0.11956583  0.63666667  0.70892857  0.48227667  0.616665    0.31247412
   0.59166667  0.70535714  0.30096234  0.55845738]
 [ 0.31678744  0.61666667  0.61428571  0.54979627  0.59374459  0.15154779
   0.60416667  0.59285714  0.49618874  0.58174603]
 [-0.16278861  0.55        0.56071429  0.36566183  0.53606061  0.46229017
   0.76416667  0.79107143  0.62463884  0.74526529]
 [ 0.57312199  0.8475      0.85714286  0.7109899   0.84303391  0.29250894
   0.60333333  0.61607143  0.48145764  0.58256854]
 [ 0.31141126  0.6525      0.67321429  0.46496181  0.63767316  0.14329106
   0.54583333  0.54821429  0.57412459  0.52977633]
 [ 0.07173735  0.55166667  0.60178571  0.45468816  0.53796537 -0.16929915
   0.40833333  0.41071429  0.2858004   0.38968254]
 [ 0.15692308  0.66166667  0.71428571  0.40137316  0.62632062  0.04553254
   0.56        0.64464286  0.38754601  0.52012654]
 [ 0.21626342  0.67833333  0.72321429  0.56277753  0.66227828  0.22872035
   0.65166667  0.71071429  0.52630994  0.62571012]
 [ 0.36391134  0.7625      0.76785714  0.77347098  0.7515404   0.29174039
   0.60583333  0.61964286  0.53353238  0.56329726]
 [ 0.20904762  0.59166667  0.81785714  0.27580252  0.57089744  0.13550649
   0.64583333  0.66071429  0.48677361  0.63327922]
 [ 0.11553254  0.56166667  0.68571429  0.5099668   0.5331352   0.23604274
   0.66083333  0.66964286  0.58115128  0.63348124]
 [ 0.22359831  0.61166667  0.675       0.55964656  0.57341298  0.0708987
   0.57083333  0.63214286  0.36468917  0.55341991]
 [ 0.09529412  0.43666667  0.61964286  0.23748647  0.42738706  0.07350427
   0.43333333  0.43214286  0.3928869   0.40472222]
 [ 0.0958495   0.45        0.45178571  0.42329988  0.43301587  0.24849396
   0.685       0.69821429  0.54692082  0.66695887]
 [-0.11710798  0.495       0.5125      0.38656256  0.47398268 -0.1158994
   0.5         0.63035714  0.1352604   0.4905378 ]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.24932869  0.64166667  0.65357143  0.61940256  0.62227633  0.4365942
   0.75833333  0.76071429  0.62982599  0.75246032]
 [ 0.36965613  0.62583333  0.64642857  0.50337681  0.61035714  0.10643982
   0.54        0.54285714  0.61307819  0.52390332]
 [ 0.19843137  0.64        0.70892857  0.46459676  0.62060606  0.60026087
   0.75416667  0.75178571  0.71971004  0.74460317]
 [ 0.17819159  0.53583333  0.6         0.45638972  0.51891414  0.42645166
   0.7775      0.78928571  0.75547549  0.77501443]
 [ 0.16903686  0.59416667  0.63392857  0.58798664  0.58297619  0.1866414
   0.57333333  0.56785714  0.51544524  0.54298701]
 [ 0.18258921  0.62166667  0.65        0.44321546  0.61420996  0.37571133
   0.585       0.64285714  0.39019884  0.5546953 ]]
RF mean:
[0.19866016 0.61571429 0.6591199  0.51747437 0.59837821 0.24253428
 0.62098214 0.64764031 0.50739203 0.60009225]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.55357143  0.          0.35571096  0.
   0.5         0.73571429  0.          0.42380952]
 [ 0.          0.5         0.55357143  0.          0.35571096  0.
   0.5         0.66071429  0.          0.3974359 ]
 [ 0.          0.5         0.57857143  0.          0.36596737  0.14494949
   0.57083333  0.58214286  0.30877132  0.47909091]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.          0.5         0.63214286  0.          0.38682984  0.122
   0.5625      0.56785714  0.18660254  0.43450216]
 [-0.05217391  0.475       0.48571429  0.          0.32494949  0.
   0.5         0.56607143  0.          0.36083916]
 [ 0.          0.5         0.59107143  0.          0.37109557  0.025
   0.5125      0.48392857  0.23901195  0.405     ]
 [ 0.          0.5         0.68571429  0.          0.40622711  0.
   0.5         0.59107143  0.          0.37109557]
 [ 0.          0.5         0.68571429  0.          0.40622711  0.
   0.5         0.71071429  0.          0.41501832]
 [ 0.          0.5         0.51428571  0.05        0.35606061  0.
   0.5         0.52857143  0.          0.34545455]
 [ 0.          0.5         0.57857143  0.          0.36596737  0.
   0.5         0.61785714  0.          0.38152681]
 [ 0.32709457  0.65083333  0.71071429  0.54179029  0.63399184  0.
   0.5         0.60357143  0.          0.37622378]
 [ 0.          0.5         0.57857143  0.          0.36596737  0.
   0.5         0.56607143  0.          0.36083916]
 [ 0.          0.5         0.66071429  0.          0.3974359   0.
   0.5         0.56607143  0.          0.36083916]
 [ 0.          0.5         0.67321429  0.          0.4018315   0.
   0.5         0.66071429  0.          0.3974359 ]
 [ 0.          0.5         0.60357143  0.          0.37622378  0.
   0.5         0.67321429  0.          0.4018315 ]
 [ 0.          0.5         0.57857143  0.          0.36596737  0.
   0.5         0.59107143  0.          0.37109557]
 [ 0.          0.5         0.83035714  0.          0.45318681  0.
   0.5         0.56607143  0.          0.36083916]
 [ 0.          0.5         0.68571429  0.          0.40622711  0.
   0.5         0.54107143  0.          0.35058275]
 [ 0.          0.5         0.67321429  0.          0.4018315   0.
   0.5         0.64642857  0.          0.39213287]
 [ 0.          0.5         0.76428571  0.          0.43278388  0.
   0.5         0.51428571  0.          0.33909091]
 [ 0.          0.5         0.51428571  0.          0.33909091  0.
   0.5         0.61785714  0.          0.38152681]
 [ 0.          0.5         0.57857143  0.          0.36596737  0.
   0.5         0.75        0.          0.4282967 ]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.          0.5         0.56607143  0.          0.36083916  0.51889855
   0.75833333  0.76071429  0.74719629  0.755     ]
 [ 0.          0.5         0.55357143  0.          0.35571096  0.
   0.5         0.55357143  0.          0.35571096]
 [ 0.          0.5         0.69821429  0.          0.41062271  0.52956522
   0.7625      0.76428571  0.73372225  0.75034632]
 [ 0.          0.5         0.63214286  0.          0.38682984  0.
   0.5         0.61785714  0.          0.38152681]
 [ 0.          0.5         0.60357143  0.          0.37622378  0.
   0.5         0.55357143  0.          0.35571096]
 [ 0.          0.5         0.61785714  0.          0.38152681  0.
   0.5         0.64642857  0.          0.39213287]]
SVM mean:
[0.00981859 0.50449405 0.62079082 0.02113537 0.38953589 0.0478719
 0.52380952 0.615625   0.07911801 0.41160482]
---------------------------
---------------------------
GBM performance:
[[ 0.39684663  0.68166667  0.7         0.64171579  0.68007215  0.23098039
   0.6         0.76428571  0.34739243  0.58633034]
 [ 0.13157801  0.57666667  0.58035714  0.53985509  0.56370491  0.13096315
   0.57166667  0.66428571  0.37314662  0.55846154]
 [ 0.06083175  0.515       0.54107143  0.3683278   0.49198052  0.35255556
   0.67916667  0.67321429  0.64356842  0.66428571]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.289001    0.61        0.68035714  0.54879473  0.60240093  0.4777971
   0.7125      0.7125      0.66038048  0.68777778]
 [ 0.04433668  0.52083333  0.52142857  0.48299166  0.50177489  0.19236364
   0.59        0.60535714  0.57463974  0.58072511]
 [ 0.12013298  0.58666667  0.63571429  0.3564565   0.53636752  0.16558586
   0.58333333  0.58035714  0.54567957  0.56279221]
 [ 0.39898007  0.68666667  0.7625      0.59731344  0.68054779  0.24869255
   0.65        0.6625      0.57058234  0.63310967]
 [ 0.06717949  0.54166667  0.66964286  0.25147851  0.49664502 -0.01291101
   0.50666667  0.68571429  0.12844571  0.46112221]
 [ 0.27306602  0.66666667  0.65892857  0.58746622  0.64305556  0.20751748
   0.60416667  0.60714286  0.56066078  0.58928571]
 [ 0.11373894  0.55916667  0.57678571  0.48573473  0.53683983  0.39222567
   0.70083333  0.725       0.65162705  0.66920829]
 [ 0.47843025  0.745       0.75357143  0.66855759  0.71490454  0.31461252
   0.66333333  0.71071429  0.49846171  0.63031413]
 [ 0.14856069  0.555       0.57678571  0.45754957  0.52205628  0.17090576
   0.5975      0.625       0.45893294  0.56383838]
 [ 0.13342599  0.56        0.67321429  0.30106606  0.52177156 -0.17282828
   0.39916667  0.42321429  0.14043204  0.33247475]
 [ 0.10149321  0.545       0.67678571  0.20466691  0.48915085 -0.01813046
   0.485       0.59285714  0.15805181  0.44088578]
 [-0.06415758  0.47        0.54821429  0.1593748   0.40853924  0.19912196
   0.575       0.68571429  0.35952054  0.54491175]
 [ 0.54953538  0.77916667  0.78035714  0.76708588  0.77055556  0.45368636
   0.72833333  0.74107143  0.64917221  0.70298701]
 [-0.03666667  0.48333333  0.80357143  0.          0.44430403  0.37349896
   0.66833333  0.6875      0.66672523  0.63862554]
 [ 0.14382353  0.56333333  0.7125      0.20466691  0.50342158  0.45291608
   0.71666667  0.73214286  0.60430939  0.69396825]
 [-0.00965094  0.50833333  0.63571429  0.18559601  0.4602331  -0.03651642
   0.48416667  0.61785714  0.05163978  0.3983683 ]
 [ 0.1         0.53166667  0.7625      0.14142136  0.49634532  0.21195652
   0.59166667  0.59285714  0.52948944  0.56279221]
 [-0.30968013  0.38333333  0.38035714  0.16035534  0.33675685  0.31738653
   0.64916667  0.7125      0.52530436  0.63416861]
 [ 0.26423388  0.62833333  0.67321429  0.45179904  0.5889899  -0.02
   0.49166667  0.7375      0.          0.4239011 ]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.27224355  0.635       0.65892857  0.50713646  0.59804473  0.49456522
   0.74583333  0.75        0.69251627  0.72361111]
 [ 0.13606061  0.57833333  0.60357143  0.45580265  0.53552309  0.24208827
   0.6225      0.62857143  0.5933955   0.60968254]
 [ 0.09977591  0.54166667  0.67142857  0.30233222  0.51822844  0.54336364
   0.77083333  0.77321429  0.75548719  0.76619048]
 [ 0.11047622  0.54916667  0.64821429  0.28782051  0.50540793  0.56937409
   0.77416667  0.80178571  0.75400747  0.77890332]
 [ 0.18488092  0.58416667  0.65892857  0.3495761   0.53452603  0.28242836
   0.6475      0.64642857  0.5640745   0.61040043]
 [ 0.04971528  0.52666667  0.59464286  0.29646664  0.4782684   0.21391126
   0.60166667  0.69821429  0.38503472  0.56742813]]
GBM mean:
[0.15172113 0.57544643 0.64783163 0.38433602 0.54144345 0.24921824
 0.62181548 0.67276786 0.48009565 0.59344823]
---------------------------
---------------------------
BDDAE performance:
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
BDDAE mean:
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.55357143 0.         0.35571096 0.
  0.5        0.73571429 0.         0.42380952]
 [0.         0.5        0.55357143 0.         0.35571096 0.
  0.5        0.66071429 0.         0.3974359 ]
 [0.         0.5        0.57857143 0.         0.36596737 0.
  0.5        0.51428571 0.         0.33909091]
 [0.         0.5        0.54107143 0.         0.35058275 0.
  0.5        0.69821429 0.         0.41062271]
 [0.         0.5        0.63214286 0.         0.38682984 0.
  0.5        0.51428571 0.         0.33909091]
 [0.         0.5        0.51428571 0.         0.33909091 0.
  0.5        0.56607143 0.         0.36083916]
 [0.         0.5        0.59107143 0.         0.37109557 0.
  0.5        0.47142857 0.         0.32      ]
 [0.         0.5        0.68571429 0.         0.40622711 0.
  0.5        0.59107143 0.         0.37109557]
 [0.         0.5        0.68571429 0.         0.40622711 0.
  0.5        0.71071429 0.         0.41501832]
 [0.         0.5        0.51428571 0.         0.33909091 0.
  0.5        0.52857143 0.         0.34545455]
 [0.         0.5        0.57857143 0.         0.36596737 0.
  0.5        0.61785714 0.         0.38152681]
 [0.         0.5        0.60357143 0.         0.37622378 0.
  0.5        0.60357143 0.         0.37622378]
 [0.         0.5        0.57857143 0.         0.36596737 0.
  0.5        0.56607143 0.         0.36083916]
 [0.         0.5        0.66071429 0.         0.3974359  0.
  0.5        0.56607143 0.         0.36083916]
 [0.         0.5        0.67321429 0.         0.4018315  0.
  0.5        0.66071429 0.         0.3974359 ]
 [0.         0.5        0.60357143 0.         0.37622378 0.
  0.5        0.67321429 0.         0.4018315 ]
 [0.         0.5        0.57857143 0.         0.36596737 0.
  0.5        0.59107143 0.         0.37109557]
 [0.         0.5        0.83035714 0.         0.45318681 0.
  0.5        0.56607143 0.         0.36083916]
 [0.         0.5        0.68571429 0.         0.40622711 0.
  0.5        0.54107143 0.         0.35058275]
 [0.         0.5        0.67321429 0.         0.4018315  0.
  0.5        0.64642857 0.         0.39213287]
 [0.         0.5        0.76428571 0.         0.43278388 0.
  0.5        0.51428571 0.         0.33909091]
 [0.         0.5        0.51428571 0.         0.33909091 0.
  0.5        0.61785714 0.         0.38152681]
 [0.         0.5        0.57857143 0.         0.36596737 0.
  0.5        0.75       0.         0.4282967 ]
 [0.         0.5        0.54107143 0.         0.35058275 0.
  0.5        0.59107143 0.         0.37109557]
 [0.         0.5        0.56607143 0.         0.36083916 0.
  0.5        0.51428571 0.         0.33909091]
 [0.         0.5        0.55357143 0.         0.35571096 0.
  0.5        0.55357143 0.         0.35571096]
 [0.         0.5        0.69821429 0.         0.41062271 0.
  0.5        0.51428571 0.         0.33909091]
 [0.         0.5        0.63214286 0.         0.38682984 0.
  0.5        0.61785714 0.         0.38152681]
 [0.         0.5        0.60357143 0.         0.37622378 0.
  0.5        0.55357143 0.         0.35571096]
 [0.         0.5        0.61785714 0.         0.38152681 0.
  0.5        0.64642857 0.         0.39213287]]
DUMMY mean:
[0.         0.5        0.61285714 0.         0.37825247 0.
 0.5        0.59654762 0.         0.37196925]
---------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_3_1_13
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  16.0
step (sec):  16.0
overlap:  True
perc. of overlap:  0.0
Number of windows / instances:  76
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.242 0.618 0.666 0.511 0.601 0.25  0.624 0.659 0.524 0.604]
 [0.184 0.604 0.635 0.522 0.586 0.215 0.616 0.636 0.543 0.599]
 [0.199 0.616 0.659 0.517 0.598 0.243 0.621 0.648 0.507 0.6  ]
 [0.01  0.504 0.621 0.021 0.39  0.048 0.524 0.616 0.079 0.412]
 [0.152 0.575 0.648 0.384 0.541 0.249 0.622 0.673 0.48  0.593]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.5   0.613 0.    0.378 0.    0.5   0.597 0.    0.372]]
Elapsed time: 2.440414468447367 minutes
Elapsed time: 0.04067357447412279 hours
