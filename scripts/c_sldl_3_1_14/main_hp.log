2024-05-22 16:26:14.425388: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:26:14.477334: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:26:14.477568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:26:14.477924: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2024-05-22 16:26:14.478594: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:26:14.478762: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:26:14.478914: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:26:14.880255: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:26:14.880467: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:26:14.880641: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2024-05-22 16:26:14.880774: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5965 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:2b:00.0, compute capability: 8.6
    
    
-------------------------------
participant:  1
-------------------------------
    
    
participant:  1
Window size (sec):  16.0
step (sec):  12.0
overlap:  True
perc. of overlap:  25.0
Nearest multiple of 16 to 16000 is: 16000
Nearest multiple of 16 to 12000 is: 12000
['gsr_chunks', 'ppg_chunks', 'val_chunks', 'aro_chunks', 'time_chunks', 'tag_chunks', 'video']
/home/marcos/Dropbox (Maestral)/c_sldl_3_1_14/functions.py:880: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!
You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.
A typical example is when you are setting values in a column of a DataFrame, like:

 Electrophysiology (1996), Heart Rate Variability, in: European Heart Journal, vol.17, issue 3, pp354-381

This warning will not repeat
  warnings.warn(msg, UserWarning)
/usr/local/lib/python3.10/dist-packages/scipy/interpolate/_fitpack2.py:280: UserWarning: 
The maximal number of iterations maxit (set to 20 by the program)
allowed for finding a smoothing spline with fp=s has been reached: s
too small.
There is an approximation returned but the corresponding weighted sum
of squared residuals does not satisfy the condition abs(fp-s)/s < tol.
  warnings.warn(message)
Class Imbalance test (arousal)....
Multiclass distribution
Class=0, n=34 (34.000%)
Class=1, n=66 (66.000%)
Class Imbalance test (valence)....
Multiclass distribution
Class=0, n=38 (38.000%)
Class=1, n=62 (62.000%)
------------- Evaluating model --------------
------------- Evaluating model --------------
------------- Evaluating model --------------
------------- Evaluating model --------------
------------- Evaluating model --------------
arousal and valece = ok
------------- Evaluating model --------------
-------------------------------------
-------------------------------------
participant:  30
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  16.0
step (sec):  12.0
overlap:  True
perc. of overlap:  25.0
Number of windows / instances:  100
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.374 0.691 0.71  0.674 0.684 0.164 0.577 0.63  0.5   0.576]
 [0.262 0.63  0.65  0.593 0.614 0.34  0.649 0.69  0.588 0.642]
 [0.225 0.666 0.7   0.506 0.655 0.377 0.607 0.65  0.612 0.598]
 [0.    0.5   0.62  0.    0.382 0.    0.5   0.66  0.    0.397]
 [0.139 0.573 0.63  0.355 0.528 0.065 0.537 0.63  0.21  0.501]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.5   0.62  0.    0.382 0.    0.5   0.66  0.    0.397]]
participant performance loaded
    
    
-------------------------------
LAST participant's saved data: 
30
-------------------------------
    
    
---------------------------
KNN performance:
[[ 0.19944056  0.60166667  0.63        0.49053056  0.56405567  0.44503736
   0.69494048  0.79        0.64403576  0.71666936]
 [ 0.41967113  0.70916667  0.71        0.70000892  0.70489483  0.3875814
   0.68988095  0.72        0.66401763  0.68856144]
 [ 0.4357971   0.71583333  0.73        0.66448312  0.70390027  0.47833333
   0.73916667  0.74        0.70876952  0.72622516]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.38703763  0.68452381  0.74        0.58661863  0.67065476  0.4365942
   0.71666667  0.72        0.69389334  0.7088345 ]
 [ 0.37130435  0.68583333  0.69        0.65829915  0.67453186  0.57492754
   0.7875      0.79        0.78363144  0.78653624]
 [ 0.25957839  0.63083333  0.65        0.60275093  0.62297425  0.06
   0.53        0.53        0.47731363  0.50468781]
 [ 0.46462681  0.71190476  0.8         0.62243798  0.71677171  0.33865707
   0.66607143  0.69        0.63715228  0.65926601]
 [ 0.29119858  0.63333333  0.74        0.50449748  0.62933123  0.09988375
   0.54464286  0.68        0.27376373  0.50417475]
 [ 0.4         0.7         0.7         0.6583113   0.67978716  0.2218323
   0.6125      0.61        0.59250558  0.60075341]
 [ 0.03140951  0.51904762  0.57        0.34371526  0.48701326  0.49215603
   0.73333333  0.79        0.64234124  0.72495852]
 [ 0.61133425  0.80535714  0.82        0.78528727  0.80071401  0.46351734
   0.72619048  0.76        0.69459794  0.72468837]
 [ 0.05618012  0.52666667  0.55        0.48392123  0.51398074  0.30487013
   0.65        0.67        0.6014441   0.63511905]
 [ 0.29745718  0.66190476  0.68        0.58861621  0.62820513  0.13231884
   0.56416667  0.58        0.48710549  0.54150183]
 [ 0.22660528  0.6         0.71        0.44116279  0.59504552  0.02764163
   0.51011905  0.56        0.42188249  0.48967865]
 [ 0.37805361  0.68916667  0.7         0.66647208  0.68084776  0.42943392
   0.70059524  0.76        0.66133002  0.70668956]
 [ 0.63619287  0.8225      0.82        0.81685352  0.81529054  0.16605778
   0.58416667  0.61        0.49270878  0.5653194 ]
 [ 0.03931624  0.51388889  0.85        0.07071068  0.49208806  0.26716783
   0.6375      0.64        0.58294975  0.61557359]
 [ 0.2598263   0.62857143  0.7         0.48359303  0.6089881   0.44501412
   0.7225      0.73        0.68252705  0.70622683]
 [ 0.26685239  0.64166667  0.69        0.53688077  0.61957418  0.15103213
   0.575       0.63        0.49316718  0.56166667]
 [-0.06492192  0.46934524  0.66        0.18573981  0.46161064  0.05454545
   0.525       0.53        0.50587131  0.51951132]
 [ 0.22        0.61083333  0.61        0.5907782   0.60125014  0.38761539
   0.69107143  0.72        0.66870773  0.68907828]
 [ 0.34986825  0.67416667  0.69        0.65053816  0.66776363  0.12153489
   0.55982143  0.73        0.27167225  0.53221405]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.33583639  0.675       0.66        0.60903408  0.63969697  0.54461538
   0.7725      0.77        0.76317391  0.76659091]
 [ 0.1189441   0.55833333  0.56        0.52287152  0.5415071   0.43298774
   0.72083333  0.72        0.71487781  0.71385115]
 [ 0.1947038   0.59285714  0.69        0.44611487  0.58393773  0.46
   0.73        0.73        0.70764708  0.72099761]
 [ 0.30462231  0.65416667  0.68        0.62112179  0.64238012  0.54115942
   0.77083333  0.78        0.76425934  0.76903652]
 [ 0.23603006  0.61488095  0.65        0.56350808  0.60244422  0.41552836
   0.71083333  0.71        0.67913227  0.69403596]
 [ 0.37373379  0.69107143  0.71        0.67410818  0.68433122  0.16420859
   0.57678571  0.63        0.49991004  0.57590659]]
KNN mean:
[0.28931068 0.64366142 0.6925     0.55603449 0.62977039 0.323009
 0.65866497 0.69       0.60037102 0.64815548]
---------------------------
---------------------------
DT performance:
[[ 0.26531861  0.6525      0.66        0.62815315  0.64079143  0.54940373
   0.73214286  0.8         0.66549995  0.71096755]
 [ 0.26357143  0.6225      0.62        0.62346288  0.61022727  0.24888449
   0.57916667  0.61        0.54136664  0.56880966]
 [ 0.36618012  0.62666667  0.63        0.63162075  0.60669969  0.5
   0.76083333  0.76        0.74813496  0.75258852]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.25203369  0.56011905  0.58        0.56649376  0.54365079  0.33979933
   0.68833333  0.69        0.62933777  0.68182678]
 [ 0.29833333  0.62916667  0.63        0.62527046  0.61283578  0.33666667
   0.67166667  0.67        0.63232229  0.66474747]
 [ 0.3088573   0.65583333  0.67        0.64234159  0.64810273  0.06
   0.55        0.55        0.52494867  0.53633866]
 [ 0.2591446   0.63035714  0.66        0.57693068  0.6108411   0.23562582
   0.6422619   0.66        0.6857746   0.63903458]
 [ 0.01538219  0.5452381   0.63        0.37282113  0.53260046  0.15381674
   0.66666667  0.71        0.50916431  0.65131327]
 [ 0.34        0.73        0.73        0.65951142  0.72395216  0.44510989
   0.685       0.69        0.70427151  0.66537352]
 [ 0.15488135  0.58571429  0.6         0.55920229  0.57299978  0.293739
   0.65        0.67        0.52330265  0.6336562 ]
 [ 0.53716645  0.72202381  0.75        0.69818356  0.72053114  0.27513885
   0.64583333  0.67        0.57795528  0.62795136]
 [-0.06760425  0.49916667  0.5         0.43145307  0.48365773  0.13576923
   0.61        0.61        0.55259327  0.59271534]
 [ 0.20166771  0.63392857  0.7         0.56092555  0.63365842 -0.17720888
   0.41416667  0.42        0.3105457   0.40142302]
 [ 0.21064003  0.64464286  0.65        0.50192453  0.61497169 -0.04349318
   0.45178571  0.49        0.36694668  0.42957487]
 [ 0.47956362  0.72416667  0.72        0.7494023   0.71060967  0.30554945
   0.65952381  0.69        0.62047506  0.6549359 ]
 [ 0.43128205  0.69833333  0.7         0.69485232  0.69049256  0.59132107
   0.79666667  0.8         0.81445162  0.79070319]
 [ 0.3064645   0.68541667  0.85        0.36042469  0.64604489  0.24333333
   0.56583333  0.58        0.5627525   0.56247919]
 [ 0.23163412  0.67678571  0.72        0.54423187  0.63641775  0.45050117
   0.7375      0.74        0.65900177  0.72271645]
 [ 0.14861933  0.63690476  0.67        0.55355652  0.59753012  0.16213163
   0.57261905  0.61        0.50822598  0.5585151 ]
 [ 0.08620094  0.55863095  0.68        0.36579192  0.53336485 -0.025
   0.52833333  0.53        0.4489911   0.52080808]
 [ 0.08307692  0.52416667  0.52        0.5304667   0.50263958  0.20295826
   0.64047619  0.65        0.56327335  0.62369048]
 [ 0.18068511  0.5975      0.61        0.58190578  0.59236846  0.04508041
   0.53363095  0.68        0.23215582  0.51763072]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.33537683  0.72833333  0.72        0.66017469  0.71364913  0.55454545
   0.72916667  0.73        0.76907311  0.72518953]
 [ 0.25974359  0.58916667  0.59        0.57319415  0.57541126  0.25404581
   0.63416667  0.65        0.59553195  0.62708541]
 [ 0.40799462  0.69107143  0.74        0.64785993  0.67935814  0.54
   0.73        0.73        0.72748639  0.71804751]
 [ 0.04177727  0.55357143  0.58        0.49739338  0.54640665  0.44372606
   0.71916667  0.72        0.70162511  0.70994977]
 [ 0.19229249  0.60357143  0.63        0.50508186  0.6015651   0.42247492
   0.65583333  0.65        0.65003282  0.63464119]
 [ 0.26150498  0.6297619   0.65        0.59283669  0.61376984  0.33988053
   0.64940476  0.69        0.5879761   0.64233322]]
DT mean:
[0.24470675 0.62982993 0.65678571 0.56912384 0.61411244 0.28156428
 0.63929209 0.65892857 0.58618632 0.62732309]
---------------------------
---------------------------
RF performance:
[[ 0.3148925   0.61083333  0.62        0.70822245  0.59843878  0.31453445
   0.67053571  0.79        0.43867559  0.6747022 ]
 [ 0.31311594  0.64416667  0.64        0.62931669  0.63126707  0.30265748
   0.61369048  0.64        0.5993197   0.60031885]
 [ 0.34242424  0.63333333  0.64        0.67349255  0.61542791  0.42307692
   0.74083333  0.74        0.77830812  0.73712121]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.31809657  0.67857143  0.71        0.59668596  0.67717865  0.39833333
   0.69083333  0.69        0.69764088  0.68006355]
 [ 0.3         0.70083333  0.7         0.63451767  0.69549256  0.44923077
   0.72916667  0.72        0.67221348  0.71608586]
 [ 0.34595216  0.68416667  0.7         0.6837707   0.68063881  0.22
   0.53        0.53        0.51258091  0.50814463]
 [ 0.5516483   0.71071429  0.76        0.65385687  0.69736695  0.38005654
   0.74107143  0.74        0.7190023   0.7253996 ]
 [ 0.00270997  0.55238095  0.64        0.42441256  0.55295788  0.37221891
   0.63809524  0.71        0.57881614  0.63072802]
 [ 0.36        0.67        0.67        0.52453538  0.66226024  0.291466
   0.68833333  0.69        0.65009597  0.67395105]
 [ 0.0863468   0.59107143  0.61        0.36458697  0.58432956  0.38516677
   0.71369048  0.75        0.56676054  0.70477939]
 [ 0.67067633  0.83869048  0.85        0.76196059  0.83708736  0.28206022
   0.67083333  0.69        0.64532989  0.65294247]
 [ 0.09385846  0.58166667  0.6         0.36348543  0.5596584   0.38006638
   0.595       0.61        0.49540357  0.58930042]
 [ 0.14323678  0.62797619  0.68        0.47311036  0.63014513  0.02463768
   0.50166667  0.51        0.4120046   0.47235986]
 [ 0.32570637  0.5672619   0.65        0.50660289  0.55153819  0.06223237
   0.46547619  0.51        0.16185956  0.45512821]
 [ 0.40705145  0.72833333  0.74        0.65401545  0.71438534  0.39082827
   0.66845238  0.72        0.59713092  0.65083764]
 [ 0.65459866  0.835       0.83        0.77188966  0.82607753  0.47190517
   0.65416667  0.67        0.66702803  0.64192141]
 [ 0.04700855  0.61041667  0.82        0.35318329  0.55698529  0.24984899
   0.66833333  0.67        0.61130017  0.65811078]
 [-0.01048798  0.58154762  0.68        0.51471591  0.56248599  0.37066253
   0.6925      0.7         0.65364962  0.68679237]
 [ 0.22949989  0.62083333  0.69        0.47293491  0.60535631  0.23342685
   0.62380952  0.68        0.41978047  0.59175824]
 [-0.02718017  0.46488095  0.68        0.26816294  0.43948996  0.25833333
   0.45666667  0.46        0.36545816  0.43426435]
 [ 0.28        0.6275      0.63        0.55376409  0.62052614  0.30848215
   0.65595238  0.67        0.54273398  0.62920857]
 [ 0.26068511  0.5975      0.62        0.48486737  0.58676324  0.13526506
   0.57291667  0.71        0.24163194  0.55593954]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.39645022  0.58666667  0.59        0.58778534  0.57494145  0.65454545
   0.81        0.81        0.7459069   0.80518953]
 [ 0.25593407  0.57583333  0.57        0.64134477  0.54750278  0.23236578
   0.6475      0.65        0.59478122  0.63710068]
 [ 0.09414585  0.54940476  0.62        0.42810098  0.533837    0.52
   0.78        0.78        0.71714278  0.77270646]
 [-0.06240032  0.6422619   0.69        0.47419349  0.63799908  0.42198693
   0.695       0.7         0.71658005  0.68257076]
 [ 0.08242424  0.56785714  0.6         0.41011446  0.55605089  0.39458194
   0.69666667  0.69        0.71858463  0.67388944]
 [ 0.2250847   0.66607143  0.7         0.50646936  0.6553151   0.37670139
   0.60714286  0.65        0.61195386  0.59844322]]
RF mean:
[0.25005281 0.63377764 0.67607143 0.54000354 0.62112513 0.3323097
 0.65065476 0.67428571 0.57613121 0.63713423]
---------------------------
---------------------------
SVM performance:
[[ 0.          0.5         0.58        0.          0.36666667  0.
   0.5         0.76        0.          0.43137255]
 [-0.06727273  0.4675      0.49        0.04564355  0.3422619   0.
   0.5         0.61        0.          0.37867647]
 [ 0.11264069  0.55166667  0.62        0.21543204  0.4652381   0.28
   0.64083333  0.64        0.6263629   0.63324509]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.          0.5         0.62        0.          0.38235294  0.41304348
   0.70416667  0.71        0.65584813  0.68527306]
 [ 0.01478261  0.50666667  0.51        0.06454972  0.36227106  0.07454545
   0.535       0.56        0.11543204  0.40357143]
 [ 0.          0.5         0.59        0.          0.37083333  0.2
   0.6         0.6         0.47100167  0.55376845]
 [ 0.22578505  0.59583333  0.74        0.30485474  0.55512255  0.
   0.5         0.61        0.          0.37867647]
 [ 0.          0.5         0.7         0.          0.41176471  0.
   0.5         0.69        0.          0.40808824]
 [ 0.22        0.61        0.61        0.57401684  0.59277251  0.
   0.5         0.54        0.          0.35      ]
 [ 0.          0.5         0.61        0.          0.37867647  0.
   0.5         0.61        0.          0.37867647]
 [ 0.63702595  0.81785714  0.83        0.80419345  0.81536602  0.
   0.5         0.61        0.          0.37867647]
 [ 0.          0.5         0.58        0.          0.36666667  0.
   0.5         0.57        0.          0.3625    ]
 [ 0.          0.5         0.65        0.          0.39338235  0.
   0.5         0.55        0.          0.35416667]
 [ 0.          0.5         0.68        0.          0.40441176  0.
   0.5         0.64        0.          0.38970588]
 [ 0.          0.5         0.59        0.          0.37083333  0.
   0.5         0.65        0.          0.39338235]
 [ 0.42542537  0.7025      0.74        0.62926186  0.68561272  0.
   0.5         0.59        0.          0.37083333]
 [ 0.          0.5         0.86        0.          0.4619883   0.
   0.5         0.58        0.          0.36666667]
 [ 0.          0.5         0.68        0.          0.40441176  0.42947299
   0.71083333  0.73        0.58159132  0.67414807]
 [ 0.          0.5         0.67        0.          0.40073529  0.
   0.5         0.64        0.          0.38970588]
 [ 0.          0.5         0.78        0.          0.4379085  -0.02962963
   0.48333333  0.48        0.02886751  0.32929293]
 [ 0.08        0.54166667  0.54        0.09128709  0.38989899  0.
   0.5         0.63        0.          0.38602941]
 [ 0.          0.5         0.58        0.          0.36666667  0.
   0.5         0.74        0.          0.4248366 ]
 [        nan         nan         nan         nan         nan         nan
          nan         nan         nan         nan]
 [ 0.          0.5         0.56        0.          0.35833333  0.65833333
   0.83083333  0.83        0.82906879  0.82866162]
 [ 0.          0.5         0.53        0.          0.34583333  0.
   0.5         0.57        0.          0.3625    ]
 [ 0.          0.5         0.69        0.          0.40808824  0.46
   0.73        0.73        0.70000409  0.71638084]
 [ 0.          0.5         0.64        0.          0.38970588  0.
   0.5         0.59        0.          0.37083333]
 [ 0.          0.5         0.61        0.          0.37867647  0.
   0.5         0.55        0.          0.35416667]
 [ 0.          0.5         0.62        0.          0.38235294  0.
   0.5         0.66        0.          0.39705882]]
SVM mean:
[0.05887096 0.52834609 0.63928571 0.09747283 0.4281726  0.08877734
 0.54410714 0.63107143 0.14314916 0.44467478]
---------------------------
---------------------------
GBM performance:
[[ 3.87589874e-01  6.75833333e-01  7.00000000e-01  6.32401356e-01
   6.65685703e-01  4.37937604e-01  6.75000000e-01  8.40000000e-01
   4.80806041e-01  6.67834967e-01]
 [ 2.18076923e-01  6.10000000e-01  6.10000000e-01  5.62429932e-01
   5.93935509e-01  3.56181065e-01  6.70833333e-01  7.20000000e-01
   5.85569047e-01  6.58171143e-01]
 [ 3.81208461e-01  6.88333333e-01  7.00000000e-01  6.19161328e-01
   6.66253191e-01  5.40000000e-01  7.70833333e-01  7.70000000e-01
   7.63463639e-01  7.66792929e-01]
 [            nan             nan             nan             nan
              nan             nan             nan             nan
              nan             nan]
 [ 4.27676119e-01  6.88690476e-01  7.50000000e-01  6.09921850e-01
   6.76981768e-01  3.99799331e-01  7.20833333e-01  7.20000000e-01
   6.72017068e-01  7.09238539e-01]
 [ 3.40000000e-01  6.60833333e-01  6.60000000e-01  6.42889499e-01
   6.51919192e-01  3.76666667e-01  6.79166667e-01  6.80000000e-01
   6.59351483e-01  6.68668831e-01]
 [ 2.06584792e-01  5.97500000e-01  6.50000000e-01  4.68387724e-01
   5.72445055e-01  1.60000000e-01  5.90000000e-01  5.90000000e-01
   4.86052836e-01  5.58934121e-01]
 [ 5.06395668e-01  7.31547619e-01  8.10000000e-01  6.44504882e-01
   7.31423723e-01  4.44450187e-01  7.20238095e-01  7.40000000e-01
   6.97711957e-01  7.15347985e-01]
 [ 8.14241486e-02  5.30952381e-01  6.90000000e-01  2.26657329e-01
   4.97622549e-01 -5.29411765e-02  4.78571429e-01  6.60000000e-01
   0.00000000e+00  3.97058824e-01]
 [ 2.40000000e-01  6.40000000e-01  6.40000000e-01  5.69841426e-01
   6.12193362e-01  3.89853480e-01  6.96666667e-01  7.00000000e-01
   6.60423277e-01  6.79596237e-01]
 [-3.27969132e-03  5.10714286e-01  5.70000000e-01  3.44248673e-01
   4.77994505e-01  3.70877589e-01  6.63690476e-01  7.20000000e-01
   5.73973049e-01  6.42801365e-01]
 [ 6.17159049e-01  8.16071429e-01  8.10000000e-01  8.06926747e-01
   8.03413531e-01  3.76463392e-01  6.83333333e-01  7.30000000e-01
   6.00553239e-01  6.69810614e-01]
 [-7.55411255e-02  4.62500000e-01  5.10000000e-01  2.65344019e-01
   4.20685426e-01  3.02292490e-01  6.46666667e-01  6.80000000e-01
   5.66578117e-01  6.21427184e-01]
 [ 1.32050715e-01  5.69047619e-01  6.80000000e-01  3.26657329e-01
   5.32083333e-01 -4.90202551e-02  4.60833333e-01  4.80000000e-01
   3.80801392e-01  4.38130481e-01]
 [ 1.10051868e-01  5.49404762e-01  7.00000000e-01  2.31897953e-01
   5.04646359e-01 -6.91397612e-02  4.92261905e-01  5.90000000e-01
   1.46542614e-01  4.34166667e-01]
 [ 1.24289479e-01  5.58333333e-01  6.00000000e-01  3.97637239e-01
   5.20026085e-01  2.57147228e-01  6.13095238e-01  6.90000000e-01
   5.11522961e-01  6.03964932e-01]
 [ 6.77622378e-01  8.29166667e-01  8.30000000e-01  8.19744333e-01
   8.23816739e-01  6.94526198e-01  8.56666667e-01  8.50000000e-01
   8.49266919e-01  8.44962260e-01]
 [-2.64957265e-02  4.88194444e-01  8.40000000e-01  0.00000000e+00
   4.55796354e-01  1.99924496e-01  5.99166667e-01  6.20000000e-01
   5.60921165e-01  5.88580586e-01]
 [ 2.69349845e-02  5.11904762e-01  6.70000000e-01  1.11187275e-01
   4.43455882e-01  4.48689571e-01  7.41666667e-01  7.50000000e-01
   6.84821884e-01  7.32553835e-01]
 [ 4.42282176e-05  5.01785714e-01  6.40000000e-01  1.52247252e-01
   4.41743697e-01  7.07002801e-02  5.35119048e-01  6.10000000e-01
   3.41006640e-01  4.97307423e-01]
 [ 4.61538462e-02  5.18750000e-01  7.80000000e-01  7.07106781e-02
   4.70588235e-01  4.46153846e-02  5.11666667e-01  5.10000000e-01
   4.49122549e-01  4.82707293e-01]
 [ 1.20000000e-01  5.70833333e-01  5.70000000e-01  5.51109385e-01
   5.60105450e-01  2.72755616e-01  6.32738095e-01  6.90000000e-01
   4.98747294e-01  6.05782967e-01]
 [ 2.22870318e-01  6.05833333e-01  6.50000000e-01  4.73128084e-01
   5.78315018e-01  6.15384615e-02  5.25000000e-01  7.50000000e-01
   7.07106781e-02  4.60784314e-01]
 [            nan             nan             nan             nan
              nan             nan             nan             nan
              nan             nan]
 [ 3.43374741e-01  6.78333333e-01  7.00000000e-01  6.24652542e-01
   6.61106116e-01  5.86832298e-01  7.90000000e-01  8.00000000e-01
   7.66654481e-01  7.85071595e-01]
 [ 3.23873518e-01  6.61666667e-01  6.70000000e-01  6.27524430e-01
   6.48559774e-01  4.69782609e-01  7.39166667e-01  7.40000000e-01
   7.27917037e-01  7.30290543e-01]
 [ 1.23001237e-01  5.43452381e-01  6.80000000e-01  3.16277162e-01
   5.21038839e-01  6.00000000e-01  8.00000000e-01  8.00000000e-01
   7.91156339e-01  7.96287879e-01]
 [ 8.96236756e-02  5.37500000e-01  6.60000000e-01  2.19270534e-01
   4.69834357e-01  5.73893281e-01  7.83333333e-01  8.00000000e-01
   7.72241348e-01  7.84486347e-01]
 [ 1.43416260e-01  5.75000000e-01  6.40000000e-01  3.95221500e-01
   5.57266484e-01  3.27750583e-01  6.45833333e-01  6.50000000e-01
   6.00350686e-01  6.35117383e-01]
 [ 1.38560631e-01  5.73214286e-01  6.30000000e-01  3.55130373e-01
   5.27700633e-01  6.45259070e-02  5.36904762e-01  6.30000000e-01
   2.09982279e-01  5.00588733e-01]]
GBM mean:
[0.2115238  0.60304989 0.68       0.43089682 0.57452275 0.30914652
 0.65211735 0.69678571 0.53958093 0.63130236]
---------------------------
---------------------------
BDDAE performance:
[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
 [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]
BDDAE mean:
[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]
---------------------------
---------------------------
DUMMY performance:
[[0.         0.5        0.58       0.         0.36666667 0.
  0.5        0.76       0.         0.43137255]
 [0.         0.5        0.53       0.         0.34583333 0.
  0.5        0.61       0.         0.37867647]
 [0.         0.5        0.58       0.         0.36666667 0.
  0.5        0.49       0.         0.32857143]
 [0.         0.5        0.54       0.         0.35       0.
  0.5        0.72       0.         0.41830065]
 [0.         0.5        0.62       0.         0.38235294 0.
  0.5        0.52       0.         0.34166667]
 [0.         0.5        0.49       0.         0.32857143 0.
  0.5        0.53       0.         0.34583333]
 [0.         0.5        0.59       0.         0.37083333 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.68       0.         0.40441176 0.
  0.5        0.61       0.         0.37867647]
 [0.         0.5        0.7        0.         0.41176471 0.
  0.5        0.69       0.         0.40808824]
 [0.         0.5        0.5        0.         0.33333333 0.
  0.5        0.54       0.         0.35      ]
 [0.         0.5        0.61       0.         0.37867647 0.
  0.5        0.61       0.         0.37867647]
 [0.         0.5        0.61       0.         0.37867647 0.
  0.5        0.61       0.         0.37867647]
 [0.         0.5        0.58       0.         0.36666667 0.
  0.5        0.57       0.         0.3625    ]
 [0.         0.5        0.65       0.         0.39338235 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.68       0.         0.40441176 0.
  0.5        0.64       0.         0.38970588]
 [0.         0.5        0.59       0.         0.37083333 0.
  0.5        0.65       0.         0.39338235]
 [0.         0.5        0.58       0.         0.36666667 0.
  0.5        0.59       0.         0.37083333]
 [0.         0.5        0.86       0.         0.4619883  0.
  0.5        0.58       0.         0.36666667]
 [0.         0.5        0.68       0.         0.40441176 0.
  0.5        0.54       0.         0.35      ]
 [0.         0.5        0.67       0.         0.40073529 0.
  0.5        0.64       0.         0.38970588]
 [0.         0.5        0.78       0.         0.4379085  0.
  0.5        0.49       0.         0.32857143]
 [0.         0.5        0.51       0.         0.3375     0.
  0.5        0.63       0.         0.38602941]
 [0.         0.5        0.58       0.         0.36666667 0.
  0.5        0.74       0.         0.4248366 ]
 [0.         0.5        0.55       0.         0.35416667 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.56       0.         0.35833333 0.
  0.5        0.52       0.         0.34166667]
 [0.         0.5        0.53       0.         0.34583333 0.
  0.5        0.57       0.         0.3625    ]
 [0.         0.5        0.69       0.         0.40808824 0.
  0.5        0.5        0.         0.33333333]
 [0.         0.5        0.64       0.         0.38970588 0.
  0.5        0.59       0.         0.37083333]
 [0.         0.5        0.61       0.         0.37867647 0.
  0.5        0.55       0.         0.35416667]
 [0.         0.5        0.62       0.         0.38235294 0.
  0.5        0.66       0.         0.39705882]]
DUMMY mean:
[0.         0.5        0.613      0.         0.37820384 0.
 0.5        0.59166667 0.         0.37006653]
---------------------------
Current folder: /home/marcos/Dropbox (Maestral)/c_sldl_3_1_14
-------------------------------------
----- RESULTS ------
-------------------------------------
Window size (sec):  16.0
step (sec):  12.0
overlap:  True
perc. of overlap:  25.0
Number of windows / instances:  100
---------------------------
ALG Means: 
-rows: alg : KNN = 0; DT = 1; RF = 2; SVM = 3; GBM = 4; BDDAE = 5; DUMMY = 5
columns:
'val_cohen','val_uar', 'val_acc', 'val_gm',  'val_f1',  'aro_cohen','aro_uar', 'aro_acc', 'aro_gm',  'aro_f1'
-------------------------------------------------------------
  v_c   v_u   v_a   v_g   v_f1  a_c   a_u   a_a   a_g   a_f1
[[0.289 0.644 0.692 0.556 0.63  0.323 0.659 0.69  0.6   0.648]
 [0.245 0.63  0.657 0.569 0.614 0.282 0.639 0.659 0.586 0.627]
 [0.25  0.634 0.676 0.54  0.621 0.332 0.651 0.674 0.576 0.637]
 [0.059 0.528 0.639 0.097 0.428 0.089 0.544 0.631 0.143 0.445]
 [0.212 0.603 0.68  0.431 0.575 0.309 0.652 0.697 0.54  0.631]
 [0.    0.    0.    0.    0.    0.    0.    0.    0.    0.   ]
 [0.    0.5   0.613 0.    0.378 0.    0.5   0.592 0.    0.37 ]]
Elapsed time: 2.822626574834188 minutes
Elapsed time: 0.04704377624723646 hours
